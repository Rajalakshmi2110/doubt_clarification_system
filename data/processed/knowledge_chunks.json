[
  {
    "text": "Cover Image by Andrii Yalanskyi/Shutterstock\nPearson Education Limited\nHockham Way\nUnited Kingdom\nand Associated Companies throughout the world\nVisit us on the World Wide Web at: www.pearsonglobaleditions.com\nPlease contact https://support.pearson.com/getsupport/s/ with any queries on this content.\n© Pearson Education Limited 2022\nThe rights of James F. Kurose and Keith W. Ross to be identified as the authors of this work\nof the network layer (Chapter 4) and a new chapter focused on the network\nlayer’s “control plane” (Chapter 5). That change turned out to be prescient,\nas software-defined networking (SDN), arguably the most important and\nexciting advance in networking in decades, has been rapidly adopted\nin practice—so much so that it’s already hard to imagine an introduction to\nmodern computer networking that doesn’t cover SDN. SDN has also\nenabled new advances in the practice of network management, which we\nalso cover in modernized and deeper detail in this edition. And as we’ll see\nin Chapter 7 of this eighth edition, the separation of the data and control\nplanes is now also deeply embedded in 4G/5G mobile cellular network\narchitectures, as is an “all-IP” approach to their core networks. The rapid\nadoption of 4G/5G networks and the mobile applications they enable are\nundoubtedly the most significant changes we’ve seen in networking since\nthe publication of our seventh edition. We’ve thus significantly updated and\ndeepened our treatment of this exciting area. Indeed, the ongoing wireless\nnetwork revolution is so important that we think it has become a critical\npart of an introductory networking course.\nIn addition to these changes, we’ve also updated many sections\nthroughout the book and added new material to reflect changes across the\nbreadth of networking. In some cases, we have also retired material from\nthe previous edition. As always, material that has been retired from the\nprinted text can always be found on our book’s Companion Website. The\nmost important changes in this eighth edition are the following:\nChapter 1 has been updated to reflect the ever-growing reach and use\nof the Internet, and of 4G/5G networks.\nChapter 2, which covers the application layer, has been significantly\nupdated, including material on the new HTTP/2 and HTTP/3 protocols\nfor the Web.\nChapter 3, has been updated to reflect advances in, and evolution in\nuse of, transport-layer congestion control and error-control protocols\nover the past five years. While this material had remained relatively\nstable for quite some time, there have been a number of important\nadvances since the seventh edition. Several new congestion-control\nalgorithms have been developed and deployed beyond the “classic”\nTCP algorithms. We provide a deeper coverage of TCP CUBIC, the\ndefault TCP protocol in many deployed systems, and examine delay-\nbased approaches to congestion control, including the new BBR\nprotocol, which is deployed in Google’s backbone network. We also\nstudy the QUIC protocol, which is being incorporated into the HTTP/3\nstandard. Although QUIC is technically not a transport-layer protocol—\nit provides application-layer reliability, congestion control, and\nconnection multiplexing services at the application layer—it uses many\nof the error- and congestion-control principles that we develop in the\nearly sections of Chapter 3.\nChapter 4, which covers the network-layer data plane, has general\nupdates throughout. We’ve added a new section on so-called\nmiddleboxes, which perform network-layer functions other than routing\nand forwarding, such as firewalling and load balancing. Middleboxes\nbuild naturally on the generalized “match plus action” forwarding\noperation of network-layer devices that we cover earlier in Chapter 4.\nWe’ve also added timely new material on topics such as the amount of\nbuffering that is “just right” in network routers, on net neutrality, and on\nthe architectural principles of the Internet.\nChapter 5, which cover the network-layer’s control plane, contains\nupdated material on SDN, and a significantly new treatment of network\nmanagement. The use of SDN has evolved beyond management of\npacket-forwarding tables to include configuration management of\nnetwork devices as well. We introduce two new protocols, NETCONF\nand YANG, whose adoption and use have fueled this new approach\ntoward network management.\nChapter 6, which covers the link layer, has been updated to reflect the\ncontinuing evolution of link-layer technologies such as Ethernet. We\nhave also updated and expanded our treatment of datacenter networks,\nwhich are at the heart of the technology driving much of today’s\nInternet commerce.\nAs noted earlier, Chapter 7 has been significantly updated and revised\nto reflect the many changes in wireless networking since the seventh\nedition, from short-range Bluetooth piconets, to medium-range wireless\n802.11 local area networks (WLANs), to wide-area 4G/5G wireless\ncellular networks. We have retired our coverage of earlier 2G and 3G\nnetworks in favor of a broader and deeper treatment of today’s 4G LTE\nnetworks and tomorrow’s 5G networks. We have also updated our\ncoverage of mobility issues, from the local issue of handover of mobile\ndevices between base stations to the global issue of identity\nmanagement and mobile device roaming among different global cellular\nChapter 8, which covers network security, has been updated to reflect\nchanges in wireless network security in particular, with new material on\nWPA3 security in WLANs, and mutual device/network mutual\nauthentication and confidentiality in 4G/5G networks.\nWe have also retired Chapter 9, on multimedia networking, from this\nedition. Over time, as multimedia applications became more prevalent, we\nhad already migrated Chapter 9 material on topics such as video streaming,\npacket scheduling, and content distribution networks into earlier chapters.\nAs noted earlier, all retired material from this and earlier editions can be\nfound on our book’s Companion Website.\nThis textbook is for a first course on computer networking. It can be used in\nboth computer science and electrical engineering departments. In terms of\nprogramming languages, the book assumes only that the student has\nexperience with C, C++, Java, or Python (and even then only in a few\nplaces). Although this book is more precise and analytical than many other\nintroductory computer networking texts, it rarely uses any mathematical\nconcepts that are not taught in high school. We have made a deliberate\neffort to avoid using any advanced calculus, probability, or stochastic\nprocess concepts (although we’ve included some homework problems for\nstudents with this advanced background). The book is therefore appropriate\nfor undergraduate courses and for first-year graduate courses. It should also\nbe useful to practitioners in the networking industry.\nWhat Is Unique About This Textbook?\nThe subject of computer networking is enormously complex, involving\nmany concepts, protocols, and technologies that are woven together in an\nintricate manner. To cope with this scope and complexity, many computer\nnetworking texts are often organized around the “layers” of a network\narchitecture. With a layered organization, students can see through the\ncomplexity of computer networking—they learn about the distinct concepts\nand protocols in one part of the architecture while seeing the big picture of\nhow all parts fit together. From a pedagogical perspective, our personal\nexperience has been that such a layered approach indeed works well.\nNevertheless, we have found that the traditional approach of teaching—\nbottom up; that is, from the physical layer toward the application layer—is\nnot the best approach for a modern course on computer networking.\nA Top-Down Approach\nOur book broke new ground 20 years ago by treating networking in a top-\ndown ­manner—that is, by beginning at the application layer and working its\nway down toward the physical layer. The feedback we received from\nteachers and students alike have confirmed that this top-down approach has\nmany advantages and does indeed work well pedagogically. First, it places\nemphasis on the application layer (a “high growth area” in networking).\nIndeed, many of the recent revolutions in computer networking—including\nthe Web, and media streaming—have taken place at the ­application layer.\nAn early emphasis on application-layer issues differs from the ­approaches\ntaken in most other texts, which have only a small amount of material on\nnetwork applications, their requirements, application-layer paradigms (e.g.,\nclient-server and peer-to-peer), and application programming interfaces.\nSecond, our experience as instructors (and that of many instructors who\nhave used this text) has been that teaching networking applications near the\nbeginning of the course is a powerful motivational tool. Students are\nthrilled to learn about how networking applications work—applications\nsuch as e-mail, streaming video, and the Web, which most students use on a\ndaily basis. Once a student understands the applications, the student can\nthen understand the network services needed to support these applications.\nThe student can then, in turn, examine the various ways in which such\nservices might be provided and implemented in the lower layers. Covering\napplications early thus provides motivation for the remainder of the text.\nThird, a top-down approach enables instructors to introduce network\napplication development at an early stage. Students not only see how\npopular applications and protocols work, but also learn how easy it is to\ncreate their own network ­applications and application-layer protocols. With\nthe top-down approach, students get early ­exposure to the notions of socket\nprogramming, service models, and ­protocols—important concepts that\nresurface in all subsequent layers. By providing socket programming\nexamples in Python, we highlight the central ideas without confusing\nstudents with complex code. Undergraduates in electrical engineering and\ncomputer science will have no difficulty following the Python code.\nAn Internet Focus\nAlthough we dropped the phrase “Featuring the Internet” from the title of\nthis book with the fourth edition, this doesn’t mean that we dropped our\nfocus on the Internet. Indeed, nothing could be further from the case!\nInstead, since the Internet has become so pervasive, we felt that any\nnetworking textbook must have a significant focus on the Internet, and thus\nthis phrase was somewhat unnecessary. We continue to use the Internet’s\narchitecture and protocols as primary vehicles for studying fundamental\ncomputer networking concepts. Of course, we also include concepts and\nprotocols from other network architectures. But the spotlight is clearly on\nthe Internet, a fact reflected in our organizing the book around the Internet’s\nfive-layer architecture: the application, transport, network, link, and\nphysical layers.\nAnother benefit of spotlighting the Internet is that most computer\nscience and electrical engineering students are eager to learn about the\nInternet and its protocols. They know that the Internet has been a\nrevolutionary and disruptive technology and can see that it is profoundly\nchanging our world. Given the enormous relevance of the Internet, students\nare naturally curious about what is “under the hood.” Thus, it is easy for an\ninstructor to get students excited about basic principles when using the\nInternet as the guiding focus.\nTeaching Networking Principles\nTwo of the unique features of the book—its top-down approach and its\nfocus on the Internet—have appeared in the titles of our book. If we could\nhave squeezed a third phrase into the subtitle, it would have contained the\nword principles. The field of networking is now mature enough that a\nnumber of fundamentally important issues can be identified. For example,\nin the transport layer, the fundamental issues include reliable\ncommunication over an unreliable network layer, connection establishment/\nteardown and handshaking, congestion and flow control, and multiplexing.\nThree fundamentally important network-layer issues are determining\n“good” paths between two routers, interconnecting a large number of\nheterogeneous networks, and managing the complexity of a modern\nnetwork. In the link layer, a fundamental problem is sharing a multiple\nconfidentiality, authentication, and message integrity are all based on\ncryptographic fundamentals. This text identifies fundamental networking\nissues and studies approaches toward ­addressing these issues. The student\nlearning these principles will gain knowledge with a long “shelf life”—long\nafter many of today’s network standards and protocols have become\nobsolete, the principles they embody will remain important and relevant.\nWe believe that the combination of using the Internet to get the student’s\nfoot in the door and then emphasizing fundamental issues and solution\napproaches will allow the student to quickly understand just about any\nnetworking technology.\nStudent Resources\nStudent resources are available on the Companion Website (CW) at www.­-\npearsonglobaleditions.com. Resources include:\nInteractive learning material. The book’s Website contains ­VideoNotes\n—video presentations of important topics throughout the book done by\nthe authors, as well as walkthroughs of solutions to problems similar to\nthose at the end of the chapter. We’ve seeded the Website with\nVideoNotes and online problems for Chapters 1 through 5. As in earlier\neditions, the Website contains the interactive animations that illustrate\nmany key networking concepts. Professors can integrate these\ninteractive features into their lectures or use them as mini labs.\nAdditional technical material. As we have added new material in each\nedition of our book, we’ve had to remove coverage of some existing\ntopics to keep the book at manageable length. Material that appeared in\nearlier editions of the text is still of ­interest, and thus can be found on\nthe book’s Website.\nProgramming assignments. The Website also provides a number of\ndetailed programming assignments, which include building a\nmultithreaded Web ­server, building an e-mail client with a GUI\ninterface, programming the sender and ­receiver sides of a reliable data\ntransport protocol, programming a distributed routing algorithm, and\nWireshark labs. One’s understanding of network protocols can be\ngreatly ­deepened by seeing them in action. The Website provides\nnumerous Wireshark assignments that enable students to actually\nobserve the sequence of messages exchanged between two protocol\nentities. The Website includes separate Wireshark labs on HTTP, DNS,\nTCP, UDP, IP, ICMP, Ethernet, ARP, WiFi, TLS and on tracing all\nprotocols involved in satisfying a request to fetch a Web page. We’ll\ncontinue to add new labs over time.\nPedagogical Features\nWe have each been teaching computer networking for more than 30 years.\nTogether, we bring more than 60 years of teaching experience to this text,\nduring which time we have taught many thousands of students. We have\nalso been active researchers in computer networking during this time. (In\nfact, Jim and Keith first met each other as master’s students in a computer\nnetworking course taught by Mischa Schwartz in 1979 at Columbia\nUniversity.) We think all this gives us a good perspective on where\nnetworking has been and where it is likely to go in the future. Nevertheless,\nwe have resisted temptations to bias the material in this book toward our\nown pet research projects. We figure you can visit our personal Websites if\nyou are interested in our research. Thus, this book is about modern\ncomputer networking—it is about contemporary protocols and technologies\nas well as the underlying principles behind these protocols and\ntechnologies. We also believe that learning (and teaching!) about\nnetworking can be fun. A sense of humor, use of analogies, and real-world\nexamples in this book will hopefully make this material more fun.\nSupplements for Instructors\nWe provide a complete supplements package to aid instructors in teaching\nthis course. This material can be accessed from Pearson’s Instructor\n(http://www.pearsonglobaleditions.com). \nInstructor Resource Center for ­information about accessing these\ninstructor’s supplements.\nPowerPoint® slides. We provide PowerPoint slides for all eight\nchapters. The slides have been completely updated with this eighth\nedition. The slides cover each chapter in detail. They use graphics and\nanimations (rather than relying only on monotonous text bullets) to\nmake the slides interesting and visually appealing. We provide the\noriginal PowerPoint slides so you can customize them to best suit your\nown teaching needs. Some of these slides have been contributed by\nother instructors who have taught from our book.\nHomework solutions. We provide a solutions manual for the homework\nproblems in the text, programming assignments, and Wireshark labs. As\nnoted ­earlier, we’ve introduced many new homework problems at each\nchapter’s end. For additional interactive problems and solutions, an\ninstructor (and students) can consult this books Companion Website at\nChapter Dependencies\nThe first chapter of this text presents a self-contained overview of computer\nnetworking. Introducing many key concepts and terminology, this chapter\nsets the stage for the rest of the book. All of the other chapters directly\ndepend on this first chapter. After completing Chapter 1, we recommend\ninstructors cover Chapters 2 through 6 in sequence, following our top-down\nphilosophy. Each of these five chapters leverages material from the\npreceding chapters. After completing the first six chapters, the instructor\nhas quite a bit of flexibility. There are no interdependencies among the last\ntwo chapters, so they can be taught in any order. However, the last two\nchapters depends on the material in the first six chapters. Many instructors\nfirst teach the first six chapters and then teach one of the last two chapters\nfor “dessert.”\nOne Final Note: We’d Love to Hear from You\nWe encourage students and instructors to e-mail us with any comments they\nmight have about our book. It’s been wonderful for us to hear from so many\ninstructors and students from around the world about our first seven\neditions. We’ve incorporated many of these suggestions into later editions\nof the book. We also encourage instructors to send us new homework\nproblems (and solutions) that would complement the current homework\nproblems. We’ll post these on the instructor-only portion of the Website. We\nalso encourage instructors and students to create new interactive animations\nthat illustrate the concepts and protocols in this book. If you have an\nanimation that you think would be appropriate for this text, please submit it\nto us. If the animation (including notation and terminology) is appropriate,\nwe’ll be happy to include it on the text’s Website, with an appropriate\nreference to the animation’s authors.\nSo, as the saying goes, “Keep those cards and letters coming!”\nSeriously, please do continue to send us interesting URLs, point out typos,\ndisagree with any of our claims, and tell us what works and what doesn’t\nwork. Tell us what you think should or shouldn’t be included in the next\nkurose@cs.umass.edu \nkeithwross@nyu.edu.\nChapter 1 Computer Networks and the Internet\nChapter 2 Application Layer\nChapter 3 Transport Layer\nChapter 4 The Network Layer: Data Plane\nChapter 5 The Network Layer: Control Plane\nChapter 6 The Link Layer and LANs\nChapter 7 Wireless and Mobile Networks\nChapter 8 Security in Computer Networks\nChapter 1 Computer Networks and the Internet\nWhat Is the Internet?\nA Nuts-and-Bolts Description\nA Services Description\nWhat Is a Protocol?\nThe Network Edge\nAccess Networks\nPhysical Media\nThe Network Core\nPacket Switching\nCircuit Switching\nA Network of Networks\nDelay, Loss, and Throughput in Packet-Switched\nOverview of Delay in Packet-Switched\nQueuing Delay and Packet Loss\nEnd-to-End Delay\nThroughput in Computer Networks\nProtocol Layers and Their Service Models\nLayered Architecture\nEncapsulation\nNetworks Under Attack\nHistory of Computer Networking and the Internet\nThe Development of Packet Switching:\nProprietary Networks and Internetworking:\nA Proliferation of Networks: 1980–1990\nThe Internet Explosion: The 1990s\nThe New Millennium\nHomework Problems and Questions\nWireshark Lab\nInterview: Leonard Kleinrock\nChapter 2 Application Layer\nPrinciples of Network Applications\nNetwork Application Architectures\nProcesses Communicating\nTransport Services Available to Applications\nTransport Services Provided by the Internet\nApplication-Layer Protocols\nNetwork Applications Covered in This Book\nThe Web and HTTP\nOverview of HTTP\nNon-Persistent and Persistent Connections\nHTTP Message Format\nUser-Server Interaction: Cookies\nWeb Caching\nElectronic Mail in the Internet\nMail Message Formats\nMail Access Protocols\nDNS—The Internet’s Directory Service\nServices Provided by DNS\nOverview of How DNS Works\nDNS Records and Messages\nPeer-to-Peer File Distribution\nVideo Streaming and Content Distribution\nInternet Video\nHTTP Streaming and DASH\nContent Distribution Networks\nCase Studies: Netflix and YouTube\nSocket Programming: Creating Network\nApplications\nSocket Programming with UDP\nSocket Programming with TCP\nHomework Problems and Questions\nSocket Programming Assignments\nWireshark Labs: HTTP, DNS\nInterview: Tim Berners-Lee\nChapter 3 Transport Layer\nIntroduction and Transport-Layer Services\nRelationship Between Transport and\nNetwork Layers\nOverview of the Transport Layer in the\nMultiplexing and Demultiplexing\nConnectionless Transport: UDP\nUDP Segment Structure\nUDP Checksum\nPrinciples of Reliable Data Transfer\nBuilding a Reliable Data Transfer Protocol\nPipelined Reliable Data Transfer Protocols\nGo-Back-N (GBN)\nSelective Repeat (SR)\nConnection-Oriented Transport: TCP\nThe TCP Connection\nTCP Segment Structure\nRound-Trip Time Estimation and Timeout\nReliable Data Transfer\nFlow Control\nTCP Connection Management\nPrinciples of Congestion Control\nThe Causes and the Costs of Congestion\nApproaches to Congestion Control\nTCP Congestion Control\nClassic TCP Congestion Control\nNetwork-Assisted Explicit Congestion\nNotification and Delayed-based Congestion\nEvolution of Transport-Layer Functionality\nHomework Problems and Questions\nProgramming Assignments\nWireshark Labs: Exploring TCP, UDP\nInterview: Van Jacobson\nChapter 4 The Network Layer: Data Plane\nOverview of Network Layer\nForwarding and Routing: The Data and\nControl Planes\nNetwork Service Model\nWhat’s Inside a Router?\nInput Port Processing and Destination-\nBased Forwarding\nOutput Port Processing\nWhere Does Queuing Occur?\nPacket Scheduling\nThe Internet Protocol (IP): IPv4, Addressing, IPv6,\nIPv4 Datagram Format\nIPv4 Addressing\nNetwork Address Translation (NAT)\nGeneralized Forwarding and SDN\nOpenFlow Examples of Match-plus-action\nMiddleboxes\nHomework Problems and Questions\nWireshark Lab: IP\nInterview: Vinton G. Cerf\nChapter 5 The Network Layer: Control Plane\nIntroduction\nRouting Algorithms\nThe Link-State (LS) Routing Algorithm\nThe Distance-Vector (DV) Routing\nIntra-AS Routing in the Internet: OSPF\nRouting Among the ISPs: BGP\nThe Role of BGP\nAdvertising BGP Route Information\nDetermining the Best Routes\nRouting Policy\nPutting the Pieces Together: Obtaining\nInternet Presence\nThe SDN Control Plane\nThe SDN Control Plane: SDN Controller\nand SDN Network-control Applications\nOpenFlow Protocol\nData and Control Plane Interaction: An\nSDN: Past and Future\nICMP: The Internet Control Message Protocol\nNetwork Management and SNMP,\nNETCONF/YANG\nThe Network Management Framework\nThe Simple Network Management Protocol\n(SNMP) and the Management Information\nThe Network Configuration Protocol\n(NETCONF) and YANG\nHomework Problems and Questions\nSocket Programming Assignment 5: ICMP Ping\nProgramming Assignment: Routing\nWireshark Lab: ICMP\nInterview: Jennifer Rexford\nChapter 6 The Link Layer and LANs\nIntroduction to the Link Layer\nThe Services Provided by the Link Layer\nWhere Is the Link Layer Implemented?\nError-Detection and -Correction Techniques\nParity Checks\nChecksumming Methods\nCyclic Redundancy Check (CRC)\nMultiple Access Links and Protocols\nChannel Partitioning Protocols\nRandom Access Protocols\nTaking-Turns Protocols\nDOCSIS: The Link-Layer Protocol for\nCable Internet Access\nSwitched Local Area Networks\nLink-Layer Addressing and ARP\nLink-Layer Switches\nVirtual Local Area Networks (VLANs)\nLink Virtualization: A Network as a Link Layer\nMultiprotocol Label Switching (MPLS)\nData Center Networking\nData Center Architectures\nTrends in Data Center Networking\nRetrospective: A Day in the Life of a Web Page\nGetting Started: DHCP, UDP, IP, and\nStill Getting Started: DNS and ARP\nStill Getting Started: Intra-Domain Routing\nto the DNS Server\nWeb Client-Server Interaction: TCP and\nHomework Problems and Questions\nWireshark Labs: 802.11 Ethernet\nInterview: Albert Greenberg\nChapter 7 Wireless and Mobile Networks\nIntroduction\nWireless Links and Network Characteristics\nWiFi: 802.11 Wireless LANs\nThe 802.11 Wireless LAN Architecture\nThe 802.11 MAC Protocol\nThe IEEE 802.11 Frame\nMobility in the Same IP Subnet\nAdvanced Features in 802.11\nPersonal Area Networks: Bluetooth\nCellular Networks: 4G and 5G\n4G LTE Cellular Networks: Architecture\nand Elements\nLTE Protocols Stacks\nLTE Radio Access Network\nAdditional LTE Functions: Network\nAttachment and Power Management\nThe Global Cellular Network: A Network of\n5G Cellular Networks\nMobility Management: Principles\nDevice Mobility: a Network-layer\nPerspective\nHome Networks and Roaming on Visited\nDirect and Indirect Routing to/from a\nMobile Device\nMobility Management in Practice\nMobility Management in 4G/5G Networks\nWireless and Mobility: Impact on Higher-Layer\nHomework Problems and Questions\nWireshark Lab: WiFi\nInterview: Deborah Estrin\nChapter 8 Security in Computer Networks\nWhat Is Network Security?\nPrinciples of Cryptography\nSymmetric Key Cryptography\nPublic Key Encryption\nMessage Integrity and Digital Signatures\nCryptographic Hash Functions\nMessage Authentication Code\nDigital Signatures\nEnd-Point Authentication\nSecuring E-Mail\nSecure E-Mail\nSecuring TCP Connections: TLS\nThe Big Picture\nA More Complete Picture\nNetwork-Layer Security: IPsec and Virtual Private\nIPsec and Virtual Private Networks (VPNs)\nThe AH and ESP Protocols\nSecurity Associations\nThe IPsec Datagram\nIKE: Key Management in IPsec\nSecuring Wireless LANs and 4G/5G Cellular\nAuthentication and Key Agreement in\n802.11 Wireless LANs\nAuthentication and Key Agreement in\n4G/5G Cellular Networks\nOperational Security: Firewalls and Intrusion\nDetection Systems\nIntrusion Detection Systems\nHomework Problems and Questions\nWireshark Lab: SSL\nInterview: Steven M. Bellovin\nChapter 2. For now, let’s draw upon a simple analogy, one that we will\nfrequently use in this book. Suppose Alice wants to send a letter to Bob\nusing the postal service. Alice, of course, can’t just write the letter (the data)\nand drop the letter out her window. Instead, the postal service requires that\nAlice put the letter in an envelope; write Bob’s full name, address, and zip\ncode in the center of the envelope; seal the envelope; put a stamp in the\nupper-right-hand corner of the envelope; and finally, drop the envelope into\nan official postal service mailbox. Thus, the postal service has its own\n“postal service interface,” or set of rules, that Alice must follow to have the\npostal service deliver her letter to Bob. In a similar manner, the Internet has\na socket interface that the program sending data must follow to have the\nInternet deliver the data to the program that will receive the data.\nThe postal service, of course, provides more than one service to its\ncustomers. It provides express delivery, reception confirmation, ordinary\nuse, and many more services. In a similar manner, the Internet provides\nmultiple services to its applications. When you develop an Internet\napplication, you too must choose one of the Internet’s services for your\napplication. We’ll describe the Internet’s services in Chapter 2.\nWe have just given two descriptions of the Internet; one in terms of its\nhardware and software components, the other in terms of an infrastructure\nfor providing services to distributed applications. But perhaps you are still\nconfused as to what the Internet is. What are packet switching and TCP/IP?\nWhat are routers? What kinds of communication links are present in the\nInternet? What is a distributed application? How can a thermostat or body\nscale be attached to the Internet? If you feel a bit overwhelmed by all of this\nnow, don’t worry—the purpose of this book is to introduce you to both the\nnuts and bolts of the Internet and the principles that govern how and why it\nworks. We’ll explain these important terms and questions in the following\nsections and chapters.\n1.1.3 What Is a Protocol?\nNow that we’ve got a bit of a feel for what the Internet is, let’s consider\nanother important buzzword in computer networking: protocol. What is a\nprotocol? What does a protocol do?\nA Human Analogy\nIt is probably easiest to understand the notion of a computer network\nprotocol by first considering some human analogies, since we humans\nexecute protocols all of the time. Consider what you do when you want to\nask someone for the time of day. A typical exchange is shown in Figure 1.2.\nHuman protocol (or good manners, at least) dictates that one first offer a\ngreeting (the first “Hi” in Figure 1.2) to initiate communication with\nsomeone else. The typical response to a “Hi” is a returned “Hi” message.\nImplicitly, one then takes a cordial “Hi” response as an indication that one\ncan proceed and ask for the time of day. A different response to the initial\n“Hi” (such as “Don’t bother me!” or “I don’t speak English,” or some\nunprintable reply) might indicate an unwillingness or inability to\ncommunicate. In this case, the human protocol would be not to ask for the\ntime of day. Sometimes one gets no response at all to a question, in which\ncase one typically gives up asking that person for the time. Note that in our\nhuman protocol, there are specific messages we send, and specific actions\nwe take in response to the received reply messages or other events (such as\nno reply within some given amount of time). Clearly, transmitted and\nreceived messages, and actions taken when these messages are sent or\nreceived or other events occur, play a central role in a human protocol. If\npeople run different protocols (for example, if one person has manners but\nthe other does not, or if one understands the concept of time and the other\ndoes not) the protocols do not interoperate and no useful work can be\naccomplished. The same is true in networking—it takes two (or more)\ncommunicating entities running the same protocol in order to accomplish a\nFigure 1.2 ♦A human protocol and a computer network protocol\nLet’s consider a second human analogy. Suppose you’re in a college\nclass (a computer networking class, for example!). The teacher is droning\non about protocols and you’re confused. The teacher stops to ask, “Are\nthere any questions?” (a message that is transmitted to, and received by, all\nstudents who are not sleeping). You raise your hand (transmitting an\nimplicit message to the teacher). Your teacher acknowledges you with a\nsmile, saying “Yes . . .” (a transmitted message encouraging you to ask your\nquestion—teachers love to be asked questions), and you then ask your\nquestion (that is, transmit your message to your teacher). Your teacher hears\nyour question (receives your question message) and answers (transmits a\nreply to you). Once again, we see that the transmission and receipt of\nmessages, and a set of conventional actions taken when these messages are\nsent and received, are at the heart of this question-and-answer protocol.\nNetwork Protocols\nA network protocol is similar to a human protocol, except that the entities\nexchanging messages and taking actions are hardware or software\ncomponents of some device (for example, computer, smartphone, tablet,\nrouter, or other network-capable device). All activity in the Internet that\ninvolves two or more communicating remote entities is governed by a\nprotocol. For example, hardware-implemented protocols in two physically\nconnected computers control the flow of bits on the “wire” between the two\nnetwork interface cards; congestion-control protocols in end systems\ncontrol the rate at which packets are transmitted between sender and\nreceiver; protocols in routers determine a packet’s path from source to\ndestination. Protocols are running everywhere in the Internet, and\nconsequently much of this book is about computer network protocols.\nAs an example of a computer network protocol with which you are\nprobably familiar, consider what happens when you make a request to a\nWeb server, that is, when you type the URL of a Web page into your Web\nbrowser. The scenario is illustrated in the right half of Figure 1.2. First, your\ncomputer will send a connection request message to the Web server and\nwait for a reply. The Web server will eventually receive your connection\nrequest message and return a connection reply message. Knowing that it is\nnow OK to request the Web document, your computer then sends the name\nof the Web page it wants to fetch from that Web server in a GET message.\nFinally, the Web server returns the Web page (file) to your computer.\nGiven the human and networking examples above, the exchange of\nmessages and the actions taken when these messages are sent and received\nare the key defining elements of a protocol:\nA protocol defines the format and the order of messages exchanged\nbetween two or more communicating entities, as well as the actions\ntaken on the transmission and/or receipt of a message or other event.\nThe Internet, and computer networks in general, make extensive use of\ncommunication tasks. As you read through this book, you will learn that\nsome protocols are simple and straightforward, while others are complex\nand intellectually deep. Mastering the field of computer networking is\nequivalent to understanding the what, why, and how of networking\n1.2 The Network Edge\nIn the previous section, we presented a high-level overview of the Internet\nand ­networking protocols. We are now going to delve a bit more deeply into\nthe components of the Internet. We begin in this section at the edge of the\nnetwork and look at the components with which we are most ­familiar—\nnamely, the computers, smartphones and other devices that we use on a\ndaily basis. In the next section, we’ll move from the network edge to the\nnetwork core and examine switching and routing in computer networks.\nRecall from the previous section that in computer networking jargon,\nthe computers and other devices connected to the Internet are often referred\nto as end systems. They are referred to as end systems because they sit at\nthe edge of the Internet, as shown in Figure 1.3. The Internet’s end systems\ninclude desktop computers (e.g., desktop PCs, Macs, and Linux boxes),\nservers (e.g., Web and e-mail servers), and mobile devices (e.g., laptops,\nsmartphones, and tablets). Furthermore, an increasing number of non-\ntraditional “things” are being attached to the Internet as end ­systems (see\nthe Case History feature).\nFigure 1.3 ♦End-system interaction\nEnd systems are also referred to as hosts because they host (that is, run)\napplication programs such as a Web browser program, a Web server\nprogram, an e-mail client program, or an e-mail server program.\nThroughout this book we will use the terms hosts and end systems\ninterchangeably; that is, host = end system. Hosts are sometimes further\ndivided into two categories: clients and servers. Informally, clients tend to\nbe desktops, laptops, smartphones, and so on, whereas servers tend to be\nmore powerful machines that store and distribute Web pages, stream video,\nrelay e-mail, and so on. Today, most of the servers from which we receive\nsearch results, e-mail, Web pages, videos and mobile app content reside in\nlarge data centers. For example, as of 2020, Google has 19 data centers on\nfour continents, collectively containing several million servers. Figure 1.3\nincludes two such data centers, and the Case History sidebar describes data\ncenters in more detail.\nInternet companies such as Google, Microsoft, Amazon, and Alibaba have built\nmassive data centers, each housing tens to hundreds of thousands of hosts. These\ndata centers are not only connected to the Internet, as shown in Figure 1.1, but also\ninternally include complex computer networks that interconnect the datacenter’s hosts.\nThe data centers are the engines behind the Internet applications that we use on a\ndaily basis.\nBroadly speaking, data centers serve three purposes, which we describe here in the\ncontext of Amazon for concreteness. First, they serve Amazon e-commerce pages to\nusers, for example, pages describing products and purchase information. Second, they\nserve as massively parallel computing infrastructures for Amazon-specific data\nprocessing tasks. Third, they provide cloud computing to other companies. Indeed,\ntoday a major trend in computing is for companies to use a cloud provider such as\nAmazon to handle essentially all of their IT needs. For example, Airbnb and many other\nInternet-based companies do not own and manage their own data centers but instead\nrun their entire Web-based services in the Amazon cloud, called Amazon Web Services\nThe worker bees in a data center are the hosts. They serve content (e.g., Web\npages and videos), store e-mails and documents, and collectively perform massively\ndistributed computations. The hosts in data centers, called blades and resembling\npizza boxes, are generally commodity hosts that include CPU, memory, and disk\nstorage. The hosts are stacked in racks, with each rack typically having 20 to\n40 blades. The racks are then interconnected using sophisticated and evolving data\ncenter network designs. Data center networks are discussed in greater detail in\n1.2.1 Access Networks\nHaving considered the applications and end systems at the “edge of the\nnetwork,” let’s next consider the access network—the network that\nphysically connects an end system to the first router (also known as the\n“edge router”) on a path from the end system to any other distant end\nsystem. Figure 1.4 shows several types of access networks with thick,\nshaded lines and the settings (home, enterprise, and wide-area mobile\nwireless) in which they are used.\nFigure 1.4 ♦Access networks\nHome Access: DSL, Cable, FTTH, and 5G Fixed Wireless\nAs of 2020, more than 80% of the households in Europe and the USA have\nInternet access [Statista 2019]. Given this widespread use of home access\nnetworks let’s begin our overview of access networks by considering how\nhomes connect to the Internet.\nToday, the two most prevalent types of broadband residential access are\ndigital subscriber line (DSL) and cable. A residence typically obtains DSL\nInternet access from the same local telephone company (telco) that provides\nits wired local phone access. Thus, when DSL is used, a customer’s telco is\nalso its ISP. As shown in Figure 1.5, each customer’s DSL modem uses the\nexisting telephone line exchange data with a digital subscriber line access\nmultiplexer (DSLAM) located in the telco’s local central office (CO). The\nhome’s DSL modem takes digital data and translates it to high-­frequency\ntones for transmission over telephone wires to the CO; the analog signals\nfrom many such houses are translated back into digital format at the\nFigure 1.5 ♦DSL Internet access\nThe residential telephone line carries both data and traditional\ntelephone signals simultaneously, which are encoded at different\nfrequencies:\nA high-speed downstream channel, in the 50 kHz to 1 MHz band\nA medium-speed upstream channel, in the 4 kHz to 50 kHz band\nAn ordinary two-way telephone channel, in the 0 to 4 kHz band\nThis approach makes the single DSL link appear as if there were three\nseparate links, so that a telephone call and an Internet connection can share\nthe DSL link at the same time. (We’ll describe this technique of frequency-\ndivision multiplexing in Section 1.3.1.) On the customer side, a splitter\nseparates the data and telephone signals arriving to the home and forwards\nthe data signal to the DSL modem. On the telco side, in the CO, the\nDSLAM separates the data and phone signals and sends the data into the\nInternet. Hundreds or even thousands of households connect to a single\nThe DSL standards define multiple transmission rates, including\ndownstream transmission rates of 24 Mbs and 52 Mbs, and upstream rates\nof 3.5 Mbps and 16  Mbps; the newest standard provides for aggregate\nupstream plus downstream rates of 1 Gbps [ITU 2014]. Because the\ndownstream and upstream rates are different, the access is said to be\nasymmetric. The actual downstream and upstream transmission rates\nachieved may be less than the rates noted above, as the DSL provider may\npurposefully limit a residential rate when tiered service (different rates,\navailable at different prices) are offered. The maximum rate is also limited\nby the distance between the home and the CO, the gauge of the twisted-pair\nline and the degree of electrical interference. Engineers have expressly\ndesigned DSL for short distances between the home and the CO; generally,\nif the residence is not located within 5 to 10 miles of the CO, the residence\nmust resort to an alternative form of Internet access.\nWhile DSL makes use of the telco’s existing local telephone\ninfrastructure, cable Internet access makes use of the cable television\ncompany’s existing cable television infrastructure. A residence obtains cable\nInternet access from the same company that provides its cable television. As\nillustrated in Figure 1.6, fiber optics connect the cable head end to\nneighborhood-level junctions, from which traditional coaxial cable is then\nused to reach individual houses and apartments. Each neighborhood\njunction typically supports 500 to 5,000 homes. Because both fiber and\ncoaxial cable are employed in this system, it is often referred to as hybrid\nfiber coax (HFC).\nFigure 1.6 ♦A hybrid fiber-coaxial access network\nCable internet access requires special modems, called cable modems.\nAs with a DSL modem, the cable modem is typically an external device and\nconnects to the home PC through an Ethernet port. (We will discuss\nEthernet in great detail in Chapter 6.) At the cable head end, the cable\nmodem termination system (CMTS) serves a similar function as the DSL\nnetwork’s DSLAM—turning the analog signal sent from the cable modems\nin many downstream homes back into digital format. Cable modems divide\nthe HFC network into two channels, a downstream and an upstream\nchannel. As with DSL, access is typically asymmetric, with the downstream\nchannel typically allocated a higher transmission rate than the upstream\nchannel. The DOCSIS 2.0 and 3.0 standards define downstream bitrates of\n40 Mbps and 1.2 Gbps, and upstream rates of  30  Mbps and 100 Mbps,\nrespectively. As in the case of DSL networks, the ­maximum achievable rate\nmay not be realized due to lower contracted data rates or media\nimpairments.\nOne important characteristic of cable Internet access is that it is a\nshared broadcast medium. In particular, every packet sent by the head end\ntravels downstream on every link to every home and every packet sent by a\nhome travels on the upstream channel to the head end. For this reason, if\nseveral users are simultaneously downloading a video file on the\ndownstream channel, the actual rate at which each user receives its video\nfile will be significantly lower than the aggregate cable downstream rate.\nOn the other hand, if there are only a few active users and they are all Web\nsurfing, then each of the users may actually receive Web pages at the full\ncable downstream rate, because the users will rarely request a Web page at\nexactly the same time. Because the upstream channel is also shared, a\ndistributed multiple access protocol is needed to coordinate transmissions\nand avoid collisions. (We’ll discuss this collision issue in some detail in\nChapter 6.)\nAlthough DSL and cable networks currently represent the majority of\nresidential broadband access in the United States, an up-and-coming\ntechnology that provides even higher speeds is fiber to the home (FTTH)\n[Fiber Broadband 2020]. As the name suggests, the FTTH concept is simple\n—provide an optical fiber path from the CO directly to the home. FTTH\ncan potentially provide Internet access rates in the gigabits per second\nThere are several competing technologies for optical distribution from\nthe CO to the homes. The simplest optical distribution network is called\ndirect fiber, with one fiber leaving the CO for each home. More commonly,\neach fiber leaving the central office is actually shared by many homes; it is\nnot until the fiber gets relatively close to the homes that it is split into\nindividual customer-specific fibers. There are two competing optical-\ndistribution network architectures that perform this splitting: active optical\nnetworks (AONs) and passive optical networks (PONs). AON is essentially\nswitched Ethernet, which is discussed in Chapter 6.\nHere, we briefly discuss PON, which is used in Verizon’s FiOS service.\nFigure 1.7 shows FTTH using the PON distribution architecture. Each\nhome has an optical network terminator (ONT), which is connected by\ndedicated optical fiber to a neighborhood splitter. The splitter combines a\nnumber of homes (typically less than 100) onto a single, shared optical\nfiber, which connects to an optical line ­terminator (OLT) in the telco’s CO.\nThe OLT, providing conversion between optical and electrical signals,\nconnects to the Internet via a telco router. At home, users connect a home\nrouter (typically a wireless router) to the ONT and access the ­Internet via\nthis home router. In the PON architecture, all packets sent from OLT to the\nsplitter are replicated at the splitter (similar to a cable head end).\nFigure 1.7 ♦FTTH Internet access\nIn addition to DSL, Cable, and FTTH, 5G fixed wireless is beginning\nto be deployed. 5G fixed wireless not only promises high-speed residential\naccess, but will do so without installing costly and failure-prone cabling\nfrom the telco’s CO to the home. With 5G fixed wireless, using beam-\nforming technology, data is sent wirelessly from a provider’s base station to\nthe a modem in the home. A WiFi wireless router is connected to the\nmodem (possibly bundled together), similar to how a WiFi wireless router\nis connected to a cable or DSL modem. 5G cellular networks are covered in\nAccess in the Enterprise (and the Home): Ethernet and WiFi\nOn corporate and university campuses, and increasingly in home settings, a\nlocal area  network (LAN) is used to connect an end system to the edge\nrouter. Although there are many types of LAN technologies, Ethernet is by\nfar the most prevalent access technology in corporate, university, and home\nnetworks. As shown in Figure 1.8, Ethernet users use twisted-pair copper\nwire to connect to an Ethernet switch, a technology discussed in detail in\nChapter 6. The Ethernet switch, or a network of such interconnected\nswitches, is then in turn connected into the larger Internet. With Ethernet\naccess, users typically have 100 Mbps to tens of Gbps access to the\nEthernet switch, whereas servers may have 1 Gbps 10 Gbps access.\nFigure 1.8 ♦Ethernet Internet access\nIncreasingly, however, people are accessing the Internet wirelessly from\nlaptops, smartphones, tablets, and other “things”. In a wireless LAN setting,\nwireless users transmit/receive packets to/from an access point that is\nconnected into the enterprise’s network (most likely using wired Ethernet),\nwhich in turn is connected to the wired Internet. A wireless LAN user must\ntypically be within a few tens of meters of the access point. Wireless LAN\naccess based on IEEE 802.11 technology, more colloquially known as WiFi,\nis now just about everywhere—universities, business offices, cafes, airports,\nhomes, and even in airplanes. As discussed in detail in Chapter 7, 802.11\ntoday provides a shared transmission rate of up to more than 100 Mbps.\nEven though Ethernet and WiFi access networks were initially\ndeployed in enterprise (corporate, university) settings, they are also\ncommon components of home networks. Many homes combine broadband\nresidential access (that is, cable modems or DSL) with these inexpensive\nwireless LAN technologies to create powerful home networks Figure 1.9\nshows a typical home network. This home network consists of a roaming\nlaptop, multiple Internet-connected home appliances, as well as a wired PC;\na base station (the wireless access point), which communicates with the\nwireless PC and other wireless devices in the home; and a home router that\nconnects the wireless access point, and any other wired home devices, to\nthe Internet. This network allows household members to have broadband\naccess to the Internet with one member roaming from the kitchen to the\nbackyard to the bedrooms.\nFigure 1.9 ♦A typical home network\nWide-Area Wireless Access: 3G and LTE 4G and 5G\nMobile devices such as iPhones and Android devices are being used to\nmessage, share photos in social networks, make mobile payments, watch\nmovies, stream music, and much more while on the run. These devices\nemploy the same wireless infrastructure used for cellular telephony to\nsend/receive packets through a base station that is operated by the cellular\nnetwork provider. Unlike WiFi, a user need only be within a few tens of\nkilometers (as opposed to a few tens of meters) of the base station.\nTelecommunications companies have made enormous investments in\nso-called fourth-generation (4G) wireless, which provides real-world\ndownload speeds of up to 60 Mbps. But even higher-speed wide-area access\ntechnologies—a fifth-generation (5G) of wide-area wireless networks—are\nalready being deployed. We’ll cover the basic principles of wireless\nnetworks and mobility, as well as WiFi, 4G and 5G technologies (and\nmore!) in Chapter 7.\n1.2.2 Physical Media\nIn the previous subsection, we gave an overview of some of the most\nimportant network access technologies in the Internet. As we described\nthese technologies, we also indicated the physical media used. For example,\nwe said that HFC uses a combination of fiber cable and coaxial cable. We\nsaid that DSL and Ethernet use copper wire. And we said that mobile access\nnetworks use the radio spectrum. In this subsection, we provide a brief\noverview of these and other transmission media that are commonly used in\nthe Internet.\nIn order to define what is meant by a physical medium, let us reflect on\nthe brief life of a bit. Consider a bit traveling from one end system, through\na series of links and routers, to another end system. This poor bit gets\nkicked around and transmitted many, many times! The source end system\nfirst transmits the bit, and shortly thereafter the first router in the series\nreceives the bit; the first router then transmits the bit, and shortly thereafter\nthe second router receives the bit; and so on. Thus our bit, when traveling\nfrom source to destination, passes through a series of transmitter-receiver\npairs. For each transmitter-receiver pair, the bit is sent by propagating\nelectromagnetic waves or optical pulses across a physical medium. The\nphysical medium can take many shapes and forms and does not have to be\nof the same type for each transmitter-receiver pair along the path. Examples\nof physical media include twisted-pair copper wire, coaxial cable,\nmultimode fiber-optic cable, terrestrial radio spectrum, and satellite radio\nspectrum. Physical media fall into two categories: guided media and\nunguided media. With guided media, the waves are guided along a solid\nmedium, such as a fiber-optic cable, a twisted-pair copper wire, or a coaxial\ncable. With unguided media, the waves propagate in the atmosphere and in\nouter space, such as in a wireless LAN or a digital satellite channel.\nBut before we get into the characteristics of the various media types, let\nus say a few words about their costs. The actual cost of the physical link\n(copper wire, fiber-optic cable, and so on) is often relatively minor\ncompared with other networking costs. In particular, the labor cost\nassociated with the installation of the physical link can be orders of\nmagnitude higher than the cost of the material. For this reason, many\nbuilders install twisted pair, optical fiber, and coaxial cable in every room in\na building. Even if only one medium is initially used, there is a good chance\nthat another medium could be used in the near future, and so money is\nsaved by not having to lay additional wires in the future.\nTwisted-Pair Copper Wire\nThe least expensive and most commonly used guided transmission medium\nis twisted-pair copper wire. For over a hundred years it has been used by\ntelephone networks. In fact, more than 99 percent of the wired connections\nfrom the telephone handset to the local telephone switch use twisted-pair\ncopper wire. Most of us have seen twisted pair in our homes (or those of\nour parents or grandparents!) and work environments. Twisted pair consists\nof two insulated copper wires, each about 1 mm thick, arranged in a regular\nspiral pattern. The wires are twisted together to reduce the electrical\ninterference from similar pairs close by. Typically, a number of pairs are\nbundled together in a cable by wrapping the pairs in a protective shield. A\nwire pair constitutes a single communication link. Unshielded twisted pair\n(UTP) is commonly used for computer networks within a building, that is,\nfor LANs. Data rates for LANs using twisted pair today range from 10\nMbps to 10 Gbps. The data rates that can be achieved depend on the\nthickness of the wire and the distance between transmitter and receiver.\nWhen fiber-optic technology emerged in the 1980s, many people\ndisparaged twisted pair because of its relatively low bit rates. Some people\neven felt that fiber-optic technology would completely replace twisted pair.\nBut twisted pair did not give up so easily. Modern twisted-pair technology,\nsuch as category 6a cable, can achieve data rates of 10 Gbps for distances\nup to a hundred meters. In the end, twisted pair has emerged as the\ndominant solution for high-speed LAN networking.\nAs discussed earlier, twisted pair is also commonly used for residential\nInternet access. We saw that dial-up modem technology enables access at\nrates of up to 56  kbps over twisted pair. We also saw that DSL (digital\nsubscriber line) technology has enabled residential users to access the\nInternet at tens of Mbps over twisted pair (when users live close to the ISP’s\ncentral office).\nCoaxial Cable\nLike twisted pair, coaxial cable consists of two copper conductors, but the\ntwo conductors are concentric rather than parallel. With this construction\nand special insulation and shielding, coaxial cable can achieve high data\ntransmission rates. Coaxial cable is quite common in cable television\nsystems. As we saw earlier, cable television systems have recently been\ncoupled with cable modems to provide residential users with Internet access\nat rates of hundreds of Mbps. In cable television and cable Internet access,\nthe transmitter shifts the digital signal to a specific frequency band, and the\nresulting analog signal is sent from the transmitter to one or more receivers.\nCoaxial cable can be used as a guided shared medium. Specifically, a\nnumber of end systems can be connected directly to the cable, with each of\nthe end systems receiving whatever is sent by the other end systems.\nFiber Optics\nAn optical fiber is a thin, flexible medium that conducts pulses of light,\nwith each pulse representing a bit. A single optical fiber can support\ntremendous bit rates, up to tens or even hundreds of gigabits per second.\nThey are immune to electromagnetic interference, have very low signal\nattenuation up to 100 kilometers, and are very hard to tap. These\ncharacteristics have made fiber optics the preferred long-haul guided\ntransmission media, particularly for overseas links. Many of the long-\ndistance telephone networks in the United States and elsewhere now use\nfiber optics exclusively. Fiber optics is also prevalent in the backbone of the\nInternet. However, the high cost of optical devices—such as transmitters,\nreceivers, and switches—has hindered their deployment for short-haul\ntransport, such as in a LAN or into the home in a residential access\nnetwork. The Optical Carrier (OC) standard link speeds range from\n51.8 Mbps to 39.8 Gbps; these specifications are often referred to as OC-n,\nwhere the link speed equals n × 51.8 Mbps. Standards in use today include\nOC-1, OC-3, OC-12, OC-24, OC-48, OC-96, OC-192, OC-768.\nTerrestrial Radio Channels\nRadio channels carry signals in the electromagnetic spectrum. They are an\nattractive medium because they require no physical wire to be installed, can\npenetrate walls, provide connectivity to a mobile user, and can potentially\ncarry a signal for long distances. The characteristics of a radio channel\ndepend significantly on the propagation environment and the distance over\nwhich a signal is to be carried. Environmental considerations determine\npath loss and shadow fading (which decrease the signal strength as the\nsignal travels over a distance and around/through obstructing objects),\nmultipath fading (due to signal reflection off of interfering objects), and\ninterference (due to other transmissions and electromagnetic signals).\nTerrestrial radio channels can be broadly classified into three groups:\nthose that operate over very short distance (e.g., with one or two meters);\nthose that operate in local areas, typically spanning from ten to a few\nhundred meters; and those that operate in the wide area, spanning tens of\nkilometers. Personal devices such as wireless headsets, keyboards, and\nmedical devices operate over short distances; the wireless LAN\ntechnologies described in Section 1.2.1 use local-area radio channels; the\ncellular access technologies use wide-area radio channels. We’ll discuss\nradio channels in detail in Chapter 7.\nSatellite Radio Channels\nA communication satellite links two or more Earth-based microwave\ntransmitter/receivers, known as ground stations. The satellite receives\ntransmissions on one frequency band, regenerates the signal using a\nrepeater (discussed below), and transmits the signal on another frequency.\nTwo types of satellites are used in  communications: geostationary\nsatellites and low-earth orbiting (LEO) satellites.\nGeostationary satellites permanently remain above the same spot on\nEarth. This stationary presence is achieved by placing the satellite in orbit at\n36,000 kilo­meters above Earth’s surface. This huge distance from ground\nstation through satellite back to ground station introduces a substantial\nsignal propagation delay of 280 milliseconds. Nevertheless, satellite links,\nwhich can operate at speeds of hundreds of Mbps, are often used in areas\nwithout access to DSL or cable-based Internet access.\nLEO satellites are placed much closer to Earth and do not remain\npermanently above one spot on Earth. They rotate around Earth (just as the\nMoon does) and may communicate with each other, as well as with ground\nstations. To provide continuous coverage to an area, many satellites need to\nbe placed in orbit. There are currently many low-altitude communication\nsystems in development. LEO satellite ­technology may be used for Internet\naccess sometime in the future.\n1.3 The Network Core\nHaving examined the Internet’s edge, let us now delve more deeply inside\nthe network core—the mesh of packet switches and links that interconnects\nthe Internet’s end systems. Figure 1.10 highlights the network core with\nthick, shaded lines.\nFigure 1.10 ♦The network core\n1.3.1 Packet Switching\nIn a network application, end systems exchange messages with each other.\nMessages can contain anything the application designer wants. Messages\nmay perform a control function (for example, the “Hi” messages in our\nhandshaking example in Figure 1.2) or can contain data, such as an e-mail\nmessage, a JPEG image, or an MP3 audio file. To send a message from a\nsource end system to a destination end system, the source breaks long\nmessages into smaller chunks of data known as packets. Between source\nand destination, each packet travels through communication links and\npacket switches (for which there are two predominant types, routers and\nlink-layer switches). Packets are transmitted over each communication link\nat a rate equal to the full transmission rate of the link. So, if a source end\nsystem or a packet switch is sending a packet of L bits over a link with\ntransmission rate R bits/sec, then the time to transmit the packet is L / R\nStore-and-Forward Transmission\nMost packet switches use store-and-forward transmission at the inputs to\nthe links. Store-and-forward transmission means that the packet switch\nmust receive the entire packet before it can begin to transmit the first bit of\nthe packet onto the outbound link. To explore store-and-forward\ntransmission in more detail, consider a simple network consisting of two\nend systems connected by a single router, as shown in Figure 1.11. A router\nwill typically have many incident links, since its job is to switch an\nincoming packet onto an outgoing link; in this simple example, the router\nhas the rather simple task of transferring a packet from one (input) link to\nthe only other attached link. In this example, the source has three packets,\neach consisting of L bits, to send to the destination. At the snapshot of time\nshown in Figure 1.11, the source has transmitted some of packet 1, and the\nfront of packet 1 has already arrived at the router. Because the router\nemploys store-and-forwarding, at this instant of time, the router cannot\ntransmit the bits it has received; instead it must first buffer (i.e., “store”) the\npacket’s bits. Only after the router has received all of the packet’s bits can it\nbegin to transmit (i.e., “forward”) the packet onto the outbound link. To\ngain some insight into store-and-forward transmission, let’s now calculate\nthe amount of time that elapses from when the source begins to send the\npacket until the destination has received the entire packet. (Here we will\nignore propagation delay—the time it takes for the bits to travel across the\nwire at near the speed of light—which will be discussed in Section 1.4.)\nThe source begins to transmit at time 0; at time L/R seconds, the source has\ntransmitted the entire packet, and the entire packet has been received and\nstored at the router (since there is no propagation delay). At time L/R\nseconds, since the router has just received the entire packet, it can begin to\ntransmit the packet onto the outbound link towards the destination; at time\n2L/R, the router has transmitted the entire packet, and the entire packet has\nbeen received by the destination. Thus, the total delay is 2L/R. If the switch\ninstead forwarded bits as soon as they arrive (without first receiving the\nentire packet), then the total delay would be L/R since bits are not held up at\nthe router. But, as we will discuss in Section 1.4, routers need to receive,\nstore, and process the entire packet before forwarding.\nFigure 1.11 ♦Store-and-forward packet switching\nNow let’s calculate the amount of time that elapses from when the\nsource begins to send the first packet until the destination has received all\nthree packets. As before, at time L/R, the router begins to forward the first\npacket. But also at time L/R the source will begin to send the second packet,\nsince it has just finished sending the entire first packet. Thus, at time 2L/R,\nthe destination has received the first packet and the router has received the\nsecond packet. Similarly, at time 3L/R, the destination has received the first\ntwo packets and the router has received the third packet. Finally, at time\n4L/R the destination has received all three packets!\nLet’s now consider the general case of sending one packet from source\nto destination over a path consisting of N links each of rate R (thus, there\nare N-1 routers between source and destination). Applying the same logic as\nabove, we see that the end-to-end delay is:\nYou may now want to try to determine what the delay would be for P\npackets sent over a series of N links.\nQueuing Delays and Packet Loss\nEach packet switch has multiple links attached to it. For each attached link,\nthe packet switch has an output buffer (also called an output queue),\nwhich stores packets that the router is about to send into that link. The\noutput buffers play a key role in packet switching. If an arriving packet\nneeds to be transmitted onto a link but finds the link busy with the\ntransmission of another packet, the arriving packet must wait in the output\nbuffer. Thus, in addition to the store-and-forward delays, packets suffer\ndend-to-end = N L\noutput buffer queuing delays. These delays are variable and depend on the\nlevel of congestion in the network. Since the amount of buffer space is\nfinite, an arriving packet may find that the buffer is completely full with\nother packets waiting for transmission. In this case, packet loss will occur\n—either the arriving packet or one of the already-queued packets will be\nFigure 1.12 illustrates a simple packet-switched network. As in Figure\n1.11, packets are represented by three-dimensional slabs. The width of a\nslab represents the number of bits in the packet. In this figure, all packets\nhave the same width and hence the same length. Suppose Hosts A and B are\nsending packets to Host E. Hosts A and B first send their packets along 100\nMbps Ethernet links to the first router. The router then directs these packets\nto the 15 Mbps link. If, during a short interval of time, the arrival rate of\npackets to the router (when converted to bits per second) exceeds 15 Mbps,\ncongestion will occur at the router as packets queue in the link’s output\nbuffer before being transmitted onto the link. For example, if Host A and B\neach send a burst of five packets back-to-back at the same time, then most\nof these packets will spend some time waiting in the queue. The situation is,\nin fact, entirely analogous to many common-day situations—for example,\nwhen we wait in line for a bank teller or wait in front of a tollbooth. We’ll\nexamine this queuing delay in more detail in Section 1.4.\nFigure 1.12 ♦Packet switching\nForwarding Tables and Routing Protocols\nEarlier, we said that a router takes a packet arriving on one of its attached\ncommunication links and forwards that packet onto another one of its\nattached communication links. But how does the router determine which\nlink it should forward the packet onto? Packet forwarding is actually done\nin different ways in different types of computer networks. Here, we briefly\ndescribe how it is done in the Internet.\nIn the Internet, every end system has an address called an IP address.\nWhen a source end system wants to send a packet to a destination end\nsystem, the source includes the destination’s IP address in the packet’s\nheader. As with postal addresses, this address has a hierarchical structure.\nWhen a packet arrives at a router in the network, the router examines a\nportion of the packet’s destination address and forwards the packet to an\nadjacent router. More specifically, each router has a forwarding table that\nmaps destination addresses (or portions of the destination addresses) to that\nrouter’s outbound links. When a packet arrives at a router, the router\nexamines the address and searches its forwarding table, using this\ndestination address, to find the appropriate outbound link. The router then\ndirects the packet to this outbound link.\nThe end-to-end routing process is analogous to a car driver who does\nnot use maps but instead prefers to ask for directions. For example, suppose\nJoe is driving from Philadelphia to 156 Lakeside Drive in Orlando, Florida.\nJoe first drives to his neighborhood gas station and asks how to get to 156\nLakeside Drive in Orlando, Florida. The gas station attendant extracts the\nFlorida portion of the address and tells Joe that he needs to get onto the\ninterstate highway I-95 South, which has an entrance just next to the gas\nstation. He also tells Joe that once he enters Florida, he should ask someone\nelse there. Joe then takes I-95 South until he gets to Jacksonville, Florida, at\nwhich point he asks another gas station attendant for directions. The\nattendant extracts the Orlando portion of the address and tells Joe that he\nshould continue on I-95 to Daytona Beach and then ask someone else. In\nDaytona Beach, another gas station attendant also extracts the Orlando\nportion of the address and tells Joe that he should take I-4 directly to\nOrlando. Joe takes I-4 and gets off at the Orlando exit. Joe goes to another\ngas station attendant, and this time the attendant extracts the Lakeside Drive\nportion of the address and tells Joe the road he must follow to get to\nLakeside Drive. Once Joe reaches Lakeside Drive, he asks a kid on a\nbicycle how to get to his destination. The kid extracts the 156 portion of the\naddress and points to the house. Joe finally reaches his ultimate destination.\nIn the above analogy, the gas station attendants and kids on bicycles are\nanalogous to routers.\nWe just learned that a router uses a packet’s destination address to\nChapter 5. But to whet your appetite here, we’ll note now that the Internet\nhas a number of special routing protocols that are used to automatically set\nthe forwarding tables. A routing protocol may, for example, determine the\nshortest path from each router to each destination and use the shortest path\nresults to configure the forwarding tables in the routers.\n1.3.2 Circuit Switching\nThere are two fundamental approaches to moving data through a network of\nlinks and switches: circuit switching and packet switching. Having\ncovered packet-switched networks in the previous subsection, we now turn\nour attention to circuit-switched networks.\nIn circuit-switched networks, the resources needed along a path\n(buffers, link transmission rate) to provide for communication between the\nend systems are reserved for the duration of the communication session\nbetween the end systems. In packet-switched networks, these resources are\nnot reserved; a session’s messages use the resources on demand and, as a\nconsequence, may have to wait (that is, queue) for access to a\ncommunication link. As a simple analogy, consider two restaurants, one that\nrequires reservations and another that neither requires reservations nor\naccepts them. For the restaurant that requires reservations, we have to go\nthrough the hassle of calling before we leave home. But when we arrive at\nthe restaurant we can, in principle, immediately be seated and order our\nmeal. For the restaurant that does not require reservations, we don’t need to\nbother to reserve a table. But when we arrive at the restaurant, we may have\nto wait for a table before we can be seated.\nTraditional telephone networks are examples of circuit-switched\nnetworks. ­Consider what happens when one person wants to send\ninformation (voice or facsimile) to another over a telephone network.\nBefore the sender can send the information, the network must establish a\nconnection between the sender and the receiver. This is a bona fide\nconnection for which the switches on the path between the sender and\nreceiver maintain connection state for that connection. In the jargon of\ntelephony, this connection is called a circuit. When the network establishes\nthe circuit, it also reserves a constant transmission rate in the network’s\nlinks (representing a fraction of each link’s transmission capacity) for the\nduration of the connection. Since a given transmission rate has been\nreserved for this sender-to-receiver connection, the sender can transfer the\ndata to the receiver at the guaranteed constant rate.\nFigure 1.13 illustrates a circuit-switched network. In this network, the\nfour circuit switches are interconnected by four links. Each of these links\nhas four circuits, so that each link can support four simultaneous\nconnections. The hosts (for example, PCs and workstations) are each\ndirectly connected to one of the switches. When two hosts want to\ncommunicate, the network establishes a dedicated end-to-end connection\nbetween the two hosts. Thus, in order for Host A to communicate with Host\nB, the network must first reserve one circuit on each of two links. In this\nexample, the dedicated end-to-end connection uses the second circuit in the\nfirst link and the fourth circuit in the second link. Because each link has\nfour circuits, for each link used by the end-to-end connection, the\nconnection gets one fourth of the link’s total transmission capacity for the\nduration of the connection. Thus, for example, if each link between adjacent\nswitches has a transmission rate of 1 Mbps, then each end-to-end circuit-\nswitch connection gets 250 kbps of dedicated transmission rate.\nFigure 1.13 ♦A simple circuit-switched network consisting of four\nswitches and four links\nIn contrast, consider what happens when one host wants to send a\npacket to another host over a packet-switched network, such as the Internet.\nAs with circuit switching, the packet is transmitted over a series of\ncommunication links. But different from circuit switching, the packet is sent\ninto the network without reserving any link resources whatsoever. If one of\nthe links is congested because other packets need to be transmitted over the\nlink at the same time, then the packet will have to wait in a buffer at the\nsending side of the transmission link and suffer a delay. The Internet makes\nits best effort to deliver packets in a timely manner, but it does not make\nany guarantees.\nMultiplexing in Circuit-Switched Networks\nA circuit in a link is implemented with either frequency-division\nmultiplexing (FDM) or time-division multiplexing (TDM). With FDM,\nthe frequency spectrum of a link is divided up among the connections\nestablished across the link. Specifically, the link dedicates a frequency band\nto each connection for the ­duration of the connection. In telephone\nnetworks, this frequency band typically has a width of 4 kHz (that is, 4,000\nhertz or 4,000 cycles per second). The width of the band is called, not\nsurprisingly, the bandwidth. FM radio stations also use FDM to share the\nfrequency spectrum between 88 MHz and 108 MHz, with each station\nbeing allocated a specific frequency band.\nFor a TDM link, time is divided into frames of fixed duration, and each\nframe is divided into a fixed number of time slots. When the network\nestablishes a connection across a link, the network dedicates one time slot\nin every frame to this connection. These slots are dedicated for the sole use\nof that connection, with one time slot available for use (in every frame) to\ntransmit the connection’s data.\nFigure 1.14 illustrates FDM and TDM for a specific network link\nsupporting up to four circuits. For FDM, the frequency domain is\nsegmented into four bands, each of bandwidth 4 kHz. For TDM, the time\ndomain is segmented into frames, with four time slots in each frame; each\ncircuit is assigned the same dedicated slot in the revolving TDM frames.\nFor TDM, the transmission rate of a circuit is equal to the frame rate\nmultiplied by the number of bits in a slot. For example, if the link transmits\n8,000 frames per second and each slot consists of 8 bits, then the\ntransmission rate of each circuit is 64 kbps.\nFigure 1.14 ♦With FDM, each circuit continuously gets a fraction of\nthe bandwidth. With TDM, each circuit gets all of the\nbandwidth periodically during brief intervals of time\n(that is, during slots)\nProponents of packet switching have always argued that circuit\nswitching is wasteful because the dedicated circuits are idle during silent\nperiods. For example, when one person in a telephone call stops talking,\nthe idle network resources (frequency bands or time slots in the links along\nthe connection’s route) cannot be used by other ongoing connections. As\nanother example of how these resources can be underutilized, consider a\nradiologist who uses a circuit-switched network to remotely access a series\nof x-rays. The radiologist sets up a connection, requests an image,\ncontemplates the image, and then requests a new image. Network resources\nare allocated to the connection but are not used (i.e., are wasted) during the\nradiologist’s contemplation periods. Proponents of packet switching also\nenjoy pointing out that establishing end-to-end circuits and reserving end-\nto-end transmission capacity is complicated and requires complex signaling\nsoftware to coordinate the operation of the switches along the end-to-end\nBefore we finish our discussion of circuit switching, let’s work through\na numerical example that should shed further insight on the topic. Let us\nconsider how long it takes to send a file of 640,000 bits from Host A to Host\nB over a circuit-switched network. Suppose that all links in the network use\nTDM with 24 slots and have a bit rate of 1.536 Mbps. Also suppose that it\ntakes 500 msec to establish an end-to-end circuit before Host A can begin to\ntransmit the file. How long does it take to send the file? Each circuit has a\ntransmission rate of (1.536 Mbps)/24 = 64 kbps, so it takes (640,000\nbits)/(64 kbps) = 10 seconds to transmit the file. To this 10 seconds we add\nthe circuit establishment time, giving 10.5 seconds to send the file. Note\nthat the transmission time is independent of the number of links: The\ntransmission time would be 10 seconds if the end-to-end circuit passed\nthrough one link or a hundred links. (The actual end-to-end delay also\nincludes a propagation delay; see Section 1.4.)\nPacket Switching Versus Circuit Switching\nHaving described circuit switching and packet switching, let us compare the\ntwo. Critics of packet switching have often argued that packet switching is\nnot suitable for real-time services (for example, telephone calls and video\nconference calls) because of its variable and unpredictable end-to-end\ndelays (due primarily to variable and unpredictable queuing delays).\nProponents of packet switching argue that (1) it offers better sharing of\ntransmission capacity than circuit switching and (2) it is simpler, more\nefficient, and less costly to implement than circuit switching. An interesting\ndiscussion of packet switching versus circuit switching is [Molinero-\nFernandez 2002]. Generally speaking, people who do not like to hassle with\n­restaurant reservations prefer packet switching to circuit switching.\nWhy is packet switching more efficient? Let’s look at a simple\nexample. Suppose users share a 1 Mbps link. Also suppose that each user\nalternates between periods of activity, when a user generates data at a\nconstant rate of 100 kbps, and periods of inactivity, when a user generates\nno data. Suppose further that a user is active only 10 percent of the time\n(and is idly drinking coffee during the remaining 90 percent of the time).\nWith circuit switching, 100 kbps must be reserved for each user at all times.\nFor example, with circuit-switched TDM, if a one-second frame is divided\ninto 10 time slots of 100 ms each, then each user would be allocated one\ntime slot per frame.\nThus, the circuit-switched link can support only 10 ( = 1 Mbps/100\nkbps) simultaneous users. With packet switching, the probability that a\nspecific user is active is 0.1 (that is, 10 percent). If there are 35 users, the\nprobability that there are 11 or more simultaneously active users is\napproximately 0.0004. (Homework Problem P8 outlines how this\nprobability is obtained.) When there are 10 or fewer simultaneously active\nusers (which happens with probability 0.9996), the aggregate arrival rate of\ndata is less than or equal to 1 Mbps, the output rate of the link. Thus, when\nthere are 10 or fewer active users, users’ packets flow through the link\nessentially without delay, as is the case with circuit switching. When there\nare more than 10 simultaneously active users, then the aggregate arrival rate\nof packets exceeds the output capacity of the link, and the output queue will\nbegin to grow. (It continues to grow until the aggregate input rate falls back\nbelow 1 Mbps, at which point the queue will begin to diminish in length.)\nBecause the probability of having more than 10 simultaneously active users\nis minuscule in this example, packet switching provides essentially the\nsame performance as circuit switching, but does so while allowing for more\nthan three times the number of users.\nLet’s now consider a second simple example. Suppose there are 10\nusers and that one user suddenly generates one thousand 1,000-bit packets,\nwhile other users remain quiescent and do not generate packets. Under\nTDM circuit switching with 10 slots per frame and each slot consisting of\n1,000 bits, the active user can only use its one time slot per frame to\ntransmit data, while the remaining nine time slots in each frame remain idle.\nIt will be 10 seconds before all of the active user’s one million bits of data\nhas been transmitted. In the case of packet switching, the active user can\ncontinuously send its packets at the full link rate of 1 Mbps, since there are\nno other users generating packets that need to be multiplexed with the\nactive user’s packets. In this case, all of the active user’s data will be\ntransmitted within 1 second.\nThe above examples illustrate two ways in which the performance of\npacket switching can be superior to that of circuit switching. They also\nhighlight the crucial difference between the two forms of sharing a link’s\ntransmission rate among multiple data streams. Circuit switching pre-\nallocates use of the transmission link regardless of demand, with allocated\nbut unneeded link time going unused. Packet switching on the other hand\nallocates link use on demand. Link transmission capacity will be shared on\na packet-by-packet basis only among those users who have packets that\nneed to be transmitted over the link.\nAlthough packet switching and circuit switching are both prevalent in\ntoday’s telecommunication networks, the trend has certainly been in the\ndirection of packet switching. Even many of today’s circuit-switched\ntelephone networks are slowly migrating toward packet switching. In\nparticular, telephone networks often use packet switching for the expensive\noverseas portion of a telephone call.\n1.3.3 A Network of Networks\nWe saw earlier that end systems (PCs, smartphones, Web servers, mail\nservers, and so on) connect into the Internet via an access ISP. The access\nISP can provide either wired or wireless connectivity, using an array of\naccess technologies including DSL, cable, FTTH, Wi-Fi, and cellular. Note\nthat the access ISP does not have to be a telco or a cable company; instead it\ncan be, for example, a university (providing Internet access to students,\nstaff, and faculty), or a company (providing access for its employees). But\nconnecting end users and content providers into an access ISP is only a\nsmall piece of solving the puzzle of connecting the billions of end systems\nthat make up the Internet. To complete this puzzle, the access ISPs\nthemselves must be interconnected. This is done by creating a network of\nnetworks—understanding this phrase is the key to understanding the\nOver the years, the network of networks that forms the Internet has\nevolved into a very complex structure. Much of this evolution is driven by\neconomics and national policy, rather than by performance considerations.\nIn order to understand today’s Internet network structure, let’s\nincrementally build a series of network structures, with each new structure\nbeing a better approximation of the complex Internet that we have today.\nRecall that the overarching goal is to interconnect the access ISPs so that all\nend systems can send packets to each other. One naive approach would be\nto have each access ISP directly connect with every other access ISP. Such a\nmesh design is, of course, much too costly for the access ISPs, as it would\nrequire each access ISP to have a separate communication link to each of\nthe hundreds of thousands of other access ISPs all over the world.\nOur first network structure, Network Structure 1, interconnects all of\nthe access ISPs with a single global transit ISP. Our (imaginary) global\ntransit ISP is a network of routers and communication links that not only\nspans the globe, but also has at least one router near each of the hundreds of\nthousands of access ISPs. Of course, it would be very costly for the global\nISP to build such an extensive network. To be profitable, it would naturally\ncharge each of the access ISPs for connectivity, with the pricing reflecting\n(but not necessarily directly proportional to) the amount of traffic an access\nISP exchanges with the global ISP. Since the access ISP pays the global\ntransit ISP, the access ISP is said to be a customer and the global transit ISP\nis said to be a provider.\nNow if some company builds and operates a global transit ISP that is\nprofitable, then it is natural for other companies to build their own global\ntransit ISPs and compete with the original global transit ISP. This leads to\nNetwork Structure 2, which consists of the hundreds of thousands of access\nISPs and multiple global ­transit ISPs. The access ISPs certainly prefer\nNetwork Structure 2 over Network Structure 1 since they can now choose\namong the competing global transit providers as a function of their pricing\nand services. Note, however, that the global transit ISPs themselves must\ninterconnect: Otherwise access ISPs connected to one of the global transit\nproviders would not be able to communicate with access ISPs connected to\nthe other global transit providers.\nNetwork Structure 2, just described, is a two-tier hierarchy with global\ntransit providers residing at the top tier and access ISPs at the bottom tier.\nThis assumes that global transit ISPs are not only capable of getting close to\neach and every access ISP, but also find it economically desirable to do so.\nIn reality, although some ISPs do have impressive global coverage and do\ndirectly connect with many access ISPs, no ISP has presence in each and\nevery city in the world. Instead, in any given region, there may be a\nregional ISP to which the access ISPs in the region connect. Each regional\nISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to our (imaginary)\nglobal transit ISP; but tier-1 ISPs, which actually do exist, do not have a\npresence in every city in the world. There are approximately a dozen tier-1\nISPs, including Level 3 Communications, AT&T, Sprint, and NTT.\nInterestingly, no group officially sanctions tier-1 status; as the saying goes\n—if you have to ask if you’re a member of a group, you’re probably not.\nReturning to this network of networks, not only are there multiple\ncompeting tier-1 ISPs, there may be multiple competing regional ISPs in a\nregion. In such a hierarchy, each access ISP pays the regional ISP to which\nit connects, and each regional ISP pays the tier-1 ISP to which it connects.\n(An access ISP can also connect directly to a tier-1 ISP, in which case it\npays the tier-1 ISP). Thus, there is customer-provider relationship at each\nlevel of the hierarchy. Note that the tier-1 ISPs do not pay anyone as they\nare at the top of the hierarchy. To further complicate matters, in some\nregions, there may be a larger regional ISP (possibly spanning an entire\ncountry) to which the smaller regional ISPs in that region connect; the\nlarger regional ISP then connects to a tier-1 ISP. For example, in China,\nthere are access ISPs in each city, which connect to provincial ISPs, which\nin turn connect to national ISPs, which finally connect to tier-1 ISPs [Tian\n2012]. We refer to this multi-tier hierarchy, which is still only a crude\napproximation of today’s Internet, as Network Structure 3.\nTo build a network that more closely resembles today’s Internet, we\nmust add points of presence (PoPs), multi-homing, peering, and Internet\nexchange points (IXPs) to the hierarchical Network Structure 3. PoPs exist\nin all levels of the hierarchy, except for the bottom (access ISP) level. A\nPoP is simply a group of one or more routers (at the same location) in the\nprovider’s network where customer ISPs can connect into the provider ISP.\nFor a customer network to connect to a provider’s PoP, it can lease a high-\nspeed link from a third-party telecommunications provider to directly\nconnect one of its routers to a router at the PoP. Any ISP (except for tier-1\nISPs) may choose to multi-home, that is, to connect to two or more\nprovider ISPs. So, for example, an access ISP may multi-home with two\nregional ISPs, or it may multi-home with two regional ISPs and also with a\ntier-1 ISP. Similarly, a regional ISP may multi-home with multiple tier-1\nISPs. When an ISP multi-homes, it can continue to send and receive packets\ninto the Internet even if one of its providers has a failure.\nAs we just learned, customer ISPs pay their provider ISPs to obtain\nglobal Internet interconnectivity. The amount that a customer ISP pays a\nprovider ISP reflects the amount of traffic it exchanges with the provider.\nTo reduce these costs, a pair of nearby ISPs at the same level of the\nhierarchy can peer, that is, they can directly connect their networks together\nso that all the traffic between them passes over the direct connection rather\nthan through upstream intermediaries. When two ISPs peer, it is typically\nsettlement-free, that is, neither ISP pays the other. As noted earlier, tier-1\nISPs also peer with one another, settlement-free. For a readable discussion\nof peering and customer-provider relationships, see [Van der Berg 2008].\nAlong these same lines, a third-party company can create an Internet\nExchange Point (IXP), which is a meeting point where multiple ISPs can\npeer together. An IXP is typically in a stand-alone building with its own\nswitches [Ager 2012]. There are over 600 IXPs in the Internet today\n[PeeringDB 2020]. We refer to this ecosystem—consisting of access ISPs,\nregional ISPs, tier-1 ISPs, PoPs, multi-homing, peering, and IXPs—as\nNetwork Structure 4.\nWe now finally arrive at Network Structure 5, which describes today’s\nInternet. Network Structure 5, illustrated in Figure 1.15, builds on top of\nNetwork Structure 4 by adding content-provider networks. Google is\ncurrently one of the leading examples of such a content-provider network.\nAs of this writing, it Google has 19 major data centers distributed across\nNorth America, Europe, Asia, South America, and Australia with each data\ncenter having tens or hundreds of thousands of servers. Additionally,\nGoogle has smaller data centers, each with a few hundred servers; these\nsmaller data centers are often located within IXPs. The Google data centers\nare all interconnected via Google’s private TCP/IP network, which spans\nthe entire globe but is nevertheless separate from the public Internet.\nImportantly, the Google private network only carries traffic to/from Google\nservers. As shown in Figure 1.15, the Google private network attempts to\n“bypass” the upper tiers of the Internet by peering (settlement free) with\nlower-tier ISPs, either by directly connecting with them or by connecting\nwith them at IXPs [Labovitz 2010]. However, because many access ISPs\ncan still only be reached by transiting through tier-1 networks, the Google\nnetwork also connects to tier-1 ISPs, and pays those ISPs for the traffic it\nexchanges with them. By creating its own network, a content provider not\nonly reduces its payments to upper-tier ISPs, but also has greater control of\nhow its services are ultimately delivered to end users. Google’s network\ninfrastructure is described in greater detail in Section 2.6.\nFigure 1.15 ♦Interconnection of ISPs\nIn summary, today’s Internet—a network of networks—is complex,\nconsisting of a dozen or so tier-1 ISPs and hundreds of thousands of lower-\ntier ISPs. The ISPs are diverse in their coverage, with some spanning\nmultiple continents and oceans, and others limited to narrow geographic\nregions. The lower-tier ISPs connect to the higher-tier ISPs, and the higher-\ntier ISPs interconnect with one another. Users and content providers are\ncustomers of lower-tier ISPs, and lower-tier ISPs are customers of higher-\ntier ISPs. In recent years, major content providers have also created their\nown networks and connect directly into lower-tier ISPs where possible.\n1.4 Delay, Loss, and Throughput in Packet-Switched Networks\nBack in Section 1.1 we said that the Internet can be viewed as an infrastructure that provides\nservices to distributed applications running on end systems. Ideally, we would like Internet\nservices to be able to move as much data as we want between any two end systems,\ninstantaneously, without any loss of data. Alas, this is a lofty goal, one that is unachievable\nin reality. Instead, computer networks necessarily constrain throughput (the amount of data\nper second that can be transferred) between end systems, introduce delays between end\nsystems, and can actually lose packets. On one hand, it is unfortunate that the physical laws\nof reality introduce delay and loss as well as constrain throughput. On the other hand,\nbecause computer networks have these problems, there are many fascinating issues\nsurrounding how to deal with the problems—more than enough issues to fill a course on\ncomputer networking and to motivate thousands of PhD theses! In this section, we’ll begin\nto examine and quantify delay, loss, and throughput in computer networks.\n1.4.1 Overview of Delay in Packet-Switched Networks\nRecall that a packet starts in a host (the source), passes through a series of routers, and ends\nits journey in another host (the destination). As a packet travels from one node (host or\nrouter) to the subsequent node (host or router) along this path, the packet suffers from\nseveral types of delays at each node along the path. The most important of these delays are\nthe nodal processing delay, queuing delay, transmission delay, and propagation delay;\ntogether, these delays accumulate to give a total nodal delay. The performance of many\nInternet applications—such as search, Web browsing, e-mail, maps, instant messaging, and\nvoice-over-IP—are greatly affected by network delays. In order to acquire a deep\nunderstanding of packet switching and computer networks, we must understand the nature\nand importance of these delays.\nTypes of Delay\nLet’s explore these delays in the context of Figure 1.16. As part of its end-to-end route\nbetween source and destination, a packet is sent from the upstream node through router A to\nrouter B. Our goal is to characterize the nodal delay at router A. Note that router A has an\noutbound link leading to router B. This link is preceded by a queue (also known as a buffer).\nWhen the packet arrives at router A from the upstream node, router A examines the packet’s\nheader to determine the appropriate outbound link for the packet and then directs the packet\nto this link. In this example, the outbound link for the packet is the one that leads to router B.\nA packet can be transmitted on a link only if there is no other packet currently being\ntransmitted on the link and if there are no other packets preceding it in the queue; if the link\nis ­currently busy or if there are other packets already queued for the link, the newly arriving\npacket will then join the queue.\nFigure 1.16 ♦The nodal delay at router A\nProcessing Delay\nThe time required to examine the packet’s header and determine where to direct the packet is\npart of the processing delay. The processing delay can also include other factors, such as the\ntime needed to check for bit-level errors in the packet that occurred in transmitting the\npacket’s bits from the upstream node to router A. Processing delays in high-speed routers are\ntypically on the order of microseconds or less. After this nodal processing, the router directs\nthe packet to the queue that precedes the link to router B. (In Chapter 4 we’ll study the\ndetails of how a router operates.)\nQueuing Delay\nAt the queue, the packet experiences a queuing delay as it waits to be transmitted onto the\nlink. The length of the queuing delay of a specific packet will depend on the number of\nearlier-arriving packets that are queued and waiting for transmission onto the link. If the\nqueue is empty and no other packet is currently being transmitted, then our packet’s queuing\ndelay will be zero. On the other hand, if the traffic is heavy and many other packets are also\nwaiting to be transmitted, the queuing delay will be long. We will see shortly that the\nnumber of packets that an arriving packet might expect to find is a function of the intensity\nand nature of the traffic arriving at the queue. ­Queuing delays can be on the order of\nmicroseconds to milliseconds in practice.\nTransmission Delay\nAssuming that packets are transmitted in a first-come-first-served manner, as is common in\npacket-switched networks, our packet can be transmitted only after all the packets that have\narrived before it have been transmitted. Denote the length of the packet by L bits, and denote\nthe transmission rate of the link from router A to router B by R bits/sec. For example, for a\n10 Mbps Ethernet link, the rate is R = 10 Mbps; for a 100 Mbps Ethernet link, the rate is R =\n100 Mbps. The transmission delay is L/R. This is the amount of time required to push (that\nis, transmit) all of the packet’s bits into the link. Transmission delays are typically on the\norder of microseconds to milliseconds in practice.\nPropagation Delay\nOnce a bit is pushed into the link, it needs to propagate to router B. The time required to\npropagate from the beginning of the link to router B is the propagation delay. The bit\npropagates at the propagation speed of the link. The propagation speed depends on the\nphysical medium of the link (that is, fiber optics, twisted-pair copper wire, and so on) and is\nin the range of\n2  ⋅ 108 meters/sec to 3  ⋅ 108 meters/sec\nwhich is equal to, or a little less than, the speed of light. The propagation delay is the\ndistance between two routers divided by the propagation speed. That is, the propagation\ndelay is d/s, where d is the distance between router A and router B and s is the propagation\nspeed of the link. Once the last bit of the packet propagates to node B, it and all the\npreceding bits of the packet are stored in router B. The whole process then continues with\nrouter B now performing the forwarding. In wide-area networks, propagation delays are on\nthe order of milliseconds.\nComparing Transmission and Propagation Delay\nExploring propagation delay and transmission delay\nNewcomers to the field of computer networking sometimes have difficulty understanding\nthe difference between transmission delay and propagation delay. The difference is subtle\nbut important. The transmission delay is the amount of time required for the router to push\nout the packet; it is a function of the packet’s length and the transmission rate of the link, but\nhas nothing to do with the distance between the two routers. The propagation delay, on the\nother hand, is the time it takes a bit to propagate from one router to the next; it is a function\nof the distance between the two routers, but has nothing to do with the packet’s length or the\ntransmission rate of the link.\nAn analogy might clarify the notions of transmission and propagation delay. Consider a\nhighway that has a tollbooth every 100 kilometers, as shown in Figure 1.17. You can think of\nthe highway segments between tollbooths as links and the tollbooths as routers. Suppose that\ncars travel (that is, propagate) on the highway at a rate of 100 km/hour (that is, when a car\nleaves a tollbooth, it instantaneously accelerates to 100 km/hour and maintains that speed\nbetween tollbooths). Suppose next that 10 cars, traveling together as a caravan, follow each\nother in a fixed order. You can think of each car as a bit and the caravan as a packet. Also\nsuppose that each tollbooth services (that is, transmits) a car at a rate of one car per 12\nseconds, and that it is late at night so that the caravan’s cars are the only cars on the highway.\nFinally, suppose that whenever the first car of the caravan arrives at a tollbooth, it waits at\nthe entrance until the other nine cars have arrived and lined up behind it. (Thus, the entire\ncaravan must be stored at the tollbooth before it can begin to be forwarded.) The time\nrequired for the tollbooth to push the entire caravan onto the highway is (10 cars)/(5\ncars/minute) = 2 minutes. This time is analogous to the transmission delay in a router. The\ntime required for a car to travel from the exit of one tollbooth to the next tollbooth is 100\nkm/(100 km/hour) = 1 hour. This time is analogous to propagation delay. Therefore, the time\nfrom when the caravan is stored in front of a tollbooth until the caravan is stored in front of\nthe next tollbooth is the sum of transmission delay and propagation delay—in this example,\n62 minutes.\nFigure 1.17 ♦Caravan analogy\nLet’s explore this analogy a bit more. What would happen if the tollbooth service time\nfor a caravan were greater than the time for a car to travel between tollbooths? For example,\nsuppose now that the cars travel at the rate of 1,000 km/hour and the tollbooth services cars\nat the rate of one car per minute. Then the traveling delay between two tollbooths is 6\nminutes and the time to serve a caravan is 10 minutes. In this case, the first few cars in the\ncaravan will arrive at the second tollbooth before the last cars in the caravan leave the first\ntollbooth. This situation also arises in packet-switched networks—the first bits in a packet\ncan arrive at a router while many of the remaining bits in the packet are still waiting to be\ntransmitted by the preceding router.\nIf a picture speaks a thousand words, then an animation must speak a million words. The\nWeb site for this textbook provides an interactive animation that nicely illustrates and\ncontrasts transmission delay and propagation delay. The reader is highly encouraged to visit\nthat animation. [Smith 2009] also provides a very readable discussion of propagation,\nqueuing, and transmission delays.\nIf we let d\n denote the processing, queuing, transmission, and\npropagation delays, then the total nodal delay is given by\ndnodal = dproc + dqueue + dtrans + dprop\nThe contribution of these delay components can vary significantly. For example, d\nnegligible (for example, a couple of microseconds) for a link connecting two routers on the\nsame university campus; however, d\n is hundreds of milliseconds for two routers\ninterconnected by a geostationary satellite link, and can be the dominant term in d\nSimilarly, d\n can range from negligible to significant. Its contribution is typically\nnegligible for transmission rates of 10 Mbps and higher (for example, for LANs); however,\nit can be hundreds of milliseconds for large Internet packets sent over low-speed dial-up\nmodem links. The processing delay, d\n, is often negligible; however, it strongly influences\na router’s maximum throughput, which is the maximum rate at which a router can forward\n1.4.2 Queuing Delay and Packet Loss\nThe most complicated and interesting component of nodal delay is the queuing delay, d\nIn fact, queuing delay is so important and interesting in computer networking that thousands\nof papers and numerous books have been written about it [Bertsekas 1991; Kleinrock 1975,\nKleinrock 1976]. We give only a high-level, intuitive discussion of queuing delay here; the\nmore curious reader may want to browse through some of the books (or even eventually\nwrite a PhD thesis on the subject!). Unlike the other three delays (namely, d\n), the queuing delay can vary from packet to packet. For example, if 10 packets arrive at\nan empty queue at the same time, the first packet transmitted will suffer no queuing delay,\nwhile the last packet transmitted will suffer a relatively large queuing delay (while it waits\nfor the other nine packets to be transmitted). Therefore, when characterizing queuing delay,\none typically uses statistical measures, such as average queuing delay, variance of queuing\ndelay, and the probability that the queuing delay exceeds some specified value.\nWhen is the queuing delay large and when is it insignificant? The answer to this\nquestion depends on the rate at which traffic arrives at the queue, the transmission rate of the\nlink, and the nature of the arriving traffic, that is, whether the traffic arrives periodically or\narrives in bursts. To gain some insight here, let a denote the average rate at which packets\narrive at the queue (a is in units of packets/sec). Recall that R is the transmission rate; that is,\nit is the rate (in bits/sec) at which bits are pushed out of the queue. Also suppose, for\nsimplicity, that all packets consist of L bits. Then the average rate at which bits arrive at the\nqueue is La bits/sec. Finally, assume that the queue is very big, so that it can hold essentially\nan infinite number of bits. The ratio La/R, called the traffic intensity, often plays an\nimportant role in estimating the extent of the queuing delay. If La/R > 1, then the average\nrate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted\nfrom the queue. In this unfortunate situation, the queue will tend to increase without bound\nand the queuing delay will approach infinity! Therefore, one of the golden rules in traffic\nengineering is: Design your system so that the traffic intensity is no greater than 1.\nNow consider the case La/R ≤ 1. Here, the nature of the arriving traffic impacts the\nqueuing delay. For example, if packets arrive periodically—that is, one packet arrives every\nL/R seconds—then every packet will arrive at an empty queue and there will be no queuing\ndelay. On the other hand, if packets arrive in bursts but periodically, there can be a\nsignificant average queuing delay. For example, suppose N packets arrive simultaneously\nevery (L/R)N seconds. Then the first packet transmitted has no queuing delay; the second\npacket transmitted has a queuing delay of L/R seconds; and more generally, the nth packet\ntransmitted has a queuing delay of (n − 1)L/R We leave it as an exercise for you to calculate\nthe average queuing delay in this example.\nThe two examples of periodic arrivals described above are a bit academic. ­Typically, the\narrival process to a queue is random; that is, the arrivals do not follow any pattern and the\npackets are spaced apart by random amounts of time. In this more realistic case, the quantity\nLa/R is not usually sufficient to fully characterize the queuing delay statistics. Nonetheless,\nit is useful in gaining an intuitive understanding of the extent of the queuing delay. In\nparticular, if the traffic intensity is close to zero, then packet arrivals are few and far between\nand it is unlikely that an arriving packet will find another packet in the queue. Hence, the\naverage queuing delay will be close to zero. On the other hand, when the traffic intensity is\nclose to 1, there will be intervals of time when the arrival rate exceeds the transmission\ncapacity (due to variations in packet arrival rate), and a queue will form during these periods\nof time; when the arrival rate is less than the transmission capacity, the length of the queue\nwill shrink. Nonetheless, as the traffic intensity approaches 1, the average queue length gets\nlarger and larger. The qualitative dependence of average queuing delay on the traffic\nintensity is shown in Figure 1.18.\nFigure 1.18 ♦Dependence of average queuing delay on traffic intensity\nOne important aspect of Figure 1.18 is the fact that as the traffic intensity approaches 1,\nthe average queuing delay increases rapidly. A small percentage increase in the intensity will\nresult in a much larger percentage-wise increase in delay. Perhaps you have experienced this\nphenomenon on the highway. If you regularly drive on a road that is typically congested, the\nfact that the road is typically congested means that its traffic intensity is close to 1. If some\nevent causes an even slightly larger-than-usual amount of traffic, the delays you experience\ncan be huge.\nTo really get a good feel for what queuing delays are about, you are encouraged once\nagain to visit the textbook Web site, which provides an interactive animation for a queue. If\nyou set the packet arrival rate high enough so that the traffic intensity exceeds 1, you will\nsee the queue slowly build up over time.\nPacket Loss\nIn our discussions above, we have assumed that the queue is capable of holding an infinite\nnumber of packets. In reality a queue preceding a link has finite capacity, although the\nqueuing capacity greatly depends on the router design and cost. Because the queue capacity\nis finite, packet delays do not really approach infinity as the traffic intensity approaches 1.\nInstead, a packet can arrive to find a full queue. With no place to store such a packet, a\nrouter will drop that packet; that is, the packet will be lost. This overflow at a queue can\nagain be seen in the interactive animation when the traffic intensity is greater than 1.\nFrom an end-system viewpoint, a packet loss will look like a packet having been\ntransmitted into the network core but never emerging from the network at the destination.\nThe fraction of lost packets increases as the traffic intensity increases. Therefore,\nperformance at a node is often measured not only in terms of delay, but also in terms of the\nprobability of packet loss. As we’ll discuss in the subsequent chapters, a lost packet may be\nretransmitted on an end-to-end basis in order to ensure that all data are eventually transferred\nfrom source to destination.\n1.4.3 End-to-End Delay\nOur discussion up to this point has focused on the nodal delay, that is, the delay at a single\nrouter. Let’s now consider the total delay from source to destination. To get a handle on this\nconcept, suppose there are N − 1 routers between the source host and the destination host.\nLet’s also suppose for the moment that the network is uncongested (so that queuing delays\nare negligible), the processing delay at each router and at the source host is d\ntransmission rate out of each router and out of the source host is R bits/sec, and the\npropagation on each link is d\n. The nodal delays accumulate and give an end-to-end delay,\nwhere, once again, d\n = L/R, where L is the packet size. Note that Equation 1.2 is a\ngeneralization of Equation 1.1, which did not take into account processing and propagation\ndelays. We leave it to you to generalize Equation 1.2 to the case of ­heterogeneous delays at\nthe nodes and to the presence of an average queuing delay at each node.\nUsing Traceroute to discover network paths and measure network delay\nTo get a hands-on feel for end-to-end delay in a computer network, we can make use of the\nTraceroute program. Traceroute is a simple program that can run in any Internet host. When\nthe user specifies a destination hostname, the program in the source host sends multiple,\nspecial packets toward that destination. As these packets work their way toward the\ndestination, they pass through a series of routers. When a router receives one of these special\npackets, it sends back to the source a short message that contains the name and address of\nthe router.\nMore specifically, suppose there are N − 1 routers between the source and the\ndestination. Then the source will send N special packets into the network, with each packet\naddressed to the ultimate destination. These N special packets are marked 1 through N, with\nthe first packet marked 1 and the last packet marked N. When the nth router receives the nth\npacket marked n, the router does not forward the packet toward its destination, but instead\nsends a message back to the source. When the destination host receives the Nth packet, it too\nreturns a message back to the source. The source records the time that elapses between when\nit sends a packet and when it receives the corresponding return message; it also records the\nname and address of the router (or the destination host) that returns the message. In this\nmanner, the source can reconstruct the route taken by packets flowing from source to\ndestination, and the source can determine the round-trip delays to all the intervening routers.\ndend-end = N(dproc + dtrans + dprop)\nTraceroute actually repeats the experiment just described three times, so the source actually\nsends 3 • N packets to the destination. RFC 1393 describes Traceroute in detail.\nHere is an example of the output of the Traceroute program, where the route was being\ntraced from the source host gaia.cs.umass.edu (at the University of ­Massachusetts) to a host\nin the computer science department at the University of Sorbonne in Paris (formerly the\nuniversity was known as UPMC). The output has six columns: the first column is the n value\ndescribed above, that is, the number of the router along the route; the second column is the\nname of the router; the third column is the address of the router (of the form\nxxx.xxx.xxx.xxx); the last three columns are the round-trip delays for three experiments. If\nthe source receives fewer than three messages from any given router (due to packet loss in\nthe network), Traceroute places an asterisk just after the router number and reports fewer\nthan three round-trip times for that router.\ngw-vlan-2451.cs.umass.edu (128.119.245.1) 1.899 ms 3.266 ms\nj-cs-gw-int-10-240.cs.umass.edu (10.119.240.254) 1.296 ms\n1.276 ms 1.245 ms\nn5-rt-1-1-xe-2-1-0.gw.umass.edu (128.119.3.33) 2.237 ms\n2.217 ms 2.187 ms\ncore1-rt-et-5-2-0.gw.umass.edu (128.119.0.9) 0.351 ms 0.392\nms 0.380 ms\nborder1-rt-et-5-0-0.gw.umass.edu (192.80.83.102) 0.345 ms\n0.345 ms 0.344 ms\nnox300gw1-umass-re.nox.org (192.5.89.101) 3.260 ms 0.416 ms\nnox300gw1-umass-re.nox.org (192.5.89.101) 3.165 ms 7.326 ms\n198.71.45.237 (198.71.45.237) 77.826 ms 77.246 ms 77.744 ms\nrenater-lb1-gw.mx1.par.fr.geant.net (62.40.124.70) 79.357\nms 77.729 79.152 ms\n193.51.180.109 (193.51.180.109) 78.379 ms 79.936 80.042 ms\n* 193.51.180.109 (193.51.180.109) 80.640 ms *\n* 195.221.127.182 (195.221.127.182) 78.408 ms *\n195.221.127.182 (195.221.127.182) 80.686 ms 80.796 ms\nr-upmc1.reseau.jussieu.fr (134.157.254.10) 78.399 ms *\nIn the trace above, there are 14 routers between the source and the destination. Most of these\nrouters have a name, and all of them have addresses. For example, the name of Router 4 is\ncore1-rt-et-5-2-0.gw.umass.edu and its address is 128.119.0.9. Looking at\nthe data provided for this same router, we see that in the first of the three trials the round-trip\ndelay between the source and the router was 0.351 msec. The round-trip delays for the\nsubsequent two trials were 0.392 and 0.380 msec. These round-trip delays include all of the\ndelays just discussed, including transmission delays, propagation delays, router processing\ndelays, and queuing delay.\nBecause the queuing delay is varying with time, the round-trip delay of packet n sent to\na router n can sometimes be longer than the round-trip delay of packet n+1 sent to router\nn+1. Indeed, we observe this phenomenon in the above example: the delay to Router 12 is\nsmaller than the delay to Router 11! Also note the big increase in the round-trip delay when\ngoing from router 7 to router 8. This is due to a transatlantic fiber-optic link between routers\n7 and 8, giving rise to a relatively large propagation delay. There are a number of free\nsoftware programs that provide a graphical interface to Traceroute; one of our favorites is\nPingPlotter [PingPlotter 2020].\nEnd System, Application, and Other Delays\nIn addition to processing, transmission, and propagation delays, there can be additional\nsignificant delays in the end systems. For example, an end system wanting to transmit a\npacket into a shared medium (e.g., as in a WiFi or cable modem scenario) may purposefully\ndelay its transmission as part of its protocol for sharing the medium with other end systems;\nwe’ll consider such protocols in detail in Chapter 6. Another important delay is media\npacketization delay, which is present in Voice-over-IP (VoIP) applications. In VoIP, the\nsending side must first fill a packet with encoded digitized speech before passing the packet\nto the Internet. This time to fill a packet—called the packetization delay—can be significant\nand can impact the user-perceived quality of a VoIP call. This issue will be further explored\nin a homework problem at the end of this chapter.\n1.4.4 Throughput in Computer Networks\nIn addition to delay and packet loss, another critical performance measure in computer\nnetworks is end-to-end throughput. To define throughput, consider transferring a large file\nfrom Host A to Host B across a computer network. This transfer might be, for example, a\nlarge video clip from one computer to another. The instantaneous throughput at any instant\nof time is the rate (in bits/sec) at which Host B is receiving the file. (Many applications\ndisplay the instantaneous throughput during downloads in the user interface—perhaps you\nhave observed this before! You might like to try measuring the end-to-end delay and\ndownload throughput between your and servers around the Internet using the speedtest\napplication [Speedtest 2020].) If the file consists of F bits and the transfer takes T seconds\nfor Host B to receive all F bits, then the average throughput of the file transfer is F/T\nbits/sec. For some applications, such as Internet telephony, it is desirable to have a low delay\nand an instantaneous throughput consistently above some threshold (for example, over 24\nkbps for some Internet telephony applications and over 256 kbps for some real-time video\napplications). For other applications, including those involving file transfers, delay is not\ncritical, but it is desirable to have the highest possible throughput.\nTo gain further insight into the important concept of throughput, let’s consider a few\nexamples. Figure 1.19(a) shows two end systems, a server and a client, connected by two\ncommunication links and a router. Consider the throughput for a file transfer from the server\nto the client. Let R  denote the rate of the link between the server and the router; and R\ndenote the rate of the link between the router and the client. Suppose that the only bits being\nsent in the entire network are those from the server to the client. We now ask, in this ideal\nscenario, what is the server-to-client throughput? To answer this question, we may think of\nbits as fluid and communication links as pipes. Clearly, the server cannot pump bits through\nits link at a rate faster than R  bps; and the router cannot forward bits at a rate faster than R\nbps. If R  < R , then the bits pumped by the server will “flow” right through the router and\narrive at the client at a rate of R  bps, giving a throughput of R  bps. If, on the other hand, R\n< R , then the router will not be able to forward bits as quickly as it receives them. In this\ncase, bits will only leave the router at rate R , giving an end-to-end throughput of R . (Note\nalso that if bits continue to arrive at the router at rate R , and continue to leave the router at\nR , the backlog of bits at the router waiting for transmission to the client will grow and grow\n—a most undesirable situation!) Thus, for this simple two-link network, the throughput is\nmin{R , R }, that is, it is the transmission rate of the bottleneck link. Having determined the\nthroughput, we can now approximate the time it takes to transfer a large file of F bits from\nserver to client as F/min{R , R }. For a specific example, suppose that you are downloading\nan MP3 file of F = 32 million bits, the server has a transmission rate of R  = 2 Mbps, and\nyou have an access link of R  = 1 Mbps. The time needed to transfer the file is then 32\nseconds. Of course, these expressions for throughput and transfer time are only\napproximations, as they do not account for store-and-forward and processing delays as well\nas protocol issues.\nFigure 1.19 ♦Throughput for a file transfer from server to client\nFigure 1.19(b) now shows a network with N links between the server and the client, with\nthe transmission rates of the N links being R , R , ..., R . Applying the same analysis as for\nthe two-link network, we find that the throughput for a file transfer from server to client is\nmin{R , R , ..., R }, which is once again the transmission rate of the bottleneck link along the\npath between server and client.\nNow consider another example motivated by today’s Internet. Figure 1.20(a) shows two\nend systems, a server and a client, connected to a computer network. Consider the\nthroughput for a file transfer from the server to the client. The server is connected to the\nnetwork with an access link of rate R  and the client is connected to the network with an\naccess link of rate R . Now suppose that all the links in the core of the communication\nnetwork have very high transmission rates, much higher than R  and R . Indeed, today, the\ncore of the Internet is over-provisioned with high speed links that experience little\ncongestion. Also suppose that the only bits being sent in the entire network are those from\nthe server to the client. Because the core of the computer network is like a wide pipe in this\nexample, the rate at which bits can flow from source to destination is again the minimum of\nR  and R , that is, throughput = min{R , R }. Therefore, the constraining factor for throughput\nin today’s Internet is typically the access network.\nFigure 1.20 ♦End-to-end throughput: (a) Client downloads a file from ­server; (b)\n10 clients ­downloading with 10 servers\nFor a final example, consider Figure 1.20(b) in which there are 10 servers and 10 clients\nconnected to the core of the computer network. In this example, there are 10 simultaneous\ndownloads taking place, involving 10 client-server pairs. Suppose that these 10 downloads\nare the only traffic in the network at the current time. As shown in the figure, there is a link\nin the core that is traversed by all 10 downloads. Denote R for the transmission rate of this\nlink R. Let’s suppose that all server access links have the same rate R , all client access links\nhave the same rate R , and the transmission rates of all the links in the core—except the one\ncommon link of rate R—are much larger than R , R , and R. Now we ask, what are the\nthroughputs of the downloads? Clearly, if the rate of the common link, R, is large—say a\nhundred times larger than both R  and R —then the throughput for each download will once\nagain be min{R , R }. But what if the rate of the common link is of the same order as R  and\nR ? What will the throughput be in this case? Let’s take a look at a specific example.\nSuppose R  = 2 Mbps, R  = 1 Mbps, R = 5 Mbps, and the common link divides its\ntransmission rate equally among the 10 downloads. Then the bottleneck for each download\nis no longer in the access network, but is now instead the shared link in the core, which only\nprovides each download with 500 kbps of throughput. Thus, the end-to-end throughput for\neach download is now reduced to 500 kbps.\nThe examples in Figure 1.19 and Figure 1.20(a) show that throughput depends on the\ntransmission rates of the links over which the data flows. We saw that when there is no other\nintervening traffic, the throughput can simply be approximated as the minimum transmission\nrate along the path between source and destination. The example in Figure 1.20(b) shows\nthat more generally the throughput depends not only on the transmission rates of the links\nalong the path, but also on the intervening traffic. In particular, a link with a high\ntransmission rate may nonetheless be the bottleneck link for a file transfer if many other data\nflows are also passing through that link. We will examine throughput in computer networks\nmore closely in the homework problems and in the subsequent chapters.\n1.5 Protocol Layers and Their Service Models\nFrom our discussion thus far, it is apparent that the Internet is an extremely\ncomplicated system. We have seen that there are many pieces to the\nInternet: numerous applications and protocols, various types of end\nsystems, packet switches, and various types of link-level media. Given this\nenormous complexity, is there any hope of organizing a network\narchitecture, or at least our discussion of network architecture? Fortunately,\nthe answer to both questions is yes.\n1.5.1 Layered Architecture\nBefore attempting to organize our thoughts on Internet architecture, let’s\nlook for a human analogy. Actually, we deal with complex systems all the\ntime in our everyday life. Imagine if someone asked you to describe, for\nexample, the airline system. How would you find the structure to describe\nthis complex system that has ticketing agents, baggage checkers, gate\npersonnel, pilots, airplanes, air traffic control, and a worldwide system for\nrouting airplanes? One way to describe this system might be to describe the\nseries of actions you take (or others take for you) when you fly on an\nairline. You purchase your ticket, check your bags, go to the gate, and\neventually get loaded onto the plane. The plane takes off and is routed to its\ndestination. After your plane lands, you deplane at the gate and claim your\nbags. If the trip was bad, you complain about the flight to the ticket agent\n(getting nothing for your effort). This scenario is shown in Figure 1.21.\nFigure 1.21 ♦Taking an airplane trip: actions\nAlready, we can see some analogies here with computer networking:\nYou are being shipped from source to destination by the airline; a packet is\nshipped from source host to destination host in the Internet. But this is not\nquite the analogy we are after. We are looking for some structure in Figure\n1.21. Looking at Figure 1.21, we note that there is a ticketing function at\neach end; there is also a baggage function for already-ticketed passengers,\nand a gate function for already-ticketed and already-baggage-checked\npassengers. For passengers who have made it through the gate (that is,\npassengers who are already ticketed, baggage-checked, and through the\ngate), there is a takeoff and landing function, and while in flight, there is an\nairplane-routing function. This suggests that we can look at the\nfunctionality in Figure 1.21 in a horizontal manner, as shown in Figure\nFigure 1.22 ♦Horizontal layering of airline functionality\nFigure 1.22 has divided the airline functionality into layers, providing a\nframework in which we can discuss airline travel. Note that each layer,\ncombined with the layers below it, implements some functionality, some\nservice. At the ticketing layer and below, airline-counter-to-airline-counter\ntransfer of a person is accomplished. At the baggage layer and below,\nbaggage-check-to-baggage-claim transfer of a person and bags is\naccomplished. Note that the baggage layer provides this service only to an\nalready-ticketed person. At the gate layer, departure-gate-to-arrival-gate\ntransfer of a person and bags is accomplished. At the takeoff/landing layer,\nrunway-to-runway transfer of people and their bags is accomplished. Each\nlayer provides its service by (1) performing certain actions within that layer\n(for example, at the gate layer, loading and unloading people from an\nairplane) and by (2) using the services of the layer directly below it (for\nexample, in the gate layer, using the runway-to-runway passenger transfer\nservice of the takeoff/landing layer).\nA layered architecture allows us to discuss a well-defined, specific part\nof a large and complex system. This simplification itself is of considerable\nvalue by providing modularity, making it much easier to change the\nimplementation of the service provided by the layer. As long as the layer\nprovides the same service to the layer above it, and uses the same services\nfrom the layer below it, the remainder of the system remains unchanged\nwhen a layer’s implementation is changed. (Note that changing the\nimplementation of a service is very different from changing the service\nitself!) For example, if the gate functions were changed (for instance, to\nhave people board and disembark by height), the remainder of the airline\nsystem would remain unchanged since the gate layer still provides the same\nfunction (loading and unloading people); it simply implements that function\nin a different manner after the change. For large and complex systems that\nare constantly being updated, the ability to change the implementation of a\nservice without affecting other components of the system is another\nimportant advantage of layering.\nProtocol Layering\nBut enough about airlines. Let’s now turn our attention to network\nprotocols. To provide structure to the design of network protocols, network\ndesigners organize protocols—and the network hardware and software that\nimplement the protocols—in layers. Each protocol belongs to one of the\nlayers, just as each function in the airline architecture in Figure 1.22\nbelonged to a layer. We are again interested in the services that a layer\noffers to the layer above—the so-called service model of a layer. Just as in\nthe case of our airline example, each layer provides its service by (1)\nperforming certain actions within that layer and by (2) using the services of\nthe layer directly below it. For example, the services provided by layer n\nmay include reliable delivery of messages from one edge of the network to\nthe other. This might be implemented by using an unreliable edge-to-edge\nmessage delivery service of layer n − 1, and adding layer n functionality to\ndetect and retransmit lost messages.\nA protocol layer can be implemented in software, in hardware, or in a\ncombination of the two. Application-layer protocols—such as HTTP and\nSMTP—are almost always implemented in software in the end systems; so\nare transport-layer protocols. Because the physical layer and data link layers\nare responsible for handling communication over a specific link, they are\ntypically implemented in a network interface card (for example, Ethernet or\nWiFi interface cards) associated with a given link. The network layer is\noften a mixed implementation of hardware and software. Also note that just\nas the functions in the layered airline architecture were distributed among\nthe various airports and flight control centers that make up the system, so\ntoo is a layer n protocol distributed among the end systems, packet\nswitches, and other components that make up the network. That is, there’s\noften a piece of a layer n protocol in each of these network components.\nProtocol layering has conceptual and structural advantages [RFC 3439].\nAs we have seen, layering provides a structured way to discuss system\ncomponents. Modularity makes it easier to update system components. We\nmention, however, that some researchers and networking engineers are\nvehemently opposed to layering [Wakeman 1992]. One potential drawback\nof layering is that one layer may duplicate lower-layer functionality. For\nexample, many protocol stacks provide error recovery on both a per-link\nbasis and an end-to-end basis. A second potential drawback is that\nfunctionality at one layer may need information (for example, a timestamp\nvalue) that is present only in another layer; this violates the goal of\nseparation of layers.\nWhen taken together, the protocols of the various layers are called the\nprotocol stack. The Internet protocol stack consists of five layers: the\nphysical, link, network, transport, and application layers, as shown in\n(DNS). We’ll see in Chapter 2 that it is very easy to create and deploy our\nown new application-layer protocols.\nAn application-layer protocol is distributed over multiple end systems,\nwith the application in one end system using the protocol to exchange\npackets of information with the application in another end system. We’ll\nrefer to this packet of information at the application layer as a message.\nTransport Layer\nThe Internet’s transport layer transports application-layer messages between\napplication endpoints. In the Internet, there are two transport protocols,\nTCP and UDP, either of which can transport application-layer messages.\nTCP provides a ­connection-oriented service to its applications. This service\nincludes guaranteed delivery of application-layer messages to the\ndestination and flow control (that is, sender/receiver speed matching). TCP\nalso breaks long messages into shorter ­segments and provides a congestion-\ncontrol mechanism, so that a source throttles its transmission rate when the\nnetwork is congested. The UDP protocol provides a connectionless service\nto its applications. This is a no-frills service that provides no reliability, no\nflow control, and no congestion control. In this book, we’ll refer to a\ntransport-layer packet as a segment.\nNetwork Layer\nThe Internet’s network layer is responsible for moving network-layer\npackets known as datagrams from one host to another. The Internet\ntransport-layer protocol (TCP or UDP) in a source host passes a transport-\nlayer segment and a destination address to the network layer, just as you\nwould give the postal service a letter with a destination address. The\nnetwork layer then provides the service of delivering the segment to the\ntransport layer in the destination host.\nThe Internet’s network layer includes the celebrated IP protocol, which\ndefines the fields in the datagram as well as how the end systems and\nrouters act on these fields. There is only one IP protocol, and all Internet\ncomponents that have a network layer must run the IP protocol. The\nInternet’s network layer also contains routing protocols that determine the\nroutes that datagrams take between sources and destinations. The Internet\nhas many routing protocols. As we saw in Section 1.3, the Internet is a\nnetwork of networks, and within a network, the network administrator can\nrun any routing protocol desired. Although the network layer contains both\nthe IP protocol and numerous routing protocols, it is often simply referred\nto as the IP layer, reflecting the fact that IP is the glue that binds the Internet\nThe Internet’s network layer routes a datagram through a series of routers\nbetween the source and destination. To move a packet from one node (host\nor router) to the next node in the route, the network layer relies on the\nservices of the link layer. In particular, at each node, the network layer\npasses the datagram down to the link layer, which delivers the datagram to\nthe next node along the route. At this next node, the link layer passes the\ndatagram up to the network layer.\nThe services provided by the link layer depend on the specific link-\nlayer protocol that is employed over the link. For example, some link-layer\nprotocols provide reliable delivery, from transmitting node, over one link, to\nreceiving node. Note that this reliable delivery service is different from the\nreliable delivery service of TCP, which provides reliable delivery from one\nend system to another. Examples of link-layer protocols include Ethernet,\nWiFi, and the cable access network’s DOCSIS protocol. As datagrams\ntypically need to traverse several links to travel from source to destination,\na datagram may be handled by different link-layer protocols at different\nlinks along its route. For example, a datagram may be handled by Ethernet\non one link and by PPP on the next link. The network layer will receive a\ndifferent service from each of the different link-layer protocols. In this\nbook, we’ll refer to the link-layer packets as frames.\nPhysical Layer\nWhile the job of the link layer is to move entire frames from one network\nelement to an adjacent network element, the job of the physical layer is to\nmove the individual bits within the frame from one node to the next. The\nprotocols in this layer are again link dependent and further depend on the\nactual transmission medium of the link (for example, twisted-pair copper\nwire, single-mode fiber optics). For example, Ethernet has many physical-\nlayer protocols: one for twisted-pair copper wire, another for coaxial cable,\nanother for fiber, and so on. In each case, a bit is moved across the link in a\ndifferent way.\n1.5.2 Encapsulation\nFigure 1.24 shows the physical path that data takes down a sending end\nsystem’s protocol stack, up and down the protocol stacks of an intervening\nlink-layer switch and router, and then up the protocol stack at the receiving\nend system. As we discuss later in this book, routers and link-layer switches\nare both packet switches. Similar to end systems, routers and link-layer\nswitches organize their networking hardware and software into layers. But\nrouters and link-layer switches do not implement all of the layers in the\nprotocol stack; they typically implement only the bottom layers. As shown\nin Figure 1.24, link-layer switches implement layers 1 and 2; routers\nimplement layers 1 through 3. This means, for example, that Internet routers\nare capable of implementing the IP protocol (a layer 3 protocol), while link-\nlayer switches are not. We’ll see later that while link-layer switches do not\nrecognize IP addresses, they are capable of recognizing layer 2 addresses,\nsuch as Ethernet addresses. Note that hosts implement all five layers; this is\nconsistent with the view that the Internet architecture puts much of its\ncomplexity at the edges of the network.\nFigure 1.24 ♦Hosts, routers, and link-layer switches; each contains\na ­different set of layers, reflecting their differences in\nfunctionality\nFigure 1.24 also illustrates the important concept of encapsulation. At\nthe sending host, an application-layer message (M in Figure 1.24) is\npassed to the transport layer. In the simplest case, the transport layer takes\nthe message and appends additional information (so-called transport-layer\nheader information, H  in Figure 1.24) that will be used by the receiver-side\ntransport layer. The application-layer message and the transport-layer\nheader information together constitute the transport-layer segment. The\ntransport-layer segment thus encapsulates the application-layer message.\nThe added information might include information allowing the receiver-side\ntransport layer to deliver the message up to the appropriate application, and\nerror-detection bits that allow the receiver to determine whether bits in the\nmessage have been changed in route. The transport layer then passes the\nsegment to the network layer, which adds network-layer header information\n(H  in Figure 1.24) such as source and destination end system addresses,\ncreating a network-layer datagram. The datagram is then passed to the\nlink layer, which (of course!) will add its own link-layer header information\nand create a link-layer frame. Thus, we see that at each layer, a packet has\ntwo types of fields: header fields and a payload field. The payload is\ntypically a packet from the layer above.\nA useful analogy here is the sending of an interoffice memo from one\ncorporate branch office to another via the public postal service. Suppose\nAlice, who is in one branch office, wants to send a memo to Bob, who is in\nanother branch office. The memo is analogous to the application-layer\nmessage. Alice puts the memo in an interoffice envelope with Bob’s name\nand department written on the front of the envelope. The interoffice\nenvelope is analogous to a transport-layer segment—it contains header\ninformation (Bob’s name and department number) and it encapsulates the\napplication-layer message (the memo). When the sending branch-office\nmailroom receives the interoffice envelope, it puts the interoffice envelope\ninside yet another envelope, which is suitable for sending through the\npublic postal service. The sending mailroom also writes the postal address\nof the sending and receiving branch offices on the postal envelope. Here,\nthe postal envelope is analogous to the datagram—it encapsulates the\ntransport-layer segment (the interoffice envelope), which encapsulates the\noriginal message (the memo). The postal service delivers the postal\nenvelope to the receiving branch-office mailroom. There, the process of de-\nencapsulation is begun. The mailroom extracts the interoffice memo and\nforwards it to Bob. Finally, Bob opens the envelope and removes the memo.\nThe process of encapsulation can be more complex than that described\nabove. For example, a large message may be divided into multiple\ntransport-layer segments (which might themselves each be divided into\nmultiple network-layer datagrams). At the receiving end, such a segment\nmust then be reconstructed from its constituent datagrams.\n1.6 Networks Under Attack\nThe Internet has become mission critical for many institutions today,\nincluding large and small companies, universities, and government\nagencies. Many individuals also rely on the Internet for many of their\nprofessional, social, and personal activities. Billions of “things,” including\nwearables and home devices, are currently being connected to the Internet.\nBut behind all this utility and excitement, there is a dark side, a side where\n“bad guys” attempt to wreak havoc in our daily lives by damaging our\nInternet-connected computers, violating our privacy, and rendering\ninoperable the Internet services on which we depend.\nThe field of network security is about how the bad guys can attack\ncomputer networks and about how we, soon-to-be experts in computer\nnetworking, can defend networks against those attacks, or better yet, design\nnew architectures that are immune to such attacks in the first place. Given\nthe frequency and variety of existing attacks as well as the threat of new\nand more destructive future attacks, network security has become a central\ntopic in the field of computer networking. One of the features of this\ntextbook is that it brings network security issues to the forefront.\nSince we don’t yet have expertise in computer networking and Internet\nprotocols, we’ll begin here by surveying some of today’s more prevalent\nsecurity-related problems. This will whet our appetite for more substantial\ndiscussions in the upcoming chapters. So we begin here by simply asking,\nwhat can go wrong? How are computer networks vulnerable? What are\nsome of the more prevalent types of attacks today?\nThe Bad Guys Can Put Malware into Your Host Via the Internet\nWe attach devices to the Internet because we want to receive/send data\nfrom/to the Internet. This includes all kinds of good stuff, including\nInstagram posts, Internet search results, streaming music, video conference\ncalls, streaming movies, and so on. But, unfortunately, along with all that\ngood stuff comes malicious stuff—­collectively known as malware—that\ncan also enter and infect our devices. Once malware infects our device it\ncan do all kinds of devious things, including deleting our files and installing\nspyware that collects our private information, such as  social ­security\nnumbers, passwords, and keystrokes, and then sends this (over the Internet,\nof course!) back to the bad guys. Our compromised host may also\nbe enrolled in a network of thousands of similarly compromised devices,\ncollectively known as a botnet, which the bad guys control and leverage for\nspam e-mail distribution or distributed denial-of-service attacks (soon to be\ndiscussed) against targeted hosts.\nMuch of the malware out there today is self-replicating: once it infects\none host, from that host it seeks entry into other hosts over the Internet, and\nfrom ­the newly infected hosts, it seeks entry into yet more hosts. In this\nmanner, self-­replicating malware can spread exponentially fast.\nThe Bad Guys Can Attack Servers and Network Infrastructure\nAnother broad class of security threats are known as denial-of-service\n(DoS) attacks. As the name suggests, a DoS attack renders a network, host,\nor other piece of infrastructure unusable by legitimate users. Web servers, e-\nmail servers, DNS servers (discussed in Chapter 2), and institutional\nnetworks can all be subject to DoS attacks. The site Digital Attack Map\nallows use to visualize the top daily DoS attacks worldwide [DAM 2020].\nMost Internet DoS attacks fall into one of three categories:\nVulnerability attack. This involves sending a few well-crafted messages\nto a vulnerable application or operating system running on a targeted\nhost. If the right sequence of packets is sent to a vulnerable application\nor operating system, the service can stop or, worse, the host can crash.\nBandwidth flooding. The attacker sends a deluge of packets to the\ntargeted host—so many packets that the target’s access link becomes\nclogged, preventing legitimate packets from reaching the server.\nConnection flooding. The attacker establishes a large number of half-\nopen or fully open TCP connections (TCP connections are discussed in\nChapter 3) at the target host. The host can become so bogged down with\nthese bogus connections that it stops accepting legitimate connections.\nLet’s now explore the bandwidth-flooding attack in more detail. Recalling\nour delay and loss analysis discussion in Section 1.4.2, it’s evident that if\nthe server has an access rate of R bps, then the attacker will need to send\ntraffic at a rate of approximately R bps to cause damage. If R is very large, a\nsingle attack source may not be able to generate enough traffic to harm the\nserver. Furthermore, if all the traffic emanates from a single source, an\nupstream router may be able to detect the attack and block all traffic from\nthat source before the traffic gets near the server. In a distributed DoS\n(DDoS) attack, illustrated in Figure 1.25, the attacker controls multiple\nsources and has each source blast traffic at the target. With this approach,\nthe aggregate traffic rate across all the controlled sources needs to be\napproximately R to cripple the ­service. DDoS attacks leveraging botnets\nwith thousands of comprised hosts are a common occurrence today [DAM\n2020]. DDos attacks are much harder to detect and defend against than a\nDoS attack from a single host.\nFigure 1.25 ♦A distributed denial-of-service attack\nWe encourage you to consider the following question as you work your\nway through this book: What can computer network designers do to defend\nagainst DoS attacks? We will see that different defenses are needed for the\nthree types of DoS attacks.\nThe Bad Guys Can Sniff Packets\nMany users today access the Internet via wireless devices, such as WiFi-\nconnected laptops or handheld devices with cellular Internet connections\n(covered in Chapter 7). While ubiquitous Internet access is extremely\nconvenient and enables marvelous new applications for mobile users, it also\ncreates a major security vulnerability—by placing a passive receiver in the\nvicinity of the wireless transmitter, that receiver can obtain a copy of every\npacket that is transmitted! These packets can contain all kinds of sensitive\ninformation, including passwords, social security numbers, trade secrets,\nand private personal messages. A passive receiver that records a copy of\nevery packet that flies by is called a packet sniffer.\nSniffers can be deployed in wired environments as well. In wired\nbroadcast environments, as in many Ethernet LANs, a packet sniffer can\nobtain copies of broadcast packets sent over the LAN. As described in\nSection 1.2, cable access technologies also broadcast packets and are thus\nvulnerable to sniffing. Furthermore, a bad guy who gains access to an\ninstitution’s access router or access link to the Internet may be able to plant\na sniffer that makes a copy of every packet going to/from the organization.\nSniffed packets can then be analyzed offline for sensitive information.\nPacket-sniffing software is freely available at various Web sites and as\ncommercial products. Professors teaching a networking course have been\nknown to assign lab exercises that involve writing a packet-sniffing and\napplication-layer data reconstruction program. Indeed, the Wireshark\n[Wireshark 2020] labs associated with this text (see the introductory\nWireshark lab at the end of this chapter) use exactly such a packet sniffer!\nBecause packet sniffers are passive—that is, they do not inject packets\ninto the channel—they are difficult to detect. So, when we send packets into\na wireless channel, we must accept the possibility that some bad guy may\nbe recording copies of our packets. As you may have guessed, some of the\nbest defenses against packet sniffing involve cryptography. We will\nexamine cryptography as it applies to network security in Chapter 8.\nThe Bad Guys Can Masquerade as Someone You Trust\nIt is surprisingly easy (you will have the knowledge to do so shortly as you\nproceed through this text!) to create a packet with an arbitrary source\naddress, packet content, and destination address and then transmit this\nhand-crafted packet into the Internet, which will dutifully forward the\npacket to its destination. Imagine the unsuspecting receiver (say an Internet\nrouter) who receives such a packet, takes the (false) source address as being\ntruthful, and then performs some command embedded in the packet’s\nmechanisms for end-point authentication in Chapter 8.\nIn closing this section, it’s worth considering how the Internet got to be\nsuch an insecure place in the first place. The answer, in essence, is that the\nInternet was originally designed to be that way, based on the model of “a\ngroup of mutually trusting users attached to a transparent network”\n[Blumenthal 2001]—a model in which (by definition) there is no need for\nsecurity. Many aspects of the original Internet architecture deeply reflect\nthis notion of mutual trust. For example, the ability for one user to send a\npacket to any other user is the default rather than a requested/granted\ncapability, and user identity is taken at declared face value, rather than\nbeing authenticated by default.\nBut today’s Internet certainly does not involve “mutually trusting\nusers.” Nonetheless, today’s users still need to communicate when they\ndon’t necessarily trust each other, may wish to communicate anonymously,\nmay communicate indirectly through third parties (e.g., Web caches, which\nwe’ll study in Chapter 2, or mobility-assisting agents, which we’ll study in\nChapter 7), and may distrust the hardware, software, and even the air\nthrough which they communicate. We now have many security-related\nchallenges before us as we progress through this book: We should seek\ndefenses against sniffing, end-point masquerading, man-in-the-middle\nattacks, DDoS attacks, malware, and more. We should keep in mind that\ncommunication among mutually trusted users is the exception rather than\nthe rule. Welcome to the world of modern computer networking!\n1.7 History of Computer Networking and the\nSections 1.1 through 1.6 presented an overview of the technology of\ncomputer networking and the Internet. You should know enough now to\nimpress your family and friends! However, if you really want to be a big hit\nat the next cocktail party, you should sprinkle your discourse with tidbits\nabout the fascinating history of the Internet [Segaller 1998].\n1.7.1 The Development of Packet Switching: 1961–\nThe field of computer networking and today’s Internet trace their\nbeginnings back to the early 1960s, when the telephone network was the\nworld’s dominant communication network. Recall from Section 1.3 that the\ntelephone network uses circuit switching to transmit information from a\nsender to a receiver—an appropriate choice given that voice is transmitted\nat a constant rate between sender and receiver. Given the increasing\nimportance of computers in the early 1960s and the advent of timeshared\ncomputers, it was perhaps natural to consider how to hook computers\ntogether so that they could be shared among geographically distributed\nusers. The traffic generated by such users was likely to be bursty—intervals\nof activity, such as the sending of a command to a remote computer,\nfollowed by periods of inactivity while waiting for a reply or while\ncontemplating the received response.\nThree research groups around the world, each unaware of the others’\nwork [Leiner 1998], began inventing packet switching as an efficient and\nrobust alternative to circuit switching. The first published work on packet-\nswitching techniques was that of Leonard Kleinrock [Kleinrock 1961;\nKleinrock 1964], then a graduate student at MIT. Using queuing theory,\nKleinrock’s work elegantly demonstrated the effectiveness of the packet-\nswitching approach for bursty traffic sources. In 1964, Paul Baran [Baran\n1964] at the Rand Institute had begun investigating the use of packet\nswitching for secure voice over military networks, and at the National\nPhysical Laboratory in England, Donald Davies and Roger Scantlebury\nwere also developing their ideas on packet switching.\nThe work at MIT, Rand, and the NPL laid the foundations for today’s\nInternet. But the Internet also has a long history of a let’s-build-it-and-\ndemonstrate-it attitude that also dates back to the 1960s. J. C. R. Licklider\n[DEC 1990] and Lawrence Roberts, both colleagues of Kleinrock’s at MIT,\nwent on to lead the computer science program at the Advanced Research\nProjects Agency (ARPA) in the United States. Roberts published an overall\nplan for the ARPAnet [Roberts 1967], the first packet-switched computer\nnetwork and a direct ancestor of today’s public Internet. On Labor Day in\n1969, the first packet switch was installed at UCLA under Kleinrock’s\nsupervision, and three additional packet switches were installed shortly\nthereafter at the Stanford Research Institute (SRI), UC Santa Barbara, and\nthe University of Utah (Figure 1.26). The fledgling precursor to the Internet\nwas four nodes large by the end of 1969. Kleinrock recalls the very first use\nof the network to perform a remote login from UCLA to SRI, crashing the\nsystem [Kleinrock 2004].\nFigure 1.26 ♦An early packet switch\nMark J. Terrill/AP Photo\nBy 1972, ARPAnet had grown to approximately 15 nodes and was given\nits first public demonstration by Robert Kahn. The first host-to-host\nprotocol between ARPAnet end systems, known as the network-control\nprotocol (NCP), was completed [RFC 001]. With an end-to-end protocol\navailable, applications could now be written. Ray Tomlinson wrote the first\ne-mail program in 1972.\n1.7.2 Proprietary Networks and Internetworking:\nThe initial ARPAnet was a single, closed network. In order to communicate\nwith an ARPAnet host, one had to be actually attached to another ARPAnet\nIMP. In the early to mid-1970s, additional stand-alone packet-switching\nnetworks besides ARPAnet came into being: ALOHANet, a microwave\nnetwork linking universities on the Hawaiian islands [Abramson 1970], as\nwell as DARPA’s packet-satellite [RFC 829] and packet-radio networks\n[Kahn 1978]; Telenet, a BBN commercial packet-switching network based\non ARPAnet technology; Cyclades, a French packet-switching network\npioneered by Louis Pouzin [Think 2012]; Time-sharing networks such as\nTymnet and the GE Information Services network, among others, in the late\n1960s and early 1970s [Schwartz 1977]; IBM’s SNA (1969–1974), which\nparalleled the ARPAnet work [Schwartz 1977].\nThe number of networks was growing. With perfect hindsight we can\nsee that the time was ripe for developing an encompassing architecture for\nconnecting networks together. Pioneering work on interconnecting\nnetworks (under the sponsorship of the Defense Advanced Research\nProjects Agency (DARPA)), in essence creating a network of networks, was\ndone by Vinton Cerf and Robert Kahn [Cerf 1974]; the term internetting\nwas coined to describe this work.\nThese architectural principles were embodied in TCP. The early\nversions of TCP, however, were quite different from today’s TCP. The early\nversions of TCP combined a reliable in-sequence delivery of data via end-\nsystem retransmission (still part of today’s TCP) with forwarding functions\n(which today are performed by IP). Early experimentation with TCP,\ncombined with the recognition of the importance of an unreliable, non-\nflow-controlled, end-to-end transport service for applications such as\npacketized voice, led to the separation of IP out of TCP and the\ndevelopment of the UDP protocol. The three key Internet protocols that we\nsee today—TCP, UDP, and IP—were conceptually in place by the end of\nIn addition to the DARPA Internet-related research, many other\nimportant networking activities were underway. In Hawaii, Norman\nAbramson was developing ALOHAnet, a packet-based radio network that\nallowed multiple remote sites on the Hawaiian Islands to communicate with\neach other. The ALOHA protocol [Abramson 1970] was the first multiple-\naccess protocol, allowing geographically distributed users to share a single\nbroadcast communication medium (a radio ­frequency). Metcalfe and Boggs\nbuilt on Abramson’s multiple-access protocol work when they developed\nthe Ethernet protocol [Metcalfe 1976] for wire-based shared broadcast\nnetworks. Interestingly, Metcalfe and Boggs’ Ethernet protocol was\nmotivated by the need to connect multiple PCs, printers, and shared disks\n[Perkins 1994]. Twenty-five years ago, well before the PC revolution and\nthe explosion of networks, Metcalfe and Boggs were laying the foundation\nfor today’s PC LANs.\n1.7.3 A Proliferation of Networks: 1980–1990\nBy the end of the 1970s, approximately two hundred hosts were connected\nto the ARPAnet. By the end of the 1980s the number of hosts connected to\nthe public ­Internet, a confederation of networks looking much like today’s\nInternet, would reach a hundred thousand. The 1980s would be a time of\ntremendous growth.\nMuch of that growth resulted from several distinct efforts to create\ncomputer networks linking universities together. BITNET provided e-mail\nand file transfers among several universities in the Northeast. CSNET\n(computer science network) was formed to link university researchers who\ndid not have access to ARPAnet. In 1986, NSFNET was created to provide\naccess to NSF-sponsored supercomputing centers. Starting with an initial\nbackbone speed of 56 kbps, NSFNET’s backbone would be running at 1.5\nMbps by the end of the decade and would serve as a primary backbone\nlinking regional networks.\nIn the ARPAnet community, many of the final pieces of today’s Internet\narchitecture were falling into place. January 1, 1983 saw the official\ndeployment of TCP/IP as the new standard host protocol for ARPAnet\n(replacing the NCP pro­tocol). The transition [RFC 801] from NCP to\nTCP/IP was a flag day event—all hosts were required to transfer over to\nTCP/IP as of that day. In the late 1980s, important extensions were made to\nTCP to implement host-based congestion control [Jacobson 1988]. The\nDNS, used to map between a human-readable Internet name (for example,\ngaia.cs.umass.edu) and its 32-bit IP address, was also developed [RFC\nParalleling this development of the ARPAnet (which was for the most\npart a US effort), in the early 1980s the French launched the Minitel project,\nan ambitious plan to bring data networking into everyone’s home.\nSponsored by the French government, the Minitel system consisted of a\npublic packet-switched network (based on the X.25 protocol suite), Minitel\nservers, and inexpensive terminals with built-in low-speed modems. The\nMinitel became a huge success in 1984 when the French government gave\naway a free Minitel terminal to each French household that wanted one.\nMinitel sites included free sites—such as a telephone directory site—as\nwell as private sites, which collected a usage-based fee from each user. At\nits peak in the mid 1990s, it offered more than 20,000 services, ranging\nfrom home banking to specialized research databases. The Minitel was in a\nlarge proportion of French homes 10 years before most Americans had ever\nheard of the Internet.\n1.7.4 The Internet Explosion: The 1990s\nThe 1990s were ushered in with a number of events that symbolized the\ncontinued evolution and the soon-to-arrive commercialization of the\nInternet. ARPAnet, the progenitor of the Internet, ceased to exist. In 1991,\nNSFNET lifted its restrictions on the use of NSFNET for commercial\npurposes. NSFNET itself would be decommissioned in 1995, with Internet\nbackbone traffic being carried by commercial Internet Service Providers.\nThe main event of the 1990s was to be the emergence of the World\nWide Web application, which brought the Internet into the homes and\nbusinesses of millions of people worldwide. The Web served as a platform\nfor enabling and deploying hundreds of new applications that we take for\ngranted today, including search (e.g., Google and Bing) Internet commerce\n(e.g., Amazon and eBay) and social networks (e.g., Facebook).\nThe Web was invented at CERN by Tim Berners-Lee between 1989 and\n1991 [Berners-Lee 1989], based on ideas originating in earlier work on\nhypertext from the 1940s by Vannevar Bush [Bush 1945] and since the\n1960s by Ted Nelson [Xanadu 2012]. Berners-Lee and his associates\ndeveloped initial versions of HTML, HTTP, a Web server, and a browser—\nthe four key components of the Web. Around the end of 1993 there were\nabout two hundred Web servers in operation, this collection of servers being\njust a harbinger of what was about to come. At about this time several\nresearchers were developing Web browsers with GUI interfaces, including\nMarc Andreessen, who along with Jim Clark, formed Mosaic\nCommunications, \nCommunications\nCorporation [Cusumano 1998; Quittner 1998]. By 1995, university students\nwere using Netscape browsers to surf the Web on a daily basis. At about this\ntime companies—big and small—began to operate Web servers and transact\ncommerce over the Web. In 1996, Microsoft started to make browsers,\nwhich started the browser war between Netscape and Microsoft, which\nMicrosoft won a few years later [Cusumano 1998].\nThe second half of the 1990s was a period of tremendous growth and\ninnovation for the Internet, with major corporations and thousands of\nstartups creating Internet products and services. By the end of the\nmillennium the Internet was supporting hundreds of popular applications,\nincluding four killer applications:\nE-mail, including attachments and Web-accessible e-mail\nThe Web, including Web browsing and Internet commerce\nInstant messaging, with contact lists\nPeer-to-peer file sharing of MP3s, pioneered by Napster\nInterestingly, the first two killer applications came from the research\ncommunity, whereas the last two were created by a few young\nentrepreneurs.\nThe period from 1995 to 2001 was a roller-coaster ride for the Internet\nin the financial markets. Before they were even profitable, hundreds of\nInternet startups made initial public offerings and started to be traded in a\nstock market. Many companies were valued in the billions of dollars\nwithout having any significant revenue streams. The Internet stocks\ncollapsed in 2000–2001, and many startups shut down. Nevertheless, a\nnumber of companies emerged as big winners in the Internet space,\nincluding Microsoft, Cisco, Yahoo, eBay, Google, and Amazon.\n1.7.5 The New Millennium\nIn the first two decades of the 21st century, perhaps no other technology has\ntransformed society more than the Internet along with Internet-connected\nsmartphones. And innovation in computer networking continues at a rapid\npace. Advances are being made on all fronts, including deployments of\nfaster routers and higher transmission speeds in both access networks and in\nnetwork backbones. But the following developments merit special attention:\nSince the beginning of the millennium, we have been seeing aggressive\ndeployment of broadband Internet access to homes—not only cable\nmodems and DSL but also fiber to the home, and now 5G fixed wireless\nas discussed in Section 1.2. This high-speed Internet access has set the\nstage for a wealth of video applications, including the distribution of\nuser-generated video (for example, YouTube), on-demand streaming of\nmovies and television shows (e.g., Netflix), and multi-person video\nconference (e.g., Skype, Facetime, and Google Hangouts).\nThe increasing ubiquity of high-speed wireless Internet access is not\nonly making it possible to remain constantly connected while on the\nmove, but also enabling new location-specific applications such as Yelp,\nTinder, and Waz. The number of wireless devices connecting to the\nInternet surpassed the number of wired devices in 2011. This high-\nspeed wireless access has set the stage for the rapid emergence of hand-\nheld computers (iPhones, Androids, iPads, and so on), which enjoy\nconstant and untethered access to the Internet.\nOnline social networks—such as Facebook, Instagram, Twitter, and\nWeChat (hugely popular in China)—have created massive people\nnetworks on top of the Internet. Many of these social networks are\nextensively used for messaging as well as photo sharing. Many Internet\nusers today “live” primarily within one or more social networks.\nThrough their APIs, the online social networks create platforms for new\nnetworked applications, including mobile payments and distributed\nAs discussed in Section 1.3.3, online service providers, such as Google\nand Microsoft, have deployed their own extensive private networks,\nwhich not only connect together their globally distributed data centers,\nbut are used to bypass the Internet as much as possible by peering\ndirectly with lower-tier ISPs. As a result, Google provides search results\nand e-mail access almost instantaneously, as if their data centers were\nrunning within one’s own computer.\nMany Internet commerce companies are now running their applications\nin the “cloud”—such as in Amazon’s EC2, in Microsoft’s Azure, or in\nthe Alibaba Cloud. Many companies and universities have also\nmigrated their Internet applications (e.g., e-mail and Web hosting) to the\ncloud. Cloud companies not only provide applications scalable\ncomputing and storage environments, but also provide the applications\nimplicit access to their high-performance private networks.\n1.8 Summary\nIn this chapter, we’ve covered a tremendous amount of material! We’ve\nlooked at the various pieces of hardware and software that make up the\nInternet in particular and computer networks in general. We started at the\nedge of the network, looking at end systems and applications, and at the\ntransport service provided to the applications running on the end systems.\nWe also looked at the link-layer technologies and physical media typically\nfound in the access network. We then dove deeper inside the network, into\nthe network core, identifying packet switching and circuit switching as the\ntwo basic approaches for transporting data through a telecommunication\nnetwork, and we examined the strengths and weaknesses of each approach.\nWe also examined the structure of the global Internet, learning that the\nInternet is a network of networks. We saw that the Internet’s hierarchical\nstructure, consisting of higher- and lower-tier ISPs, has allowed it to scale\nto include thousands of networks.\nIn the second part of this introductory chapter, we examined several\ntopics central to the field of computer networking. We first examined the\ncauses of delay, throughput and packet loss in a packet-switched network.\nWe developed simple quantitative models for transmission, propagation,\nand queuing delays as well as for throughput; we’ll make extensive use of\nthese delay models in the homework problems throughout this book. Next\nwe examined protocol layering and service models, key architectural\nprinciples in networking that we will also refer back to throughout this\nbook. We also surveyed some of the more prevalent security attacks in the\nInternet day. We finished our introduction to networking with a brief history\nof computer networking. The first chapter in itself constitutes a mini-course\nin computer networking.\nSo, we have indeed covered a tremendous amount of ground in this first\nchapter! If you’re a bit overwhelmed, don’t worry. In the following\nchapters, we’ll revisit all of these ideas, covering them in much more detail\n(that’s a promise, not a threat!). At this point, we hope you leave this\nchapter with a still-developing intuition for the pieces that make up a\nnetwork, a still-developing command of the vocabulary of networking\n(don’t be shy about referring back to this chapter), and an ever-growing\ndesire to learn more about networking. That’s the task ahead of us for the\nrest of this book.\nRoad-Mapping This Book\nBefore starting any trip, you should always glance at a road map in order to\nbecome familiar with the major roads and junctures that lie ahead. For the\ntrip we are about to embark on, the ultimate destination is a deep\nunderstanding of the how, what, and why of computer networks. Our road\nmap is the sequence of chapters of this book:\n1. Computer Networks and the Internet\n2. Application Layer\n3. Transport Layer\n4. Network Layer: Data Plane\n5. Network Layer: Control Plane\n6. The Link Layer and LANs\n7. Wireless and Mobile Networks\n8. Security in Computer Networks\nChapters 2 through 6 are the five core chapters of this book. You should\nnotice that these chapters are organized around the top four layers of the\nfive-layer Internet protocol. Further note that our journey will begin at the\ntop of the Internet protocol stack, namely, the application layer, and will\nwork its way downward. The rationale behind this top-down journey is that\nonce we understand the applications, we can understand the network\nservices needed to support these applications. We can then, in turn, examine\nthe various ways in which such services might be implemented by a\nnetwork architecture. Covering applications early thus provides motivation\nfor the remainder of the text.\nThe second half of the book—Chapters 7 and 8—zooms in on two\nenormously important (and somewhat independent) topics in modern\ncomputer networking. In Chapter 7, we examine wireless and mobile\nnetworks, including wireless LANs (including WiFi and Bluetooth),\nCellular networks (including 4G and 5G), and mobility. Chapter 8, which\naddresses security in computer networks, first looks at the underpinnings of\nencryption and network security, and then we examine how the basic theory\nis being applied in a broad range of Internet contexts.\nHomework Problems and Questions\nChapter 1 Review Questions\nSECTION 1.1\nR1. What is the difference between a host and an end system? List several\ndifferent types of end systems. Is a Web server an end system?\nR2. Describe the protocol that might be used by two people having a\ntelephonic conversation to initiate and end the conversation, i.e., the\nway that they talk.\nR3. Why are standards important for protocols?\nSECTION 1.2\nR4. List four access technologies. Classify each one as home access,\nenterprise access, or wide-area wireless access.\nR5. Is HFC transmission rate dedicated or shared among users? Are\ncollisions possible in a downstream HFC channel? Why or why not?\nR6. What access network technologies would be most suitable for\nproviding internet access in rural areas?\nR7. Dial-up modems and DSL both use the telephone line (a twisted-pair\ncopper cable) as their transmission medium. Why then is DSL much\nfaster than dial-up access?\nR8. What are some of the physical media that Ethernet can run over?\nR9. HFC, DSL, and FTTH are all used for residential access. For each of\nthese access technologies, provide a range of ­transmission rates and\ncomment on whether the transmission rate is shared or dedicated.\nR10. Describe the different wireless technologies you use during the day\nand their characteristics. If you have a choice between multiple\ntechnologies, why do you prefer one over another?\nSECTION 1.3\nR11. Suppose there is exactly one packet switch between a sending host\nand a receiving host. The transmission rates between the sending host\nand the switch and between the switch and the receiving host are R\nand R , respectively. Assuming that the switch uses store-and-forward\npacket switching, what is the total end-to-end delay to send a packet\nof length L? (Ignore queuing, propagation delay, and processing\nR12. What advantage does a circuit-switched network have over a packet-\nswitched network? What advantages does TDM have over FDM in a\ncircuit-switched network?\nR13. Suppose users share a 2 Mbps link. Also suppose each user transmits\ncontinuously at 1 Mbps when transmitting, but each user transmits\nonly 20 percent of the time. (See the discussion of statistical\nmultiplexing in Section 1.3.)\na. When circuit switching is used, how many users can be\nb. For the remainder of this problem, suppose packet switching is\nused. Why will there be essentially no queuing delay before the\nlink if two or fewer users transmit at the same time? Why will\nthere be a queuing delay if three users transmit at the same time?\nc. Find the probability that a given user is transmitting.\nd. Suppose now there are three users. Find the probability that at\nany given time, all three users are transmitting simultaneously.\nFind the fraction of time during which the queue grows.\nR14. Why will two ISPs at the same level of the hierarchy often peer with\neach other? How does an IXP earn money?\nR15. Why is a content provider considered a different Internet entity\ntoday? How does a content provider connect to other ISPs? Why?\nSECTION 1.4\nR16. Consider sending a packet from a source host to a destination host\nover a fixed route. List the delay components in the end-to-end delay.\nWhich of these delays are constant and which are variable?\nR17. Visit the Transmission Versus Propagation Delay interactive\nanimation at the Companion Website. Among the rates, propagation\ndelay, and packet sizes available, find a combination for which the\nsender finishes transmitting before the first bit of the packet reaches\nthe receiver. Find another combination for which the first bit of the\npacket reaches the receiver before the sender finishes transmitting.\nR18. A user can directly connect to a server through either long-range\nwireless or a twisted-pair cable for transmitting a 1500-bytes file. The\ntransmission rates of the wireless and wired media are 2 and 100\nMbps, respectively. Assume that the propagation speed in air is 3 ×\n10  m/s, while the speed in the twisted pair is 2 × 10  m/s. If the user\nis located 1 km away from the server, what is the nodal delay when\nusing each of the two technologies?\nR19. Suppose Host A wants to send a large file to Host B. The path from\nHost A to Host B has three links, of rates R  = 500 kbps, R  = 2 Mbps,\nand R  = 1 Mbps.\na. Assuming no other traffic in the network, what is the throughput\nfor the file transfer?\nb. Suppose the file is 4 million bytes. Dividing the file size by the\nthroughput, roughly how long will it take to transfer the file to\nc. Repeat (a) and (b), but now with R  reduced to 100 kbps.\nR20. Suppose end system A wants to send a large file to end system B. At a\nvery high level, describe how end system A creates packets from the\nfile. When one of these packets arrives to a router, what information\nin the packet does the router use to determine the link onto which the\npacket is forwarded? Why is packet switching in the Internet\nanalogous to driving from one city to another and asking directions\nalong the way?\nR21. Visit the Queuing and Loss interactive animation at the Companion\nWebsite. What is the maximum emission rate and the minimum\ntransmission rate? With those rates, what is the traffic intensity? Run\nthe interactive animation with these rates and determine how long it\ntakes for packet loss to occur. Then repeat the experiment a second\ntime and determine again how long it takes for packet loss to occur.\nAre the values different? Why or why not?\nSECTION 1.5\nR22. If two end-systems are connected through multiple routers and the\ndata-link level between them ensures reliable data delivery, is a\ntransport protocol ­offering reliable data delivery between these two\nend-systems necessary? Why?\nR23. What are the five layers in the Internet protocol stack? What are the\nprincipal responsibilities of each of these layers?\nR24. What do encapsulation and de-encapsulation mean? Why are they\nneeded in a layered protocol stack?\nR25. Which layers in the Internet protocol stack does a router process?\nWhich layers does a link-layer switch process? Which layers does a\nhost process?\nSECTION 1.6\nR26. What is self-replicating malware?\nR27. Describe how a botnet can be created and how it can be used for a\nDDoS attack.\nR28. Suppose Alice and Bob are sending packets to each other over a\ncomputer network. Suppose Trudy positions herself in the network so\nthat she can capture all the packets sent by Alice and send whatever\nshe wants to Bob; she can also capture all the packets sent by Bob\nand send whatever she wants to Alice. List some of the malicious\nthings Trudy can do from this position.\nP1. Design and describe an application-level protocol to be used between\nan automatic teller machine and a bank’s centralized computer. Your\nprotocol should allow a user’s card and password to be verified, the\naccount balance (which is maintained at the centralized computer) to\nbe queried, and an account withdrawal to be made (that is, money\ndisbursed to the user). Your protocol entities should be able to handle\nthe all-too-common case in which there is not enough money in the\naccount to cover the withdrawal. Specify your protocol by listing the\nmessages exchanged and the action taken by the automatic teller\nmachine or the bank’s centralized computer on transmission and\nreceipt of messages. Sketch the operation of your protocol for the\ncase of a simple withdrawal with no errors, using a diagram similar to\nthat in Figure 1.2. Explicitly state the assumptions made by your\nprotocol about the underlying end-to-end transport service.\nP2. Equation 1.1 gives a formula for the end-to-end delay of sending one\npacket of length L over N links of transmission rate R. Generalize this\nformula for sending P such packets back-to-back over the N links.\nP3. Consider an application that transmits data at a steady rate (for\nexample, the sender generates an N-bit unit of data every k time units,\nwhere k is small and fixed). Also, when such an application starts, it\nwill continue running for a relatively long period of time. Answer the\nfollowing questions, briefly justifying your answer:\na. Would a packet-switched network or a circuit-switched network\nbe more appropriate for this application? Why?\nb. Suppose that a packet-switched network is used and the only\ntraffic in this network comes from such applications as described\nabove. Furthermore, assume that the sum of the application data\nrates is less than the capacities of each and every link. Is some\nform of congestion control needed? Why?\nP4. Consider the circuit-switched network in Figure 1.13. Recall that\nthere are four circuits on each link. Label the four switches A, B, C,\nand D, going in the clockwise direction.\na. What is the maximum number of simultaneous connections that\ncan be in progress at any one time in this network?\nb. Suppose that all connections are between switches A and C. What\nis the maximum number of simultaneous connections that can be\nin progress?\nc. Suppose we want to make four connections between switches A\nand C, and another four connections between switches B and D.\nCan we route these calls through the four links to accommodate\nall eight ­connections?\nP5. Review the car-caravan analogy in Section 1.4. Assume a propagation\nspeed of 100 km/hour.\na. Suppose the caravan travels 175 km, beginning in front of one\ntollbooth, passing through a second tollbooth, and finishing just\nafter a third tollbooth. What is the end-to-end delay?\nb. Repeat (a), now assuming that there are eight cars in the caravan\ninstead of ten.\nP6. This elementary problem begins to explore propagation delay and\ntransmission delay, two central concepts in data networking. Consider\ntwo hosts, A and B, connected by a single link of rate R bps. Suppose\nthat the two hosts are separated by m meters, and suppose the\npropagation speed along the link is s meters/sec. Host A is to send a\npacket of size L bits to Host B.\nExploring propagation delay and transmission delay\na. Express the propagation delay, d\n, in terms of m and s.\nb. Determine the transmission time of the packet, d\n, in terms of L\nc. Ignoring processing and queuing delays, obtain an expression for\nthe end-to-end delay.\nd. Suppose Host A begins to transmit the packet at time t = 0. At\n, where is the last bit of the packet?\ne. Suppose d\n is greater than d\n. At time t = d\n, where is the\nfirst bit of the packet?\nf. Suppose d\n is less than d\n. At time t = d\n, where is the first\nbit of the packet?\ng. Suppose s = 2.5 · 10 , L = 1500 bytes, and R = 10 Mbps. Find the\ndistance m so that d\nP7. In this problem, we consider sending real-time voice from Host A to\nHost B over a packet-switched network (VoIP). Host A converts\nanalog voice to a digital 64 kbps bit stream on the fly. Host A then\ngroups the bits into 56-byte packets. There is one link between Hosts\nA and B; its transmission rate is 10 Mbps and its propagation delay is\n10 msec. As soon as Host A gathers a packet, it sends it to Host B. As\nsoon as Host B receives an entire packet, it converts the packet’s bits\nto an analog signal. How much time elapses from the time a bit is\ncreated (from the original analog signal at Host A) until the bit is\ndecoded (as part of the analog signal at Host B)?\nP8. Suppose users share a 10 Mbps link. Also suppose each user requires\n200 kbps when transmitting, but each user transmits only 10 percent\nof the time. (See the discussion of packet switching versus circuit\nswitching in Section 1.3.)\na. When circuit switching is used, how many users can be\nb. For the remainder of this problem, suppose packet switching is\nused. Find the probability that a given user is transmitting.\nc. Suppose there are 120 users. Find the probability that at any\ngiven time, exactly n users are transmitting simultaneously.\n(Hint: Use the binomial distribution.)\nd. Find the probability that there are 51 or more users transmitting ­-\nsimultaneously.\nP9. Consider the discussion in Section 1.3 of packet switching versus\ncircuit switching in which an example is provided with a 1 Mbps link.\nUsers are generating data at a rate of 100 kbps when busy, but are\nbusy generating data only with probability p = 0.1. Suppose that the 1\nMbps link is replaced by a 1 Gbps link.\na. What is N, the maximum number of users that can be supported\nsimultaneously under circuit switching?\nb. Now consider packet switching and a user population of M users.\nGive a formula (in terms of p, M, N) for the probability that more\nthan N users are sending data.\nP10. Consider the network illustrated in Figure 1.16. Assume the two hosts\non the left of the figure start transmitting packets of 1500 bytes at the\nsame time towards Router B. Suppose the link rates between the hosts\nand Router A is 4-Mbps. One link has a 6-ms propagation delay and\nthe other has a 2-ms propagation delay. Will queuing delay occur at\nP11. Consider the scenario in Problem P10 again, but now assume the links\nbetween the hosts and Router A have different rates R  and R  byte/s\nin addition to different propagation delays d  and d . Assume the\npacket lengths for the two hosts are of L bytes. For what values of the\npropagation delay will no queuing delay occur at Router A?\nP12. Consider a client and a server connected through one router. Assume\nthe router can start transmitting an incoming packet after receiving its\nfirst h bytes instead of the whole packet. Suppose that the link rates\nare R byte/s and that the client transmits one packet with a size of L\nbytes to the server. What is the end-to-end delay? Assume the\npropagation, processing, and queuing delays are negligible.\nGeneralize the previous result to a scenario where the client and the\nserver are interconnected by N routers.\nP13. (a) Suppose N packets arrive simultaneously to a link at which no\npackets are currently being transmitted or queued. Each packet is\nof length L and the link has transmission rate R. What is the\naverage queuing delay for the N packets?\n(b) Now suppose that N such packets arrive to the link every LN/R\nseconds. What is the average queuing delay of a packet?\nP14. Consider the queuing delay in a router buffer. Let I denote traffic\nintensity; that is, I = La/R. Suppose that the queuing delay takes the\nform IL/R (1 − I) for I < 1.\na. Provide a formula for the total delay, that is, the queuing delay\nplus the transmission delay.\nb. Plot the total delay as a function of L /R.\nP15. Let a denote the rate of packets arriving at a link in packets/sec, and\nlet µ denote the link’s transmission rate in packets/sec. Based on the\nformula for the total delay (i.e., the queuing delay plus the\ntransmission delay) derived in the previous problem, derive a formula\nfor the total delay in terms of a and µ.\nP16. Consider a router buffer preceding an outbound link. In this problem,\nyou will use Little’s formula, a famous formula from queuing theory.\nLet N denote the average number of packets in the buffer plus the\npacket being transmitted. Let a denote the rate of packets arriving at\nthe link. Let d denote the average total delay (i.e., the queuing delay\nplus the transmission delay) experienced by a packet. Little’s formula\nis N = a · d. Suppose that on average, the buffer contains 100 packets,\nand the average packet queuing delay is 20 msec. The link’s\ntransmission rate is 100 packets/sec. Using Little’s formula, what is\nthe average packet arrival rate, assuming there is no packet loss?\nP17. Consider the network illustrated in Figure 1.12. Would Equation 1.2\nhold in such a scenario? If so, under which conditions? If not, why?\n(Assume N is the number of links between a source and a destination\nin the figure.)\nP18. Perform a Traceroute between source and destination on the same\ncontinent at three different hours of the day.\nUsing Traceroute to discover network paths and measure network delay\na. Find the average and standard deviation of the round-trip delays\nat each of the three hours.\nb. Find the number of routers in the path at each of the three hours.\nDid the paths change during any of the hours?\nc. Try to identify the number of ISP networks that the Traceroute\npackets pass through from source to destination. Routers with\nsimilar names and/or similar IP addresses should be considered\nas part of the same ISP. In your experiments, do the largest delays\noccur at the peering interfaces between adjacent ISPs?\nd. Repeat the above for a source and destination on different\ncontinents. Compare the intra-continent and inter-continent\nP19. Metcalfe’s law states the value of a computer network is proportional\nto the square of the number of connected users of the system. Let n\ndenote the number of users in a computer network. Assuming each\nuser sends one message to each of the other users, how many\nmessages will be sent? Does your answer support Metcalfe’s law?\nP20. Consider the throughput example corresponding to Figure 1.20(b).\nNow suppose that there are M client-server pairs rather than 10.\nDenote R , R , and R for the rates of the server links, client links, and\nnetwork link. Assume all other links have abundant capacity and that\nthere is no other traffic in the network besides the traffic generated by\nthe M client-server pairs. Derive a general expression for throughput\nin terms of R , R , R, and M.\nP21. Assume a client and a server can connect through either network (a)\nor (b) in Figure 1.19. Assume that R  = (R  + R ) / i, for i = 1, 2, ..., N.\nIn what case will network (a) have a higher throughput than network\nP22. Consider Figure 1.19(b). Suppose that each link between the server\nand the client has a packet loss probability p, and the packet loss\nprobabilities for these links are independent. What is the probability\nthat a packet (sent by the server) is successfully received by the\nreceiver? If a packet is lost in the path from the server to the client,\nthen the server will re-transmit the packet. On average, how many\ntimes will the server re-transmit the packet in order for the client to\nsuccessfully receive the packet?\nP23. Consider Figure 1.19(a). Assume that we know the bottleneck link\nalong the path from the server to the client is the first link with rate R\nbits/sec. Suppose we send a pair of packets back to back from the\nserver to the client, and there is no other traffic on this path. Assume\neach packet of size L bits, and both links have the same propagation\na. What is the packet inter-arrival time at the destination? That is,\nhow much time elapses from when the last bit of the first packet\narrives until the last bit of the second packet arrives?\nb. Now assume that the second link is the bottleneck link (i.e., R  <\nR ). Is it possible that the second packet queues at the input queue\nof the second link? Explain. Now suppose that the server sends\nthe second packet T seconds after sending the first packet. How\nlarge must T be to ensure no queuing before the second link?\nP24. Consider a user who needs to transmit 1.5 gigabytes of data to a\nserver. The user lives in a village where only dial-up access is\navailable. As an alternative, a bus collects data from users in rural\nareas and transfer them to the Internet through a 1 Gbps link once it\ngets back to the city. The bus visits the village once a day and stops in\nfront of the user’s house just long enough to receive the data. The bus\nhas a 100 Mbps WiFi connection. Suppose the average speed of the\nbus is 60 km/h and that the distance between the village and the city\nis 150 km. What is the fastest way the user can transfer the data to the\nP25. Suppose two hosts, A and B, are separated by 20,000 kilometers and\nare connected by a direct link of R = 5 Mbps. Suppose the\npropagation speed over the link is 2.5 · 10  meters/sec.\na. Calculate the bandwidth-delay product, R · d\nb. Consider sending a file of 800,000 bits from Host A to Host B.\nSuppose the file is sent continuously as one large message. What\nis the maximum number of bits that will be in the link at any\ngiven time?\nc. Provide an interpretation of the bandwidth-delay product.\nd. What is the width (in meters) of a bit in the link? Is it longer than\na ­football field?\ne. Derive a general expression for the width of a bit in terms of the\npropagation speed s, the transmission rate R, and the length of the\nP26. Consider problem P25 but now with a link of R = 1 Gbps.\na. Calculate the bandwidth-delay product, R · d\nb. Consider sending a file of 800,000 bits from Host A to Host B.\nSuppose the file is sent continuously as one big message. What is\nthe maximum number of bits that will be in the link at any given\nc. What is the width (in meters) of a bit in the link?\nP27. Consider the scenario illustrated in Figure 1.19(a). Assume R  is 20\nMbps, R  is 10 Mbps, and the server is continuously sending traffic to\nthe client. Also assume the router between the server and the client\ncan buffer at most four messages. After how many messages sent by\nthe server will packet loss starts occurring at the router?\nP28. Generalize the result obtained in Problem P27 for the case where the\nrouter can buffer m messages.\nP29. Suppose there is a 10 Mbps microwave link between a geostationary\nsatellite and its base station on Earth. Every minute the satellite takes\na digital photo and sends it to the base station. Assume a propagation\nspeed of 2.4 · 10  meters/sec.\na. What is the propagation delay of the link?\nb. What is the bandwidth-delay product, R · d\nc. Let x denote the size of the photo. What is the minimum value of\nx for the microwave link to be continuously transmitting?\nP30. Consider the airline travel analogy in our discussion of layering in\nSection 1.5, and the addition of headers to protocol data units as they\nflow down the protocol stack. Is there an equivalent notion of header\ninformation that is added to passengers and baggage as they move\ndown the airline protocol stack?\nP31. In modern packet-switched networks, including the Internet, the\nsource host segments long, application-layer messages (for example,\nan image or a music file) into smaller packets and sends the packets\ninto the network. The receiver then reassembles the packets back into\nthe original message. We refer to this process as message\nsegmentation. Figure 1.27 illustrates the end-to-end transport of a\nmessage with and without message segmentation. Consider a message\nthat is 10  bits long that is to be sent from source to destination in\nFigure 1.27. Suppose each link in the figure is 5 Mbps. Ignore\npropagation, queuing, and processing delays.\nFigure 1.27 ♦End-to-end message transport: (a) without\nmessage segmentation; (b) with message\nsegmentation\na. Consider sending the message from source to destination without\nmessage segmentation. How long does it take to move the\nmessage from the source host to the first packet switch? Keeping\nin mind that each switch uses store-and-forward packet\nswitching, what is the total time to move the message from\nsource host to destination host?\nb. Now suppose that the message is segmented into 100 packets,\nwith each packet being 10,000 bits long. How long does it take to\nmove the first packet from source host to the first switch? When\nthe first packet is being sent from the first switch to the second\nswitch, the second packet is being sent from the source host to\nthe first switch. At what time will the second packet be fully\nreceived at the first switch?\nc. How long does it take to move the file from source host to\ndestination host when message segmentation is used? Compare\nthis result with your answer in part (a) and comment.\nd. In addition to reducing delay, what are reasons to use message ­-\nsegmentation?\ne. Discuss the drawbacks of message segmentation.\nP32. Consider Problem P31 and assume that the propagation delay is 250\nms. Recalculate the total time needed to transfer the source data with\nand without segmentation. Is segmentation more beneficial or less if\nthere is propagation delay?\nP33. Consider sending a large file of F bits from Host A to Host B. There\nare three links (and two switches) between A and B, and the links are\nuncongested (that is, no queuing delays). Host A segments the file\ninto segments of S bits each and adds 80 bits of header to each\nsegment, forming packets of L = 80 + S bits. Each link has a\ntransmission rate of R bps. Find the value of S that minimizes the\ndelay of moving the file from Host A to Host B. Disregard\npropagation delay.\nP34. Early versions of TCP combined functions for both forwarding and\nreliable delivery. How are these TCP variants located in the ISO/OSI\nprotocol stack? Why were forwarding functions later separated from\nTCP? What were the consequences?\nWireshark Lab\n“Tell me and I forget. Show me and I remember. Involve me and I\nunderstand.”\nChinese proverb\nOne’s understanding of network protocols can often be greatly deepened by\nseeing them in action and by playing around with them—observing the\nsequence of messages exchanged between two protocol entities, delving\ninto the details of protocol operation, causing protocols to perform certain\nactions, and observing these actions and their consequences. This can be\ndone in simulated scenarios or in a real network environment such as the\nInternet. The interactive animations at the textbook Web site take the first\napproach. In the Wireshark labs, we’ll take the latter approach. You’ll run\nnetwork applications in various scenarios using a computer on your desk, at\nhome, or in a lab. You’ll observe the network protocols in your computer,\ninteracting and exchanging messages with protocol entities executing\nelsewhere in the Internet. Thus, you and your computer will be an integral\npart of these live labs. You’ll observe—and you’ll learn—by doing.\nThe basic tool for observing the messages exchanged between\nexecuting protocol entities is called a packet sniffer. As the name suggests,\na packet sniffer passively copies (sniffs) messages being sent from and\nChapter 1, and as shown earlier in Figure 1.24, network-core devices do not\nfunction at the application layer but instead function at lower layers—\nspecifically at the network layer and below. This basic design—namely,\nconfining application software to the end systems—as shown in Figure 2.1,\nhas facilitated the rapid development and deployment of a vast array of\nnetwork applications.\n2.1.1 Network Application Architectures\nBefore diving into software coding, you should have a broad architectural\nplan for your application. Keep in mind that an application’s architecture is\ndistinctly different from the network architecture (e.g., the five-layer\nInternet architecture discussed in Chapter 1). From the application\ndeveloper’s perspective, the network architecture is fixed and provides a\nspecific set of services to applications. The application architecture, on\nthe other hand, is designed by the application developer and dictates how\nthe application is structured over the various end systems. In choosing the\napplication architecture, an application developer will likely draw on one of\nthe two predominant architectural paradigms used in modern network\napplications: the client-server architecture or the peer-to-peer (P2P)\narchitecture.\nIn a client-server architecture, there is an always-on host, called the\nserver, which services requests from many other hosts, called clients. A\nclassic example is the Web application for which an always-on Web server\nservices requests from browsers running on client hosts. When a Web server\nreceives a request for an object from a client host, it responds by sending\nthe requested object to the client host. Note that with the client-server\narchitecture, clients do not directly communicate with each other; for\nexample, in the Web application, two browsers do not directly\ncommunicate. Another characteristic of the client-server architecture is that\nthe server has a fixed, well-known address, called an IP address (which\nwe’ll discuss soon). Because the server has a fixed, well-known address,\nand because the server is always on, a client can always contact the server\nby sending a packet to the server’s IP address. Some of the better-known\napplications with a client-server architecture include the Web, FTP, Telnet,\nand e-mail. The client-server architecture is shown in Figure 2.2(a).\nFigure 2.2 ♦(a) Client-server architecture; (b) P2P architecture\nOften in a client-server application, a single-server host is incapable of\nkeeping up with all the requests from clients. For example, a popular social-\nnetworking site can quickly become overwhelmed if it has only one server\nhandling all of its requests. For this reason, a data center, housing a large\nnumber of hosts, is often used to create a powerful virtual server. The most\npopular Internet services—such as search engines (e.g., Google, Bing,\nBaidu), Internet commerce (e.g., Amazon, eBay, Alibaba), Web-based e-\nmail (e.g., Gmail and Yahoo Mail), social media (e.g., Facebook, Instagram,\nTwitter, and WeChat)—run in one or more data centers. As discussed in\nSection 1.3.3, Google has 19 data centers distributed around the world,\nwhich collectively handle search, YouTube, Gmail, and other services. A\ndata center can have hundreds of thousands of servers, which must be\npowered and maintained. Additionally, the service providers must pay\nrecurring interconnection and bandwidth costs for sending data from their\ndata centers.\nIn a P2P architecture, there is minimal (or no) reliance on dedicated\nservers in data centers. Instead the application exploits direct\ncommunication between pairs of intermittently connected hosts, called\npeers. The peers are not owned by the service provider, but are instead\ndesktops and laptops controlled by users, with most of the peers residing in\nhomes, universities, and offices. Because the peers communicate without\npassing through a dedicated server, the architecture is called peer-to-peer.\nAn example of a popular P2P application is the file-sharing application\nBitTorrent.\nOne of the most compelling features of P2P architectures is their self-\nscalability. For example, in a P2P file-sharing application, although each\npeer ­generates workload by requesting files, each peer also adds service\ncapacity to the system by distributing files to other peers. P2P architectures\nare also cost effective, since they normally don’t require significant server\ninfrastructure and server bandwidth (in contrast with clients-server designs\nwith datacenters). However, P2P applications face challenges of security,\nperformance, and reliability due to their highly ­decentralized structure.\n2.1.2 Processes Communicating\nBefore building your network application, you also need a basic\nunderstanding of how the programs, running in multiple end systems,\ncommunicate with each other. In the jargon of operating systems, it is not\nactually programs but processes that communicate. A process can be\nthought of as a program that is running within an end system. When\nprocesses are running on the same end system, they can communicate with\neach other with interprocess communication, using rules that are governed\nby the end system’s operating system. But in this book, we are not\nparticularly interested in how processes in the same host communicate, but\ninstead in how processes running on different hosts (with potentially\ndifferent operating systems) communicate.\nProcesses on two different end systems communicate with each other\nby exchanging messages across the computer network. A sending process\ncreates and sends messages into the network; a receiving process receives\nthese messages and possibly responds by sending messages back. Figure\n2.1 illustrates that processes communicating with each other reside in the\napplication layer of the five-layer protocol stack.\nClient and Server Processes\nA network application consists of pairs of processes that send messages to\neach other over a network. For example, in the Web application a client\nbrowser process exchanges messages with a Web server process. In a P2P\nfile-sharing system, a file is transferred from a process in one peer to a\nprocess in another peer. For each pair of communicating processes, we\ntypically label one of the two processes as the client and the other process\nas the server. With the Web, a browser is a client process and a Web server\nis a server process. With P2P file sharing, the peer that is downloading the\nfile is labeled as the client, and the peer that is uploading the file is labeled\nas the server.\nYou may have observed that in some applications, such as in P2P file\nsharing, a process can be both a client and a server. Indeed, a process in a\nP2P file-sharing system can both upload and download files. Nevertheless,\nin the context of any given communication session between a pair of\nprocesses, we can still label one process as the client and the other process\nas the server. We define the client and server processes as follows:\nIn the context of a communication session between a pair of processes,\nthe process that initiates the communication (that is, initially contacts\nthe other process at the beginning of the session) is labeled as the\nclient. The process that waits to be contacted to begin the session is the\nIn the Web, a browser process initializes contact with a Web server\nprocess; hence the browser process is the client and the Web server process\nis the server. In P2P file sharing, when Peer A asks Peer B to send a specific\nfile, Peer A is the client and Peer B is the server in the context of this\nspecific communication session. When there’s no confusion, we’ll\nsometimes also use the terminology “client side and server side of an\napplication.” At the end of this chapter, we’ll step through simple code for\nboth the client and server sides of network applications.\nThe Interface Between the Process and the Computer Network\nAs noted above, most applications consist of pairs of communicating\nprocesses, with the two processes in each pair sending messages to each\nother. Any message sent from one process to another must go through the\nunderlying network. A process sends messages into, and receives messages\nfrom, the network through a software interface called a socket. Let’s\nconsider an analogy to help us understand processes and sockets. A process\nis analogous to a house and its socket is analogous to its door. When a\nprocess wants to send a message to another process on another host, it\nshoves the message out its door (socket). This sending process assumes that\nthere is a transportation infrastructure on the other side of its door that will\ntransport the message to the door of the destination process. Once the\nmessage arrives at the destination host, the message passes through the\nreceiving process’s door (socket), and the receiving process then acts on the\nFigure 2.3 illustrates socket communication between two processes that\ncommunicate over the Internet. (Figure 2.3 assumes that the underlying\ntransport protocol used by the processes is the Internet’s TCP protocol.) As\nshown in this figure, a socket is the interface between the application layer\nand the transport layer within a host. It is also referred to as the Application\nProgramming Interface (API) between the application and the network,\nsince the socket is the programming interface with which network\napplications are built. The application developer has control of everything\non the application-layer side of the socket but has little control of the\ntransport-layer side of the socket. The only control that the application\ndeveloper has on the transport-layer side is (1) the choice of transport\nprotocol and (2) perhaps the ability to fix a few transport-layer parameters\nsuch as maximum buffer and maximum segment sizes (to be covered in\nChapter 3). Once the application developer chooses a transport protocol (if\na choice is available), the application is built using the transport-layer\nservices provided by that protocol. We’ll explore sockets in some detail in\nSection 2.7.\nFigure 2.3 ♦Application processes, sockets, and underlying\ntransport protocol\nAddressing Processes\nIn order to send postal mail to a particular destination, the destination needs\nto have an address. Similarly, in order for a process running on one host to\nsend packets to a process running on another host, the receiving process\nneeds to have an address. To identify the receiving process, two pieces of\ninformation need to be specified: (1)  the address of the host and (2) an\nidentifier that specifies the receiving process in the destination host.\nIn the Internet, the host is identified by its IP address. We’ll discuss IP\naddresses in great detail in Chapter 4. For now, all we need to know is that\nan IP address is a 32-bit quantity that we can think of as uniquely\nidentifying the host. In addition to knowing the address of the host to which\na message is destined, the sending process must also identify the receiving\nprocess (more specifically, the receiving socket) running in the host. This\ninformation is needed because in general a host could be running many\nnetwork applications. A destination port number serves this purpose.\nPopular applications have been assigned specific port numbers. For\nexample, a Web server is identified by port number 80. A mail server\nprocess (using the SMTP protocol) is identified by port number 25. A list of\nwell-known port numbers for all Internet standard protocols can be found at\nwww.iana.org. We’ll examine port numbers in detail in Chapter 3.\n2.1.3 Transport Services Available to Applications\nRecall that a socket is the interface between the application process and the\ntransport-layer protocol. The application at the sending side pushes\nmessages through the socket. At the other side of the socket, the transport-\nlayer protocol has the responsibility of getting the messages to the socket of\nthe receiving process.\nMany networks, including the Internet, provide more than one\ntransport-layer protocol. When you develop an application, you must\nchoose one of the available transport-layer protocols. How do you make\nthis choice? Most likely, you would study the services provided by the\navailable transport-layer protocols, and then pick the protocol with the\nservices that best match your application’s needs. The situation is similar to\nchoosing either train or airplane transport for travel between two cities. You\nhave to choose one or the other, and each transportation mode offers\ndifferent services. (For example, the train offers downtown pickup and\ndrop-off, whereas the plane offers shorter travel time.)\nWhat are the services that a transport-layer protocol can offer to\napplications invoking it? We can broadly classify the possible services\nalong four dimensions: reliable data transfer, throughput, timing, and\nReliable Data Transfer\nAs discussed in Chapter 1, packets can get lost within a computer network.\nFor example, a packet can overflow a buffer in a router, or can be discarded\nby a host or router after having some of its bits corrupted. For many\napplications—such as electronic mail, file transfer, remote host access, Web\ndocument transfers, and financial applications—data loss can have\ndevastating consequences (in the latter case, for either the bank or the\ncustomer!). Thus, to support these applications, something has to be done to\nguarantee that the data sent by one end of the application is delivered\ncorrectly and completely to the other end of the application. If a protocol\nprovides such a guaranteed data delivery service, it is said to provide\nreliable data transfer. One important service that a transport-layer\nprotocol can potentially provide to an application is process-to-process\nreliable data transfer. When a transport protocol provides this service, the\nsending process can just pass its data into the socket and know with\ncomplete confidence that the data will arrive without errors at the receiving\nWhen a transport-layer protocol doesn’t provide reliable data transfer,\nsome of the data sent by the sending process may never arrive at the\nreceiving process. This may be acceptable for loss-tolerant applications,\nmost notably multimedia applications such as conversational audio/video\nthat can tolerate some amount of data loss. In these multimedia\napplications, lost data might result in a small glitch in the audio/video—not\na crucial impairment.\nIn Chapter 1, we introduced the concept of available throughput, which, in\nthe context of a communication session between two processes along a\nnetwork path, is the rate at which the sending process can deliver bits to the\nreceiving process. Because other sessions will be sharing the bandwidth\nalong the network path, and because these other sessions will be coming\nand going, the available throughput can fluctuate with time. These\nobservations lead to another natural service that a transport-layer protocol\ncould provide, namely, guaranteed available throughput at some specified\nrate. With such a service, the application could request a guaranteed\nthroughput of r bits/sec, and the transport protocol would then ensure that\nthe available throughput is always at least r bits/sec. Such a guaranteed\nthroughput service would appeal to many applications. For example, if an\nInternet telephony application encodes voice at 32 kbps, it needs to send\ndata into the network and have data delivered to the receiving application at\nthis rate. If the transport protocol cannot provide this throughput, the\napplication would need to encode at a lower rate (and receive enough\nthroughput to sustain this lower coding rate) or may have to give up, since\nreceiving, say, half of the needed throughput is of little or no use to this\nInternet telephony application. Applications that have throughput\nrequirements are said to be bandwidth-sensitive applications. Many\ncurrent multimedia applications are bandwidth sensitive, although some\nmultimedia applications may use adaptive coding techniques to encode\ndigitized voice or video at a rate that matches the currently available\nthroughput.\nWhile bandwidth-sensitive applications have specific throughput\nrequirements, elastic applications can make use of as much, or as little,\nthroughput as happens to be available. Electronic mail, file transfer, and\nWeb transfers are all elastic applications. Of course, the more throughput,\nthe better. There’s an adage that says that one cannot be too rich, too thin, or\nhave too much throughput!\nA transport-layer protocol can also provide timing guarantees. As with\nthroughput guarantees, timing guarantees can come in many shapes and\nforms. An example guarantee might be that every bit that the sender pumps\ninto the socket arrives at the receiver’s socket no more than 100 msec later.\nSuch a service would be appealing to interactive real-time applications,\nsuch as Internet telephony, virtual environments, teleconferencing, and\nmultiplayer games, all of which require tight timing constraints on data\ndelivery in order to be effective, see [Gauthier 1999; Ramjee 1994]. Long\ndelays in Internet telephony, for example, tend to result in unnatural pauses\nin the conversation; in a multiplayer game or virtual interactive\nenvironment, a long delay between taking an action and seeing the response\nfrom the environment (for example, from another player at the end of an\nend-to-end connection) makes the application feel less realistic. For non-\nreal-time applications, lower delay is always preferable to higher delay, but\nno tight constraint is placed on the end-to-end delays.\nFinally, a transport protocol can provide an application with one or more\nsecurity services. For example, in the sending host, a transport protocol can\nencrypt all data transmitted by the sending process, and in the receiving\nhost, the transport-layer protocol can decrypt the data before delivering the\ndata to the receiving process. Such a service would provide confidentiality\nbetween the two processes, even if the data is somehow observed between\nsending and receiving processes. A transport protocol can also provide other\nsecurity services in addition to confidentiality, including data integrity and\nend-point authentication, topics that we’ll cover in detail in Chapter 8.\n2.1.4 Transport Services Provided by the Internet\nUp until this point, we have been considering transport services that a\ncomputer network could provide in general. Let’s now get more specific\nand examine the type of transport services provided by the Internet. The\nInternet (and, more generally, TCP/IP networks) makes two transport\nprotocols available to applications, UDP and TCP. When you (as an\napplication developer) create a new network application for the Internet,\none of the first decisions you have to make is whether to use UDP or TCP.\nEach of these protocols offers a different set of services to the invoking\napplications. Figure 2.4 shows the service requirements for some selected\napplications.\nFigure 2.4 ♦Requirements of selected network applications\nTCP Services\nThe TCP service model includes a connection-oriented service and a\nreliable data transfer service. When an application invokes TCP as its\ntransport protocol, the application receives both of these services from TCP.\nConnection-oriented service. TCP has the client and server exchange\ntransport-layer control information with each other before the\napplication-level messages begin to flow. This so-called handshaking\nprocedure alerts the client and server, allowing them to prepare for an\nonslaught of packets. After the handshaking phase, a TCP connection\nis said to exist between the sockets of the two processes. The\nconnection is a full-duplex connection in that the two processes can\nsend messages to each other over the connection at the same time.\nWhen the application finishes sending messages, it must tear down the\nconnection. In Chapter 3, we’ll discuss connection-oriented service in\ndetail and examine how it is implemented.\nReliable data transfer service. The communicating processes can rely\non TCP to deliver all data sent without error and in the proper order.\nWhen one side of the application passes a stream of bytes into a socket,\nit can count on TCP to deliver the same stream of bytes to the receiving\nsocket, with no missing or duplicate bytes.\nTCP also includes a congestion-control mechanism, a service for the ­-\ngeneral welfare of the Internet rather than for the direct benefit of the\ncommunicating processes. The TCP congestion-control mechanism throttles\na sending process (client or server) when the network is congested between\nsender and receiver. As we will see in Chapter 3, TCP congestion control\nalso attempts to limit each TCP connection to its fair share of network\nUDP Services\nUDP is a no-frills, lightweight transport protocol, providing minimal\nservices. UDP is connectionless, so there is no handshaking before the two\nprocesses start to communicate. UDP provides an unreliable data transfer\nservice—that is, when a process sends a message into a UDP socket, UDP\nprovides no guarantee that the message will ever reach the receiving\nprocess. Furthermore, messages that do arrive at the receiving process may\narrive out of order.\nNeither TCP nor UDP provides any encryption—the data that the sending process\npasses into its socket is the same data that travels over the network to the destination\nprocess. So, for example, if the sending process sends a password in cleartext (i.e.,\nunencrypted) into its socket, the cleartext password will travel over all the links\nbetween sender and receiver, potentially getting sniffed and discovered at any of the\nintervening links. Because privacy and other security issues have become critical for\nmany applications, the Internet community has developed an enhancement for TCP,\ncalled Transport Layer Security (TLS) [RFC 5246]. TCP-enhanced-with-TLS not only\ndoes everything that traditional TCP does but also provides critical process-to-process\nsecurity services, including encryption, data integrity, and end-point authentication. We\nemphasize that TLS is not a third Internet transport protocol, on the same level as TCP\nand UDP, but instead is an enhancement of TCP, with the enhancements being\nimplemented in the application layer. In particular, if an application wants to use the\nservices of TLS, it needs to include TLS code (existing, highly optimized libraries and\nclasses) in both the client and server sides of the application. TLS has its own socket\nAPI that is similar to the traditional TCP socket API. When an application uses TLS, the\nsending process passes cleartext data to the TLS socket; TLS in the sending host then\nencrypts the data and passes the encrypted data to the TCP socket. The encrypted\ndata travels over the Internet to the TCP socket in the receiving process. The receiving\nsocket passes the encrypted data to TLS, which decrypts the data. Finally, TLS passes\nthe cleartext data through its TLS socket to the receiving process. We’ll cover TLS in\nsome detail in Chapter 8.\nUDP does not include a congestion-control mechanism, so the sending\nside of UDP can pump data into the layer below (the network layer) at any\nrate it pleases. (Note, however, that the actual end-to-end throughput may\nbe less than this rate due to the limited transmission capacity of intervening\nlinks or due to congestion).\nServices Not Provided by Internet Transport Protocols\nWe have organized transport protocol services along four dimensions:\nreliable data transfer, throughput, timing, and security. Which of these\nservices are provided by TCP and UDP? We have already noted that TCP\nprovides reliable end-to-end data transfer. And we also know that TCP can\nbe easily enhanced at the application layer with TLS to provide security\nservices. But in our brief description of TCP and UDP, conspicuously\nmissing was any mention of throughput or timing guarantees—services not\nprovided by today’s Internet transport protocols. Does this mean that time-\nsensitive applications such as Internet telephony cannot run in today’s\nInternet? The answer is clearly no—the Internet has been hosting time-\nsensitive applications for many years. These applications often work fairly\nwell because they have been designed to cope, to the greatest extent\npossible, with this lack of guarantee. Nevertheless, clever design has its\nlimitations when delay is excessive, or the end-to-end throughput is limited.\nIn summary, today’s Internet can often provide satisfactory service to time-\nsensitive applications, but it cannot provide any timing or throughput\nguarantees.\nFigure 2.5 indicates the transport protocols used by some popular\nInternet applications. We see that e-mail, remote terminal access, the Web,\nand file transfer all use TCP. These applications have chosen TCP primarily\nbecause TCP provides reliable data transfer, guaranteeing that all data will\neventually get to its destination. Because Internet telephony applications\n(such as Skype) can often tolerate some loss but require a minimal rate to\nbe effective, developers of Internet telephony applications usually prefer to\nrun their applications over UDP, thereby circumventing TCP’s congestion\ncontrol mechanism and packet overheads. But because many firewalls are\nconfigured to block (most types of) UDP traffic, Internet telephony\napplications often are designed to use TCP as a backup if UDP\ncommunication fails.\nFigure 2.5 ♦Popular Internet applications, their application-layer\nprotocols, and their underlying transport protocols\n2.1.5 Application-Layer Protocols\nWe have just learned that network processes communicate with each other\nby sending messages into sockets. But how are these messages structured?\nWhat are the meanings of the various fields in the messages? When do the\nprocesses send the messages? These questions bring us into the realm of\napplication-layer protocols. An application-layer protocol defines how an\napplication’s processes, running on different end systems, pass messages to\neach other. In particular, an application-layer protocol defines:\nThe types of messages exchanged, for example, request messages and\nresponse messages\nThe syntax of the various message types, such as the fields in the\nmessage and how the fields are delineated\nThe semantics of the fields, that is, the meaning of the information in\nRules for determining when and how a process sends messages and\nresponds to messages\nSome application-layer protocols are specified in RFCs and are therefore in\nthe public domain. For example, the Web’s application-layer protocol,\nHTTP (the HyperText Transfer Protocol [RFC 7230]), is available as an\nRFC. If a browser developer follows the rules of the HTTP RFC, the\nbrowser will be able to retrieve Web pages from any Web server that has\nalso followed the rules of the HTTP RFC. Many other application-layer\nprotocols are proprietary and intentionally not available in the public\ndomain. For example, Skype uses proprietary application-layer protocols.\nIt is important to distinguish between network applications and\napplication-layer protocols. An application-layer protocol is only one piece\nof a network application (albeit, a very important piece of the application\nfrom our point of view!). Let’s look at a couple of examples. The Web is a\nclient-server application that allows users to obtain documents from Web\nservers on demand. The Web application consists of many components,\nincluding a standard for document formats (that is, HTML), Web browsers\n(for example, Chrome and Microsoft Internet Explorer), Web servers\n(for  example, Apache and Microsoft servers), and an application-layer\nprotocol. The Web’s application-layer protocol, HTTP, defines the format\nand sequence of  messages exchanged between browser and Web server.\nThus, HTTP is only one  piece (albeit, an important piece) of the Web\napplication. As another example, we’ll see in Section 2.6 that Netflix’s\nvideo service also has many components, including servers that store and\ntransmit videos, other servers that manage billing and other ­client functions,\nclients (e.g., the Netflix app on your smartphone, tablet, or ­computer), and\nan application-level DASH protocol defines the format and sequence of\nmessages exchanged between a Netflix server and client. Thus, DASH is\nonly one piece (albeit, an important piece) of the Netflix application.\n2.1.6 Network Applications Covered in This Book\nNew applications are being developed every day. Rather than covering a\nlarge number of Internet applications in an encyclopedic manner, we have\nchosen to focus on a small number of applications that are both pervasive\nand important. In this chapter, we discuss five important applications: the\nWeb, electronic mail, directory service, video streaming, and P2P\napplications. We first discuss the Web, not only because it is an enormously\npopular application, but also because its application-layer protocol, HTTP,\nis straightforward and easy to understand. We then discuss electronic mail,\nthe Internet’s first killer application. E-mail is more complex than the Web\nin the sense that it makes use of not one but several application-layer\nprotocols. After e-mail, we cover DNS, which provides a directory service\nfor the Internet. Most users do not interact with DNS directly; instead, users\ninvoke DNS indirectly through other applications (including the Web, file\ntransfer, and electronic mail). DNS illustrates nicely how a piece of core\nnetwork functionality (network-name to network-address translation) can be\nimplemented at the application layer in the Internet. We then discuss P2P\nfile sharing applications, and complete our application study by discussing\nvideo streaming on demand, including distributing stored video over\ncontent distribution networks.\n2.2 The Web and HTTP\nUntil the early 1990s, the Internet was used primarily by researchers,\nacademics, and university students to log in to remote hosts, to transfer files\nfrom local hosts to remote hosts and vice versa, to receive and send news,\nand to receive and send electronic mail. Although these applications were\n(and continue to be) extremely useful, the Internet was essentially unknown\noutside of the academic and research communities. Then, in the early\n1990s, a major new application arrived on the scene—the World Wide Web\n[Berners-Lee 1994]. The Web was the first Internet application that caught\nthe general public’s eye. It dramatically changed how people interact inside\nand outside their work environments. It elevated the Internet from just one\nof many data networks to essentially the one and only data network.\nPerhaps what appeals the most to users is that the Web operates on\ndemand. Users receive what they want, when they want it. This is unlike\ntraditional broadcast radio and television, which force users to tune in when\nthe content provider makes the content available. In addition to being\navailable on demand, the Web has many other wonderful features that\npeople love and cherish. It is enormously easy for any individual to make\ninformation available over the Web—everyone can become a publisher at\nextremely low cost. Hyperlinks and search engines help us navigate through\nan ocean of information. Photos and videos stimulate our senses. Forms,\nJavaScript, video, and many other devices enable us to interact with pages\nand sites. And the Web and its protocols serve as a platform for YouTube,\nWeb-based e-mail (such as Gmail), and most mobile Internet applications,\nincluding Instagram and Google Maps.\n2.2.1 Overview of HTTP\nThe HyperText Transfer Protocol (HTTP), the Web’s application-layer\nprotocol, is at the heart of the Web. It is defined in [RFC 1945], [RFC 7230]\nand [RFC 7540]. HTTP is implemented in two programs: a client program\nand a server program. The client program and server program, executing on\ndifferent end systems, talk to each other by exchanging HTTP messages.\nHTTP defines the structure of these messages and how the client and server\nexchange the messages. Before explaining HTTP in detail, we should\nreview some Web terminology.\nA Web page (also called a document) consists of objects. An object is ­-\nsimply a file—such as an HTML file, a JPEG image, a Javascrpt file, a CCS\nstyle sheet file, or a video clip—that is addressable by a single URL. Most\nWeb pages consist of a base HTML file and several referenced objects. For\nexample, if a Web page contains HTML text and five JPEG images, then\nthe Web page has six objects: the base HTML file plus the five images. The\nTCP congestion control, discussed in detail in Chapter 3, also provides\nbrowsers an unintended incentive to use multiple parallel TCP connections\nrather than a single persistent connection. Very roughly speaking, TCP\ncongestion control aims to give each TCP connection sharing a bottleneck\nlink an equal share of the available bandwidth of that link; so if there are n\nTCP connections operating over a bottleneck link, then each connection\napproximately gets 1/nth of the bandwidth. By opening multiple parallel\nTCP connections to transport a single Web page, the browser can “cheat”\nand grab a larger portion of the link bandwidth. Many HTTP/1.1 browsers\nopen up to six parallel TCP connections not only to circumvent HOL\nblocking but also to obtain more bandwidth.\nOne of the primary goals of HTTP/2 is to get rid of (or at least reduce\nthe number of) parallel TCP connections for transporting a single Web page.\nThis not only reduces the number of sockets that need to be open and\nmaintained at servers, but also allows TCP congestion control to operate as\nintended. But with only one TCP connection to transport a Web page,\nHTTP/2 requires carefully designed mechanisms to avoid HOL blocking.\nHTTP/2 Framing\nThe HTTP/2 solution for HOL blocking is to break each message into small\nframes, and interleave the request and response messages on the same TCP\nconnection. To understand this, consider again the example of a Web page\nconsisting of one large video clip and, say, 8 smaller objects. Thus the\nserver will receive 9 concurrent requests from any browser wanting to see\nthis Web page. For each of these requests, the server needs to send 9\ncompeting HTTP response messages to the browser. ­Suppose all frames are\nof fixed length, the video clip consists of 1000 frames, and each of the\nsmaller objects consists of two frames. With frame interleaving, after\nsending one frame from the video clip, the first frames of each of the small\nobjects are sent. Then after sending the second frame of the video clip, the\nlast frames of each of the small objects are sent. Thus, all of the smaller\nobjects are sent after sending a total of 18 frames. If interleaving were not\nused, the smaller objects would be sent only after sending 1016 frames.\nThus the HTTP/2 framing mechanism can significantly decrease user-\nperceived delay.\nThe ability to break down an HTTP message into independent frames,\ninterleave them, and then reassemble them on the other end is the single\nmost important enhancement of HTTP/2. The framing is done by the\nframing sub-layer of the HTTP/2 protocol. When a server wants to send an\nHTTP response, the response is processed by the framing sub-layer, where\nit is broken down into frames. The header field of the response becomes one\nframe, and the body of the message is broken down into one for more\nadditional frames. The frames of the response are then interleaved by the\nframing sub-layer in the server with the frames of other responses and sent\nover the single persistent TCP connection. As the frames arrive at the client,\nthey are first reassembled into the original response messages at the framing\nsub-layer and then processed by the browser as usual. Similarly, a client’s\nHTTP requests are broken into frames and interleaved.\nIn addition to breaking down each HTTP message into independent\nframes, the framing sublayer also binary encodes the frames. Binary\nprotocols are more efficient to parse, lead to slightly smaller frames, and are\nless error-prone.\nResponse Message Prioritization and Server Pushing\nMessage prioritization allows developers to customize the relative priority\nof requests to better optimize application performance. As we just learned,\nthe framing sub-layer organizes messages into parallel streams of data\ndestined to the same requestor. When a client sends concurrent requests to a\nserver, it can prioritize the responses it is requesting by assigning a weight\nbetween 1 and 256 to each message. The higher number indicates higher\npriority. Using these weights, the server can send first the frames for the\nresponses with the highest priority. In addition to this, the client also states\neach message’s dependency on other messages by specifying the ID of the\nmessage on which it depends.\nAnother feature of HTTP/2 is the ability for a server to send multiple\nresponses for a single client request. That is, in addition to the response to\nthe original request, the server can push additional objects to the client,\nwithout the client having to request each one. This is possible since the\nHTML base page indicates the objects that will be needed to fully render\nthe Web page. So instead of waiting for the HTTP requests for these\nobjects, the server can analyze the HTML page, identify the objects that are\nneeded, and send them to the client before receiving explicit requests for\nthese objects. Server push eliminates the extra latency due to waiting for the\nQUIC, discussed in Chapter 3, is a new “transport” protocol that is\nimplemented in the application layer over the bare-bones UDP protocol.\nQUIC has several features that are desirable for HTTP, such as message\nmultiplexing (interleaving), per-stream flow control, and low-latency\nconnection establishment. HTTP/3 is yet a new HTTP protocol that is\ndesigned to operate over QUIC. As of 2020, HTTP/3 is described in\nInternet drafts and has not yet been fully standardized. Many of the HTTP/2\nfeatures (such as message interleaving) are subsumed by QUIC, allowing\nfor a simpler, streamlined design for HTTP/3.\n2.3 Electronic Mail in the Internet\nElectronic mail has been around since the beginning of the Internet. It was\nthe most popular application when the Internet was in its infancy [Segaller\n1998], and has become more elaborate and powerful over the years. It\nremains one of the Internet’s most important and utilized applications.\nAs with ordinary postal mail, e-mail is an asynchronous communication\nmedium—people send and read messages when it is convenient for them,\nwithout having to coordinate with other people’s schedules. In contrast with\npostal mail, electronic mail is fast, easy to distribute, and inexpensive.\nModern e-mail has many powerful features, including messages with\nattachments, hyperlinks, HTML-formatted text, and embedded photos.\nIn this section, we examine the application-layer protocols that are at\nthe heart of Internet e-mail. But before we jump into an in-depth discussion\nof these protocols, let’s take a high-level view of the Internet mail system\nand its key components.\nFigure 2.14 presents a high-level view of the Internet mail system. We\nsee from this diagram that it has three major components: user agents, mail\nservers, and the Simple Mail Transfer Protocol (SMTP). We now\ndescribe each of these components in the context of a sender, Alice, sending\nan e-mail message to a recipient, Bob. User agents allow users to read,\nreply to, forward, save, and compose ­messages. Examples of user agents for\ne-mail include Microsoft Outlook, Apple Mail, Web-based Gmail, the\nGmail App running in a smartphone, and so on. When Alice is finished\ncomposing her message, her user agent sends the message to her mail\nserver, where the message is placed in the mail server’s outgoing message\nqueue. When Bob wants to read a message, his user agent retrieves the\nmessage from his mailbox in his mail server.\nFigure 2.14 ♦A high-level view of the Internet e-mail system\nMail servers form the core of the e-mail infrastructure. Each recipient,\nsuch as Bob, has a mailbox located in one of the mail servers. Bob’s\nmailbox manages and maintains the messages that have been sent to him. A\ntypical message starts its journey in the sender’s user agent, then travels to\nthe sender’s mail server, and then travels to the recipient’s mail server,\nwhere it is deposited in the recipient’s mailbox. When Bob wants to access\nthe messages in his mailbox, the mail server containing his mailbox\nauthenticates Bob (with his username and password). Alice’s mail server\nmust also deal with failures in Bob’s mail server. If Alice’s server cannot\ndeliver mail to Bob’s server, Alice’s server holds the message in a message\nqueue and attempts to transfer the message later. Reattempts are often done\nevery 30 minutes or so; if there is no success after several days, the server\nremoves the message and notifies the sender (Alice) with an e-mail\nSMTP is the principal application-layer protocol for Internet electronic\nmail. It uses the reliable data transfer service of TCP to transfer mail from\nthe sender’s mail server to the recipient’s mail server. As with most\napplication-layer protocols, SMTP has two sides: a client side, which\nexecutes on the sender’s mail server, and a server side, which executes on\nthe recipient’s mail server. Both the client and server sides of SMTP run on\nevery mail server. When a mail server sends mail to other mail servers, it\nacts as an SMTP client. When a mail server receives mail from other mail\nservers, it acts as an SMTP server.\nSMTP, defined in RFC 5321, is at the heart of Internet electronic mail. As\nmentioned above, SMTP transfers messages from senders’ mail servers to\nthe recipients’ mail servers. SMTP is much older than HTTP. (The original\nSMTP RFC dates back to 1982, and SMTP was around long before that.)\nAlthough SMTP has numerous wonderful qualities, as evidenced by its\nubiquity in the Internet, it is nevertheless a legacy technology that possesses\ncertain archaic characteristics. For example, it restricts the body (not just\nthe headers) of all mail messages to simple 7-bit ASCII. This restriction\nmade sense in the early 1980s when transmission capacity was scarce and\nno one was e-mailing large attachments or large image, audio, or video\nfiles. But today, in the multimedia era, the 7-bit ASCII restriction is a bit of\na pain—it requires binary multimedia data to be encoded to ASCII before\nbeing sent over SMTP; and it requires the corresponding ASCII message to\nbe decoded back to binary after SMTP transport. Recall from Section 2.2\nthat HTTP does not require multimedia data to be ASCII encoded before\nTo illustrate the basic operation of SMTP, let’s walk through a common\nscenario. Suppose Alice wants to send Bob a simple ASCII message.\n1. Alice invokes her user agent for e-mail, provides Bob’s e-mail address\n(for example, bob@someschool.edu), composes a message, and\ninstructs the user agent to send the message.\n2. Alice’s user agent sends the message to her mail server, where it is\nplaced in a message queue.\n3. The client side of SMTP, running on Alice’s mail server, sees the\nmessage in the message queue. It opens a TCP connection to an SMTP\nserver, running on Bob’s mail server.\n4. After some initial SMTP handshaking, the SMTP client sends Alice’s\nmessage into the TCP connection.\n5. At Bob’s mail server, the server side of SMTP receives the message.\nBob’s mail server then places the message in Bob’s mailbox.\n6. Bob invokes his user agent to read the message at his convenience.\nThe scenario is summarized in Figure 2.15.\nFigure 2.15 ♦Alice sends a message to Bob\nIt is important to observe that SMTP does not normally use\nintermediate mail servers for sending mail, even when the two mail servers\nare located at opposite ends of the world. If Alice’s server is in Hong Kong\nand Bob’s server is in St. Louis, the TCP connection is a direct connection\nbetween the Hong Kong and St. Louis servers. In particular, if Bob’s mail\nserver is down, the message remains in Alice’s mail server and waits for a\nnew attempt—the message does not get placed in some intermediate mail\nLet’s now take a closer look at how SMTP transfers a message from a\nsending mail server to a receiving mail server. We will see that the SMTP\nprotocol has many similarities with protocols that are used for face-to-face\nhuman interaction. First, the client SMTP (running on the sending mail\nserver host) has TCP establish a connection to port 25 at the server SMTP\n(running on the receiving mail server host). If the server is down, the client\ntries again later. Once this connection is established, the server and client\nperform some application-layer handshaking—just as humans often\nintroduce themselves before transferring information from one to another,\nSMTP clients and servers introduce themselves before transferring\ninformation. During this SMTP handshaking phase, the SMTP client\nindicates the e-mail address of the sender (the person who generated the\nmessage) and the e-mail address of the recipient. Once the SMTP client and\nserver have introduced themselves to each other, the client sends the\nmessage. SMTP can count on the reliable data transfer service of TCP to get\nthe message to the server without errors. The client then repeats this process\nover the same TCP connection if it has other messages to send to the server;\notherwise, it instructs TCP to close the connection.\nLet’s next take a look at an example transcript of messages exchanged\nbetween an SMTP client (C) and an SMTP server (S). The hostname of the\nclient is crepes.fr and the hostname of the server is hamburger.edu.\nWe discuss IP addresses in some detail in Chapter 4, but it is useful to\nsay a few brief words about them now. An IP address consists of four bytes\nand has a rigid hierarchical structure. An IP address looks like\n121.7.106.83, where each period separates one of the bytes expressed\nin decimal notation from 0 to 255. An IP address is hierarchical because as\nwe scan the address from left to right, we obtain more and more specific\ninformation about where the host is located in the Internet (that is, within\nwhich network, in the network of networks). Similarly, when we scan a\npostal address from bottom to top, we obtain more and more specific\ninformation about where the addressee is located.\n2.4.1 Services Provided by DNS\nWe have just seen that there are two ways to identify a host—by a hostname\nand by an IP address. People prefer the more mnemonic hostname\nidentifier, while routers prefer fixed-length, hierarchically structured IP\nDHCP, which is discussed in Chapter 4). You can easily determine the IP\naddress of your local DNS server by accessing network status windows in\nWindows or UNIX. A host’s local DNS server is typically “close to” the\nhost. For an institutional ISP, the local DNS server may be on the same\nLAN as the host; for a residential ISP, it is typically separated from the host\nby no more than a few routers. When a host makes a DNS query, the query\nis sent to the local DNS server, which acts a proxy, ­forwarding the query\ninto the DNS server hierarchy, as we’ll discuss in more detail below.\nLet’s take a look at a simple example. Suppose the host\ncse.nyu.edu desires the IP address of gaia.cs.umass.edu. Also\nsuppose that NYU’s local DNS server for cse.nyu.edu is called\ndns.nyu.edu \nauthoritative \ngaia.cs.umass.edu is called dns.umass.edu. As shown in ­Figure\n2.19, the host cse.nyu.edu first sends a DNS query message to its local\nDNS server, dns.nyu.edu. The query message contains the hostname to\nbe translated, namely, gaia.cs.umass.edu. The local DNS server\nforwards the query message to a root DNS server. The root DNS server\ntakes note of the edu suffix and returns to the local DNS server a list of IP\naddresses for TLD servers responsible for edu. The local DNS server then\nresends the query message to one of these TLD servers. The TLD server\ntakes note of the umass.edu suffix and responds with the IP address of\nthe authoritative DNS server for the University of Massachusetts, namely,\ndns.umass.edu. Finally, the local DNS server resends the query\nmessage directly to dns.umass.edu, which responds with the IP address\nof gaia.cs.umass.edu. Note that in this example, in order to obtain\nthe mapping for one hostname, eight DNS messages were sent: four query\nmessages and four reply messages! We’ll soon see how DNS caching\nreduces this query traffic.\nFigure 2.19 ♦Interaction of the various DNS servers\nOur previous example assumed that the TLD server knows the\nauthoritative DNS server for the hostname. In general, this is not always\ntrue. Instead, the TLD server may know only of an intermediate DNS\nserver, which in turn knows the authoritative DNS server for the hostname.\nFor example, suppose again that the University of Massachusetts has a DNS\nserver for the university, called dns.umass.edu. Also suppose that each\nof the departments at the University of Massachusetts has its own DNS\nserver, and that each departmental DNS server is authoritative for all hosts\nin the department. In this case, when the intermediate DNS server,\ndns.umass.edu, receives a query for a host with a hostname ending\nwith cs.umass.edu, it returns to dns.nyu.edu the IP address of\ndns.cs.umass.edu, which is authoritative for all hostnames ending\nwith cs.umass.edu. The local DNS server dns.nyu.edu then sends\nthe query to the authoritative DNS server, which returns the desired\nmapping to the local DNS server, which in turn returns the mapping to the\nrequesting host. In this case, a total of 10 DNS messages are sent!\nThe example shown in Figure 2.19 makes use of both recursive\nqueries and iterative queries. The query sent from cse.nyu.edu to\ndns.nyu.edu is a recursive query, since the query asks dns.nyu.edu\nto obtain the mapping on its behalf. However, the subsequent three queries\nare iterative since all of the replies are directly returned to dns.nyu.edu.\nIn theory, any DNS query can be iterative or recursive. For example, Figure\n2.20 shows a DNS query chain for which all of the queries are recursive. In\npractice, the queries typically follow the pattern in Figure 2.19: The query\nfrom the requesting host to the local DNS server is recursive, and the\nremaining queries are iterative.\nFigure 2.20 ♦Recursive queries in DNS\nDNS Caching\nOur discussion thus far has ignored DNS caching, a critically important\nfeature of the DNS system. In truth, DNS extensively exploits DNS caching\nin order to improve the delay performance and to reduce the number of\nDNS messages ricocheting around the Internet. The idea behind DNS\ncaching is very simple. In a query chain, when a DNS server receives a\nDNS reply (containing, for example, a mapping from a hostname to an IP\naddress), it can cache the mapping in its local memory. For example, in\nFigure 2.19, each time the local DNS server dns.nyu.edu receives a\nreply from some DNS server, it can cache any of the information contained\nin the reply. If a hostname/IP address pair is cached in a DNS server and\nanother query arrives to the DNS server for the same hostname, the DNS\nserver can provide the desired IP address, even if it is not authoritative for\nthe hostname. Because hosts and mappings between hostnames and IP\naddresses are by no means permanent, DNS servers discard cached\ninformation after a period of time (often set to two days).\nAs an example, suppose that a host apricot.nyu.edu queries\ndns.nyu.edu for the IP address for the hostname cnn.com.\nFurthermore, ­suppose that a few hours later, another NYU host, say,\nkiwi.nyu.edu, also queries dns.nyu.edu with the same hostname.\nBecause of caching, the local DNS server will be able to immediately return\nthe IP address of cnn.com to this second requesting host without having to\nquery any other DNS servers. A local DNS server can also cache the IP\naddresses of TLD servers, thereby allowing the local DNS server to bypass\nthe root DNS servers in a query chain. In fact, because of caching, root\nservers are bypassed for all but a very small fraction of DNS queries.\n2.4.3 DNS Records and Messages\nThe DNS servers that together implement the DNS distributed database\nstore resource records (RRs), including RRs that provide hostname-to-IP\naddress mappings. Each DNS reply message carries one or more resource\nrecords. In this and the following subsection, we provide a brief overview\nof DNS resource records and messages; more details can be found in\n[Albitz 1993] or in the DNS RFCs [RFC 1034; RFC 1035].\nA resource record is a four-tuple that contains the following fields:\n(Name, Value, Type, TTL)\nTTL is the time to live of the resource record; it determines when a resource\nshould be removed from a cache. In the example records given below, we\nignore the TTL field. The meaning of Name and Value depend on Type:\nIf Type=A, then Name is a hostname and Value is the IP address for\nthe hostname. Thus, a Type A record provides the standard hostname-to-\nIP address mapping. As an example, (relay1.bar.foo.com,\n145.37.93.126, A) is a Type A record.\nIf Type=NS, then Name is a domain (such as foo.com) and Value\nis the hostname of an authoritative DNS server that knows how to\nobtain the IP addresses for hosts in the domain. This record is used to\nroute DNS queries further along in the query chain. As an example,\n(foo.com, dns.foo.com, NS) is a Type NS record.\nIf Type=CNAME, then Value is a canonical hostname for the alias\nhostname Name. This record can provide querying hosts the canonical\nrelay1.bar.foo.com, CNAME) is a CNAME record.\nIf Type=MX, then Value is the canonical name of a mail server that\nhas an alias hostname Name. As an example, (foo.com,\nmail.bar.foo.com, MX) is an MX record. MX records allow the\nhostnames of mail servers to have simple aliases. Note that by using the\nMX record, a company can have the same aliased name for its mail\nserver and for one of its other servers (such as its Web server). To obtain\nthe canonical name for the mail server, a DNS client would query for an\nMX record; to obtain the canonical name for the other server, the DNS\nclient would query for the CNAME record.\nIf a DNS server is authoritative for a particular hostname, then the DNS\nserver will contain a Type A record for the hostname. (Even if the DNS\nserver is not authoritative, it may contain a Type A record in its cache.) If a\nserver is not authoritative for a hostname, then the server will contain a\nType NS record for the domain that includes the hostname; it will also\ncontain a Type A record that provides the IP address of the DNS server in\nthe Value field of the NS record. As an example, suppose an edu TLD\nserver is not authoritative for the host gaia.cs.umass.edu. Then this\nserver will contain a record for a domain that includes the host\ngaia.cs.umass.edu, \n(umass.edu,\ndns.umass.edu, NS). The edu TLD server would also contain a Type\nA record, which maps the DNS server dns.umass.edu to an IP address,\nfor example, (dns.umass.edu, 128.119.40.111, A).\nDNS Messages\nEarlier in this section, we referred to DNS query and reply messages. These\nare the only two kinds of DNS messages. Furthermore, both query and\nreply messages have the same format, as shown in Figure 2.21.The\nsemantics of the various fields in a DNS message are as follows:\nThe first 12 bytes is the header section, which has a number of fields.\nThe first field is a 16-bit number that identifies the query. This identifier\nis copied into the reply message to a query, allowing the client to match\nreceived replies with sent queries. There are a number of flags in the\nflag field. A 1-bit query/reply flag indicates whether the message is a\nquery (0) or a reply (1). A 1-bit authoritative flag is set in a reply\nmessage when a DNS server is an authoritative server for a queried\nname. A 1-bit recursion-desired flag is set when a client (host or DNS\nserver) desires that the DNS server perform recursion when it doesn’t\nhave the record. A 1-bit recursion-available field is set in a reply if the\nDNS server supports recursion. In the header, there are also four\nnumber-of fields. These fields indicate the number of occurrences of the\nfour types of data sections that follow the header.\nThe question section contains information about the query that is being\nmade. This section includes (1) a name field that contains the name that\nis being queried, and (2) a type field that indicates the type of question\nbeing asked about the name—for example, a host address associated\nwith a name (Type A) or the mail server for a name (Type MX).\nFigure 2.21 ♦DNS message format\nIn a reply from a DNS server, the answer section contains the resource\nrecords for the name that was originally queried. Recall that in each\nresource record there is the Type (for example, A, NS, CNAME, and\nMX), the Value, and the TTL. A reply can return multiple RRs in the\nanswer, since a hostname can have multiple IP addresses (for example,\nfor replicated Web servers, as discussed earlier in this section).\nThe authority section contains records of other authoritative servers.\nThe additional section contains other helpful records. For example, the\nanswer field in a reply to an MX query contains a resource record\nproviding the canonical hostname of a mail server. The additional\nsection contains a Type A record providing the IP address for the\ncanonical hostname of the mail server.\nHow would you like to send a DNS query message directly from the\nhost you’re working on to some DNS server? This can easily be done with\nthe nslookup program, which is available from most Windows and UNIX\nplatforms. For example, from a Windows host, open the Command Prompt\nand invoke the nslookup program by simply typing “nslookup.” After\ninvoking nslookup, you can send a DNS query to any DNS server (root,\nTLD, or authoritative). After receiving the reply message from the DNS\nserver, nslookup will display the records included in the reply (in a human-\nreadable format). As an alternative to running nslookup from your own\nhost, you can visit one of many Web sites that allow you to remotely\nemploy nslookup. (Just type “nslookup” into a search engine and you’ll be\nbrought to one of these sites.) The DNS Wireshark lab at the end of this\nchapter will allow you to explore the DNS in much more detail.\nInserting Records into the DNS Database\nThe discussion above focused on how records are retrieved from the DNS\ndatabase. You might be wondering how records get into the database in the\nfirst place. Let’s look at how this is done in the context of a specific\nexample. Suppose you have just created an exciting new startup company\ncalled Network Utopia. The first thing you’ll surely want to do is register\nthe domain name networkutopia.com at a registrar. A registrar is a\ncommercial entity that verifies the uniqueness of the domain name, enters\nthe domain name into the DNS database (as discussed below), and collects\na small fee from you for its services. Prior to 1999, a single registrar,\nNetwork Solutions, had a monopoly on domain name registration for com,\nnet, and org domains. But now there are many registrars competing for\ncustomers, and the Internet Corporation for Assigned Names and Numbers\n(ICANN) accredits the various registrars. A complete list of accredited\nregistrars is available at http://www.internic.net.\nWhen you register the domain name networkutopia.com with\nsome registrar, you also need to provide the registrar with the names and IP\naddresses of your primary and secondary authoritative DNS servers.\nSuppose the names and IP addresses are dns1.networkutopia.com,\ndns2.networkutopia.com, \n212.2.212.1, \n212.212.212.2. For each of these two authoritative DNS servers, the\nregistrar would then make sure that a Type NS and a Type A record are\nentered into the TLD com servers. Specifically, for the primary authoritative\nserver for networkutopia.com, the registrar would insert the following\ntwo resource records into the DNS system:\n(networkutopia.com, dns1.networkutopia.com, NS)\n(dns1.networkutopia.com, 212.212.212.1, A)\nWe have seen that DNS is a critical component of the Internet infrastructure, with many\nimportant services—including the Web and e-mail—simply incapable of functioning\nwithout it. We therefore naturally ask, how can DNS be attacked? Is DNS a sitting\nduck, waiting to be knocked out of service, while taking most Internet applications\ndown with it?\nThe first type of attack that comes to mind is a DDoS bandwidth-flooding attack (see\nSection 1.6) against DNS servers. For example, an attacker could attempt to send to\neach DNS root server a deluge of packets, so many that the majority of legitimate DNS\nqueries never get answered. Such a large-scale DDoS attack against DNS root servers\nactually took place on October 21, 2002. In this attack, the attackers leveraged a\nbotnet to send truck loads of ICMP ping messages to each of the 13 DNS root IP\naddresses. (ICMP messages are discussed in Section 5.6. For now, it suffices to know\nthat ICMP packets are special types of IP datagrams.) Fortunately, this large-scale\nattack caused minimal damage, having little or no impact on users’ Internet experience.\nThe attackers did succeed at directing a deluge of packets at the root servers. But\nmany of the DNS root servers were protected by packet filters, configured to always\nblock all ICMP ping messages directed at the root servers. These protected servers\nwere thus spared and functioned as normal. Furthermore, most local DNS servers\ncache the IP addresses of top-level-domain servers, allowing the query process to\noften bypass the DNS root servers.\nA potentially more effective DDoS attack against DNS is send a deluge of DNS\nqueries to top-level-domain servers, for example, to top-level-domain servers that\nhandle the .com domain. It is harder to filter DNS queries directed to DNS servers; and\ntop-level-domain servers are not as easily bypassed as are root servers. Such an\nattack took place against the top-level-domain service provider Dyn on October 21,\n2016. This DDoS attack was accomplished through a large number of DNS lookup\nrequests from a botnet consisting of about one hundred thousand IoT devices such as\nprinters, IP cameras, residential gateways and baby monitors that had been infected\nwith Mirai malware. For almost a full day, Amazon, Twitter, Netflix, Github and Spotify\nwere disturbed.\nDNS could potentially be attacked in other ways. In a man-in-the-middle attack, the\nattacker intercepts queries from hosts and returns bogus replies. In the DNS poisoning\nattack, the attacker sends bogus replies to a DNS server, tricking the server into\naccepting bogus records into its cache. Either of these attacks could be used, for\nexample, to redirect an unsuspecting Web user to the attacker’s Web site. The DNS\nSecurity Extensions (DNSSEC [Gieben 2004; RFC 4033] have been designed and\ndeployed to protect against such exploits. DNSSEC, a secured version of DNS,\naddresses many of these possible attacks and is gaining popularity in the Internet.\nYou’ll also have to make sure that the Type A resource record for your Web\nserver www.networkutopia.com and the Type MX resource record for\nyour mail server mail.networkutopia.com are entered into your\nuser. (Recall from Chapter 1 that the end-to-end throughput of a stream is\ngoverned by the throughput at the bottleneck link.) The likelihood of this\nhappening increases as the number of links in the end-to-end path increases.\nA second drawback is that a popular video will likely be sent many times\nover the same communication links. Not only does this waste network\nbandwidth, but the Internet video company itself will be paying its provider\nISP (connected to the data center) for sending the same bytes into the\nInternet over and over again. A third problem with this solution is that a\nsingle data center represents a single point of failure—if the data center or\nits links to the Internet goes down, it would not be able to distribute any\nvideo streams.\nIn order to meet the challenge of distributing massive amounts of video\ndata to users distributed around the world, almost all major video-streaming\ncompanies make use of Content Distribution Networks (CDNs). A CDN\nmanages servers in multiple geographically distributed locations, stores\ncopies of the videos (and other types of Web content, including documents,\nimages, and audio) in its servers, and attempts to direct each user request to\na CDN location that will provide the best user experience. The CDN may be\na private CDN, that is, owned by the content provider itself; for example,\nGoogle’s CDN distributes YouTube videos and other types of content. The\nCDN may alternatively be a third-party CDN that distributes content on\nbehalf of multiple content providers; Akamai, Limelight and Level-3 all\noperate third-party CDNs. A very readable overview of modern CDNs is\n[Leighton 2009; Nygren 2010].\nCDNs typically adopt one of two different server placement\nphilosophies [Huang 2008]:\nEnter Deep. One philosophy, pioneered by Akamai, is to enter deep\ninto the access networks of Internet Service Providers, by deploying\nserver clusters in access ISPs all over the world. (Access networks are\ndescribed in Section 1.3.) Akamai takes this approach with clusters in\nthousands of locations. The goal is to get close to end users, thereby\nimproving user-perceived delay and throughput by decreasing the\nnumber of links and routers between the end user and the CDN server\nfrom which it receives content. Because of this highly distributed\ndesign, the task of maintaining and managing the clusters becomes\nchallenging.\nBring Home. A second design philosophy, taken by Limelight and\nmany other CDN companies, is to bring the ISPs home by building\nlarge clusters at a smaller number (for example, tens) of sites. Instead of\ngetting inside the access ISPs, these CDNs typically place their clusters\nin Internet Exchange Points (IXPs) (see Section 1.3). Compared with\nthe enter-deep design philosophy, the bring-home design typically\nresults in lower maintenance and management overhead, possibly at the\nexpense of higher delay and lower throughput to end users.\nOnce its clusters are in place, the CDN replicates content across its clusters.\nThe CDN may not want to place a copy of every video in each cluster, since\nsome videos are rarely viewed or are only popular in some countries. In\nfact, many CDNs do not push videos to their clusters but instead use a\nsimple pull strategy: If a client requests a video from a cluster that is not\nstoring the video, then the cluster retrieves the video (from a central\nrepository or from another cluster) and stores a copy locally while\nstreaming the video to the client at the same time. Similar Web caching (see\nSection 2.2.5), when a cluster’s storage becomes full, it removes videos that\nare not frequently requested.\nCDN Operation\nHaving identified the two major approaches toward deploying a CDN, let’s\nnow dive down into the nuts and bolts of how a CDN operates. When a\nbrowser in a user’s host is instructed to retrieve a specific video (identified\nby a URL), the CDN must intercept the request so that it can (1) determine\na suitable CDN server cluster for that client at that time, and (2) redirect the\nclient’s request to a server in that cluster. We’ll shortly discuss how a CDN\ncan determine a suitable cluster. But first let’s examine the mechanics\nbehind intercepting and redirecting a request.\nGOOGLE’S NETWORK INFRASTRUCTURE\nTo support its vast array of services—including search, Gmail, calendar, YouTube\nvideo, maps, documents, and social networks—Google has deployed an extensive\nprivate network and CDN infrastructure. Google’s CDN infrastructure has three tiers of\nserver clusters:\nNineteen “mega data centers” in North America, Europe, and Asia [Google\nLocations 2020], with each data center having on the order of 100,000 servers.\nThese mega data centers are responsible for serving dynamic (and often\npersonalized) content, including search results and Gmail messages.\nWith about 90 clusters in IXPs scattered throughout the world, with each cluster\nconsisting of hundreds of servers servers [Adhikari 2011a] [Google CDN 2020].\nThese clusters are responsible for serving static content, including YouTube videos.\nMany hundreds of “enter-deep” clusters located within an access ISP. Here a\ncluster typically consists of tens of servers within a single rack. These enter-deep ­-\nservers perform TCP splitting (see Section 3.7) and serve static content [Chen\n2011], including the static portions of Web pages that embody search results.\nAll of these data centers and cluster locations are networked together with Google’s\nown private network. When a user makes a search query, often the query is first sent\nover the local ISP to a nearby enter-deep cache, from where the static content is\nretrieved; while providing the static content to the client, the nearby cache also\nforwards the query over Google’s private network to one of the mega data centers,\nfrom where the personalized search results are retrieved. For a YouTube video, the\nvideo itself may come from one of the bring-home caches, whereas portions of the\nWeb page surrounding the video may come from the nearby enter-deep cache, and the\nadvertisements surrounding the video come from the data centers. In summary, except\nfor the local ISPs, the Google cloud services are largely provided by a network\ninfrastructure that is independent of the public Internet.\nMost CDNs take advantage of DNS to intercept and redirect requests;\nan interesting discussion of such a use of the DNS is [Vixie 2009]. Let’s\nconsider a simple example to illustrate how the DNS is typically involved.\nSuppose a content provider, NetCinema, employs the third-party CDN\ncompany, KingCDN, to distribute its videos to its customers. On the\nNetCinema Web pages, each of its videos is assigned a URL that includes\nthe string “video” and a unique identifier for the video itself; for example,\nTransformers 7 might be assigned http://video.netcinema.com/6Y7B23V.\nSix steps then occur, as shown in Figure 2.25:\nFigure 2.25 ♦DNS redirects a user’s request to a CDN server\n1. The user visits the Web page at NetCinema.\n2. When the user clicks on the link http://video.netcinema.com/6Y7B23V,\nthe user’s host sends a DNS query for video.netcinema.com.\n3. The user’s Local DNS Server (LDNS) relays the DNS query to an\nauthoritative DNS server for NetCinema, which observes the string\n“video” in the hostname video.netcinema.com. To “hand over” the DNS\nquery to KingCDN, instead of returning an IP address, the NetCinema\nauthoritative DNS server returns to the LDNS a hostname in the\nKingCDN’s domain, for example, a1105.kingcdn.com.\n4. From this point on, the DNS query enters into KingCDN’s private DNS\ninfrastructure. The user’s LDNS then sends a second query, now for\na1105.kingcdn.com, and KingCDN’s DNS system eventually returns the\nIP addresses of a KingCDN content server to the LDNS. It is thus here,\nwithin the KingCDN’s DNS system, that the CDN server from which\nthe client will receive its content is specified.\n5. The LDNS forwards the IP address of the content-serving CDN node to\nthe user’s host.\n6. Once the client receives the IP address for a KingCDN content server, it\nestablishes a direct TCP connection with the server at that IP address\nand issues an HTTP GET request for the video. If DASH is used, the\nserver will first send to the client a manifest file with a list of URLs, one\nfor each version of the video, and the client will dynamically select\nchunks from the different versions.\nCluster Selection Strategies\nAt the core of any CDN deployment is a cluster selection strategy, that is,\na mechanism for dynamically directing clients to a server cluster or a data\ncenter within the CDN. As we just saw, the CDN learns the IP address of\nthe client’s LDNS server via the client’s DNS lookup. After learning this IP\naddress, the CDN needs to select an appropriate cluster based on this IP\naddress. CDNs generally employ proprietary cluster selection strategies. We\nnow briefly survey a few approaches, each of which has its own advantages\nand disadvantages.\nOne simple strategy is to assign the client to the cluster that is\ngeographically closest. Using commercial geo-location databases (such as\nQuova [Quova 2020] and Max-Mind [MaxMind 2020]), each LDNS IP\naddress is mapped to a geographic location. When a DNS request is\nreceived from a particular LDNS, the CDN chooses the geographically\nclosest cluster, that is, the cluster that is the fewest kilometers from the\nLDNS “as the bird flies.” Such a solution can work reasonably well for a\nlarge fraction of the clients [Agarwal 2009]. However, for some clients, the\nsolution may perform poorly, since the geographically closest cluster may\nnot be the closest cluster in terms of the length or number of hops of the\nnetwork path. Furthermore, a problem inherent with all DNS-based\napproaches is that some end-users are configured to use remotely located\nLDNSs [Shaikh 2001; Mao 2002], in which case the LDNS location may be\nfar from the client’s location. Moreover, this simple strategy ignores the\nvariation in delay and available bandwidth over time of Internet paths,\nalways assigning the same cluster to a particular client.\nIn order to determine the best cluster for a client based on the current\ntraffic conditions, CDNs can instead perform periodic real-time\nmeasurements of delay and loss performance between their clusters and\nclients. For instance, a CDN can have each of its clusters periodically send\nprobes (for example, ping messages or DNS queries) to all of the LDNSs\naround the world. One drawback of this approach is that many LDNSs are\nconfigured to not respond to such probes.\n2.6.4 Case Studies: Netflix and YouTube\nWe conclude our discussion of streaming stored video by taking a look at\ntwo highly successful large-scale deployments: Netflix and YouTube. We’ll\nsee that each of these systems take a very different approach, yet employ\nmany of the underlying principles discussed in this section.\nAs of 2020, Netflix is the leading service provider for online movies and\nTV series in North America. As we discuss below, Netflix video distribution\nhas two major components: the Amazon cloud and its own private CDN\ninfrastructure.\nNetflix has a Web site that handles numerous functions, including user\nregistration and login, billing, movie catalogue for browsing and searching,\nand a movie recommendation system. As shown in Figure 2.26, this Web\nsite (and its associated backend databases) run entirely on Amazon servers\nin the Amazon cloud. Additionally, the Amazon cloud handles the following\ncritical functions:\nContent ingestion. Before Netflix can distribute a movie to its\ncustomers, it must first ingest and process the movie. Netflix receives\nstudio master versions of movies and uploads them to hosts in the\nAmazon cloud.\nContent processing. The machines in the Amazon cloud create many\ndifferent formats for each movie, suitable for a diverse array of client\nvideo players running on desktop computers, smartphones, and game\nconsoles connected to televisions. A different version is created for each\nof these formats and at multiple bit rates, allowing for adaptive\nstreaming over HTTP using DASH.\nUploading versions to its CDN. Once all of the versions of a movie\nhave been created, the hosts in the Amazon cloud upload the versions to\nFigure 2.26 ♦Netflix video streaming platform\nWhen Netflix first rolled out its video streaming service in 2007, it\nemployed three third-party CDN companies to distribute its video content.\nNetflix has since created its own private CDN, from which it now streams\nall of its videos. To create its own CDN, Netflix has installed server racks\nboth in IXPs and within residential ISPs themselves. Netflix currently has\nserver racks in over 200 IXP locations; see [Bottger 2018] [Netflix Open\nConnect 2020] for a current list of IXPs housing Netflix racks. There are\nalso hundreds of ISP locations housing Netflix racks; also see [Netflix Open\nConnect 2020], where Netflix provides to potential ISP partners instructions\nabout installing a (free) Netflix rack for their networks. Each server in the\nrack has several 10 Gbps Ethernet ports and over 100 terabytes of storage.\nThe number of servers in a rack varies: IXP installations often have tens of\nservers and contain the entire Netflix streaming video library, including\nmultiple versions of the videos to support DASH. Netflix does not use pull-\ncaching (Section 2.2.5) to populate its CDN servers in the IXPs and ISPs.\nInstead, Netflix distributes by pushing the videos to its CDN servers during\noff-peak hours. For those locations that cannot hold the entire library,\nNetflix pushes only the most popular videos, which are determined on a\nday-to-day basis. The Netflix CDN design is described in some detail in the\nYouTube videos [Netflix Video 1] and [Netflix Video 2]; see also [Bottger\nHaving described the components of the Netflix architecture, let’s take\na closer look at the interaction between the client and the various servers\nthat are involved in movie delivery. As indicated earlier, the Web pages for\nbrowsing the Netflix video library are served from servers in the Amazon\ncloud. When a user selects a movie to play, the Netflix software, running in\nthe Amazon cloud, first determines which of its CDN servers have copies of\nthe movie. Among the servers that have the movie, the software then\ndetermines the “best” server for that client request. If the client is using a\nresidential ISP that has a Netflix CDN server rack installed in that ISP, and\nthis rack has a copy of the requested movie, then a server in this rack is\ntypically selected. If not, a server at a nearby IXP is typically selected.\nOnce Netflix determines the CDN server that is to deliver the content, it\nsends the client the IP address of the specific server as well as a manifest\nfile, which has the URLs for the different versions of the requested movie.\nThe client and that CDN server then directly interact using a proprietary\nversion of DASH. Specifically, as described in Section 2.6.2, the client uses\nthe byte-range header in HTTP GET request messages, to request chunks\nfrom the different versions of the movie. Netflix uses chunks that are\napproximately four-seconds long [Adhikari 2012]. While the chunks are\nbeing downloaded, the client measures the received throughput and runs a\nrate-determination algorithm to determine the quality of the next chunk to\nNetflix embodies many of the key principles discussed earlier in this\nsection, including adaptive streaming and CDN distribution. However,\nbecause Netflix uses its own private CDN, which distributes only video\n(and not Web pages), Netflix has been able to simplify and tailor its CDN\ndesign. In particular, Netflix does not need to employ DNS redirect, as\ndiscussed in Section 2.6.3, to connect a particular client to a CDN server;\ninstead, the Netflix software (running in the Amazon cloud) directly tells\nthe client to use a particular CDN server. Furthermore, the Netflix CDN\nuses push caching rather than pull caching (Section 2.2.5): content is\npushed into the servers at scheduled times at off-peak hours, rather than\ndynamically during cache misses.\nWith hundreds of hours of video uploaded to YouTube every minute and\nseveral billion video views per day, YouTube is indisputably the world’s\nlargest video-sharing site. YouTube began its service in April 2005 and was\nacquired by Google in November 2006. Although the Google/YouTube\ndesign and protocols are proprietary, through several independent\nmeasurement efforts we can gain a basic understanding about how YouTube\noperates [Zink 2009; Torres 2011; Adhikari 2011a]. As with Netflix,\nYouTube makes extensive use of CDN technology to distribute its videos\n[Torres 2011]. Similar to Netflix, Google uses its own private CDN to\ndistribute YouTube videos, and has installed server clusters in many\nhundreds of different IXP and ISP locations. From these locations and\ndirectly from its huge data centers, Google distributes YouTube videos\n[Adhikari 2011a]. Unlike Netflix, however, Google uses pull caching, as\ndescribed in Section 2.2.5, and DNS redirect, as described in Section 2.6.3.\nMost of the time, Google’s cluster-selection strategy directs the client to the\ncluster for which the RTT between client and cluster is the lowest; however,\nin order to balance the load across clusters, sometimes the client is directed\n(via DNS) to a more distant cluster [Torres 2011].\nYouTube employs HTTP streaming, often making a small number of\ndifferent versions available for a video, each with a different bit rate and\ncorresponding quality level. YouTube does not employ adaptive streaming\n(such as DASH), but instead requires the user to manually select a version.\nIn order to save bandwidth and server resources that would be wasted by\nrepositioning or early termination, YouTube uses the HTTP byte range\nrequest to limit the flow of transmitted data after a target amount of video is\nprefetched.\nSeveral million videos are uploaded to YouTube every day. Not only\nare YouTube videos streamed from server to client over HTTP, but YouTube\nuploaders also upload their videos from client to server over HTTP.\nYouTube processes each video it receives, converting it to a YouTube video\nformat and creating multiple versions at different bit rates. This processing\ntakes place entirely within Google data centers. (See the case study on\nGoogle’s network infrastructure in Section 2.6.3.)\n2.7 Socket Programming: Creating Network\nApplications\nNow that we’ve looked at a number of important network applications, let’s\nexplore how network application programs are actually created. Recall from\nSection 2.1 that a typical network application consists of a pair of programs\n—a client program and a server program—residing in two different end\nsystems. When these two programs are executed, a client process and a\nserver process are created, and these processes communicate with each\nother by reading from, and writing to, sockets. When creating a network\napplication, the developer’s main task is therefore to write the code for both\nthe client and server programs.\nThere are two types of network applications. One type is an\nimplementation whose operation is specified in a protocol standard, such as\nan RFC or some other standards document; such an application is\nsometimes referred to as “open,” since the rules specifying its operation are\nknown to all. For such an implementation, the client and server programs\nmust conform to the rules dictated by the RFC. For example, the client\nprogram could be an implementation of the client side of the HTTP\nprotocol, described in Section 2.2 and precisely defined in RFC 2616;\nsimilarly, the server program could be an implementation of the HTTP\nserver protocol, also precisely defined in RFC 2616. If one developer writes\ncode for the client program and another developer writes code for the server\nprogram, and both developers carefully follow the rules of the RFC, then\nthe two programs will be able to interoperate. Indeed, many of today’s\nnetwork applications involve communication between client and server\nprograms that have been created by independent developers—for example,\na Google Chrome browser communicating with an Apache Web server, or a\nBitTorrent client communicating with BitTorrent tracker.\nThe other type of network application is a proprietary network\napplication. In this case, the client and server programs employ an\napplication-layer protocol that has not been openly published in an RFC or\nelsewhere. A single developer (or development team) creates both the client\nand server programs, and the developer has complete control over what\ngoes in the code. But because the code does not implement an open\nprotocol, other independent developers will not be able to develop code that\ninteroperates with the application.\nIn this section, we’ll examine the key issues in developing a client-\nserver application, and we’ll “get our hands dirty” by looking at code that\nimplements a very simple client-server application. During the development\nphase, one of the first decisions the developer must make is whether the\napplication is to run over TCP or over UDP. Recall that TCP is connection\noriented and provides a reliable byte-stream channel through which data\nflows between two end systems. UDP is connectionless and sends\nindependent packets of data from one end system to the other, without any\nguarantees about delivery. Recall also that when a client or server program\nimplements a protocol defined by an RFC, it should use the well-known\nport number associated with the protocol; conversely, when developing a\nproprietary application, the developer must be careful to avoid using such\nwell-known port numbers. (Port numbers were briefly discussed in Section\n2.1. They are covered in more detail in Chapter 3.)\nWe introduce UDP and TCP socket programming by way of a simple\nUDP application and a simple TCP application. We present the simple UDP\nand TCP applications in Python 3. We could have written the code in Java,\nC, or C++, but we chose Python mostly because Python clearly exposes the\nkey socket concepts. With Python there are fewer lines of code, and each\nline can be explained to the novice programmer without difficulty. But\nthere’s no need to be frightened if you are not familiar with Python. You\nshould be able to easily follow the code if you have experience\nprogramming in Java, C, or C++.\nIf you are interested in client-server programming with Java, you are\nencouraged to see the Companion Website for this textbook; in fact, you can\nfind there all the examples in this section (and associated labs) in Java. For\nreaders who are interested in client-server programming in C, there are\nwe will discuss IPv4 in Chapter 4.) The second parameter indicates that the\nsocket is of type SOCK_DGRAM, which means it is a UDP socket (rather\nthan a TCP socket). Note that we are not specifying the port number of the\nclient socket when we create it; we are instead letting the operating system\ndo this for us. Now that the client process’s door has been created, we will\nwant to create a message to send through the door.\nmessage = input(’Input lowercase sentence:’)\ninput() is a built-in function in Python. When this command is executed,\nthe user at the client is prompted with the words “Input lowercase\nsentence:” The user then uses her keyboard to input a line, which is put into\nthe variable message. Now that we have a socket and a message, we will\nwant to send the message through the socket to the destination host.\nclientSocket.sendto(message.encode(), (serverName,\n serverPort))\nIn the above line, we first convert the message from string type to byte type,\nas we need to send bytes into a socket; this is done with the encode()\nmethod. The method sendto() attaches the destination address\n(serverName, serverPort) to the message and sends the resulting\npacket into the process’s socket, clientSocket. (As mentioned earlier,\nthe source address is also attached to the packet, although this is done\nautomatically rather than explicitly by the code.) Sending a client-to-server\nmessage via a UDP socket is that simple! After sending the packet, the\nclient waits to receive data from the server.\nmodifiedMessage, serverAddress =\nclientSocket.recvfrom(2048)\nWith the above line, when a packet arrives from the Internet at the client’s\nsocket, the packet’s data is put into the variable modifiedMessage and\nthe packet’s source address is put into the variable serverAddress. The\nvariable serverAddress contains both the server’s IP address and the\nserver’s port number. The program UDPClient doesn’t actually need this\nserver address information, since it already knows the server address from\nthe outset; but this line of Python provides the server address nevertheless.\nThe method recvfrom also takes the buffer size 2048 as input. (This\nbuffer size works for most purposes.)\nprint(modifiedMessage.decode())\nThis line prints out modifiedMessage on the user’s display, after converting\nthe message from bytes to string. It should be the original line that the user\ntyped, but now capitalized.\nclientSocket.close()\nThis line closes the socket. The process then terminates.\nUDPServer.py\nLet’s now take a look at the server side of the application:\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET, SOCK_DGRAM)\nserverSocket.bind((’’, serverPort))\nprint(”The server is ready to receive”)\nwhile True:\n    message, clientAddress =\nserverSocket.recvfrom(2048)\n    modifiedMessage = message.decode().upper()\n    serverSocket.sendto(modifiedMessage.encode(),\nclientAddress)\nNote that the beginning of UDPServer is similar to UDPClient. It also\nimports the socket module, also sets the integer variable serverPort to\n12000, and also creates a socket of type SOCK_DGRAM (a UDP socket).\nThe first line of code that is significantly different from UDPClient is:\nserverSocket.bind((’’, serverPort))\nThe above line binds (that is, assigns) the port number 12000 to the server’s\nsocket. Thus, in UDPServer, the code (written by the application developer)\nis explicitly assigning a port number to the socket. In this manner, when\nanyone sends a packet to port 12000 at the IP address of the server, that\npacket will be directed to this socket. UDPServer then enters a while loop;\nthe while loop will allow UDPServer to receive and process packets from\nclients indefinitely. In the while loop, UDPServer waits for a packet to\nmessage, clientAddress =\nserverSocket.recvfrom(2048)\nThis line of code is similar to what we saw in UDPClient. When a packet\narrives at the server’s socket, the packet’s data is put into the variable\nmessage and the packet’s source address is put into the variable\nclientAddress. The variable ­clientAddress contains both the client’s IP\naddress and the client’s port number. Here, UDPServer will make use of this\naddress information, as it provides a return address, similar to the return\naddress with ordinary postal mail. With this source address information, the\nserver now knows to where it should direct its reply.\nmodifiedMessage = message.decode().upper()\nThis line is the heart of our simple application. It takes the line sent by the\nclient and, after converting the message to a string, uses the method\nupper() to capitalize it.\nserverSocket.sendto(modifiedMessage.encode(),\nclientAddress)\nThis last line attaches the client’s address (IP address and port number) to\nthe capitalized message (after converting the string to bytes), and sends the\nresulting packet into the server’s socket. (As mentioned earlier, the server\naddress is also attached to the packet, although this is done automatically\nrather than explicitly by the code.) The Internet will then deliver the packet\nto this client address. After the server sends the packet, it remains in the\nwhile loop, waiting for another UDP packet to arrive (from any client\nrunning on any host).\nTo test the pair of programs, you run UDPClient.py on one host and\nUDPServer.py on another host. Be sure to include the proper hostname or\nIP address of the server in UDPClient.py. Next, you execute UDPServer.py,\nthe compiled server program, in the server host. This creates a process in\nthe server that idles until it is contacted by some client. Then you execute\nUDPClient.py, the compiled client program, in the client. This creates a\nprocess in the client. Finally, to use the application at the client, you type a\nsentence followed by a carriage return.\nTo develop your own UDP client-server application, you can begin by\nslightly modifying the client or server programs. For example, instead of\nconverting all the letters to uppercase, the server could count the number of\ntimes the letter s appears and return this number. Or you can modify the\nclient so that after receiving a capitalized sentence, the user can continue to\nsend more sentences to the server.\n2.7.2 Socket Programming with TCP\nUnlike UDP, TCP is a connection-oriented protocol. This means that before\nthe client and server can start to send data to each other, they first need to\nhandshake and establish a TCP connection. One end of the TCP connection\nis attached to the client socket and the other end is attached to a server\nsocket. When creating the TCP connection, we associate with it the client\nsocket address (IP address and port number) and the server socket address\n(IP address and port number). With the TCP connection established, when\none side wants to send data to the other side, it just drops the data into the\nTCP connection via its socket. This is different from UDP, for which the\nserver must attach a destination address to the packet before dropping it into\nthe socket.\nNow let’s take a closer look at the interaction of client and server\nprograms in TCP. The client has the job of initiating contact with the server.\nIn order for the server to be able to react to the client’s initial contact, the\nserver has to be ready. This implies two things. First, as in the case of UDP,\nthe TCP server must be running as a process before the client attempts to\ninitiate contact. Second, the server program must have a special door—\nmore precisely, a special socket—that welcomes some initial contact from a\nclient process running on an arbitrary host. Using our house/door analogy\nfor a process/socket, we will sometimes refer to the client’s initial contact as\n“knocking on the welcoming door.”\nWith the server process running, the client process can initiate a TCP\nconnection to the server. This is done in the client program by creating a\nTCP socket. When the client creates its TCP socket, it specifies the address\nof the welcoming socket in the server, namely, the IP address of the server\nhost and the port number of the socket. After creating its socket, the client\ninitiates a three-way handshake and establishes a TCP connection with the\nserver. The three-way handshake, which takes place within the transport\nlayer, is completely invisible to the client and server programs.\nDuring the three-way handshake, the client process knocks on the\nwelcoming door of the server process. When the server “hears” the\nknocking, it creates a new door—more precisely, a new socket that is\ndedicated to that particular ­client. In our example below, the welcoming\ndoor is a TCP socket object that we call ­serverSocket; the newly\ncreated socket dedicated to the client making the connection is called\nconnectionSocket. Students who are encountering TCP sockets for\nthe first time sometimes confuse the welcoming socket (which is the initial\npoint of contact for all clients wanting to communicate with the server), and\neach newly created server-side connection socket that is subsequently\ncreated for communicating with each client.\nFrom the application’s perspective, the client’s socket and the server’s\nconnection socket are directly connected by a pipe. As shown in Figure\n2.28, the client process can send arbitrary bytes into its socket, and TCP\nguarantees that the server process will receive (through the connection\nsocket) each byte in the order sent. TCP thus provides a reliable service\nbetween the client and server processes. Furthermore, just as people can go\nin and out the same door, the client process not only sends bytes into but\nalso receives bytes from its socket; similarly, the server process not only\nreceives bytes from but also sends bytes into its connection socket.\nFigure 2.28 ♦The TCPServer process has two sockets\nWe use the same simple client-server application to demonstrate socket\nprogramming with TCP: The client sends one line of data to the server, the\nserver capitalizes the line and sends it back to the client. Figure 2.29\nhighlights the main socket-related activity of the client and server that\ncommunicate over the TCP transport service.\nFigure 2.29 ♦The client-server application using TCP\nTCPClient.py\nHere is the code for the client side of the application:\nfrom socket import *\nserverName = ’servername’\nserverPort = 12000\nclientSocket = socket(AF_INET, SOCK_STREAM)\nclientSocket.connect((serverName,serverPort))\nsentence = input(’Input lowercase sentence:’)\nclientSocket.send(sentence.encode())\nmodifiedSentence = clientSocket.recv(1024)\nprint(’From Server: ’, modifiedSentence.decode()) \nclientSocket.close()\nLet’s now take a look at the various lines in the code that differ significantly\nfrom the UDP implementation. The first such line is the creation of the\nclient socket.\nclientSocket = socket(AF_INET, SOCK_STREAM)\nThis line creates the client’s socket, called clientSocket. The first\nparameter again indicates that the underlying network is using IPv4. The\nsecond parameter indicates that the socket is of type SOCK_STREAM,\nwhich means it is a TCP socket (rather than a UDP socket). Note that we are\nagain not specifying the port number of the client socket when we create it;\nwe are instead letting the operating system do this for us. Now the next line\nof code is very different from what we saw in UDPClient:\nclientSocket.connect((serverName,serverPort))\nRecall that before the client can send data to the server (or vice versa) using\na TCP socket, a TCP connection must first be established between the client\nand server. The above line initiates the TCP connection between the client\nand server. The parameter of the connect() method is the address of the\nserver side of the connection. After this line of code is executed, the three-\nway handshake is performed and a TCP connection is established between\nthe client and server.\nsentence = input(’Input lowercase sentence:’)\nAs with UDPClient, the above obtains a sentence from the user. The string\nsentence continues to gather characters until the user ends the line by\ntyping a carriage return. The next line of code is also very different from\nclientSocket.send(sentence.encode())\nThe above line sends the sentence through the client’s socket and into\nthe TCP connection. Note that the program does not explicitly create a\npacket and attach the destination address to the packet, as was the case with\nUDP sockets. Instead the client program simply drops the bytes in the string\nsentence into the TCP connection. The client then waits to receive bytes\nfrom the server.\nmodifiedSentence = clientSocket.recv(2048)\nWhen characters arrive from the server, they get placed into the string\nmodifiedSentence. \nmodifiedSentence until the line ends with a carriage return character.\nAfter printing the capitalized sentence, we close the client’s socket:\nclientSocket.close()\nThis last line closes the socket and, hence, closes the TCP connection\nbetween the client and the server. It causes TCP in the client to send a TCP\nmessage to TCP in the server (see Section 3.5).\nTCPServer.py\nNow let’s take a look at the server program.\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET,SOCK_STREAM)\nserverSocket.bind((’’,serverPort))\nserverSocket.listen(1)\nprint(’The server is ready to receive’)\nwhile True:\n    connectionSocket, addr = serverSocket.accept()\nconnectionSocket.recv(1024).decode()\n    capitalizedSentence = sentence.upper()\n    connectionSocket.send(capitalizedSentence.enco\n    connectionSocket.close()\nLet’s now take a look at the lines that differ significantly from UDPServer\nand TCPClient. As with TCPClient, the server creates a TCP socket with:\nserverSocket=socket(AF_INET,SOCK_STREAM)\nSimilar to UDPServer, we associate the server port number, serverPort,\nwith this socket:\nserverSocket.bind((’’,serverPort))\nBut with TCP, serverSocket will be our welcoming socket. After\nestablishing this welcoming door, we will wait and listen for some client to\nknock on the door:\nserverSocket.listen(1)\nThis line has the server listen for TCP connection requests from the client.\nThe parameter specifies the maximum number of queued connections (at\nconnectionSocket, addr = serverSocket.accept()\nWhen a client knocks on this door, the program invokes the accept()\nmethod for serverSocket, which creates a new socket in the server, called ­-\nconnectionSocket, dedicated to this particular client. The client and\nserver then complete the handshaking, creating a TCP connection between\nthe client’s clientSocket and the server’s connectionSocket.\nWith the TCP connection established, the client and server can now send\nbytes to each other over the connection. With TCP, all bytes sent from one\nside are only guaranteed to arrive at the other side but also guaranteed to\narrive in order.\nconnectionSocket.close()\nIn this program, after sending the modified sentence to the client, we close\nthe connection socket. But since serverSocket remains open, another\nclient can now knock on the door and send the server a sentence to modify.\nThis completes our discussion of socket programming in TCP. You are\nencouraged to run the two programs in two separate hosts, and also to\nmodify them to achieve slightly different goals. You should compare the\nUDP program pair with the TCP program pair and see how they differ. You\nshould also do many of the socket programming assignments described at\nthe ends of Chapter 2 and 5. Finally, we hope someday, after mastering\nthese and more advanced socket programs, you will write your own popular\nnetwork application, become very rich and famous, and remember the\nauthors of this textbook!\n2.8 Summary\nIn this chapter, we’ve studied the conceptual and the implementation\naspects of network applications. We’ve learned about the ubiquitous client-\nserver architecture adopted by many Internet applications and seen its use in\nthe HTTP, SMTP, and DNS protocols. We’ve studied these important\napplication-level protocols, and their corresponding associated applications\n(the Web, file transfer, e-mail, and DNS) in some detail. We’ve learned\nabout the P2P architecture and contrasted it with the client-server\narchitecture. We’ve also learned about streaming video, and how modern\nvideo distribution systems leverage CDNs. We’ve examined how the socket\nAPI can be used to build network applications. We’ve walked through the\nuse of sockets for connection-oriented (TCP) and connectionless (UDP)\nend-to-end transport services. The first step in our journey down the layered\nnetwork architecture is now complete!\nAt the very beginning of this book, in Section 1.1, we gave a rather\nvague, bare-bones definition of a protocol: “the format and the order of\nmessages exchanged between two or more communicating entities, as well\nas the actions taken on the transmission and/or receipt of a message or other\nevent.” The material in this chapter, and in particular our detailed study of\nthe HTTP, SMTP, and DNS protocols, has now added considerable\nsubstance to this definition. Protocols are a key concept in networking; our\nstudy of application protocols has now given us the opportunity to develop\na more intuitive feel for what protocols are all about.\nIn Section 2.1, we described the service models that TCP and UDP\noffer to applications that invoke them. We took an even closer look at these\nservice models when we developed simple applications that run over TCP\nand UDP in Section 2.7. However, we have said little about how TCP and\nUDP provide these service models. For example, we know that TCP\nprovides a reliable data service, but we haven’t said yet how it does so. In\nthe next chapter, we’ll take a careful look at not only the what, but also the\nhow and why of transport protocols.\nEquipped with knowledge about Internet application structure and\napplication-level protocols, we’re now ready to head further down the\nprotocol stack and examine the transport layer in Chapter 3.\nHomework Problems and Questions\nChapter 2 Review Questions\nSECTION 2.1\nR1. List five nonproprietary Internet applications and the application-\nlayer protocols that they use.\nR2. What is the difference between network architecture and application\narchitecture?\nR3. For a communication session between a pair of processes, which\nprocess is the client and which is the server?\nR4. Why are the terms client and server still used in peer-to-peer\napplications?\nR5. What information is used by a process running on one host to identify\na process running on another host?\nR6. What is the role of HTTP in a network application? What other\ncomponents are needed to complete a Web application?\nR7. Referring to Figure 2.4, we see that none of the applications listed in\nFigure 2.4 requires both no data loss and timing. Can you conceive of\nan application that requires no data loss and that is also highly time-\nR8. List the four broad classes of services that a transport protocol can\nprovide. For each of the service classes, indicate if either UDP or\nTCP (or both) provides such a service.\nR9. Recall that TCP can be enhanced with TLS to provide process-to-\nprocess security services, including encryption. Does TLS operate at\nthe transport layer or the application layer? If the application\ndeveloper wants TCP to be enhanced with TLS, what does the\ndeveloper have to do?\nSECTIONS 2.2-2.5\nR10. What is meant by a handshaking protocol?\nR11. What does a stateless protocol mean? Is IMAP stateless? What about\nR12. How can websites keep track of users? Do they always need to use\nR13. Describe how Web caching can reduce the delay in receiving a\nrequested object. Will Web caching reduce the delay for all objects\nrequested by a user or for only some of the objects? Why?\nR14. Telnet into a Web server and send a multiline request message.\nInclude in the request message the If-modified-since: header\nline to force a response message with the 304 Not Modified\nstatus code.\nR15. Are there any constraints on the format of the HTTP body? What\nabout the email message body sent with SMTP? How can arbitrary\ndata be transmitted over SMTP?\nR16. Suppose Alice, with a Web-based e-mail account (such as Hotmail or\nGmail), sends a message to Bob, who accesses his mail from his mail\nserver using IMAP. Discuss how the message gets from Alice’s host\nto Bob’s host. Be sure to list the series of application-layer protocols\nthat are used to move the message between the two hosts.\nR17. Print out the header of an e-mail message you have recently received.\nHow many Received: header lines are there? Analyze each of the\nheader lines in the message.\nR18. What is the HOL blocking issue in HTTP/1.1? How does HTTP/2\nattempt to solve it?\nR19. Why are MX records needed? Would it not be enough to use a\nCNAME record? (Assume the email client looks up email addresses\nthrough a Type A query and that the target host only runs an email\nR20. What is the difference between recursive and iterative DNS queries?\nSECTION 2.5\nR21. Under what circumstances is file downloading through P2P much\nfaster than through a centralized client-server approach? Justify your\nanswer using Equation 2.2.\nR22. Consider a new peer Alice that joins BitTorrent without possessing\nany chunks. Without any chunks, she cannot become a top-four\nuploader for any of the other peers, since she has nothing to upload.\nHow then will Alice get her first chunk?\nR23. Assume a BitTorrent tracker suddenly becomes unavailable. What are\nits consequences? Can files still be downloaded?\nSECTION 2.6\nR24. CDNs typically adopt one of two different server placement\nphilosophies. Name and briefly describe them.\nR25. Besides network-related considerations such as delay, loss, and\nbandwidth performance, there are other important factors that go into\ndesigning a CDN server selection strategy. What are they?\nSECTION 2.7\nR26. In Section 2.7, the UDP server described needed only one socket,\nwhereas the TCP server needed two sockets. Why? If the TCP server\nwere to support n simultaneous connections, each from a different\nclient host, how many sockets would the TCP server need?\nR27. For the client-server application over TCP described in Section 2.7,\nwhy must the server program be executed before the client program?\nFor the client-server application over UDP, why may the client\nprogram be executed before the server program?\nP1. True or false?\na. A user requests a Web page that consists of some text and three\nimages. For this page, the client will send one request message\nand receive four response messages.\nb. Two distinct Web pages (for example,\nwww.mit.edu/research.html and\nwww.mit.edu/students.html) can be sent over the same\npersistent connection.\nc. With nonpersistent connections between browser and origin\nserver, it is possible for a single TCP segment to carry two\ndistinct HTTP request messages.\nd. The Date: header in the HTTP response message indicates\nwhen the object in the response was last modified.\ne. HTTP response messages never have an empty message body.\nP2. SMS, iMessage, Wechat, and WhatsApp are all smartphone real-time\nmessaging systems. After doing some research on the Internet, for\neach of these systems write one paragraph about the protocols they\nuse. Then write a paragraph explaining how they differ.\nP3. Assume you open a browser and enter\nhttp://yourbusiness.com/about.html in the address bar.\nWhat happens until the webpage is ­displayed? Provide details about\nthe protocol(s) used and a high-level description of the messages\nP4. Consider the following string of ASCII characters that were captured\nby Wireshark when the browser sent an HTTP GET message (i.e., this\nis the actual content of an HTTP GET message). The characters <cr>\n<lf> are carriage return and line-feed characters (that is, the italized\ncharacter string <cr> in the text below represents the single carriage-\nreturn character that was contained at that point in the HTTP header).\nAnswer the following questions, indicating where in the HTTP GET\nmessage below you find the answer.\nmakes use of the ICMP protocol and is summarized at the end of Chapter 5.\nIt is highly recommended that students complete several, if not all, of these\nassignments. Students can find full details of these assignments, as well as\nwww.pearsonglobaleditions.com.\nAssignment 1: Web Server\nIn this assignment, you will develop a simple Web server in Python that is\ncapable of processing only one request. Specifically, your Web server will\n(i) create a connection socket when contacted by a client (browser); (ii)\nreceive the HTTP request from this connection; (iii) parse the request to\ndetermine the specific file being requested; (iv) get the requested file from\nthe server’s file system; (v) create an HTTP response message consisting of\nthe requested file preceded by header lines; and (vi) send the response over\nthe TCP connection to the requesting browser. If a browser requests a file\nthat is not present in your server, your server should return a “404 Not\nFound” error message.\nIn the Companion Website, we provide the skeleton code for your\nserver. Your job is to complete the code, run your server, and then test your\nserver by sending requests from browsers running on different hosts. If you\nrun your server on a host that already has a Web server running on it, then\nyou should use a different port than port 80 for your Web server.\nAssignment 2: UDP Pinger\nIn this programming assignment, you will write a client ping program in\nPython. Your client will send a simple ping message to a server, receive a\ncorresponding pong message back from the server, and determine the delay\nbetween when the client sent the ping message and received the pong\nmessage. This delay is called the Round Trip Time (RTT). The functionality\nprovided by the client and server is similar to the functionality provided by\nstandard ping program available in modern operating systems. However,\nstandard ping programs use the Internet Control Message Protocol (ICMP)\n(which we will study in Chapter 5). Here we will create a nonstandard (but\nsimple!) UDP-based ping program.\nYour ping program is to send 10 ping messages to the target server over\nUDP. For each message, your client is to determine and print the RTT when\nthe corresponding pong message is returned. Because UDP is an unreliable\nprotocol, a packet sent by the client or server may be lost. For this reason,\nthe client cannot wait indefinitely for a reply to a ping message. You should\nhave the client wait up to one second for a reply from the server; if no reply\nis received, the client should assume that the packet was lost and print a\nmessage accordingly.\nIn this assignment, you will be given the complete code for the server\n(available in the Companion Website). Your job is to write the client code,\nwhich will be very similar to the server code. It is recommended that you\nfirst study carefully the server code. You can then write your client code,\nliberally cutting and pasting lines from the server code.\nAssignment 3: Mail Client\nThe goal of this programming assignment is to create a simple mail client\nthat sends e-mail to any recipient. Your client will need to establish a TCP\nconnection with a mail server (e.g., a Google mail server), dialogue with the\nmail server using the SMTP protocol, send an e-mail message to a recipient\n(e.g., your friend) via the mail server, and finally close the TCP connection\nwith the mail server.\nFor this assignment, the Companion Website provides the skeleton code\nfor your client. Your job is to complete the code and test your client by\nsending e-mail to different user accounts. You may also try sending through\ndifferent servers (for example, through a Google mail server and through\nyour university mail server).\nAssignment 4: Web Proxy\nIn this assignment, you will develop a Web proxy. When your proxy\nreceives an HTTP request for an object from a browser, it generates a new\nHTTP request for the same object and sends it to the origin server. When\nthe proxy receives the corresponding HTTP response with the object from\nthe origin server, it creates a new HTTP response, including the object, and\nsends it to the client.\nFor this assignment, the Companion Website provides the skeleton code\nfor the proxy server. Your job is to complete the code, and then test it by\nhaving different browsers request Web objects via your proxy.\nWireshark Lab: HTTP\nHaving gotten our feet wet with the Wireshark packet sniffer in Lab 1,\nwe’re now ready to use Wireshark to investigate protocols in operation. In\nthis lab, we’ll explore several aspects of the HTTP protocol: the basic\nGET/reply interaction, HTTP message formats, retrieving large HTML\nfiles, retrieving HTML files with embedded URLs, persistent and non-\npersistent connections, and HTTP authentication and security.\nAs is the case with all Wireshark labs, the full description of this lab is\navailable at this book’s Web site, www.pearsonglobaleditions.com.\nWireshark Lab: DNS\nIn this lab, we take a closer look at the client side of the DNS, the protocol\nthat translates Internet hostnames to IP addresses. Recall from Section 2.5\nthat the client’s role in the DNS is relatively simple—a client sends a query\nto its local DNS server and receives a response back. Much can go on under\nthe covers, invisible to the DNS clients, as the hierarchical DNS servers\ncommunicate with each other to either recursively or iteratively resolve the\nclient’s DNS query. From the DNS client’s standpoint, however, the\nprotocol is quite simple—a query is formulated to the local DNS server and\na response is received from that server. We observe DNS in action in this\nAs is the case with all Wireshark labs, the full description of this lab is\navailable at this book’s Web site, www.pearsonglobaleditions.com.\nAN INTERVIEW WITH…\nTim Berners-Lee\nSir Tim Berners-Lee is known as the inventor of the World\nWide Web. In 1989, while working as a fellow at CERN, he\nproposed an Internet-based distributed information\nmanagement system including the original version of the\nHTTP protocol. In the same year he successfully\nimplemented his design on a client and server. He received\nthe 2016 Turing award for “inventing the World Wide Web, the\nfirst Web browser, and the fundamental protocols and\nalgorithms allowing the Web to scale.” He is the Co-Founder\nof the World Wide Web Foundation, and currently is a\nProfessorial Fellow of Computer Science at the University of\nOxford and a professor at CSAIL at MIT.\nCourtesy of Tim Berners-Lee\nYou originally studied physics. How is\nnetworking similar to physics?\nWhen you study physics, you imagine what rules\nof behavior on the very small scale could possibly\ngive rise to the large-scale world as we see it.\nWhen you design a global system like the Web,\nyou try to invent rules of behavior of Web pages\nand links and things that could in the large create a\nlarge-scale world as we would like it. One is\nanalysis and the other synthesis, but they are very\nWhat influenced you to specialize in\nnetworking?\nAfter my physics degree, the telecommunications\nresearch companies seemed to be the most\ninteresting places. The microprocessor had just\ncome out, and telecommunications was switching\nvery fast from hardwired logic to microprocessor-\nbased systems. It was very exciting.\nWhat is the most challenging part of your\nWhen two groups disagree strongly about\nsomething, but want in the end to achieve a\ncommon goal, finding exactly what they each\nmean and where the misunderstandings are can be\nvery demanding. The chair of any working group\nknows that. However, this is what it takes to make\nprogress toward consensus on a large scale.\nWhat people have inspired you\nprofessionally?\nMy parents, who were involved in the early days\nof computing, gave me a fascination with the\nwhole subject. Mike Sendall and Peggie Rimmer,\nfor whom I worked at various times at CERN are\namong the people who taught me and encouraged\nme. I later learned to admire the people, including\nVanevar Bush, Doug Englebart, and Ted Nelson,\nwho had had similar dreams in their time but had\nnot had the benefit of the existence for PCs and the\nInternet to be able to realize it.\nTransport Layer\nResiding between the application and network\nlayers, the transport layer is a central piece of\nthe layered network architecture. It has the\ncritical role of providing communication\nservices directly to the application processes\nrunning on different hosts. The pedagogic\napproach we take in this chapter is to\nalternate between discussions of transport-\nlayer principles and discussions of how these\nimplemented in \nprotocols; as usual, particular emphasis will\nbe given to Internet protocols, in particular\nthe TCP and UDP transport-layer protocols.\nWe’ll begin by discussing the relationship\nbetween the transport and network layers.\nThis sets the stage for examining the first\ncritical function of the transport layer—\nextending the network layer’s delivery service\nbetween two end systems to a delivery service\nbetween two application-layer processes\nrunning on the end systems. We’ll illustrate\nthis function in our coverage of the Internet’s\nconnectionless transport protocol, UDP.\nWe’ll then return to principles and\nconfront one of the most fundamental\nproblems in computer networking—how two\nentities can communicate reliably over a\nmedium that may lose and corrupt data.\nThrough a series of increasingly complicated\n(and realistic!) scenarios, we’ll build up an\narray of techniques that transport protocols\nuse to solve this problem. We’ll then show\nhow these principles are embodied in TCP,\nthe Internet’s connection-oriented transport\nWe’ll next move on to a second\nfundamentally \nnetworking—controlling the transmission rate\nof transport-layer entities in order to avoid, or\nrecover from, congestion within the network.\nWe’ll consider the causes and consequences\nof congestion, as well as commonly used\ncongestion-control \ntechniques. \nobtaining a solid understanding of the issues\nbehind congestion control, we’ll study TCP’s\napproach to congestion control.\n3.1 Introduction and Transport-Layer Services\nIn the previous two chapters, we touched on the role of the transport layer\nand the services that it provides. Let’s quickly review what we have already\nlearned about the transport layer.\nA transport-layer protocol provides for logical communication\nbetween application processes running on different hosts. By logical\ncommunication, we mean that from an application’s perspective, it is as if\nthe hosts running the processes were directly connected; in reality, the hosts\nmay be on opposite sides of the planet, connected via numerous routers and\na wide range of link types. Application processes use the logical\ncommunication provided by the transport layer to send messages to each\nother, free from the worry of the details of the physical infrastructure used\nto carry these messages. Figure 3.1 illustrates the notion of logical\ncommunication.\nFigure 3.1 ♦The transport layer provides logical rather than\nphysical communication between application\nAs shown in Figure 3.1, transport-layer protocols are implemented in\nthe end systems but not in network routers. On the sending side, the\ntransport layer converts the application-layer messages it receives from a\nsending application process into transport-layer packets, known as\ntransport-layer segments in Internet terminology. This is done by (possibly)\nbreaking the application messages into smaller chunks and adding a\ntransport-layer header to each chunk to create the transport-layer segment.\nThe transport layer then passes the segment to the network layer at the\nsending end system, where the segment is encapsulated within a network-\nlayer packet (a datagram) and sent to the destination. It’s important to note\nthat network routers act only on the network-layer fields of the datagram;\nthat is, they do not examine the fields of the transport-layer segment\nencapsulated with the datagram. On the receiving side, the network layer\nextracts the transport-layer segment from the datagram and passes the\nsegment up to the transport layer. The transport layer then processes the\nreceived segment, making the data in the segment available to the receiving\napplication.\nMore than one transport-layer protocol may be available to network\napplications. For example, the Internet has two protocols—TCP and UDP.\nEach of these protocols provides a different set of transport-layer services to\nthe invoking application.\n3.1.1 Relationship Between Transport and Network\nRecall that the transport layer lies just above the network layer in the\nprotocol stack. Whereas a transport-layer protocol provides logical\ncommunication between processes running on different hosts, a network-\nlayer protocol provides logical-communication between hosts. This\ndistinction is subtle but important. Let’s examine this distinction with the\naid of a household analogy.\nConsider two houses, one on the East Coast and the other on the West\nCoast, with each house being home to a dozen kids. The kids in the East\nCoast household are cousins of the kids in the West Coast household. The\nkids in the two households love to write to each other—each kid writes each\ncousin every week, with each letter delivered by the traditional postal\nservice in a separate envelope. Thus, each household sends 144 letters to the\nother household every week. (These kids would save a lot of money if they\nhad e-mail!) In each of the households, there is one kid—Ann in the West\nCoast house and Bill in the East Coast house—responsible for mail\ncollection and mail distribution. Each week Ann visits all her brothers and\nsisters, collects the mail, and gives the mail to a postal-service mail carrier,\nwho makes daily visits to the house. When letters arrive at the West Coast\nhouse, Ann also has the job of distributing the mail to her brothers and\nsisters. Bill has a similar job on the East Coast.\nIn this example, the postal service provides logical communication\nbetween the two houses—the postal service moves mail from house to\nhouse, not from person to person. On the other hand, Ann and Bill provide\nlogical communication among the cousins—Ann and Bill pick up mail\nfrom, and deliver mail to, their brothers and sisters. Note that from the\ncousins’ perspective, Ann and Bill are the mail service, even though Ann\nand Bill are only a part (the end-system part) of the end-to-end delivery\nprocess. This household example serves as a nice analogy for explaining\nhow the transport layer relates to the network layer:\napplication messages = letters in envelopes\nprocesses = cousins\nhosts (also called end systems) = houses\ntransport-layer protocol = Ann and Bill\nnetwork-layer protocol = postal service (including mail carriers)\nContinuing with this analogy, note that Ann and Bill do all their work\nwithin their respective homes; they are not involved, for example, in sorting\nmail in any intermediate mail center or in moving mail from one mail center\nto another. Similarly, transport-layer protocols live in the end systems.\nWithin an end system, a transport protocol moves messages from\napplication processes to the network edge (that is, the network layer) and\nvice versa, but it doesn’t have any say about how the messages are moved\nwithin the network core. In fact, as illustrated in Figure 3.1, intermediate\nrouters neither act on, nor recognize, any information that the transport\nlayer may have added to the application messages.\nContinuing with our family saga, suppose now that when Ann and Bill\ngo on vacation, another cousin pair—say, Susan and Harvey—substitute for\nthem and provide the household-internal collection and delivery of mail.\nUnfortunately for the two families, Susan and Harvey do not do the\ncollection and delivery in exactly the same way as Ann and Bill. Being\nyounger kids, Susan and Harvey pick up and drop off the mail less\nfrequently and occasionally lose letters (which are sometimes chewed up by\nthe family dog). Thus, the cousin-pair Susan and Harvey do not provide the\nsame set of services (that is, the same service model) as Ann and Bill. In an\nanalogous manner, a computer network may make available multiple\ntransport protocols, with each protocol offering a different service model to\napplications.\nThe possible services that Ann and Bill can provide are clearly\nconstrained by the possible services that the postal service provides. For\nexample, if the postal service doesn’t provide a maximum bound on how\nlong it can take to deliver mail between the two houses (for example, three\ndays), then there is no way that Ann and Bill can guarantee a maximum\ndelay for mail delivery between any of the cousin pairs. In a similar manner,\nthe services that a transport protocol can provide are often constrained by\nthe service model of the underlying network-layer protocol. If the network-\nlayer protocol cannot provide delay or bandwidth guarantees for transport-\nlayer segments sent between hosts, then the transport-layer protocol cannot\nprovide delay or bandwidth guarantees for application messages sent\nbetween processes.\nNevertheless, certain services can be offered by a transport protocol\neven when the underlying network protocol doesn’t offer the corresponding\nservice at the network layer. For example, as we’ll see in this chapter, a\ntransport protocol can offer reliable data transfer service to an application\neven when the underlying network protocol is unreliable, that is, even when\nthe network protocol loses, garbles, or duplicates packets. As another\nexample (which we’ll explore in Chapter 8 when we discuss network\nsecurity), a transport protocol can use encryption to guarantee that\napplication messages are not read by intruders, even when the network\nlayer cannot guarantee the confidentiality of transport-layer segments.\n3.1.2 Overview of the Transport Layer in the\nRecall that the Internet makes two distinct transport-layer protocols\navailable to the application layer. One of these protocols is UDP (User\nDatagram Protocol), which provides an unreliable, connectionless service to\nthe invoking application. The second of these protocols is TCP\n(Transmission Control Protocol), which provides a reliable, connection-\noriented service to the invoking application. When designing a network\napplication, the application developer must specify one of these two\ntransport protocols. As we saw in Section 2.7, the application developer\nselects between UDP and TCP when creating sockets.\nTo simplify terminology, we refer to the transport-layer packet as a\nsegment. We mention, however, that the Internet literature (for example, the\nRFCs) also refers to the transport-layer packet for TCP as a segment but\noften refers to the packet for UDP as a datagram. However, this same\nInternet literature also uses the term datagram for the network-layer packet!\nFor an introductory book on computer networking such as this, we believe\nthat it is less confusing to refer to both TCP and UDP packets as segments,\nand reserve the term datagram for the network-layer packet.\nBefore proceeding with our brief introduction of UDP and TCP, it will\nbe useful to say a few words about the Internet’s network layer. (We’ll learn\nabout the network layer in detail in Chapters 4 and 5.) The Internet’s\nnetwork-layer protocol has a name—IP, for Internet Protocol. IP provides\nlogical communication between hosts. The IP service model is a best-effort\ndelivery service. This means that IP makes its “best effort” to deliver\nsegments between communicating hosts, but it makes no guarantees. In\nparticular, it does not guarantee segment delivery, it does not guarantee\norderly delivery of segments, and it does not guarantee the integrity of the\ndata in the segments. For these reasons, IP is said to be an unreliable\nservice. We also mention here that every host has at least one network-layer\naddress, a so-called IP address. We’ll examine IP addressing in detail in\nChapter 4; for this chapter we need only keep in mind that each host has an\nIP address.\nHaving taken a glimpse at the IP service model, let’s now summarize\nthe service models provided by UDP and TCP. The most fundamental\nresponsibility of UDP and TCP is to extend IP’s delivery service between\ntwo end systems to a delivery service between two processes running on the\nend systems. Extending host-to-host delivery to process-to-process delivery\nis called transport-layer multiplexing and demultiplexing. We’ll discuss\ntransport-layer multiplexing and demultiplexing in the next section. UDP\nand TCP also provide integrity checking by including error-detection fields\nin their segments’ headers. These two minimal transport-layer services—\nprocess-to-process data delivery and error checking—are the only two\nservices that UDP provides! In particular, like IP, UDP is an unreliable\nservice—it does not guarantee that data sent by one process will arrive\nintact (or at all!) to the destination process. UDP is discussed in detail in\nSection 3.3.\nTCP, on the other hand, offers several additional services to\napplications. First and foremost, it provides reliable data transfer. Using\nprogramming assignment in Chapter 2, you built a Web server that does just\nthis. For such a server, at any given time there may be many connection\nsockets (with different identifiers) attached to the same process.\nIf the client and server are using persistent HTTP, then throughout the\nduration of the persistent connection the client and server exchange HTTP\nmessages via the same server socket. However, if the client and server use\nnon-persistent HTTP, then a new TCP connection is created and closed for\nevery request/response, and hence a new socket is created and later closed\nfor every request/response. This frequent creating and closing of sockets\ncan severely impact the performance of a busy Web server (although a\nnumber of operating system tricks can be used to mitigate the problem).\nReaders interested in the operating system issues surrounding persistent and\nnon-persistent HTTP are encouraged to see [Nielsen 1997; Nahum 2002].\ntransport-layer \nmultiplexing \ndemultiplexing, let’s move on and discuss one of the Internet’s transport\nprotocols, UDP. In the next section, we’ll see that UDP adds little more to\nthe network-layer protocol than a multiplexing/demultiplexing service.\n3.3 Connectionless Transport: UDP\nIn this section, we’ll take a close look at UDP, how it works, and what it\ndoes. We encourage you to refer back to Section 2.1, which includes an\noverview of the UDP service model, and to Section 2.7.1, which discusses\nsocket programming using UDP.\nTo motivate our discussion about UDP, suppose you were interested in\ndesigning a no-frills, bare-bones transport protocol. How might you go\nabout doing this? You might first consider using a vacuous transport\nprotocol. In particular, on the sending side, you might consider taking the\nmessages from the application process and passing them directly to the\nnetwork layer; and on the receiving side, you might consider taking the\nmessages arriving from the network layer and passing them directly to the\napplication process. But as we learned in the previous section, we have to\ndo a little more than nothing! At the very least, the transport layer has to\nprovide a multiplexing/demultiplexing service in order to pass data between\nthe network layer and the correct application-level process.\nUDP, defined in [RFC 768], does just about as little as a transport\nprotocol can do. Aside from the multiplexing/demultiplexing function and\nsome light error checking, it adds nothing to IP. In fact, if the application\ndeveloper chooses UDP instead of TCP, then the application is almost\ndirectly talking with IP. UDP takes messages from the application process,\ndestination \nmultiplexing/demultiplexing service, adds two other small fields, and\npasses the resulting segment to the network layer. The network layer\nencapsulates the transport-layer segment into an IP datagram and then\nmakes a best-effort attempt to deliver the segment to the receiving host. If\nthe segment arrives at the receiving host, UDP uses the destination port\nnumber to deliver the segment’s data to the correct application process.\nNote that with UDP there is no handshaking between sending and receiving\ntransport-layer entities before sending a segment. For this reason, UDP is\nsaid to be connectionless.\nDNS is an example of an application-layer protocol that typically uses\nUDP. When the DNS application in a host wants to make a query, it\nconstructs a DNS query message and passes the message to UDP. Without\nperforming any handshaking with the UDP entity running on the destination\nend system, the host-side UDP adds header fields to the message and passes\nthe resulting segment to the network layer. The network layer encapsulates\nthe UDP segment into a datagram and sends the datagram to a name server.\nThe DNS application at the querying host then waits for a reply to its query.\nIf it doesn’t receive a reply (possibly because the underlying network lost\nthe query or the reply), it might try resending the query, try sending the\nquery to another name server, or inform the invoking application that it\ncan’t get a reply.\nNow you might be wondering why an application developer would ever\nchoose to build an application over UDP rather than over TCP. Isn’t TCP\nalways preferable, since TCP provides a reliable data transfer service, while\nUDP does not? The answer is no, as some applications are better suited for\nUDP for the following reasons:\nFiner application-level control over what data is sent, and when. Under\nUDP, as soon as an application process passes data to UDP, UDP will\npackage the data inside a UDP segment and immediately pass the\nsegment to the network layer. TCP, on the other hand, has a congestion-\ncontrol mechanism that throttles the transport-layer TCP sender when\none or more links between the source and destination hosts become\nexcessively congested. TCP will also continue to resend a segment until\nthe receipt of the segment has been acknowledged by the destination,\nregardless of how long reliable delivery takes. Since real-time\napplications often require a minimum sending rate, do not want to\noverly delay segment transmission, and can tolerate some data loss,\nTCP’s service model is not particularly well matched to these\napplications’ needs. As discussed below, these applications can use\nUDP and implement, as part of the application, any additional\nfunctionality that is needed beyond UDP’s no-frills segment-delivery\nNo connection establishment. As we’ll discuss later, TCP uses a three-\nway handshake before it starts to transfer data. UDP just blasts away\nwithout any formal preliminaries. Thus UDP does not introduce any\ndelay to establish a connection. This is probably the principal reason\nwhy DNS runs over UDP rather than TCP—DNS would be much\nslower if it ran over TCP. HTTP uses TCP rather than UDP, since\nreliability is critical for Web pages with text. But, as we briefly\ndiscussed in Section 2.2, the TCP connection-establishment delay in\nHTTP is an important contributor to the delays associated with\ndownloading Web documents. Indeed, the QUIC protocol (Quick UDP\nInternet Connection, [IETF QUIC 2020]), used in Google’s Chrome\nbrowser, uses UDP as its underlying transport protocol and implements\nreliability in an application-layer protocol on top of UDP. We’ll take a\ncloser look at QUIC in Section 3.8.\nNo connection state. TCP maintains connection state in the end systems.\nThis connection state includes receive and send buffers, congestion-\nof TCP. We learned in Chapter 2 that early versions of HTTP ran over TCP\nbut that more recent versions of HTTP run over UDP, providing their own\nerror control and congestion control (among other services) at the\napplication layer. Nevertheless, many important applications run over UDP\nrather than TCP. For example, UDP is used to carry network management\n(SNMP; see Section 5.7) data. UDP is preferred to TCP in this case, since\nnetwork management applications must often run when the network is in a\nstressed state—precisely when reliable, congestion-controlled data transfer\nis difficult to achieve. Also, as we mentioned earlier, DNS runs over UDP,\nthereby avoiding TCP’s connection-establishment delays.\nFigure 3.6 ♦Popular Internet applications and their underlying\ntransport protocols\nAs shown in Figure 3.6, both UDP and TCP are sometimes used today\nwith multimedia applications, such as Internet phone, real-time video\nconferencing, and streaming of stored audio and video. We just mention\nnow that all of these applications can tolerate a small amount of packet loss,\nso that reliable data transfer is not absolutely critical for the application’s\nsuccess. Furthermore, real-time applications, like Internet phone and video\nconferencing, react very poorly to TCP’s congestion control. For these\nreasons, developers of multimedia applications may choose to run their\napplications over UDP instead of TCP. When packet loss rates are low, and\nwith some organizations blocking UDP traffic for security reasons (see\nChapter 8), TCP becomes an increasingly attractive protocol for streaming\nmedia transport.\nAlthough commonly done today, running multimedia applications over\nUDP needs to be done with care. As we mentioned above, UDP has no\ncongestion control. But congestion control is needed to prevent the network\nfrom entering a congested state in which very little useful work is done. If\neveryone were to start streaming high-bit-rate video without using any\ncongestion control, there would be so much packet overflow at routers that\nvery few UDP packets would successfully traverse the source-to-destination\npath. Moreover, the high loss rates induced by the uncontrolled UDP\nsenders would cause the TCP senders (which, as we’ll see, do decrease their\nsending rates in the face of congestion) to dramatically decrease their rates.\nThus, the lack of congestion control in UDP can result in high loss rates\nbetween a UDP sender and receiver, and the crowding out of TCP sessions.\nMany researchers have proposed new mechanisms to force all sources,\nincluding UDP sources, to perform adaptive congestion control [Mahdavi\n1997; Floyd 2000; Kohler 2006: RFC 4340].\nBefore discussing the UDP segment structure, we mention that it is ­-\npossible for an application to have reliable data transfer when using UDP.\nThis can be done if reliability is built into the application itself (for\nChapter 6, we’ll examine error-detection and -correction techniques in\ngreater detail; these techniques allow the receiver to detect and possibly\ncorrect packet bit errors. For now, we need only know that these\ntechniques require that extra bits (beyond the bits of original data to be\ntransferred) be sent from the sender to the receiver; these bits will be\ngathered into the packet checksum field of the rdt2.0 data packet.\nReceiver feedback. Since the sender and receiver are typically executing\non different end systems, possibly separated by thousands of miles, the\nonly way for the sender to learn of the receiver’s view of the world (in\nthis case, whether or not a packet was received correctly) is for the\nreceiver to provide explicit feedback to the sender. The positive (ACK)\nhosts. Unlike the bulk data transfer applications discussed in Chapter 2,\nTelnet is an interactive application. We discuss a Telnet example here, as it\nstudy CSMA/CD in Chapter 6.\nFast Retransmit\nOne of the problems with timeout-triggered retransmissions is that the\ntimeout period can be relatively long. When a segment is lost, this long\ntimeout period forces the sender to delay resending the lost packet, thereby\nincreasing the end-to-end delay. Fortunately, the sender can often detect\npacket loss well before the timeout event occurs by noting so-called\nduplicate ACKs. A duplicate ACK is an ACK that reacknowledges a\nsegment for which the sender has already received an earlier\nthe client TCP. (We’ll see in Chapter 8 that the allocation of these\nbuffers and variables before completing the third step of the three-way\nhandshake makes TCP vulnerable to a denial-of-service attack known as\nSYN flooding.) This connection-granted segment also contains no\napplication-layer data. However, it does contain three important pieces\nof information in the segment header. First, the SYN bit is set to 1.\nexamples from Chapter 2). This causes TCP in the client to send a SYN\nsegment to TCP in the server. After having sent the SYN segment, the client\nTCP enters the SYN_SENT state. While in the SYN_SENT state, the client\nTCP waits for a segment from the server TCP that includes an\ndiscussed in Chapter 5.\nNow that we have a good understanding of TCP connection\nmanagement, let’s revisit the nmap port-scanning tool and examine more\nclosely how it works. To explore a specific TCP port, say port 6789, on a\ntarget host, nmap will send a TCP SYN segment with destination port 6789\nto that host. There are three possible outcomes:\nThe source host receives a TCP SYNACK segment from the target host.\nSince this means that an application is running with TCP port 6789 on\nthe target post, nmap returns “open.”\nWe’ve seen in our discussion of TCP’s three-way handshake that a server allocates\nand initializes connection variables and buffers in response to a received SYN. The\nserver then sends a SYNACK in response, and awaits an ACK segment from the client.\nIf the client does not send an ACK to complete the third step of this 3-way handshake,\neventually (often after a minute or more) the server will terminate the half-open\nconnection and reclaim the allocated resources.\nThis TCP connection management protocol sets the stage for a classic Denial of\nService (DoS) attack known as the SYN flood attack. In this attack, the attacker(s)\nsend a large number of TCP SYN segments, without completing the third handshake\nstep. With this deluge of SYN segments, the server’s connection resources become\nexhausted as they are allocated (but never used!) for half-open connections; legitimate\nclients are then denied service. Such SYN flooding attacks were among the first\ndocumented DoS attacks [CERT SYN 1996]. Fortunately, an effective defense known\nas SYN cookies [RFC 4987] are now deployed in most major operating systems. SYN\ncookies work as follows:\nWhen the server receives a SYN segment, it does not know if the segment is\ncoming from a legitimate user or is part of a SYN flood attack. So, instead of\ncreating a half-open TCP connection for this SYN, the server creates an initial TCP\nsequence number that is a complicated function (hash function) of source and\ndestination IP addresses and port numbers of the SYN segment, as well as a\nsecret number only known to the server. This carefully crafted initial sequence\nnumber is the so-called “cookie.” The server then sends the client a SYNACK\npacket with this special initial sequence number. Importantly, the server does not\nremember the cookie or any other state information corresponding to the SYN.\nA legitimate client will return an ACK segment. When the server receives this ACK,\nit must verify that the ACK corresponds to some SYN sent earlier. But how is this\ndone if the server maintains no memory about SYN segments? As you may have\nguessed, it is done with the cookie. Recall that for a legitimate ACK, the value in the\n(Firewalls are discussed in Chapter 8.)\nThe source receives nothing. This likely means that the SYN segment\nwas blocked by an intervening firewall and never reached the target\nNmap is a powerful tool that can “case the joint” not only for open TCP\nports, but also for open UDP ports, for firewalls and their configurations,\nand even for the versions of applications and operating systems. Most of\nthis is done by manipulating TCP connection-management segments. You\ncan download nmap from www.nmap.org.\nThis completes our introduction to error control and flow control in\nTCP. In Section 3.7, we’ll return to TCP and look at TCP congestion control\nin some depth. Before doing so, however, we first step back and examine\ncongestion-control issues in a broader context.\n3.6 Principles of Congestion Control\nIn the previous sections, we examined both the general principles and\nspecific TCP mechanisms used to provide for a reliable data transfer service\nin the face of packet loss. We mentioned earlier that, in practice, such loss\ntypically results from the overflowing of router buffers as the network\nbecomes congested. Packet retransmission thus treats a symptom of\nnetwork congestion (the loss of a specific transport-layer segment) but does\nnot treat the cause of network congestion—too many sources attempting to\nsend data at too high a rate. To treat the cause of network congestion,\nmechanisms are needed to throttle senders in the face of network\ncongestion.\nIn this section, we consider the problem of congestion control in a\ngeneral context, seeking to understand why congestion is a bad thing, how\nnetwork congestion is manifested in the performance received by upper-\nlayer applications, and various approaches that can be taken to avoid, or\nreact to, network congestion. This more general study of congestion control\nis appropriate since, as with reliable data transfer, it is high on our “top-ten”\nlist of fundamentally important problems in networking. The following\nsection contains a detailed study of TCP’s congestion-control algorithm.\n3.6.1 The Causes and the Costs of Congestion\nLet’s begin our general study of congestion control by examining three\nincreasingly complex scenarios in which congestion occurs. In each case,\nwe’ll look at why congestion occurs in the first place and at the cost of\ncongestion (in terms of resources not fully utilized and poor performance\nreceived by the end systems). We’ll not (yet) focus on how to react to, or\navoid, congestion but rather focus on the simpler issue of understanding\nwhat happens as hosts increase their transmission rate and the network\nbecomes congested.\nScenario 1: Two Senders, a Router with Infinite Buffers\nWe begin by considering perhaps the simplest congestion scenario possible:\nTwo hosts (A and B) each have a connection that shares a single hop\nbetween source and destination, as shown in Figure 3.43.\nFigure 3.43 ♦Congestion scenario 1: Two connections sharing a\nsingle hop with infinite buffers\nLet’s assume that the application in Host A is sending data into the\nconnection (for example, passing data to the transport-level protocol via a\nsocket) at an average rate of λ  bytes/sec. These data are original in the\nsense that each unit of data is sent into the socket only once. The underlying\ntransport-level protocol is a simple one. Data is encapsulated and sent; no\nerror recovery (e.g., retransmission), flow control, or congestion control is\nperformed. Ignoring the additional overhead due to adding transport- and\nlower-layer header information, the rate at which Host A offers traffic to the\nrouter in this first scenario is thus λ  bytes/sec. Host B operates in a similar\nmanner, and we assume for simplicity that it too is sending at a rate of λ\nbytes/sec. Packets from Hosts A and B pass through a router and over a\nshared outgoing link of capacity R. The router has buffers that allow it to\nstore incoming packets when the packet-arrival rate exceeds the outgoing\nlink’s capacity. In this first scenario, we assume that the router has an\ninfinite amount of buffer space.\nFigure 3.44 plots the performance of Host A’s connection under this\nfirst scenario. The left graph plots the per-connection throughput (number\nof bytes per second at the receiver) as a function of the connection-sending\nrate. For a sending rate between 0 and R/2, the throughput at the receiver\nequals the sender’s sending rate—everything sent by the sender is received\nat the receiver with a finite delay. When the sending rate is above R/2,\nhowever, the throughput is only R/2. This upper limit on throughput is a\nconsequence of the sharing of link capacity between two connections. The\nlink simply cannot deliver packets to a receiver at a steady-state rate that\nexceeds R/2. No matter how high Hosts A and B set their sending rates, they\nwill each never see a throughput higher than R/2.\nFigure 3.44 ♦Congestion scenario 1: Throughput and delay as a\nfunction of host sending rate\nAchieving a per-connection throughput of R/2 might actually appear to\nbe a good thing, because the link is fully utilized in delivering packets to\ntheir destinations. The right-hand graph in Figure 3.44, however, shows the\nconsequence of operating near link capacity. As the sending rate approaches\nR/2 (from the left), the average delay becomes larger and larger. When the\nsending rate exceeds R/2, the average number of queued packets in the\nrouter is unbounded, and the average delay between source and destination\nbecomes infinite (assuming that the connections operate at these sending\nrates for an infinite period of time and there is an infinite amount of\nbuffering available). Thus, while operating at an aggregate throughput of\nnear R may be ideal from a throughput standpoint, it is far from ideal from a\ndelay standpoint. Even in this (extremely) idealized scenario, we’ve already\nfound one cost of a congested network—large queuing delays are\nexperienced as the packet-arrival rate nears the link capacity.\nScenario 2: Two Senders and a Router with Finite Buffers\nLet’s now slightly modify scenario 1 in the following two ways (see Figure\n3.45). First, the amount of router buffering is assumed to be finite. A\nconsequence of this real-world assumption is that packets will be dropped\nwhen arriving to an already-full buffer. Second, we assume that each\nconnection is reliable. If a packet containing a transport-level segment is\ndropped at the router, the sender will eventually retransmit it. Because\npackets can be retransmitted, we must now be more careful with our use of\nthe term sending rate. Specifically, let us again denote the rate at which the\napplication sends original data into the socket by λ  bytes/sec. The rate at\nwhich the transport layer sends segments (containing original data and\nretransmitted data) into the network will be denoted λ'  bytes/sec. λ'  is\nsometimes referred to as the offered load to the network.\nFigure 3.45 ♦Scenario 2: Two hosts (with retransmissions) and a\nrouter with finite buffers\nThe performance realized under scenario 2 will now depend strongly on\nhow retransmission is performed. First, consider the unrealistic case that\nHost A is able to somehow (magically!) determine whether or not a buffer is\nfree in the router and thus sends a packet only when a buffer is free. In this\ncase, no loss would occur, λ  would be equal to λ' , and the throughput of\nthe connection would be equal to λ . This case is shown in Figure 3.46(a).\nFrom a throughput standpoint, performance is ideal—everything that is sent\nis received. Note that the average host sending rate cannot exceed R/2 under\nthis scenario, since packet loss is assumed never to occur.\nFigure 3.46 ♦Scenario 2 performance with finite buffers\nConsider next the slightly more realistic case that the sender retransmits\nonly when a packet is known for certain to be lost. (Again, this assumption\nis a bit of a stretch. However, it is possible that the sending host might set\nits timeout large enough to be virtually assured that a packet that has not\nbeen acknowledged has been lost.) In this case, the performance might look\nsomething like that shown in Figure 3.46(b). To appreciate what is\nhappening here, consider the case that the offered load, λ'  (the rate of\noriginal data transmission plus retransmissions), equals R/2. According to\nFigure 3.46(b), at this value of the offered load, the rate at which data are\ndelivered to the receiver application is R/3. Thus, out of the 0.5R units of\ndata transmitted, 0.333R bytes/sec (on average) are original data and 0.166R\nbytes/sec (on average) are retransmitted data. We see here another cost of a\ncongested network—the sender must perform retransmissions in order to\ncompensate for dropped (lost) packets due to buffer overflow.\nFinally, let us consider the case that the sender may time out\nprematurely and retransmit a packet that has been delayed in the queue but\nnot yet lost. In this case, both the original data packet and the\nretransmission may reach the receiver. Of course, the receiver needs but one\ncopy of this packet and will discard the retransmission. In this case, the\nwork done by the router in forwarding the retransmitted copy of the original\npacket was wasted, as the receiver will have already received the original\ncopy of this packet. The router would have better used the link transmission\ncapacity to send a different packet instead. Here then is yet another cost of a\ncongested network—unneeded retransmissions by the sender in the face of\nlarge delays may cause a router to use its link bandwidth to forward\nunneeded copies of a packet. Figure 3.46 (c) shows the throughput versus\noffered load when each packet is assumed to be forwarded (on average)\ntwice by the router. Since each packet is forwarded twice, the throughput\nwill have an asymptotic value of R/4 as the offered load approaches R/2.\nScenario 3: Four Senders, Routers with Finite Buffers, and\nMultihop Paths\nIn our final congestion scenario, four hosts transmit packets, each over\noverlapping two-hop paths, as shown in Figure 3.47. We again assume that\neach host uses a timeout/retransmission mechanism to implement a reliable\ndata transfer service, that all hosts have the same value of λ , and that all\nrouter links have capacity R bytes/sec.\nFigure 3.47 ♦Four senders, routers with finite buffers, and multihop\nLet’s consider the connection from Host A to Host C, passing through\nrouters R1 and R2. The A–C connection shares router R1 with the D–B\nconnection and shares router R2 with the B–D connection. For extremely\nsmall values of λ , buffer overflows are rare (as in congestion scenarios 1\nand 2), and the throughput approximately equals the offered load. For\nslightly larger values of λ , the corresponding throughput is also larger,\nsince more original data is being transmitted into the network and delivered\nto the destination, and overflows are still rare. Thus, for small values of λ ,\nan increase in λ  results in an increase in λ .\nHaving considered the case of extremely low traffic, let’s next examine\nthe case that λ  (and hence λ' ) is extremely large. Consider router R2. The\nA–C traffic arriving to router R2 (which arrives at R2 after being forwarded\nfrom R1) can have an arrival rate at R2 that is at most R, the capacity of the\nlink from R1 to R2, regardless of the value of λ . If λ'  is extremely large\nfor all connections (including the B–D connection), then the arrival rate of\nB–D traffic at R2 can be much larger than that of the A–C traffic. Because\nthe A–C and B–D traffic must compete at router R2 for the limited amount\nof buffer space, the amount of A–C traffic that successfully gets through R2\n(that is, is not lost due to buffer overflow) becomes smaller and smaller as\nthe offered load from B–D gets larger and larger. In the limit, as the offered\nload approaches infinity, an empty buffer at R2 is immediately filled by a\nB–D packet, and the throughput of the A–C connection at R2 goes to zero.\nThis, in turn, implies that the A–C end-to-end throughput goes to zero in the\nlimit of heavy traffic. These considerations give rise to the offered load\nversus throughput tradeoff shown in Figure 3.48.\nFigure 3.48 ♦Scenario 3 performance with finite buffers and\nmultihop paths\nThe reason for the eventual decrease in throughput with increasing\noffered load is evident when one considers the amount of wasted work done\nby the network. In the high-traffic scenario outlined above, whenever a\npacket is dropped at a second-hop router, the work done by the first-hop\nrouter in forwarding a packet to the second-hop router ends up being\n“wasted.” The network would have been equally well off (more accurately,\nequally bad off) if the first router had simply discarded that packet and\nremained idle. More to the point, the transmission capacity used at the first\nrouter to forward the packet to the second router could have been much\nmore profitably used to transmit a different packet. (For example, when\nselecting a packet for transmission, it might be better for a router to give\npriority to packets that have already traversed some number of upstream\nrouters.) So here we see yet another cost of dropping a packet due to\ncongestion—when a packet is dropped along a path, the transmission\ncapacity that was used at each of the upstream links to forward that packet\nto the point at which it is dropped ends up having been wasted.\n3.6.2 Approaches to Congestion Control\nIn Section 3.7, we’ll examine TCP’s specific approach to congestion control\nin great detail. Here, we identify the two broad approaches to congestion\ncontrol that are taken in practice and discuss specific network architectures\nand congestion-control protocols embodying these approaches.\nAt the highest level, we can distinguish among congestion-control\napproaches by whether the network layer provides explicit assistance to the\ntransport layer for congestion-control purposes:\nEnd-to-end congestion control. In an end-to-end approach to congestion\ncontrol, the network layer provides no explicit support to the transport\nlayer for congestion-control purposes. Even the presence of network\ncongestion must be inferred by the end systems based only on observed\nnetwork behavior (for example, packet loss and delay). We’ll see\nshortly in Section 3.7.1 that TCP takes this end-to-end approach toward\ncongestion control, since the IP layer is not required to provide\nfeedback to hosts regarding network congestion. TCP segment loss (as\nwe’ll study in Chapter 8), thus providing faster establishment than the\nprotocol stack in Figure 3.58(a), where multiple RTTs are required to\nfirst establish a TCP connection, and then establish a TLS connection\nover the TCP connection.\nFigure 3.58 ♦(a) traditional secure HTTP protocol stack, and\nthe (b) secure QUIC-based HTTP/3 protocol stack\nStreams. QUIC allows several different application-level “streams” to\nbe multiplexed through a single QUIC connection, and once a QUIC\nconnection is established, new streams can be quickly added. A stream\nis an abstraction for the reliable, in-order bi-directional delivery of data\nbetween two QUIC endpoints. In the context of HTTP/3, there would be\na different stream for each object in a Web page. Each connection has a\nconnection ID, and each stream within a connection has a stream ID;\nboth of these IDs are contained in a QUIC packet header (along with\nother header information). Data from multiple streams may be\ncontained within a single QUIC segment, which is carried over UDP.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC\n3286] is an earlier reliable, message-oriented protocol that pioneered\nthe notion of multiplexing multiple application-level “streams” through\na single SCTP connection. We’ll see in Chapter 7 that SCTP is used in\ncontrol plane protocols in 4G/5G cellular wireless networks.\nReliable, TCP-friendly congestion-controlled data transfer. As\nillustrated in Figure 3.59(b), QUIC provides reliable data transfer to\neach QUIC stream separately. Figure 3.59(a) shows the case of\nHTTP/1.1 sending multiple HTTP requests, all over a single TCP\nconnection. Since TCP provides reliable, in-order byte delivery, this\nmeans that the multiple HTTP requests must be delivered in-order at the\ndestination HTTP server. Thus, if bytes from one HTTP request are lost,\nthe remaining HTTP requests can not be delivered until those lost bytes\nare retransmitted and correctly received by TCP at the HTTP server—\nthe so-called HOL blocking problem that we encountered earlier in\nSection 2.2.5. Since QUIC provides a reliable in-order delivery on a\nper-stream basis, a lost UDP segment only impacts those streams whose\ndata was carried in that segment; HTTP messages in other streams can\ncontinue to be received and delivered to the application. QUIC provides\nIn Chapter 1, we said that a computer network can be partitioned into\nthe ­“network edge” and the “network core.” The network edge covers\neverything that happens in the end systems. Having now covered the\napplication layer and the t­ransport layer, our discussion of the network edge\nis complete. It is time to explore the network core! This journey begins in\nthe next two chapters, where we’ll study the network layer, and continues\ninto Chapter 6, where we’ll study the link layer.\nHomework Problems and Questions\nChapter 3 Review Questions\nSECTIONS 3.1–3.3\nR1. Suppose the network layer provides the following service. The\nnetwork layer in the source host accepts a segment of maximum size\n1,200 bytes and a destination host address from the transport layer.\nThe network layer then guarantees to deliver the segment to the\ntransport layer at the destination host. Suppose many network\napplication processes can be running at the destination host.\na. Design the simplest possible transport-layer protocol that will\nget application data to the desired process at the destination host.\nAssume the operating system in the destination host has\nassigned a 4-byte port number to each running application\nb. Modify this protocol so that it provides a “return address” to the\ndestination process.\nc. In your protocols, does the transport layer “have to do anything”\nin the core of the computer network?\nR2. Consider a planet where everyone belongs to a family of six, every\nfamily lives in its own house, each house has a unique address, and\neach person in a given house has a unique name. Suppose this planet\nhas a mail service that delivers letters from source house to\ndestination house. The mail service requires that (1) the letter be in an\nenvelope, and that (2) the address of the destination house (and\nnothing more) be clearly written on the envelope. Suppose each\nfamily has a delegate family member who collects and distributes\nletters for the other family members. The letters do not necessarily\nprovide any indication of the recipients of the letters.\na. Using the solution to Problem R1 above as inspiration, describe\na protocol that the delegates can use to deliver letters from a\nsending family member to a receiving family member.\nb. In your protocol, does the mail service ever have to open the\nenvelope and examine the letter in order to provide its service?\nR3. How is a UDP socket fully identified? What about a TCP socket?\nWhat is the difference between the full identification of both sockets?\nR4. Describe why an application developer might choose to run an\napplication over UDP rather than TCP.\nR5. Why is it that voice and video traffic is often sent over TCP rather\nthan UDP in today’s Internet? (Hint: The answer we are looking for\nhas nothing to do with TCP’s congestion-control mechanism.)\nR6. Is it possible for an application to enjoy reliable data transfer even\nwhen the application runs over UDP? If so, how?\nR7. Suppose a process in Host C has a UDP socket with port number\n6789. Suppose both Host A and Host B each send a UDP segment to\nHost C with destination port number 6789. Will both of these\nsegments be directed to the same socket at Host C? If so, how will the\nprocess at Host C know that these two segments originated from two\ndifferent hosts?\nR8. Suppose that a Web server runs in Host C on port 80. Suppose this\nWeb server uses persistent connections, and is currently receiving\nrequests from two different Hosts, A and B. Are all of the requests\nbeing sent through the same socket at Host C? If they are being\npassed through different sockets, do both of the sockets have port 80?\nDiscuss and explain.\nSECTION 3.4\nR9. In our rdt protocols, why did we need to introduce sequence\nR10. In our rdt protocols, why did we need to introduce timers?\nR11. Suppose that the roundtrip delay between sender and receiver is\nconstant and known to the sender. Would a timer still be necessary in\nprotocol rdt 3.0, assuming that packets can be lost? Explain.\nR12. Visit the Go-Back-N interactive animation at the Companion\na. Have the source send five packets, and then pause the animation\nbefore any of the five packets reach the destination. Then kill the\nfirst packet and resume the animation. Describe what happens.\nb. Repeat the experiment, but now let the first packet reach the\ninteracting parts, the data plane and the control plane. In Chapter 4,\nwe’ll first cover the data plane functions of the network layer—the\nper-router functions in the network layer that determine how a\ndatagram (that is, a network-layer packet) arriving on one of a router’s\ninput links is forwarded to one of that router’s output links. We’ll\ncover both traditional IP forwarding (where forwarding is based on a\ndatagram’s destination address) and generalized forwarding (where\nforwarding and other functions may be performed using values in\nseveral different fields in the datagram’s header). We’ll study the IPv4\nand IPv6 protocols and addressing in detail. In Chapter 5, we’ll cover\nthe control plane functions of the network layer—the network-wide\nlogic that controls how a datagram is routed among routers along an\nend-to-end path from the source host to the destination host. We’ll\ncover routing algorithms, as well as routing protocols, such as OSPF\nand BGP, that are in widespread use in today’s Internet. Traditionally,\nthese control-plane routing protocols and data-plane forwarding\nfunctions have been implemented together, monolithically, within a\nrouter. Software-defined networking (SDN) explicitly separates the\ndata plane and control plane by implementing these control plane\nfunctions as a separate service, typically in a remote “controller.” We’ll\nalso cover SDN controllers in Chapter 5.\nThis distinction between data-plane and control-plane functions in the\nnetwork layer is an important concept to keep in mind as you learn about\nthe network layer —it will help structure your thinking about the network\nlayer and reflects a modern view of the network layer’s role in computer\nnetworking.\n4.1 Overview of Network Layer\nFigure 4.1 shows a simple network with two hosts, H1 and H2, and several\nrouters on the path between H1 and H2. Let’s suppose that H1 is sending\ninformation to H2, and consider the role of the network layer in these hosts\nand in the intervening routers. The network layer in H1 takes segments\nfrom the transport layer in H1, encapsulates each segment into a datagram,\nand then sends the datagrams to its nearby router, R1. At the receiving host,\nH2, the network layer receives the datagrams from its nearby router R2,\nextracts the transport-layer segments, and delivers the segments up to the\ntransport layer at H2. The primary data-plane role of each router is to\nforward datagrams from its input links to its output links; the primary role\nof the network control plane is to coordinate these local, per-router\nforwarding actions so that datagrams are ultimately transferred end-to-end,\nalong paths of routers between source and destination hosts. Note that the\nrouters in Figure 4.1 are shown with a truncated protocol stack, that is, with\nno upper layers above the network layer, because routers do not run\napplication- and transport-layer protocols such as those we examined in\nChapters 2 and 3.\n4.1.1 Forwarding and Routing: The Data and\nControl Planes\nThe primary role of the network layer is deceptively simple—to move\npackets from a sending host to a receiving host. To do so, two important\nnetwork-layer functions can be identified:\nForwarding. When a packet arrives at a router’s input link, the router\nmust move the packet to the appropriate output link. For example, a\npacket arriving from Host H1 to Router R1 in Figure 4.1 must be\nforwarded to the next router on a path to H2. As we will see, forwarding\nis but one function (albeit the most common and important one!)\nimplemented in the data plane. In the more general case, which we’ll\ncover in Section 4.4, a packet might also be blocked from exiting a\nrouter (for example, if the packet originated at a known malicious\nsending host, or if the packet were destined to a forbidden destination\nhost), or might be duplicated and sent over multiple outgoing links.\nRouting. The network layer must determine the route or path taken by\npackets as they flow from a sender to a receiver. The algorithms that\ncalculate these paths are referred to as routing algorithms. A routing\nalgorithm would determine, for example, the path along which packets\nflow from H1 to H2 in Figure 4.1. Routing is implemented in the\ncontrol plane of the network layer.\nFigure 4.1 ♦The network layer\nThe terms forwarding and routing are often used interchangeably by\nauthors discussing the network layer. We’ll use these terms much more\nprecisely in this book. Forwarding refers to the router-local action of\ntransferring a packet from an input link interface to the appropriate output\nlink interface. Forwarding takes place at very short timescales (typically a\nfew nanoseconds), and thus is typically implemented in hardware. Routing\nrefers to the network-wide process that determines the end-to-end paths that\npackets take from source to destination. Routing takes place on much\nlonger timescales (typically seconds), and as we will see is often\nimplemented in software. Using our driving analogy, consider the trip from\nPennsylvania to Florida undertaken by our traveler back in Section 1.3.1.\nDuring this trip, our driver passes through many interchanges en route to\nFlorida. We can think of forwarding as the process of getting through a\nsingle interchange: A car enters the interchange from one road and\ndetermines which road it should take to leave the interchange. We can think\nof routing as the process of planning the trip from Pennsylvania to Florida:\nBefore embarking on the trip, the driver has consulted a map and chosen\none of many paths possible, with each path consisting of a series of road\nsegments connected at interchanges.\nA key element in every network router is its forwarding table. A router\nforwards a packet by examining the value of one or more fields in the\nAn Overview of Chapter 4\nHaving now provided an overview of the network layer, we’ll cover the\ndata-plane component of the network layer in the following sections in this\nchapter. In Section 4.2, we’ll dive down into the internal hardware\noperations of a router, including input and output packet processing, the\nrouter’s internal switching mechanism, and packet queuing and scheduling.\nIn Section 4.3, we’ll take a look at traditional IP forwarding, in which\npackets are forwarded to output ports based on their destination IP\naddresses. We’ll encounter IP addressing, the celebrated IPv4 and IPv6\nprotocols and more. In Section 4.4, we’ll cover more generalized\nforwarding, where packets may be forwarded to output ports based on a\nlarge number of header values (i.e., not only based on destination IP\naddress). Packets may be blocked or duplicated at the router, or may have\ncertain header field values rewritten—all under software control. This more\ngeneralized form of packet forwarding is a key component of a modern\nnetwork data plane, including the data plane in software-defined networks\n(SDN). In Section 4.5, we’ll learn about “middleboxes” that can perform\nfunctions in addition to forwarding.\nWe mention here in passing that the terms forwarding and switching are\noften used interchangeably by computer-networking researchers and\npractitioners; we’ll use both terms interchangeably in this textbook as well.\nWhile we’re on the topic of terminology, it’s also worth mentioning two\nother terms that are often used interchangeably, but that we will use more\ncarefully. We’ll reserve the term packet switch to mean a general packet-\nswitching device that transfers a packet from input link interface to output\nlink interface, according to values in a packet’s header fields. Some packet\nswitches, called link-layer switches (examined in Chapter 6), base their\nforwarding decision on values in the fields of the link-layer frame; switches\nare thus referred to as link-layer (layer 2) devices. Other packet switches,\ncalled routers, base their forwarding decision on header field values in the\nnetwork-layer datagram. Routers are thus network-layer (layer 3) devices.\n(To fully appreciate this important distinction, you might want to review\nSection 1.5.2, where we discuss network-layer datagrams and link-layer\nframes and their relationship.) Since our focus in this chapter is on the\nnetwork layer, we’ll mostly use the term router in place of packet switch.\n4.2 What’s Inside a Router?\nNow that we’ve overviewed the data and control planes within the network\nlayer, the important distinction between forwarding and routing, and the\nservices and functions of the network layer, let’s turn our attention to its\nforwarding function—the actual transfer of packets from a router’s\nincoming links to the appropriate outgoing links at that router.\nA high-level view of a generic router architecture is shown in Figure\n4.4. Four router components can be identified:\nFigure 4.4 ♦Router architecture\nInput ports. An input port performs several key functions. It performs\nthe physical layer function of terminating an incoming physical link at a\nrouter; this is shown in the leftmost box of an input port and the\nrightmost box of an output port in Figure 4.4. An input port also\nperforms link-layer functions needed to interoperate with the link layer\nat the other side of the incoming link; this is represented by the middle\nboxes in the input and output ports. Perhaps most crucially, a lookup\nfunction is also performed at the input port; this will occur in the\nrightmost box of the input port. It is here that the forwarding table is\nconsulted to determine the router output port to which an arriving\npacket will be forwarded via the switching fabric. Control packets (for\nexample, packets carrying routing protocol information) are forwarded\nfrom an input port to the routing processor. Note that the term “port”\nhere—referring to the physical input and output router interfaces—is\ndistinctly different from the software ports associated with network\napplications and sockets discussed in Chapters 2 and 3. In practice, the\nnumber of ports supported by a router can range from a relatively small\nnumber in enterprise routers, to hundreds of 10 Gbps ports in a router at\nan ISP’s edge, where the number of incoming lines tends to be the\ngreatest. The Juniper MX2020, edge router, for example, supports up to\n800 100 Gbps Ethernet ports, with an overall router system capacity of\n800 Tbps [Juniper MX 2020 2020].\nSwitching fabric. The switching fabric connects the router’s input ports\nto its output ports. This switching fabric is completely contained within\nthe router—a network inside of a network router!\nOutput ports. An output port stores packets received from the\nswitching fabric and transmits these packets on the outgoing link by\nperforming the necessary link-layer and physical-layer functions. When\na link is bidirectional (that is, carries traffic in both directions), an\noutput port will typically be paired with the input port for that link on\nthe same line card.\nRouting processor. The routing processor performs control-plane\nfunctions. In traditional routers, it executes the routing protocols (which\nwe’ll study in Sections 5.3 and 5.4), maintains routing tables and\nattached link state information, and computes the forwarding table for\nthe router. In SDN routers, the routing processor is responsible for\ncommunicating with the remote controller in order to (among other\nactivities) receive forwarding table entries computed by the remote\ncontroller, and install these entries in the router’s input ports. The\nrouting processor also performs the network management functions that\nwe’ll study in Section 5.7.\nA router’s input ports, output ports, and switching fabric are almost\nalways implemented in hardware, as shown in Figure 4.4. To appreciate\nwhy a hardware implementation is needed, consider that with a 100 Gbps\ninput link and a 64-byte IP datagram, the input port has only 5.12 ns to\nprocess the datagram before another datagram may arrive. If N ports are\ncombined on a line card (as is often done in practice), the datagram-\nprocessing pipeline must operate N times faster—far too fast for software\nimplementation. Forwarding hardware can be implemented either using a\nrouter vendor’s own hardware designs, or constructed using purchased\nmerchant-silicon chips (for example, as sold by companies such as Intel and\nWhile the data plane operates at the nanosecond time scale, a router’s\ncontrol functions—executing the routing protocols, responding to attached\nlinks that go up or down, communicating with the remote controller (in the\nSDN case) and performing management functions—operate at the\nmillisecond or second timescale. These control plane functions are thus\nusually implemented in software and execute on the routing processor\n(typically a traditional CPU).\nBefore delving into the details of router internals, let’s return to our\nanalogy from the beginning of this chapter, where packet forwarding was\ncompared to cars entering and leaving an interchange. Let’s suppose that\nthe interchange is a roundabout, and that as a car enters the roundabout, a\nbit of processing is required. Let’s consider what information is required for\nthis processing:\nDestination-based forwarding. Suppose the car stops at an entry station\nand indicates its final destination (not at the local roundabout, but the\nultimate destination of its journey). An attendant at the entry station\nlooks up the final destination, determines the roundabout exit that leads\nto that final destination, and tells the driver which roundabout exit to\nGeneralized forwarding. The attendant could also determine the car’s\nexit ramp on the basis of many other factors besides the destination. For\nexample, the selected exit ramp might depend on the car’s origin, for\nexample the state that issued the car’s license plate. Cars from a certain\nset of states might be directed to use one exit ramp (that leads to the\ndestination via a slow road), while cars from other states might be\ndirected to use a different exit ramp (that leads to the destination via\nsuperhighway). The same decision might be made based on the model,\nmake and year of the car. Or a car not deemed roadworthy might be\nblocked and not be allowed to pass through the roundabout. In the case\nof generalized forwarding, any number of factors may contribute to the\nattendant’s choice of the exit ramp for a given car.\nOnce the car enters the roundabout (which may be filled with other cars\nentering from other input roads and heading to other roundabout exits), it\neventually leaves at the prescribed roundabout exit ramp, where it may\nencounter other cars leaving the roundabout at that exit.\nWe can easily recognize the principal router components in Figure 4.4\nin this analogy—the entry road and entry station correspond to the input\nport (with a lookup function to determine to local outgoing port); the\nroundabout corresponds to the switch fabric; and the roundabout exit road\ncorresponds to the output port. With this analogy, it’s instructive to consider\nwhere bottlenecks might occur. What happens if cars arrive blazingly fast\n(for example, the roundabout is in Germany or Italy!) but the station\nattendant is slow? How fast must the attendant work to ensure there’s no\nbackup on an entry road? Even with a blazingly fast attendant, what\nhappens if cars traverse the roundabout slowly—can backups still occur?\nAnd what happens if most of the cars entering at all of the roundabout’s\nentrance ramps all want to leave the roundabout at the same exit ramp—can\nbackups occur at the exit ramp or elsewhere? How should the roundabout\noperate if we want to assign priorities to different cars, or block certain cars\nfrom entering the roundabout in the first place? These are all analogous to\ncritical questions faced by router and switch designers.\nIn the following subsections, we’ll look at router functions in more\ndetail. [Turner 1988; McKeown 1997a; Partridge 1998; Iyer 2008; Serpanos\n2011; Zilberman 2019] provide a discussion of specific router architectures.\nFor concreteness and simplicity, we’ll initially assume in this section that\nforwarding decisions are based only on the packet’s destination address,\nrather than on a generalized set of packet header fields. We will cover the\ncase of more generalized packet forwarding in Section 4.4.\n4.2.1 Input Port Processing and Destination-Based\nA more detailed view of input processing is shown in Figure 4.5. As just\ndiscussed, the input port’s line-termination function and link-layer\nprocessing implement the physical and link layers for that individual input\nlink. The lookup performed in the input port is central to the router’s\noperation—it is here that the router uses the forwarding table to look up the\noutput port to which an arriving packet will be forwarded via the switching\nfabric. The forwarding table is either computed and updated by the routing\nprocessor (using a routing protocol to interact with the routing processors in\nother network routers) or is received from a remote SDN controller. The\nforwarding table is copied from the routing processor to the line cards over\na separate bus (e.g., a PCI bus) indicated by the dashed line from the\nrouting processor to the input line cards in Figure 4.4. With such a shadow\ncopy at each line card, forwarding decisions can be made locally, at each\ninput port, without invoking the centralized routing processor on a per-\npacket basis and thus avoiding a centralized processing bottleneck.\nFigure 4.5 ♦Input port processing\nLet’s now consider the “simplest” case that the output port to which an\nincoming packet is to be switched is based on the packet’s destination\naddress. In the case of 32-bit IP addresses, a brute-force implementation of\nthe forwarding table would have one entry for every possible destination\naddress. Since there are more than 4 billion possible addresses, this option\nis totally out of the question.\nAs an example of how this issue of scale can be handled, let’s suppose\nthat our router has four links, numbered 0 through 3, and that packets are to\nbe forwarded to the link interfaces as follows:\nClearly, for this example, it is not necessary to have 4 billion entries in the\nrouter’s forwarding table. We could, for example, have the following\nforwarding table with just four entries:\nWith this style of forwarding table, the router matches a prefix of the\npacket’s destination address with the entries in the table; if there’s a match,\nthe router forwards the packet to a link associated with the match. For\nexample, suppose the packet’s destination address is 11001000\n00010111 00010110 10100001; because the 21-bit prefix of this\naddress matches the first entry in the table, the router forwards the packet to\nlink interface 0. If a prefix doesn’t match any of the first three entries, then\nthe router forwards the packet to the default interface 3. Although this\nsounds simple enough, there’s a very important subtlety here. You may have\nnoticed that it is possible for a destination address to match more than one\nentry. For example, the first 24 bits of the address 11001000 00010111\n00011000 10101010 match the second entry in the table, and the first\n21 bits of the address match the third entry in the table. When there are\nmultiple matches, the router uses the longest prefix matching rule; that is,\nit finds the longest matching entry in the table and forwards the packet to\nthe link interface associated with the longest prefix match. We’ll see exactly\nwhy this longest prefix-matching rule is used when we study Internet\naddressing in more detail in Section 4.3.\nGiven the existence of a forwarding table, lookup is conceptually\nsimple—­hardware logic just searches through the forwarding table looking\nfor the longest prefix match. But at Gigabit transmission rates, this lookup\nmust be performed in nanoseconds (recall our earlier example of a 10 Gbps\nlink and a 64-byte IP datagram). Thus, not only must lookup be performed\nin hardware, but techniques beyond a simple linear search through a large\ntable are needed; surveys of fast lookup algorithms can be found in [Gupta\n2001, Ruiz-Sanchez 2001]. Special attention must also be paid to memory\naccess times, resulting in designs with embedded on-chip DRAM and faster\nSRAM (used as a DRAM cache) memories. In practice, Ternary Content\nAddressable Memories (TCAMs) are also often used for lookup [Yu 2004].\nWith a TCAM, a 32-bit IP address is presented to the memory, which\nreturns the content of the forwarding table entry for that address in\nessentially constant time. The Cisco Catalyst 6500 and 7600 Series routers\nand switches can hold upwards of a million TCAM forwarding table entries\n[Cisco TCAM 2014].\nOnce a packet’s output port has been determined via the lookup, the\npacket can be sent into the switching fabric. In some designs, a packet may\nbe temporarily blocked from entering the switching fabric if packets from\nother input ports are currently using the fabric. A blocked packet will be\nqueued at the input port and then scheduled to cross the fabric at a later\npoint in time. We’ll take a closer look at the blocking, queuing, and\nscheduling of packets (at both input ports and output ports) shortly.\nAlthough “lookup” is arguably the most important action in input port\nprocessing, many other actions must be taken: (1) physical- and link-layer\nprocessing must occur, as discussed previously; (2) the packet’s version\nnumber, checksum and time-to-live field—all of which we’ll study in\nSection 4.3—must be checked and the latter two fields rewritten; and (3)\ncounters used for network management (such as the number of IP\ndatagrams received) must be updated.\nLet’s close our discussion of input port processing by noting that the\ninput port steps of looking up a destination IP address (“match”) and then\nsending the packet into the switching fabric to the specified output port\n(“action”) is a specific case of a more general “match plus action”\nabstraction that is performed in many networked devices, not just routers. In\nlink-layer switches (covered in Chapter 6), link-layer destination addresses\nare looked up and several actions may be taken in addition to sending the\nframe into the switching fabric towards the output port. In firewalls\n(covered in Chapter 8)—devices that filter out selected incoming packets—\nan incoming packet whose header matches a given criteria (e.g., a\ncombination of source/destination IP addresses and transport-layer port\nnumbers) may be dropped (action). In a network address translator (NAT,\ncovered in Section 4.3), an incoming packet whose transport-layer port\nnumber matches a given value will have its port number rewritten before\nforwarding (action). Indeed, the “match plus action” abstraction [Bosshart\n2013] is both powerful and prevalent in network devices today, and is\ncentral to the notion of generalized forwarding that we’ll study in Section\n4.2.2 Switching\nThe switching fabric is at the very heart of a router, as it is through this\nfabric that the packets are actually switched (that is, forwarded) from an\ninput port to an output port. Switching can be accomplished in a number of\nways, as shown in Figure 4.6:\nSwitching via memory. The simplest, earliest routers were traditional\ncomputers, with switching between input and output ports being done\nunder direct control of the CPU (routing processor). Input and output\nports functioned as traditional I/O devices in a traditional operating\nsystem. An input port with an arriving packet first signaled the routing\nprocessor via an interrupt. The packet was then copied from the input\nport into processor memory. The routing processor then extracted the\ndestination address from the header, looked up the appropriate output\nport in the forwarding table, and copied the packet to the output port’s\nbuffers. In this scenario, if the memory bandwidth is such that a\nmaximum of B packets per second can be written into, or read from,\nmemory, then the overall forwarding throughput (the total rate at which\npackets are transferred from input ports to output ports) must be less\nthan B/2. Note also that two packets cannot be forwarded at the same\ntime, even if they have different destination ports, since only one\nmemory read/write can be done at a time over the shared system bus.\nSome modern routers switch via memory. A major difference from early\nrouters, however, is that the lookup of the destination address and the\nstoring of the packet into the appropriate memory location are\nperformed by processing on the input line cards. In some ways, routers\nthat switch via memory look very much like shared-memory\nmultiprocessors, with the processing on a line card switching (writing)\npackets into the memory of the appropriate output port. Cisco’s Catalyst\n8500 series switches [Cisco 8500 2020] internally switches packets via\na shared memory.\nSwitching via a bus. In this approach, an input port transfers a packet\ndirectly to the output port over a shared bus, without intervention by the\nrouting processor. This is typically done by having the input port pre-\npend a switch-internal label (header) to the packet indicating the local\noutput port to which this packet is being transferred and transmitting the\npacket onto the bus. All output ports receive the packet, but only the\nport that matches the label will keep the packet. The label is then\nremoved at the output port, as this label is only used within the switch\nto cross the bus. If multiple packets arrive to the router at the same time,\neach at a different input port, all but one must wait since only one\npacket can cross the bus at a time. Because every packet must cross the\nsingle bus, the switching speed of the router is limited to the bus speed;\nin our roundabout analogy, this is as if the roundabout could only\ncontain one car at a time. Nonetheless, switching via a bus is often\nsufficient for routers that operate in small local area and enterprise\nnetworks. The Cisco 6500 router [Cisco 6500 2020] internally switches\npackets over a 32-Gbps-backplane bus.\nSwitching via an interconnection network. One way to overcome the\nbandwidth limitation of a single, shared bus is to use a more\nsophisticated interconnection network, such as those that have been\nused in the past to interconnect processors in a multiprocessor computer\narchitecture. A crossbar switch is an interconnection network consisting\nof 2N buses that connect N input ports to N output ports, as shown in\nFigure 4.6. Each vertical bus intersects each horizontal bus at a\ncrosspoint, which can be opened or closed at any time by the switch\nfabric controller (whose logic is part of the switching fabric itself).\nWhen a packet arrives from port A and needs to be forwarded to port Y,\nthe switch controller closes the crosspoint at the intersection of busses A\nand Y, and port A then sends the packet onto its bus, which is picked up\n(only) by bus Y. Note that a packet from port B can be forwarded to port\nX at the same time, since the A-to-Y and B-to-X packets use different\ninput and output busses. Thus, unlike the previous two switching\napproaches, crossbar switches are capable of forwarding multiple\npackets in parallel. A crossbar switch is non-blocking—a packet being\nforwarded to an output port will not be blocked from reaching that\noutput port as long as no other packet is currently being forwarded to\nthat output port. However, if two packets from two different input ports\nare destined to that same output port, then one will have to wait at the\ninput, since only one packet can be sent over any given bus at a time.\nCisco 12000 series switches [Cisco 12000 2020] use a crossbar\nswitching network; the Cisco 7600 series can be configured to use\neither a bus or crossbar switch [Cisco 7600 2020].\nMore sophisticated interconnection networks use multiple stages of\nswitching elements to allow packets from different input ports to\nproceed towards the same output port at the same time through the\nmulti-stage switching fabric. See [Tobagi 1990] for a survey of switch\narchitectures. The Cisco CRS employs a three-stage non-blocking\nswitching strategy. A router’s switching capacity can also be scaled by\nrunning multiple switching fabrics in parallel. In this approach, input\nports and output ports are connected to N switching fabrics that operate\nin parallel. An input port breaks a packet into K smaller chunks, and\nsends (“sprays”) the chunks through K of these N switching fabrics to\nthe selected output port, which reassembles the K chunks back into the\noriginal packet.\nFigure 4.6 ♦Three switching techniques\n4.2.3 Output Port Processing\nOutput port processing, shown in Figure 4.7, takes packets that have been\nstored in the output port’s memory and transmits them over the output link.\nThis includes selecting (i.e., scheduling) and de-queuing packets for\ntransmission, and performing the needed link-layer and physical-layer\ntransmission functions.\nFigure 4.7 ♦Output port processing\n4.2.4 Where Does Queuing Occur?\nIf we consider input and output port functionality and the configurations\nshown in Figure 4.6, it’s clear that packet queues may form at both the input\nports and the output ports, just as we identified cases where cars may wait\nat the inputs and outputs of the traffic intersection in our roundabout\nanalogy. The location and extent of ­queuing (either at the input port queues\nor the output port queues) will depend on the traffic load, the relative speed\nof the switching fabric, and the line speed. Let’s now consider these queues\nin a bit more detail, since as these queues grow large, the router’s memory\ncan eventually be exhausted and packet loss will occur when no memory is\navailable to store arriving packets. Recall that in our earlier ­discussions, we\nsaid that packets were “lost within the network” or “dropped at a router.” It\nis here, at these queues within a router, where such packets are actually\ndropped and lost.\nSuppose that the input and output line speeds (transmission rates) all\nhave an identical transmission rate of R\n packets per second, and that there\nare N input ports and N output ports. To further simplify the discussion, let’s\nassume that all packets have the same fixed length, and that packets arrive\nto input ports in a synchronous manner. That is, the time to send a packet on\nany link is equal to the time to receive a packet on any link, and during such\nan interval of time, either zero or one packets can arrive on an input link.\nDefine the switching fabric transfer rate R\n as the rate at which packets\ncan be moved from input port to output port. If R\n is N times faster than\n, then only negligible queuing will occur at the input ports. This is\nbecause even in the worst case, where all N input lines are receiving\npackets, and all packets are to be forwarded to the same output port, each\nbatch of N packets (one packet per input port) can be cleared through the\nswitch fabric before the next batch arrives.\nInput Queuing\nBut what happens if the switch fabric is not fast enough (relative to the\ninput line speeds) to transfer all arriving packets through the fabric without\ndelay? In this case, packet queuing can also occur at the input ports, as\npackets must join input port queues to wait their turn to be transferred\nthrough the switching fabric to the output port. To illustrate an important\nconsequence of this queuing, consider a crossbar switching fabric and\nsuppose that (1) all link speeds are identical, (2) that one packet can be\ntransferred from any one input port to a given output port in the same\namount of time it takes for a packet to be received on an input link, and (3)\npackets are moved from a given input queue to their desired output queue in\nan FCFS manner. Multiple packets can be transferred in parallel, as long as\ntheir output ports are different. However, if two packets at the front of two\ninput queues are destined for the same output queue, then one of the packets\nwill be blocked and must wait at the input queue—the switching fabric can\ntransfer only one packet to a given output port at a time.\nFigure 4.8 shows an example in which two packets (darkly shaded) at\nthe front of their input queues are destined for the same upper-right output\nport. Suppose that the switch fabric chooses to transfer the packet from the\nfront of the upper-left queue. In this case, the darkly shaded packet in the\nlower-left queue must wait. But not only must this darkly shaded packet\nwait, so too must the lightly shaded packet that is queued behind that packet\nin the lower-left queue, even though there is no contention for the middle-\nright output port (the destination for the lightly shaded packet). This\nphenomenon is known as head-of-the-line (HOL) blocking in an input-\nqueued switch—a queued packet in an input queue must wait for transfer\nthrough the fabric (even though its output port is free) because it is blocked\nby another packet at the head of the line. [Karol 1987] shows that due to\nHOL blocking, the input queue will grow to unbounded length (informally,\nthis is equivalent to saying that significant packet loss will occur) under\ncertain assumptions as soon as the packet arrival rate on the input links\nreaches only 58 percent of their capacity. A number of solutions to HOL\nblocking are discussed in [McKeown 1997].\nFigure 4.8 ♦HOL blocking at and input-queued switch\nOutput Queuing\nLet’s next consider whether queuing can occur at a switch’s output ports.\nSuppose that R\n is again N times faster than R\n and that packets arriving\nat each of the N input ports are destined to the same output port. In this\ncase, in the time it takes to send a single packet onto the outgoing link, N\nnew packets will arrive at this output port (one from each of the N input\nports). Since the output port can transmit only a single packet in a unit of\ntime (the packet transmission time), the N arriving packets will have to\nqueue (wait) for transmission over the outgoing link. Then N more packets\ncan possibly arrive in the time it takes to transmit just one of the N packets\nthat had just previously been queued. And so on. Thus, packet queues can\nform at the output ports even when the switching fabric is N times faster\nthan the port line speeds. Eventually, the number of queued packets can\ngrow large enough to exhaust available memory at the output port.\nWhen there is not enough memory to buffer an incoming packet, a\ndecision must be made to either drop the arriving packet (a policy known as\ndrop-tail) or remove one or more already-queued packets to make room for\nthe newly arrived packet. In some cases, it may be advantageous to drop (or\nmark the header of) a packet before the buffer is full in order to provide a\ncongestion signal to the sender. This marking could be done using the\nExplicit Congestion Notification bits that we studied in Section 3.7.2. A\nnumber of proactive packet-dropping and -marking policies (which\ncollectively have become known as active queue management (AQM)\nalgorithms) have been proposed and analyzed [Labrador 1999, Hollot\n2002]. One of the most widely studied and implemented AQM algorithms is\nthe Random Early Detection (RED) algorithm [Christiansen 2001]. More\nrecent AQM policies include PIE (the Proportional Integral controller\nEnhanced [RFC 8033]), and CoDel [Nichols 2012].\nOutput port queuing is illustrated in Figure 4.9. At time t, a packet has\narrived at each of the incoming input ports, each destined for the uppermost\noutgoing port. Assuming identical line speeds and a switch operating at\nthree times the line speed, one time unit later (that is, in the time needed to\nreceive or send a packet), all three original packets have been transferred to\nthe outgoing port and are queued awaiting transmission. In the next time\nunit, one of these three packets will have been transmitted over the outgoing\nlink. In our example, two new packets have arrived at the incoming side of\nthe switch; one of these packets is destined for this uppermost output port.\nA consequence of such queuing is that a packet scheduler at the output\nport must choose one packet, among those queued, for transmission—a\ntopic we’ll cover in the following section.\nFigure 4.9 ♦Output port queuing\nHow Much Buffering Is “Enough?”\nOur study above has shown how a packet queue forms when bursts of\npackets arrive at a router’s input or (more likely) output port, and the packet\narrival rate temporarily exceeds the rate at which packets can be forwarded.\nThe longer the amount of time that this mismatch persists, the longer the\nqueue will grow, until eventually a port’s buffers become full and packets\nare dropped. One natural question is how much buffering should be\nprovisioned at a port. It turns out the answer to this question is much more\ncomplicated than one might imagine and can teach us quite a bit about the\nsubtle interaction among congestion-aware senders at the network’s edge\nand the network core!\nFor many years, the rule of thumb [RFC 3439] for buffer sizing was\nthat the amount of buffering (B) should be equal to an average round-trip\ntime (RTT, say 250 msec) times the link capacity (C). Thus, a 10-Gbps link\nwith an RTT of 250 msec would need an amount of buffering equal to B =\nRTT · C = 2.5 Gbits of buffers. This result was based on an analysis of the\nqueuing dynamics of a relatively small number of TCP flows [Villamizar\n1994]. More recent theoretical and experimental efforts [Appenzeller 2004],\nhowever, suggest that when a large number of independent TCP flows (N)\npass through a link, the amount of buffering needed is B = RTT · C/√N. In\ncore networks, where a large number of TCP flows typically pass through\nlarge backbone router links, the value of N can be large, with the decrease\nin needed buffer size becoming quite significant. [Appenzeller 2004;\nWischik 2005; Beheshti 2008] provide very readable discussions of the\nbuffer-sizing problem from a theoretical, implementation, and operational\nstandpoint.\nIt’s temping to think that more buffering must be better—larger buffers\nwould allow a router to absorb larger fluctuations in the packet arrival rate,\nthereby decreasing the router’s packet loss rate. But larger buffers also\nmean potentially longer queuing delays. For gamers and for interactive\nteleconferencing users, tens of milliseconds count. Increasing the amount of\nper-hop buffer by a factor of 10 to decrease packet loss could increase the\nend-end delay by a factor of 10! Increased RTTs also make TCP senders\nless responsive and slower to respond to incipient congestion and/or packet\nloss. These delay-based considerations show that buffering is a double-\nedged sword—buffering can be used to absorb short-term statistical\nfluctuations in traffic but can also lead to increased delay and the attendant\nconcerns. Buffering is a bit like salt—just the right amount of salt makes\nfood better, but too much makes it inedible!\nIn the discussion above, we’ve implicitly assumed that many\nindependent senders are competing for bandwidth and buffers at a\ncongested link. While this is probably an excellent assumption for routers\nwithin the network core, at the network edge this may not hold. Figure\n4.10(a) shows a home router sending TCP segments to a remote game\nserver. Following [Nichols 2012], suppose that it takes 20 ms to transmit a\npacket (containing a gamer’s TCP segment), that there are negligible\nqueuing delays elsewhere on the path to the game server, and that the RTT\nis 200 ms. As shown in Figure 4.10(b), suppose that at time t = 0, a burst of\n25 packets arrives to the queue. One of these queued packets is then\ntransmitted once every 20 ms, so that at t = 200 msec, the first ACK arrives,\njust as the 21st packet is being transmitted. This ACK arrival causes the\nTCP sender to send another packet, which is queued at the outgoing link of\nthe home router. At t = 220, the next ACK arrives, and another TCP\nsegment is released by the gamer and is queued, as the 22nd packet is being\ntransmitted, and so on. You should convince yourself that in this scenario,\nACK clocking results in a new packet arriving at the queue every time a\nqueued packet is sent, resulting in queue size at the home router’s outgoing\nlink that is always five packets! That is, the end-end-pipe is full (delivering\npackets to the destination at the path bottleneck rate of one packet every 20\nms), but the amount of queuing delay is constant and persistent. As a result,\nthe gamer is unhappy with the delay, and the parent (who even knows\nwireshark!) is confused because he or she doesn’t understand why delays\nare persistent and excessively long, even when there is no other traffic on\nthe home network.\nFigure 4.10 ♦Bufferbloat: persistent queues\nThis scenario above of long delay due to persistent buffering is known\nas bufferbloat and illustrates that not only is throughput important, but also\nminimal delay is important as well [Kleinrock 2018], and that the\ninteraction among senders at the network edge and queues within the\nnetwork can indeed be complex and subtle. The DOCSIS 3.1 standard for\ncable networks that we will study in Chapter 6, recently added a specific\nAQM mechanism [RFC 8033, RFC 8034] to combat bufferbloat, while\npreserving bulk throughput performance.\n4.2.5 Packet Scheduling\nLet’s now return to the question of determining the order in which queued\npackets are transmitted over an outgoing link. Since you yourself have\nundoubtedly had to wait in long lines on many occasions and observed how\nwaiting customers are served, you’re no doubt familiar with many of the\nqueuing disciplines commonly used in routers. There is first-come-first-\nserved (FCFS, also known as first-in-first-out, FIFO). The British are\nfamous for patient and orderly FCFS queuing at bus stops and in the\nmarketplace (“Oh, are you queuing?”). Other countries operate on a priority\nbasis, with one class of waiting customers given priority service over other\nwaiting customers. There is also round-robin queuing, where customers are\nagain divided into classes (as in priority queuing) but each class of\ncustomer is given service in turn.\nFirst-in-First-Out (FIFO)\nFigure 4.11 shows the queuing model abstraction for the FIFO link-\nscheduling discipline. Packets arriving at the link output queue wait for\ntransmission if the link is currently busy transmitting another packet. If\nthere is not sufficient buffering space to hold the arriving packet, the\nqueue’s packet-discarding policy then determines whether the packet will\nbe dropped (lost) or whether other packets will be removed from the queue\nto make space for the arriving packet, as discussed above. In our discussion\nbelow, we’ll ignore packet discard. When a packet is completely transmitted\nover the outgoing link (that is, receives service) it is removed from the\nFigure 4.11 ♦FIFO queuing abstraction\nThe FIFO (also known as first-come-first-served, or FCFS) scheduling\ndiscipline selects packets for link transmission in the same order in which\nthey arrived at the output link queue. We’re all familiar with FIFO queuing\nfrom service centers, where arriving customers join the back of the single\nwaiting line, remain in order, and are then served when they reach the front\nof the line. Figure 4.12 shows the FIFO queue in operation. Packet arrivals\nare indicated by numbered arrows above the upper timeline, with the\nnumber indicating the order in which the packet arrived. Individual packet\ndepartures are shown below the lower timeline. The time that a packet\nspends in service (being transmitted) is indicated by the shaded rectangle\nbetween the two timelines. In our examples here, let’s assume that each\npacket takes three units of time to be transmitted. Under the FIFO\ndiscipline, packets leave in the same order in which they arrived. Note that\nafter the departure of packet 4, the link remains idle (since packets 1\nthrough 4 have been transmitted and removed from the queue) until the\narrival of packet 5.\nFigure 4.12 ♦The FIFO queue in operation\nPriority Queuing\nUnder priority queuing, packets arriving at the output link are classified into\npriority classes upon arrival at the queue, as shown in Figure 4.13. In\npractice, a network operator may configure a queue so that packets carrying\nnetwork management information (for example, as indicated by the source\nor destination TCP/UDP port number) receive priority over user traffic;\nadditionally, real-time voice-over-IP packets might receive priority over\nnon-real-time traffic such e-mail packets. Each priority class typically has\nits own queue. When choosing a packet to transmit, the priority queuing\ndiscipline will transmit a packet from the highest priority class that has a\nnonempty queue (that is, has packets waiting for transmission). The choice\namong packets in the same priority class is typically done in a FIFO\nFigure 4.13 ♦The priority queuing model\nFigure 4.14 illustrates the operation of a priority queue with two\npriority classes. Packets 1, 3, and 4 belong to the high-priority class, and\npackets 2 and 5 belong to the low-priority class. Packet 1 arrives and,\nfinding the link idle, begins transmission. During the transmission of packet\n1, packets 2 and 3 arrive and are queued in the low- and high-priority\nqueues, respectively. After the transmission of packet 1, packet 3 (a high-\npriority packet) is selected for transmission over packet 2 (which, even\nthough it arrived earlier, is a low-priority packet). At the end of the\ntransmission of packet 3, packet 2 then begins transmission. Packet 4 (a\nhigh-priority packet) arrives during the transmission of packet 2 (a low-\npriority packet). Under a non-preemptive priority queuing discipline, the\ntransmission of a packet is not interrupted once it has begun. In this case,\npacket 4 queues for transmission and begins being transmitted after the\ntransmission of packet 2 is completed.\nFigure 4.14 ♦The priority queue in operation\nWe’ve seen that packet scheduling mechanisms (e.g., priority traffic scheduling disciplines\nsuch a strict priority, and WFQ) can be used to provide different levels of service to different\n“classes” of traffic. The definition of what precisely constitutes a “class” of traffic is up to an\nISP to decide, but could be potentially based on any set of fields in the IP datagram header.\nFor example, the port field in the IP datagram header could be used to classify datagrams\naccording to the “well-know service” associated with that port: SNMP network management\ndatagram (port 161) might be assigned to a higher priority class than an IMAP e-mail\nprotocol (ports 143, or 993) datagram and therefore receive better service. An ISP could\nalso potentially use a datagram’s source IP address to provide priority to datagrams being\nsent by certain companies (who have presumably paid the ISP for this privilege) over\ndatagrams being sent from other companies (who have not paid); an ISP could even block\ntraffic with a source IP address in a given company, or country. There are many\nmechanisms that would allow an ISP to provide different levels of service to different\nclasses of traffic. The real question is what policies and laws determine what an ISP can\nactually do. Of course, these laws will vary by country; see [Smithsonian 2017] for a brief\nsurvey. Here, we’ll briefly consider US policy on what has come to be known as “net\nneutrality.”\nThe term “net neutrality” doesn’t have a precise decision, but the March 2015 Order on\nProtecting and Promoting an Open Internet [FCC 2015] by the US Federal Communications\nCommission provides three “clear, bright line” rules that are now often associated with net\nneutrality:\n“No Blocking. . . . A person engaged in the provision of broadband Internet access\nservice, . . . shall not block lawful content, applications, services, or non-harmful\ndevices, subject to reasonable network management.”\n“No Throttling. . . . A person engaged in the provision of broadband Internet access\nservice, . . . shall not impair or degrade lawful Internet traffic on the basis of Internet\ncontent, application, or service, or use of a non-harmful device, subject to reasonable\nnetwork management.”\n“No Paid Prioritization. . . . A person engaged in the provision of broadband Internet\naccess service, . . . shall not engage in paid prioritization. “Paid prioritization” refers to\nthe management of a broadband provider’s network to directly or indirectly favor some\ntraffic over other traffic, including through use of techniques such as traffic shaping,\nprioritization, resource reservation, or other forms of preferential traffic\nmanagement, . . .”\nQuite interestingly, before the Order, ISP behaviors violating the first two of these rules had\nbeen observed [Faulhaber 2012]. In 2005, an ISP in North Carolina agreed to stop its\npractice of blocking its customers from using Vonage, a voice-over-IP service that competed\nwith its own telephone service. In 2007, Comcast was judged to be interfering with\nBitTorrent P2P traffic by internally creating and sending TCP RST packets to BitTorrent\nsenders and receivers, which caused them to close their BitTorrent connection [FCC 2008].\nBoth sides of the net neutrality debate have been argued strenuously, mostly focused on\nthe extent to which net neutrality provides benefits to customers, while at the same time\npromoting innovation. See [Peha 2006, Faulhaber 2012, Economides 2017, Madhyastha\nThe 2015 FCC Order on Protecting and Promoting an Open Internet, which banned ISPs\nfrom blocking, throttling, or providing paid prioritizing, was superseded by the 2017 FCC\nRestoring Internet Freedom Order, [FCC 2017] which rolled back these prohibitions and\nfocused instead on ISP transparency. With so much interest and so many changes, it’s\nprobably safe to say we aren’t close to having seen the final chapter written on net neutrality\nin the United States, or elsewhere.\nRound Robin and Weighted Fair Queuing (WFQ)\nUnder the round robin queuing discipline, packets are sorted into classes as\nwith priority queuing. However, rather than there being a strict service\npriority among classes, a round robin scheduler alternates service among\nthe classes. In the simplest form of round robin scheduling, a class 1 packet\nis transmitted, followed by a class 2 packet, followed by a class 1 packet,\nfollowed by a class 2 packet, and so on. A so-called work-conserving\nqueuing discipline will never allow the link to remain idle whenever there\nare packets (of any class) queued for transmission. A work-conserving\nround robin discipline that looks for a packet of a given class but finds none\nwill immediately check the next class in the round robin sequence.\nFigure 4.15 illustrates the operation of a two-class round robin queue.\nIn this example, packets 1, 2, and 4 belong to class 1, and packets 3 and 5\nbelong to the second class. Packet 1 begins transmission immediately upon\narrival at the output queue. Packets 2 and 3 arrive during the transmission\nof packet 1 and thus queue for transmission. After the transmission of\npacket 1, the link scheduler looks for a class 2 packet and thus transmits\npacket 3. After the transmission of packet 3, the scheduler looks for a class\n1 packet and thus transmits packet 2. After the transmission of packet 2,\npacket 4 is the only queued packet; it is thus transmitted immediately after\nFigure 4.15 ♦The two-class robin queue in operation\nA generalized form of round robin queuing that has been widely\nimplemented in routers is the so-called weighted fair queuing (WFQ)\ndiscipline [Demers 1990; Parekh 1993. WFQ is illustrated in Figure 4.16.\nHere, arriving packets are classified and queued in the appropriate per-class\nwaiting area. As in round robin scheduling, a WFQ scheduler will serve\nclasses in a circular manner—first serving class 1, then serving class 2, then\nserving class 3, and then (assuming there are three classes) repeating the\nservice pattern. WFQ is also a work-conserving queuing discipline and thus\nwill immediately move on to the next class in the service sequence when it\nfinds an empty class queue.\nFigure 4.16 ♦Weighted fair queuing\nWFQ differs from round robin in that each class may receive a\ndifferential amount of service in any interval of time. Specifically, each\nclass, i, is assigned a weight, w . Under WFQ, during any interval of time\nduring which there are class i packets to send, class i will then be\nguaranteed to receive a fraction of service equal to w /(Σw ), where the sum\nin the denominator is taken over all classes that also have packets queued\nfor transmission. In the worst case, even if all classes have queued packets,\nclass i will still be guaranteed to receive a fraction w /(Σw ) of the\nbandwidth, where in this worst case the sum in the denominator is over all\nclasses. Thus, for a link with transmission rate R, class i will always achieve\na throughput of at least R · w  / (Σw ) Our description of WFQ has been\nidealized, as we have not considered the fact that packets are discrete and a\npacket’s transmission will not be interrupted to begin transmission of\nanother packet; [Demers 1990; Parekh 1993] discuss this packetization\n4.3 The Internet Protocol (IP): IPv4, Addressing,\nIPv6, and More\nOur study of the network layer thus far in Chapter 4—the notion of the data\nand control plane component of the network layer, our distinction between\nforwarding and routing, the identification of various network service\nmodels, and our look inside a router—have often been without reference to\nany specific computer network architecture or protocol. In this section,\nwe’ll focus on key aspects of the network layer on today’s Internet and the\ncelebrated Internet Protocol (IP).\nThere are two versions of IP in use today. We’ll first examine the\nwidely deployed IP protocol version 4, which is usually referred to simply\nas IPv4 [RFC 791] in Section 4.3.1. We’ll examine IP version 6 [RFC 2460;\nRFC 4291], which has been proposed to replace IPv4, in Section 4.3.4. In\nbetween, we’ll primarily cover Internet addressing—a topic that might\nseem rather dry and detail-oriented but we’ll see is crucial to understanding\nhow the Internet’s network layer works. To master IP addressing is to\nmaster the Internet’s network layer itself!\n4.3.1 IPv4 Datagram Format\nRecall that the Internet’s network-layer packet is referred to as a datagram.\nWe begin our study of IP with an overview of the syntax and semantics of\nthe IPv4 datagram. You might be thinking that nothing could be drier than\nthe syntax and semantics of a packet’s bits. Nevertheless, the datagram\nplays a central role in the Internet—every networking student and\nprofessional needs to see it, absorb it, and master it. (And just to see that\nprotocol headers can indeed be fun to study, check out [Pomeranz 2010]).\nThe IPv4 datagram format is shown in Figure 4.17. The key fields in the\nIPv4 datagram are the following:\nFigure 4.17 ♦IPv4 datagram format\nVersion number. These 4 bits specify the IP protocol version of the\ndatagram. By looking at the version number, the router can determine\nhow to interpret the remainder of the IP datagram. Different versions of\nIP use different datagram formats. The datagram format for IPv4 is\nshown in Figure 4.17. The datagram format for the new version of IP\n(IPv6) is discussed in Section 4.3.4.\nHeader length. Because an IPv4 datagram can contain a variable\nnumber of options (which are included in the IPv4 datagram header),\nthese 4 bits are needed to determine where in the IP datagram the\npayload (for example, the transport-layer segment being encapsulated in\nthis datagram) actually begins. Most IP datagrams do not contain\noptions, so the typical IP datagram has a 20-byte header.\nType of service. The type of service (TOS) bits were included in the\nIPv4 header to allow different types of IP datagrams to be distinguished\nfrom each other. For example, it might be useful to distinguish real-time\ndatagrams (such as those used by an IP telephony application) from\nnon-real-time traffic (e.g., FTP). The ­specific level of service to be\nprovided is a policy issue determined and configured by the network\nadministrator for that router. We also learned in Section 3.7.2 that two\nof the TOS bits are used for Explicit Congestion ­Notification.\nDatagram length. This is the total length of the IP datagram (header\nplus data), measured in bytes. Since this field is 16 bits long, the\ntheoretical maximum size of the IP datagram is 65,535 bytes. However,\ndatagrams are rarely larger than 1,500 bytes, which allows an IP\ndatagram to fit in the payload field of a maximally sized Ethernet frame.\nIdentifier, flags, fragmentation offset. These three fields have to do with\nso-called IP fragmentation, when a large IP datagram is broken into\nseveral smaller IP datagrams which are then forwarded independently to\nthe destination, where they are reassembled before their payload data\n(see below) is passed up to the transport layer at the destination host.\nInterestingly, the new version of IP, IPv6, does not allow for\nfragmentation. We’ll not cover fragmentation here; but readers can find\na detailed discussion online, among the “retired” material from earlier\nversions of this book.\nTime-to-live. The time-to-live (TTL) field is included to ensure that\ndatagrams do not circulate forever (due to, for example, a long-lived\nrouting loop) in the network. This field is decremented by one each time\nthe datagram is processed by a router. If the TTL field reaches 0, a\nrouter must drop that datagram.\nProtocol. This field is typically used only when an IP datagram reaches\nits final destination. The value of this field indicates the specific\ntransport-layer protocol to which the data portion of this IP datagram\nshould be passed. For example, a value of 6 indicates that the data\nportion is passed to TCP, while a value of 17 indicates that the data is\npassed to UDP. For a list of all possible values, see [IANA Protocol\nNumbers 2016]. Note that the protocol number in the IP datagram has a\nrole that is analogous to the role of the port number field in the\ntransport-layer segment. The protocol number is the glue that binds the\nnetwork and transport layers together, whereas the port number is the\nglue that binds the transport and application layers together. We’ll see in\nChapter 6 that the link-layer frame also has a special field that binds the\nlink layer to the network layer.\nHeader checksum. The header checksum aids a router in detecting bit\nerrors in a received IP datagram. The header checksum is computed by\ntreating each 2  bytes in the header as a number and summing these\nnumbers using 1s complement arithmetic. As discussed in Section 3.3,\nthe 1s complement of this sum, known as the Internet checksum, is\nstored in the checksum field. A router computes the header checksum\nfor each received IP datagram and detects an error condition if the\nchecksum carried in the datagram header does not equal the computed\nchecksum. Routers typically discard datagrams for which an error has\nbeen detected. Note that the checksum must be recomputed and stored\nagain at each router, since the TTL field, and possibly the options field\nas well, will change. An interesting discussion of fast algorithms for\ncomputing the Internet checksum is [RFC 1071]. A question often asked\nat this point is, why does TCP/IP perform error checking at both the\ntransport and network layers? There are several reasons for this\nrepetition. First, note that only the IP header is checksummed at the IP\nlayer, while the TCP/UDP checksum is computed over the entire\nTCP/UDP segment. Second, TCP/UDP and IP do not necessarily both\nhave to belong to the same protocol stack. TCP can, in principle, run\nover a different network-layer protocol (for example, ATM) [Black\n1995]) and IP can carry data that will not be passed to TCP/UDP.\nSource and destination IP addresses. When a source creates a datagram,\nit inserts its IP address into the source IP address field and inserts the\naddress of the ultimate destination into the destination IP address field.\nOften the source host determines the destination address via a DNS\nlookup, as discussed in Chapter 2. We’ll discuss IP addressing in detail\nin Section 4.3.2.\nOptions. The options fields allow an IP header to be extended. Header\noptions were meant to be used rarely—hence the decision to save\noverhead by not including the information in options fields in every\ndatagram header. However, the mere existence of options does\ncomplicate matters—since datagram headers can be of variable length,\none cannot determine a priori where the data field will start. Also, since\nsome datagrams may require options processing and others may not, the\namount of time needed to process an IP datagram at a router can vary\ngreatly. These considerations become particularly important for IP\nprocessing in high-performance routers and hosts. For these reasons and\nothers, IP options were not included in the IPv6 header, as discussed in\nSection 4.3.4.\nData (payload). Finally, we come to the last and most important field—\nthe raison d’etre for the datagram in the first place! In most\ncircumstances, the data field of the IP datagram contains the transport-\nlayer segment (TCP or UDP) to be delivered to the destination.\nHowever, the data field can carry other types of data, such as ICMP\nmessages (discussed in Section 5.6).\nNote that an IP datagram has a total of 20 bytes of header (assuming no\noptions). If the datagram carries a TCP segment, then each datagram carries\na total of 40 bytes of header (20 bytes of IP header plus 20 bytes of TCP\nheader) along with the application-layer message.\n4.3.2 IPv4 Addressing\nWe now turn our attention to IPv4 addressing. Although you may be\nthinking that addressing must be a straightforward topic, hopefully by the\nend of this section you’ll be convinced that Internet addressing is not only a\njuicy, subtle, and interesting topic but also one that is of central importance\nto the Internet. An excellent treatment of IPv4 addressing can be found in\nthe first chapter in [Stewart 1999].\nBefore discussing IP addressing, however, we’ll need to say a few\nwords about how hosts and routers are connected into the Internet. A host\ntypically has only a single link into the network; when IP in the host wants\nto send a datagram, it does so over this link. The boundary between the host\nand the physical link is called an interface. Now consider a router and its\ninterfaces. Because a router’s job is to receive a datagram on one link and\nforward the datagram on some other link, a router necessarily has two or\nmore links to which it is connected. The boundary between the router and\nany one of its links is also called an interface. A router thus has multiple\ninterfaces, one for each of its links. Because every host and router is\ncapable of sending and receiving IP datagrams, IP requires each host and\nrouter interface to have its own IP address. Thus, an IP address is\ntechnically associated with an interface, rather than with the host or router\ncontaining that interface.\nEach IP address is 32 bits long (equivalently, 4 bytes), and there are\nthus a total of 232 (or approximately 4 billion) possible IP addresses. These\naddresses are typically written in so-called dotted-decimal notation, in\nwhich each byte of the address is written in its decimal form and is\nseparated by a period (dot) from other bytes in the address. For example,\nconsider the IP address 193.32.216.9. The 193 is the decimal equivalent of\nthe first 8 bits of the address; the 32 is the decimal equivalent of the second\n8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary\nnotation is\n11000001 00100000 11011000 00001001\nEach interface on every host and router in the global Internet must have an\nIP address that is globally unique (except for interfaces behind NATs, as\ndiscussed in Section 4.3.3). These addresses cannot be chosen in a willy-\nnilly manner, however. A portion of an interface’s IP address will be\ndetermined by the subnet to which it is connected.\nFigure 4.18 provides an example of IP addressing and interfaces. In this\nfigure, one router (with three interfaces) is used to interconnect seven hosts.\nTake a close look at the IP addresses assigned to the host and router\ninterfaces, as there are several things to notice. The three hosts in the upper-\nleft portion of Figure 4.18, and the router interface to which they are\nconnected, all have an IP address of the form 223.1.1.xxx. That is, they all\nhave the same leftmost 24 bits in their IP address. These four interfaces are\nalso interconnected to each other by a network that contains no routers.\nThis network could be interconnected by an Ethernet LAN, in which case\nthe interfaces would be interconnected by an Ethernet switch (as we’ll\ndiscuss in Chapter 6), or by a wireless access point (as we’ll discuss in\nChapter 7). We’ll represent this routerless network connecting these hosts as\na cloud for now, and dive into the internals of such networks in Chapters 6\nFigure 4.18 ♦Interface addresses and subnets\nIn IP terms, this network interconnecting three host interfaces and one\nrouter interface forms a subnet [RFC 950]. (A subnet is also called an IP\nnetwork or simply a network in the Internet literature.) IP addressing assigns\nan address to this subnet: 223.1.1.0/24, where the /24 (“slash-24”) notation,\nsometimes known as a subnet mask, indicates that the leftmost 24 bits of\nthe 32-bit quantity define the subnet address. The 223.1.1.0/24 subnet thus\nconsists of the three host interfaces (223.1.1.1, 223.1.1.2, and 223.1.1.3)\nand one router interface (223.1.1.4). Any additional hosts attached to the\n223.1.1.0/24 subnet would be required to have an address of the form\n223.1.1.xxx. There are two additional subnets shown in Figure 4.18: the\n223.1.2.0/24 network and the 223.1.3.0/24 subnet. Figure 4.19 illustrates\nthe three IP subnets present in Figure 4.18.\nFigure 4.19 ♦Subnet addresses\nThe IP definition of a subnet is not restricted to Ethernet segments that\nconnect multiple hosts to a router interface. To get some insight here,\nconsider Figure 4.20, which shows three routers that are interconnected\nwith each other by point-to-point links. Each router has three interfaces, one\nfor each point-to-point link and one for the broadcast link that directly\nconnects the router to a pair of hosts. What subnets are present here? Three\nsubnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are similar to the\nsubnets we encountered in Figure 4.18. But note that there are three\nadditional subnets in this example as well: one subnet, 223.1.9.0/24, for the\ninterfaces that connect routers R1 and R2; another subnet, 223.1.8.0/24, for\nthe interfaces that connect routers R2 and R3; and a third subnet,\n223.1.7.0/24, for the interfaces that connect routers R3 and R1. For a\ngeneral interconnected system of routers and hosts, we can use the\nfollowing recipe to define the subnets in the system:\nTo determine the subnets, detach each interface from its host or router,\ncreating islands of isolated networks, with interfaces terminating the\nend points of the isolated networks. Each of these isolated networks is\ncalled a subnet.\nIf we apply this procedure to the interconnected system in Figure 4.20, we\nget six islands or subnets.\nFigure 4.20 ♦Three routers interconnecting six subnets\nFrom the discussion above, it’s clear that an organization (such as a\ncompany or academic institution) with multiple Ethernet segments and\npoint-to-point links will have multiple subnets, with all of the devices on a\ngiven subnet having the same subnet address. In principle, the different\nsubnets could have quite different subnet addresses. In practice, however,\ntheir subnet addresses often have much in common. To understand why,\nlet’s next turn our attention to how addressing is handled in the global\nThe Internet’s address assignment strategy is known as Classless\nInterdomain Routing (CIDR—pronounced cider) [RFC 4632]. CIDR\ngeneralizes the notion of subnet addressing. As with subnet addressing, the\n32-bit IP address is divided into two parts and again has the dotted-decimal\nform a.b.c.d/x, where x indicates the number of bits in the first part of the\nThe x most significant bits of an address of the form a.b.c.d/x constitute\nthe network portion of the IP address, and are often referred to as the prefix\n(or network prefix) of the address. An organization is typically assigned a\nblock of contiguous addresses, that is, a range of addresses with a common\nprefix (see the Principles in Practice feature). In this case, the IP addresses\nof devices within the organization will share the common prefix. When we\ncover the Internet’s BGP routing protocol in Section 5.4, we’ll see that only\nthese x leading prefix bits are considered by routers outside the\norganization’s network. That is, when a router outside the organization\nforwards a datagram whose destination address is inside the organization,\nonly the leading x bits of the address need be considered. This considerably\nreduces the size of the forwarding table in these routers, since a single entry\nof the form a.b.c.d/x will be sufficient to forward packets to any destination\nwithin the organization.\nThe remaining 32-x bits of an address can be thought of as\ndistinguishing among the devices within the organization, all of which have\nthe same network prefix. These are the bits that will be considered when\nforwarding packets at routers within the organization. These lower-order\nbits may (or may not) have an additional subnetting structure, such as that\ndiscussed above. For example, suppose the first 21 bits of the CIDRized\naddress a.b.c.d/21 specify the organization’s network prefix and are\ncommon to the IP addresses of all devices in that organization. The\nremaining 11 bits then identify the specific hosts in the organization. The\norganization’s internal structure might be such that these 11 rightmost bits\nare used for subnetting within the organization, as discussed above. For\nexample, a.b.c.d/24 might refer to a specific subnet within the organization.\nThis example of an ISP that connects eight organizations to the Internet nicely illustrates\nhow carefully allocated CIDRized addresses facilitate routing. Suppose, as shown in Figure\n4.21, that the ISP (which we’ll call Fly-By-Night-ISP) advertises to the outside world that it\nshould be sent any datagrams whose first 20 address bits match 200.23.16.0/20. The rest\nof the world need not know that within the address block 200.23.16.0/20 there are in fact\neight other organizations, each with its own subnets. This ability to use a single prefix to\nadvertise multiple networks is often referred to as address aggregation (also route\naggregation or route summarization).\nAddress aggregation works extremely well when addresses are allocated in blocks to\nISPs and then from ISPs to client organizations. But what happens when addresses are not\nallocated in such a hierarchical manner? What would happen, for example, if Fly-By-Night-\nISP acquires ISPs-R-Us and then has Organization 1 connect to the Internet through its\nsubsidiary ISPs-R-Us? As shown in Figure 4.21, the subsidiary ISPs-R-Us owns the\naddress block 199.31.0.0/16, but Organization 1’s IP addresses are unfortunately outside of\nthis address block. What should be done here? Certainly, Organization 1 could renumber all\nof its routers and hosts to have addresses within the ISPs-R-Us address block. But this is a\ncostly solution, and Organization 1 might well be reassigned to another subsidiary in the\nfuture. The solution typically adopted is for Organization 1 to keep its IP addresses in\n200.23.18.0/23. In this case, as shown in Figure 4.22, Fly-By-Night-ISP continues to\nadvertise the address block 200.23.16.0/20 and ISPs-R-Us continues to advertise\n199.31.0.0/16. However, ISPs-R-Us now also advertises the block of addresses for\nOrganization 1, 200.23.18.0/23. When other routers in the larger Internet see the address\nblocks 200.23.16.0/20 (from Fly-By-Night-ISP) and 200.23.18.0/23 (from ISPs-R-Us) and\nwant to route to an address in the block 200.23.18.0/23, they will use longest prefix\nmatching (see Section 4.2.1), and route toward ISPs-R-Us, as it advertises the longest (i.e.,\nmost-specific) address prefix that matches the destination address.\nFigure 4.21 ♦Hierarchical addressing and route aggregation\nFigure 4.22 ♦ISPs-R-Us has a more specific route to Organization\nBefore CIDR was adopted, the network portions of an IP address were\nconstrained to be 8, 16, or 24 bits in length, an addressing scheme known as\nclassful addressing, since subnets with 8-, 16-, and 24-bit subnet addresses\nwere known as class A, B, and C networks, respectively. The requirement\nthat the subnet portion of an IP address be exactly 1, 2, or 3 bytes long\nturned out to be problematic for supporting the rapidly growing number of\norganizations with small and medium-sized subnets. A class C (/24) subnet\ncould accommodate only up to 2  − 2 = 254 hosts (two of the 2  = 256\naddresses are reserved for special use)—too small for many organizations.\nHowever, a class B (/16) subnet, which supports up to 65,634 hosts, was too\nlarge. Under classful addressing, an organization with, say, 2,000 hosts was\ntypically allocated a class B (/16) subnet address. This led to a rapid\ndepletion of the class B address space and poor utilization of the assigned\naddress space. For example, the organization that used a class B address for\nits 2,000 hosts was allocated enough of the address space for up to 65,534\ninterfaces—leaving more than 63,000 addresses that could not be used by\nother organizations.\nWe would be remiss if we did not mention yet another type of IP\naddress, the IP broadcast address 255.255.255.255. When a host sends a\ndatagram with destination address 255.255.255.255, the message is\ndelivered to all hosts on the same subnet. Routers optionally forward the\nmessage into neighboring subnets as well (although they usually don’t).\nHaving now studied IP addressing in detail, we need to know how hosts\nand subnets get their addresses in the first place. Let’s begin by looking at\nhow an organization gets a block of addresses for its devices, and then look\nat how a device (such as a host) is assigned an address from within the\norganization’s block of addresses.\nObtaining a Block of Addresses\nIn order to obtain a block of IP addresses for use within an organization’s\nsubnet, a network administrator might first contact its ISP, which would\nprovide addresses from a larger block of addresses that had already been\nallocated to the ISP. For example, the ISP may itself have been allocated the\naddress block 200.23.16.0/20. The ISP, in turn, could divide its address\nblock into eight equal-sized contiguous address blocks and give one of\nthese address blocks out to each of up to eight organizations that are\nsupported by this ISP, as shown below. (We have underlined the subnet part\nof these addresses for your convenience.)\nWhile obtaining a set of addresses from an ISP is one way to get a\nblock of addresses, it is not the only way. Clearly, there must also be a way\nfor the ISP itself to get a block of addresses. Is there a global authority that\nhas ultimate responsibility for managing the IP address space and allocating\naddress blocks to ISPs and other organizations? Indeed there is! IP\naddresses are managed under the authority of the Internet Corporation for\nAssigned Names and Numbers (ICANN) [ICANN 2020], based on\nguidelines set forth in [RFC 7020]. The role of the nonprofit ICANN\norganization is not only to allocate IP addresses, but also to manage the\nDNS root servers. It also has the very contentious job of assigning domain\nnames and resolving domain name disputes. The ICANN allocates\naddresses to regional Internet registries (for example, ARIN, RIPE, APNIC,\nand LACNIC, which together form the Address Supporting Organization of\nICANN [ASO-ICANN 2020]), and handle the allocation/management of\naddresses within their regions.\nObtaining a Host Address: The Dynamic Host Configuration\nOnce an organization has obtained a block of addresses, it can assign\nindividual IP addresses to the host and router interfaces in its organization.\nA system administrator will typically manually configure the IP addresses\ninto the router (often remotely, with a network management tool). Host\naddresses can also be configured manually, but typically this is done using\nthe Dynamic Host Configuration Protocol (DHCP) [RFC 2131]. DHCP\nallows a host to obtain (be allocated) an IP address automatically. A network\nadministrator can configure DHCP so that a given host receives the same IP\naddress each time it connects to the network, or a host may be assigned a\ntemporary IP address that will be different each time the host connects to\nthe network. In addition to host IP address assignment, DHCP also allows a\nhost to learn additional information, such as its subnet mask, the address of\nits first-hop router (often called the default gateway), and the address of its\nlocal DNS server.\nBecause of DHCP’s ability to automate the network-related aspects of\nconnecting a host into a network, it is often referred to as a plug-and-play\nor zeroconf (zero-configuration) protocol. This capability makes it very\nattractive to the network administrator who would otherwise have to\nperform these tasks manually! DHCP is also enjoying widespread use in\nresidential Internet access networks, enterprise networks, and in wireless\nLANs, where hosts join and leave the network frequently. Consider, for\nexample, the student who carries a laptop from a dormitory room to a\nlibrary to a classroom. It is likely that in each location, the student will be\nconnecting into a new subnet and hence will need a new IP address at each\nlocation. DHCP is ideally suited to this situation, as there are many users\ncoming and going, and addresses are needed for only a limited amount of\ntime. The value of DHCP’s plug-and-play capability is clear, since it’s\nunimaginable that a system administrator would be able to reconfigure\nlaptops at each location, and few students (except those taking a computer\nnetworking class!) would have the expertise to configure their laptops\nDHCP is a client-server protocol. A client is typically a newly arriving\nhost wanting to obtain network configuration information, including an IP\naddress for itself. In the simplest case, each subnet (in the addressing sense\nof Figure 4.20) will have a DHCP server. If no server is present on the\nsubnet, a DHCP relay agent (typically a router) that knows the address of a\nDHCP server for that network is needed. Figure 4.23 shows a DHCP server\nattached to subnet 223.1.2/24, with the router serving as the relay agent for\narriving clients attached to subnets 223.1.1/24 and 223.1.3/24. In our\ndiscussion below, we’ll assume that a DHCP server is available on the\nFor a newly arriving host, the DHCP protocol is a four-step process, as\nshown in Figure 4.24 for the network setting shown in Figure 4.23. In this\nfigure, yiaddr (as in “your Internet address”) indicates the address being\nallocated to the newly arriving client. The four steps are:\nFigure 4.23 ♦DHCP client and server\nDHCP server discovery. The first task of a newly arriving host is to find\na DHCP server with which to interact. This is done using a DHCP\ndiscover message, which a client sends within a UDP packet to port 67.\nThe UDP packet is encapsulated in an IP datagram. But to whom should\nthis datagram be sent? The host doesn’t even know the IP address of the\nnetwork to which it is attaching, much less the address of a DHCP\nserver for this network. Given this, the DHCP client creates an IP\ndatagram containing its DHCP discover message along with the\nbroadcast destination IP address of 255.255.255.255 and a “this host”\nsource IP address of 0.0.0.0. The DHCP client passes the IP datagram to\nthe link layer, which then broadcasts this frame to all nodes attached to\nthe subnet (we will cover the details of link-layer broadcasting in\nSection 6.4).\nDHCP server offer(s). A DHCP server receiving a DHCP discover\nmessage responds to the client with a DHCP offer message that is\nbroadcast to all nodes on the subnet, again using the IP broadcast\naddress of 255.255.255.255. (You might want to think about why this\nserver reply must also be broadcast). Since several DHCP servers can\nbe present on the subnet, the client may find itself in the enviable\nposition of being able to choose from among several offers. Each server\noffer message contains the transaction ID of the received discover\nmessage, the proposed IP address for the client, the network mask, and\nan IP address lease time—the amount of time for which the IP address\nwill be valid. It is common for the server to set the lease time to several\nhours or days [Droms 2002].\nDHCP request. The newly arriving client will choose from among one\nor more server offers and respond to its selected offer with a DHCP\nrequest message, echoing back the configuration parameters.\nDHCP ACK. The server responds to the DHCP request message with a\nDHCP ACK message, confirming the requested parameters.\nFigure 4.24 ♦DHCP client-server interaction\nOnce the client receives the DHCP ACK, the interaction is complete\nand the client can use the DHCP-allocated IP address for the lease duration.\nSince a client may want to use its address beyond the lease’s expiration,\nDHCP also provides a mechanism that allows a client to renew its lease on\nan IP address.\nFrom a mobility aspect, DHCP does have one very significant\nshortcoming. Since a new IP address is obtained from DHCP each time a\nnode connects to a new subnet, a TCP connection to a remote application\ncannot be maintained as a mobile node moves between subnets. In Chapter\n7, we will learn how mobile cellular networks allow a host to retain its IP\naddress and ongoing TCP connections as it moves between base stations in\na provider’s cellular network. Additional details about DHCP can be found\nin [Droms 2002] and [dhc 2020]. An open source reference implementation\nof DHCP is available from the Internet Systems Consortium [ISC 2020].\n4.3.3 Network Address Translation (NAT)\nGiven our discussion about Internet addresses and the IPv4 datagram\nformat, we’re now well aware that every IP-capable device needs an IP\naddress. With the ­proliferation of small office, home office (SOHO)\nsubnets, this would seem to imply that whenever a SOHO wants to install a\nLAN to connect multiple machines, a range of addresses would need to be\nallocated by the ISP to cover all of the SOHO’s IP devices (including\nphones, tablets, gaming devices, IP TVs, printers and more). If the subnet\ngrew bigger, a larger block of addresses would have to be allocated. But\nwhat if the ISP had already allocated the contiguous portions of the SOHO ­-\nnetwork’s current address range? And what typical homeowner wants (or\nshould need) to know how to manage IP addresses in the first place?\nFortunately, there is a simpler approach to address allocation that has found\nincreasingly widespread use in such scenarios: network address\ntranslation (NAT) [RFC 2663; RFC 3022; Huston 2004, Zhang 2007;\nHuston 2017].\nFigure 4.25 shows the operation of a NAT-enabled router. The NAT-\nenabled router, residing in the home, has an interface that is part of the\nhome network on the right of Figure 4.25. Addressing within the home\nnetwork is exactly as we have seen above—all four interfaces in the home\nnetwork have the same subnet address of 10.0.0.0/24. The address space\n10.0.0.0/8 is one of three portions of the IP address space that is reserved in\n[RFC 1918] for a private network or a realm with private addresses,\nsuch as the home network in Figure 4.25. A realm with private addresses\nrefers to a network whose addresses only have meaning to devices within\nthat network. To see why this is important, consider the fact that there are\nhundreds of thousands of home networks, many using the same address\nspace, 10.0.0.0/24. Devices within a given home network can send packets\nto each other using 10.0.0.0/24 addressing. However, packets forwarded\nbeyond the home network into the larger global Internet clearly cannot use\nthese addresses (as either a source or a destination address) because there\nare hundreds of thousands of networks using this block of addresses. That\nis, the 10.0.0.0/24 addresses can only have meaning within the given home\nnetwork. But if private addresses only have meaning within a given\nnetwork, how is addressing handled when packets are sent to or received\nfrom the global Internet, where addresses are necessarily unique? The\nanswer lies in understanding NAT.\nFigure 4.25 ♦Network address translation\nThe NAT-enabled router does not look like a router to the outside\nworld. Instead the NAT router behaves to the outside world as a single\ndevice with a single IP address. In Figure 4.25, all traffic leaving the home\nrouter for the larger Internet has a source IP address of 138.76.29.7, and all\ntraffic entering the home router must have a destination address of\n138.76.29.7. In essence, the NAT-enabled router is hiding the details of the\nhome network from the outside world. (As an aside, you might wonder\nwhere the home network computers get their addresses and where the router\ngets its single IP address. Often, the answer is the same—DHCP! The router\ngets its address from the ISP’s DHCP server, and the router runs a DHCP\nserver to provide addresses to computers within the NAT-DHCP-router-\ncontrolled home network’s address space.)\nIf all datagrams arriving at the NAT router from the WAN have the\nsame destination IP address (specifically, that of the WAN-side interface of\nthe NAT router), then how does the router know the internal host to which it\nshould forward a given datagram? The trick is to use a NAT translation\ntable at the NAT router, and to include port numbers as well as IP addresses\nin the table entries.\nConsider the example in Figure 4.25. Suppose a user sitting in a home\nnetwork behind host 10.0.0.1 requests a Web page on some Web server\n(port 80) with IP address 128.119.40.186. The host 10.0.0.1 assigns the\n(arbitrary) source port number 3345 and sends the datagram into the LAN.\nThe NAT router receives the datagram, generates a new source port number\n5001 for the datagram, replaces the source IP address with its WAN-side IP\naddress 138.76.29.7, and replaces the original source port number 3345\nwith the new source port number 5001. When generating a new source port\nnumber, the NAT router can select any source port number that is not\ncurrently in the NAT translation table. (Note that because a port number\nfield is 16 bits long, the NAT protocol can support over 60,000\nsimultaneous connections with a single WAN-side IP address for the\nrouter!) NAT in the router also adds an entry to its NAT translation table.\nThe Web server, blissfully unaware that the arriving datagram containing\nthe HTTP request has been manipulated by the NAT router, responds with a\ndatagram whose destination address is the IP address of the NAT router, and\nwhose destination port number is 5001. When this datagram arrives at the\nas we have seen in Chapter 2, server processes wait for incoming requests\nat well-known port numbers and peers in a P2P protocol need to accept\nincoming connections when acting as servers. How can one peer connect to\nanother peer that is behind a NAT server, and has a DHCP-provided NAT\naddress? Technical solutions to these problems include NAT traversal tools\n[RFC 5389] [RFC 5389, RFC 5128, Ford 2005].\nMore “philosophical” arguments have also been raised against NAT by\narchitectural purists. Here, the concern is that routers are meant to be layer\n3 (i.e., network-layer) devices, and should process packets only up to the\nnetwork layer. NAT violates this principle that hosts should be talking\ndirectly with each other, without interfering nodes modifying IP addresses,\nmuch less port numbers. We’ll return to this debate later in Section 4.5,\nwhen we cover middleboxes.\nINSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION SYSTEMS\nSuppose you are assigned the task of administering a home, departmental, university,\nor corporate network. Attackers, knowing the IP address range of your network, can\neasily send IP datagrams to addresses in your range. These datagrams can do all\nkinds of devious things, including mapping your network with ping sweeps and port\nscans, crashing vulnerable hosts with malformed packets, scanning for open TCP/UDP\nports on servers in your network, and infecting hosts by including malware in the\npackets. As the network administrator, what are you going to do about all those bad\nguys out there, each capable of sending malicious packets into your network? Two\npopular defense mechanisms to malicious packet attacks are firewalls and intrusion\ndetection systems (IDSs).\nAs a network administrator, you may first try installing a firewall between your\nnetwork and the Internet. (Most access routers today have firewall capability.) Firewalls\ninspect the datagram and segment header fields, denying suspicious datagrams entry\ninto the internal network. For example, a firewall may be configured to block all ICMP\necho request packets (see Section 5.6), thereby preventing an attacker from doing a\ntraditional port scan across your IP address range. Firewalls can also block packets\nbased on source and destination IP addresses and port numbers. Additionally, firewalls\ncan be configured to track TCP connections, granting entry only to datagrams that\nbelong to approved connections.\nAdditional protection can be provided with an IDS. An IDS, typically situated at the\nnetwork boundary, performs “deep packet inspection,” examining not only header fields\nbut also the payloads in the datagram (including application-layer data). An IDS has a\ndatabase of packet signatures that are known to be part of attacks. This database is\nautomatically updated as new attacks are discovered. As packets pass through the\nIDS, the IDS attempts to match header fields and payloads to the signatures in its\nsignature database. If such a match is found, an alert is created. An intrusion\nprevention system (IPS) is similar to an IDS, except that it actually blocks packets in\naddition to creating alerts. We’ll explore firewalls and IDSs in more detail in Section 4.5\nand in again Chapter 8.\nCan firewalls and IDSs fully shield your network from all attacks? The answer is\nclearly no, as attackers continually find new attacks for which signatures are not yet\navailable. But firewalls and traditional signature-based IDSs are useful in protecting\nyour network from known attacks.\nIn the early 1990s, the Internet Engineering Task Force began an effort to\ndevelop a successor to the IPv4 protocol. A prime motivation for this effort\nwas the realization that the 32-bit IPv4 address space was beginning to be\nused up, with new subnets and IP nodes being attached to the Internet (and\nbeing allocated unique IP addresses) at a breathtaking rate. To respond to\nthis need for a large IP address space, a new IP protocol, IPv6, was\ndeveloped. The designers of IPv6 also took this opportunity to tweak and\naugment other aspects of IPv4, based on the accumulated operational\nexperience with IPv4.\nThe point in time when IPv4 addresses would be completely allocated\n(and hence no new networks could attach to the Internet) was the subject of\nconsiderable debate. The estimates of the two leaders of the IETF’s Address\nLifetime Expectations working group were that addresses would become\nexhausted in 2008 and 2018, respectively [Solensky 1996]. In February\n2011, IANA allocated out the last remaining pool of unassigned IPv4\naddresses to a regional registry. While these registries still have available\nIPv4 addresses within their pool, once these addresses are exhausted, there\nare no more available address blocks that can be allocated from a central\npool [Huston 2011a]. A recent survey of IPv4 address-space exhaustion, and\nthe steps taken to prolong the life of the address space is [Richter 2015]; a\nrecent analysis of IPv4 address use is [Huston 2019].\nAlthough the mid-1990s estimates of IPv4 address depletion suggested\nthat a considerable amount of time might be left until the IPv4 address\nspace was exhausted, it was realized that considerable time would be\nneeded to deploy a new technology on such an extensive scale, and so the\nprocess to develop IP version 6 (IPv6) [RFC 2460] was begun [RFC 1752].\n(An often-asked question is what happened to IPv5? It was initially\nenvisioned that the ST-2 protocol would become IPv5, but ST-2 was later\ndropped.) An excellent source of information about IPv6 is [Huitema 1998].\nIPv6 Datagram Format\nThe format of the IPv6 datagram is shown in Figure 4.26. The most\nimportant changes introduced in IPv6 are evident in the datagram format:\nExpanded addressing capabilities. IPv6 increases the size of the IP\naddress from 32 to 128 bits. This ensures that the world won’t run out\nof IP addresses. Now, every grain of sand on the planet can be IP-\naddressable. In addition to unicast and multicast addresses, IPv6 has\nintroduced a new type of address, called an anycast address, that\nallows a datagram to be delivered to any one of a group of hosts. (This\nfeature could be used, for example, to send an HTTP GET to the nearest\nof a number of mirror sites that contain a given document.)\nA streamlined 40-byte header. As discussed below, a number of IPv4\nfields have been dropped or made optional. The resulting 40-byte fixed-\nlength header allows for faster processing of the IP datagram by a\nrouter. A new encoding of options allows for more flexible options\nprocessing.\nFlow labeling. IPv6 has an elusive definition of a flow. RFC 2460 states\nthat this allows “labeling of packets belonging to particular flows for\nwhich the sender requests special handling, such as a non-default\nquality of service or real-time service.” For example, audio and video\ntransmission might likely be treated as a flow. On the other hand, the\nmore traditional applications, such as file transfer and e-mail, might not\nbe treated as flows. It is possible that the traffic carried by a high-\npriority user (for example, someone paying for better service for their\ntraffic) might also be treated as a flow. What is clear, however, is that\nthe designers of IPv6 foresaw the eventual need to be able to\ndifferentiate among the flows, even if the exact meaning of a flow had\nyet to be determined.\nAs noted above, a comparison of Figure 4.26 with Figure 4.17 reveals\nthe simpler, more streamlined structure of the IPv6 datagram. The following\nfields are defined in IPv6:\nFigure 4.26 ♦IPv6 datagram format\nVersion. This 4-bit field identifies the IP version number. Not\nsurprisingly, IPv6 carries a value of 6 in this field. Note that putting a 4\nin this field does not create a valid IPv4 datagram. (If it did, life would\nbe a lot simpler—see the discussion below regarding the transition from\nIPv4 to IPv6.)\nTraffic class. The 8-bit traffic class field, like the TOS field in IPv4, can\nbe used to give priority to certain datagrams within a flow, or it can be\nused to give priority to datagrams from certain applications (for\nexample, voice-over-IP) over datagrams from other applications (for\nexample, SMTP e-mail).\nFlow label. As discussed above, this 20-bit field is used to identify a\nflow of datagrams.\nPayload length. This 16-bit value is treated as an unsigned integer\ngiving the number of bytes in the IPv6 datagram following the fixed-\nlength, 40-byte datagram header.\nthat we’ll cover in Chapter 7—is the following. Suppose two IPv6 nodes (in\nthis example, B and E in Figure 4.27) want to interoperate using IPv6\ndatagrams but are connected to each other by intervening IPv4 routers. We\nrefer to the intervening set of IPv4 routers between two IPv6 routers as a\ntunnel, as illustrated in Figure 4.27. With tunneling, the IPv6 node on the\nsending side of the tunnel (in this example, B) takes the entire IPv6\ndatagram and puts it in the data (payload) field of an IPv4 datagram. This\nIPv4 datagram is then addressed to the IPv6 node on the receiving side of\nthe tunnel (in this example, E) and sent to the first node in the tunnel (in this\nexample, C). The intervening IPv4 routers in the tunnel route this IPv4\ndatagram among themselves, just as they would any other datagram,\nblissfully unaware that the IPv4 datagram itself contains a complete IPv6\ndatagram. The IPv6 node on the receiving side of the tunnel eventually\nreceives the IPv4 datagram (it is the destination of the IPv4 datagram!),\ndetermines that the IPv4 datagram contains an IPv6 datagram (by observing\nthat the protocol number field in the IPv4 datagram is 41 [RFC 4213],\nindicating that the IPv4 payload is a IPv6 datagram), extracts the IPv6\ndatagram, and then routes the IPv6 datagram exactly as it would if it had\nreceived the IPv6 datagram from a directly connected IPv6 neighbor.\nFigure 4.27 ♦Tunneling\nWe end this section by noting that while the adoption of IPv6 was\ninitially slow to take off [Lawton 2001; Huston 2008b], momentum has\nbeen building. NIST [NIST IPv6 2020] reports that more than a third of US\ngovernment second-level domains are IPv6-enabled. On the client side,\nGoogle reports that about 25 percent of the clients accessing Google\nservices do so via IPv6 [Google IPv6 2020]. Other recent measurements\n[Czyz 2014] indicate that IPv6 adoption has been accelerating. The\nproliferation of devices such as IP-enabled phones and other portable\ndevices provides an additional push for more widespread deployment of\nIPv6. Europe’s Third Generation Partnership Program [3GPP 2020] has\nspecified IPv6 as the standard addressing scheme for mobile multimedia.\nOne important lesson that we can learn from the IPv6 experience is that\nit is enormously difficult to change network-layer protocols. Since the early\n1990s, numerous new network-layer protocols have been trumpeted as the\nnext major revolution for the Internet, but most of these protocols have had\nlimited penetration to date. These protocols include IPv6, multicast\nprotocols, and resource reservation protocols; a discussion of these latter\ntwo classes of protocols can be found in the online supplement to this text.\nIndeed, introducing new protocols into the network layer is like replacing\nthe foundation of a house—it is difficult to do without tearing the whole\nhouse down or at least temporarily relocating the house’s residents. On the\nother hand, the Internet has witnessed rapid deployment of new protocols at\nthe application layer. The classic examples, of course, are the Web, instant\nmessaging, streaming media, distributed games, and various forms of social\nmedia. Introducing new application-layer protocols is like adding a new\nlayer of paint to a house—it is relatively easy to do, and if you choose an\nattractive color, others in the neighborhood will copy you. In summary, in\nthe future, we can certainly expect to see changes in the Internet’s network\nlayer, but these changes will likely occur on a time scale that is much\nslower than the changes that will occur at the application layer.\n4.4 Generalized Forwarding and SDN\nRecall that Section 4.2.1 characterized destination-based forwarding as the\ntwo steps of looking up a destination IP address (“match”), then sending the\npacket into the switching fabric to the specified output port (“action”). Let’s\nnow consider a significantly more general “match-plus-action” paradigm,\nwhere the “match” can be made over multiple header fields associated with\ndifferent protocols at different layers in the protocol stack. The “action” can\ninclude forwarding the packet to one or more output ports (as in\ndestination-based forwarding), load balancing packets across multiple\noutgoing interfaces that lead to a service (as in load balancing), rewriting\nheader values (as in NAT), purposefully blocking/dropping a packet (as in a\nfirewall), sending a packet to a special server for further processing and\naction (as in DPI), and more.\nIn generalized forwarding, a match-plus-action table generalizes the\nnotion of the destination-based forwarding table that we encountered in\nSection 4.2.1. Because forwarding decisions may be made using network-\nlayer and/or link-layer source and destination addresses, the forwarding\ndevices shown in Figure 4.28 are more accurately described as “packet\nswitches” rather than layer 3 “routers” or layer 2 “switches.” Thus, in the\nremainder of this section, and in Section 5.5, we’ll refer to these devices as\npacket switches, adopting the terminology that is gaining widespread\nadoption in SDN literature.\nFigure 4.28 shows a match-plus-action table in each packet switch, with\nthe table being computed, installed, and updated by a remote controller. We\nnote that while it is possible for the control components at the individual\npacket switches to interact with each other (e.g., in a manner similar to that\nin Figure 4.2), in practice, generalized match-plus-action capabilities are\nimplemented via a remote controller that computes, installs, and updates\nthese tables. You might take a minute to compare Figures 4.2, 4.3, and 4.28\n—what similarities and differences do you notice between destination-based\nforwarding shown in Figures 4.2 and 4.3, and generalized forwarding\nshown in Figure 4.28?\nFigure 4.28 ♦Generalized forwarding: Each packet switch contains\na match-plus-action table that is computed and\ndistributed by a remote controller\nOur following discussion of generalized forwarding will be based on\nOpenFlow [McKeown 2008, ONF 2020, Casado 2014, Tourrilhes 2014]—a\nhighly visible standard that has pioneered the notion of the match-plus-\naction forwarding abstraction and controllers, as well as the SDN revolution\nmore generally [Feamster 2013]. We’ll primarily consider OpenFlow 1.0,\nwhich introduced key SDN abstractions and functionality in a particularly\nclear and concise manner. Later versions of ­OpenFlow introduced\nadditional capabilities as a result of experience gained through\nimplementation and use; current and earlier versions of the OpenFlow\nstandard can be found at [ONF 2020].\nEach entry in the match-plus-action forwarding table, known as a flow\ntable in OpenFlow, includes:\nA set of header field values to which an incoming packet will be\nmatched. As in the case of destination-based forwarding, hardware-\nbased matching is most rapidly performed in TCAM memory, with\nmore than a million destination address entries being possible [Bosshart\n2013]. A packet that matches no flow table entry can be dropped or sent\nto the remote controller for more processing. In practice, a flow table\nmay be implemented by multiple flow tables for performance or cost\nreasons [Bosshart 2013], but we’ll focus here on the abstraction of a\nsingle flow table.\nA set of counters that are updated as packets are matched to flow table\nentries. These counters might include the number of packets that have\nbeen matched by that table entry, and the time since the table entry was\nlast updated.\nA set of actions to be taken when a packet matches a flow table entry.\nThese actions might be to forward the packet to a given output port, to\ndrop the packet, makes copies of the packet and sent them to multiple\noutput ports, and/or to rewrite selected header fields.\nWe’ll explore matching and actions in more detail in Sections 4.4.1 and\n4.4.2, respectively. We’ll then study how the network-wide collection of\nper-packet switch matching rules can be used to implement a wide range of\nfunctions including routing, layer-2 switching, firewalling, load-balancing,\nvirtual networks, and more in Section 4.4.3. In closing, we note that the\nflow table is essentially an API, the abstraction through which an individual\npacket switch’s behavior can be programmed; we’ll see in Section 4.4.3 that\nnetwork-wide behaviors can similarly be programmed by appropriately\nprogramming/configuring these tables in a collection of network packet\nswitches [Casado 2014].\n4.4.1 Match\nFigure 4.29 shows the 11 packet-header fields and the incoming port ID that\ncan be matched in an OpenFlow 1.0 match-plus-action rule. Recall from\nSection 1.5.2 that a link-layer (layer 2) frame arriving to a packet switch\nwill contain a network-layer (layer 3) datagram as its payload, which in turn\nwill typically contain a transport-layer (layer 4) segment. The first\nobservation we make is that OpenFlow’s match abstraction allows for a\nmatch to be made on selected fields from three layers of protocol headers\n(thus rather brazenly defying the layering principle we studied in Section\n1.5). Since we’ve not yet covered the link layer, suffice it to say that the\nsource and destination MAC addresses shown in Figure 4.29 are the link-\nlayer addresses associated with the frame’s sending and receiving\ninterfaces; by forwarding on the basis of Ethernet addresses rather than IP\naddresses, we can see that an OpenFlow-enabled device can equally\nperform as a router (layer-3 device) forwarding datagrams as well as a\nswitch (layer-2 device) forwarding frames. The Ethernet type field\ncorresponds to the upper layer protocol (e.g., IP) to which the frame’s\npayload will be de-multiplexed, and the VLAN fields are concerned with\nso-called virtual local area networks that we’ll study in Chapter 6. The set\nof 12 values that can be matched in the OpenFlow 1.0 specification has\ngrown to 41 values in more recent OpenFlow specifications [Bosshart\nFigure 4.29 ♦Packet matching fields, OpenFlow 1.0 flow table\nThe ingress port refers to the input port at the packet switch on which a\npacket is received. The packet’s IP source address, IP destination address,\nIP protocol field, and IP type of service fields were discussed earlier in\nSection 4.3.1. The transport-layer source and destination port number fields\ncan also be matched.\nFlow table entries may also have wildcards. For example, an IP address\nof 128.119.*.* in a flow table will match the corresponding address field of\nany datagram that has 128.119 as the first 16 bits of its address. Each flow\ntable entry also has an associated priority. If a packet matches multiple flow\ntable entries, the selected match and corresponding action will be that of the\nhighest priority entry with which the packet matches.\nLastly, we observe that not all fields in an IP header can be matched.\nFor example OpenFlow does not allow matching on the basis of TTL field\nor datagram length field. Why are some fields allowed for matching, while\nothers are not? Undoubtedly, the answer has to do with the tradeoff between\nfunctionality and complexity. The “art” in choosing an abstraction is to\nprovide for enough functionality to accomplish a task (in this case to\nimplement, configure, and manage a wide range of network-layer functions\nthat had previously been implemented through an assortment of ­network-\nlayer devices), without over-burdening the abstraction with so much detail\nand generality that it becomes bloated and unusable. Butler Lampson has\nfamously noted [Lampson 1983]:\nDo one thing at a time, and do it well. An interface should capture the\nminimum essentials of an abstraction. Don’t generalize; generalizations\nare generally wrong.\nGiven OpenFlow’s success, one can surmise that its designers indeed chose\ntheir abstraction well. Additional details of OpenFlow matching can be\nfound in [ONF 2020].\n4.4.2 Action\nAs shown in Figure 4.28, each flow table entry has a list of zero or more\nactions that determine the processing that is to be applied to a packet that\nmatches a flow table entry. If there are multiple actions, they are performed\nin the order specified in the list.\nAmong the most important possible actions are:\nForwarding. An incoming packet may be forwarded to a particular\nphysical output port, broadcast over all ports (except the port on which\nit arrived) or multicast over a selected set of ports. The packet may be\nencapsulated and sent to the remote controller for this device. That\ncontroller then may (or may not) take some action on that packet,\nincluding installing new flow table entries, and may return the packet to\nthe device for forwarding under the updated set of flow table rules.\nDropping. A flow table entry with no action indicates that a matched\npacket should be dropped.\nModify-field. The values in 10 packet-header fields (all layer 2, 3, and 4\nfields shown in Figure 4.29 except the IP Protocol field) may be re-\nwritten before the packet is forwarded to the chosen output port.\n4.4.3 OpenFlow Examples of Match-plus-action in\nHaving now considered both the match and action components of\ngeneralized forwarding, let’s put these ideas together in the context of the\nsample network shown in Figure 4.30. The network has 6 hosts (h1, h2, h3,\nh4, h5 and h6) and three packet switches (s1, s2 and s3), each with four\nlocal interfaces (numbered 1 through 4). We’ll consider a number of\nnetwork-wide behaviors that we’d like to implement, and the flow table\nentries in s1, s2 and s3 needed to implement this behavior.\nFigure 4.30 ♦OpenFlow match-plus-action network with three\npacket switches, 6 hosts, and an OpenFlow controller\nA First Example: Simple Forwarding\nAs a very simple example, suppose that the desired forwarding behavior is\nthat packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3\nto s1, and then from s1 to s2 (thus completely avoiding the use of the link\nbetween s3 and s2). The flow table entry in s1 would be:\nOf course, we’ll also need a flow table entry in s3 so that datagrams\nsent from h5 or h6 are forwarded to s1 over outgoing interface 3:\nLastly, we’ll also need a flow table entry in s2 to complete this first\nexample, so that datagrams arriving from s1 are forwarded to their\ndestination, either host h3 or h4:\nA Second Example: Load Balancing\nAs a second example, let’s consider a load-balancing scenario, where\ndatagrams from h3 destined to 10.1.*.* are to be forwarded over the direct\nlink between s2 and s1, while datagrams from h4 destined to 10.1.*.* are to\nbe forwarded over the link between s2 and s3 (and then from s3 to s1). Note\nthat this behavior couldn’t be achieved with IP’s destination-based\nforwarding. In this case, the flow table in s2 would be:\nFlow table entries are also needed at s1 to forward the datagrams\nreceived from s2 to either h1 or h2; and flow table entries are needed at s3\nto forward datagrams received on interface 4 from s2 over interface 3\ntoward s1. See if you can figure out these flow table entries at s1 and s3.\nA Third Example: Firewalling\nAs a third example, let’s consider a firewall scenario in which s2 wants only\nto receive (on any of its interfaces) traffic sent from hosts attached to s3.\nIf there were no other entries in s2’s flow table, then only traffic from\n10.3.*.* would be forwarded to the hosts attached to s2.\nAlthough we’ve only considered a few basic scenarios here, the\nversatility and advantages of generalized forwarding are hopefully apparent.\nIn homework problems, we’ll explore how flow tables can be used to create\nmany different logical behaviors, including virtual networks—two or more\nlogically separate networks (each with their own independent and distinct\nforwarding behavior)—that use the same physical set of packet switches\nand links. In Section 5.5, we’ll return to flow tables when we study the\nSDN controllers that compute and distribute the flow tables, and the\nprotocol used for communicating between a packet switch and its controller.\nThe match-plus-action flow tables that we’ve seen in this section are\nactually a limited form of programmability, specifying how a router should\nforward and manipulate (e.g., change a header field) a datagram, based on\nthe match between the datagram’s header values and the matching\nconditions. One could imagine an even richer form of programmability—a\nprogramming language with higher-level constructs such as variables,\ngeneral purpose arithmetic and Boolean operations, variables, functions,\nand conditional statements, as well as constructs specifically designed for\ndatagram processing at line rate. P4 (Programming Protocol-independent\nPacket Processors) [P4 2020] is such a language, and has gained\nconsiderable interest and traction since its introduction five years ago\n[Bosshart 2014].\n4.5 Middleboxes\nRouters are the workhorses of the network layer, and in this chapter, we’ve\nlearned how they accomplish their “bread and butter” job of forwarding IP\ndatagrams toward their destination. But in this chapter, and in earlier\nchapters, we’ve also encountered other network equipment (“boxes”) within\nthe network that sit on the data path and perform functions other than\nforwarding. We encountered Web caches in Section 2.2.5; TCP connection\nsplitters in section 3.7; and network address translation (NAT), firewalls,\nand intrusion detection systems in Section 4.3.4. We learned in Section 4.4\nthat generalized forwarding allows a modern router to easily and naturally\nperform firewalling and load balancing with generalized “match plus\naction” operations.\nIn the past 20 years, we’ve seen tremendous growth in such\nmiddleboxes, which RFC 3234 defines as:\n“any intermediary box performing functions apart from normal,\nstandard functions of an IP router on the data path between a source\nhost and destination host”\nWe can broadly identify three types of services performed by middleboxes:\nNAT Translation. As we saw in Section 4.3.4, NAT boxes implement\nprivate network addressing, rewriting datagram header IP addresses and\nport numbers.\nSecurity Services. Firewalls block traffic based on header-field values or\nredirect packets for additional processing, such as deep packet\ninspection (DPI). Intrusion Detection Systems (IDS) are able to detect\npredetermined patterns and filter packets accordingly. Application-level\ne-mail filters block e-mails considered to be junk, phishing or otherwise\nposing a security threat.\nPerformance Enhancement. These middleboxes perform services such\nas compression, content caching, and load balancing of service requests\n(e.g., an HTTP request, or a search engine query) to one of a set of\nservers that can provide the desired service.\nMany other middleboxes [RFC 3234] provide capabilities belonging to\nthese three types of services, in both wired and wireless cellular [Wang\n2011] networks.\nWith the proliferation of middleboxes comes the attendant need to\noperate, manage, and upgrade this equipment. Separate specialized\nmanagement/operation skills translate to significant operational and capital\ncosts. It is perhaps not surprising then that researchers are exploring the use\nof commodity hardware (networking, computing, and storage) with\nspecialized software built on top of a common software stack—exactly the\napproach taken in SDN a decade earlier—to implement these services. This\napproach has become known as network function virtualization (NFV)\n[Mijumbi 2016]. An alternate approach that has also been explored is to\noutsource middlebox functionality to the cloud [Sherry 2012].\nFor many years, the Internet architecture had a clear separation between\nthe network layer and the transport/application layers. In these “good old\ndays,” the network layer consisted of routers, operating within the network\ncore, to forward datagrams toward their destinations using fields only in the\nIP datagram header. The transport and application layers were implemented\nin hosts operating at the network edge. Hosts exchanged packets among\nthemselves in transport-layer segments and application-layer messages.\nToday’s middleboxes clearly violate this separation: a NAT box, sitting\nbetween a router and host, rewrites network-layer IP addresses and\ntransport-layer port numbers; an in-network firewall blocks suspect\ndatagrams using application-layer (e.g., HTTP), transport-layer, and\nnetwork-layer header fields; e-mail security gateways are injected between\nthe e-mail sender (whether malicious or not) and the intended e-mail\napplication-layer \nwhitelisted/blacklisted IP addresses as well as e-mail message content.\nWhile there are those who have considered such middleboxes as a bit of an\narchitectural abomination [Garfinkel 2003], others have adopted the\nphilosophy that such middleboxes “exist for important and permanent\nreasons”—that they fill an important need—and that we’ll have more, not\nfewer, middleboxes in the future [Walfish 2004]. See the section in attached\nsidebar on “The end-to-end argument” for a slightly different lens on the\nquestion of where to place service functionality in a network.\nGiven the phenomenal success of the Internet, one might naturally wonder about the\narchitectural principles that have guided the development of what is arguably the largest\nand most complex engineered system ever built by humankind. RFC 1958, entitled\n“Architectural Principles of the Internet,” suggests that these principles, if indeed they exist,\nare truly minimal:\n“Many members of the Internet community would argue that there is no architecture,\nbut only a tradition, which was not written down for the first 25 years (or at least not by\nthe IAB). However, in very general terms, the community believes that the goal is\nconnectivity, the tool is the Internet Protocol, and the intelligence is end to end rather\nthan hidden in the network.” [RFC 1958]\nSo there we have it! The goal was to provide connectivity, there would be just one network-\nlayer protocol (the celebrated IP protocol we have studied in this chapter), and “intelligence”\n(one might say the “complexity”) would be placed at the network edge, rather than in the\nnetwork core. Let’s look these last two considerations in a bit more detail.\nBy now, we’re well acquainted with the five-layer Internet protocol stack that we first\nencountered in Figure 1.23. Another visualization of this stack, shown in Figure 4.31 and\nsometimes known as the “IP hourglass,” illustrates the “narrow waist” of the layered\nInternet architecture. While the Internet has many protocols in the physical, link, transport,\nand application layers, there is only one network layer protocol—the IP protocol. This is the\none protocol that must be implemented by each and every of the billions of Internet-\nconnected devices. This narrow waist has played a critical role in the phenomenal growth of\nthe Internet. The relative simplicity of the IP protocol, and the fact that it is the only universal\nrequirement for Internet connectivity has allowed a rich variety of networks—with very\ndifferent underlying link-layer technologies, from Ethernet to WiFi to cellular to optical\nnetworks to become part of the Internet. [Clark 1997] notes that role of the narrow waist,\nwhich he refers to as a “spanning layer,” is to “… hide the detailed differences among these\nvarious [underlying] technologies and present a uniform service interface to the applications\nabove.” For the IP layer in particular: “How does the IP spanning layer achieve its purpose?\nIt defines a basic set of services, which were carefully designed so that they could be\nconstructed from a wide range of underlying network technologies. Software, as a part of\nthe Internet [i.e., network] layer, translates what each of these lower-layer technologies\noffers into the common service of the Internet layer.”\nFor a discussion the narrow waist, including examples beyond the Internet, see [Beck\n2019; Akhshabi 2011]. We note here that as the Internet architecture enters mid-life\n(certainly, the Internet’s age of 40 to 50 years qualifies it for middle age!), one might observe\nthat its “narrow waist” may indeed be widening a bit (as often happens in middle age!) via\nthe rise of middleboxes.\nFigure 4.31 ♦The narrow-waisted Internet hourglass\nTHE END-TO-END ARGUMENT\nThe third principle in RFC 1958—that “intelligence is end to end rather than hidden in the\nnetwork”—speaks to the placement of functionality within the network. Here, we’ve seen\nthat until the recent rise of middleboxes, most Internet functionality was indeed placed at the\nnetwork’s edge. It’s worth noting that, in direct contrast with the 20th century telephone\nnetwork—which had “dumb” (non-programmable) endpoints and smart switches—the\nInternet has always had smart endpoints (programmable computers), enabling complex\nfunctionality to be placed at those endpoints. But a more principled argument for actually\nplacing functionality at the endpoints was made in an extremely influential paper [Saltzer\n1984] that articulated the “end-to-end argument.” It stated:\n“ . . . there is a list of functions each of which might be implemented in any of several\nways: by the communication subsystem, by its client, as a joint venture, or perhaps\nredundantly, each doing its own version. In reasoning about this choice, the\nrequirements of the application provide the basis for a class of arguments, which go as\nThe function in question can completely and correctly be implemented only with\nthe knowledge and help of the application standing at the end points of the\ncommunication system. Therefore, providing that questioned function as a feature\nof the communication system itself is not possible. (Sometimes an incomplete\nversion of the function provided by the communication system may be useful as a\nperformance enhancement.)\nWe call this line of reasoning against low-level function implementation the “end-to-end\nAn example illustrating the end-to-end argument is that of reliable data transfer. Since\npackets can be lost within the network (e.g., even without buffer overflows, a router holding\na queued packet could crash, or a portion of the network in which a packet is queued\nbecomes detached due to link failures), the endpoints (in this case via the TCP protocol)\nmust perform error control. As we will see in Chapter 6, some link-layer protocols do indeed\nperform local error control, but this local error control alone is “incomplete” and not sufficient\nto provide end-to-end reliable data transfer. And so reliable data transfer must be\nimplemented end to end.\nwe’re now ready to dive into the network layer’s control plane in Chapter 5!\nHomework Problems and Questions\nChapter 4 Review Questions\nSECTION 4.1\nR1. Let’s review some of the terminology used in this textbook. Recall\nthat the name of a transport-layer packet is segment and that the name\nof a link-layer packet is frame. What is the name of a network-layer\npacket? Recall that both routers and link-layer switches are called\npacket switches. What is the fundamental difference between a router\nand link-layer switch?\nR2. We noted that network layer functionality can be broadly divided into\ndata plane functionality and control plane functionality. What are the\nmain functions of the data plane? Of the control plane?\nR3. We made a distinction between the forwarding function and the\nrouting function performed in the network layer. What are the key\ndifferences between routing and forwarding?\nR4. What is the role of the forwarding table within a router?\nR5. We said that a network layer’s service model “defines the\ncharacteristics of end-to-end transport of packets between sending\nand receiving hosts.” What is the service model of the Internet’s\nnetwork layer? What guarantees are made by the Internet’s service\nmodel regarding the host-to-host delivery of datagrams?\nSECTION 4.2\nR6. In Section 4.2, we saw that a router typically consists of input ports,\noutput ports, a switching fabric and a routing processor. Which of\nthese are implemented in hardware and which are implemented in\nsoftware? Why? Returning to the notion of the network layer’s data\nplane and control plane, which are implemented in hardware and\nwhich are implemented in software? Why?\nR7. How can the input ports of a high-speed router facilitate fast\nforwarding ­decisions?\nR8. What is meant by destination-based forwarding? How does this differ\nfrom generalized forwarding (assuming you’ve read Section 4.4,\nwhich of the two approaches are adopted by Software-Defined\nNetworking)?\nR9. Suppose that an arriving packet matches two or more entries in a\nrouter’s forwarding table. With traditional destination-based\nforwarding, what rule does a router apply to determine which of these\nrules should be applied to determine the output port to which the\narriving packet should be switched?\nR10. Switching in a router forwards data from an input port to an output\nport. What is the advantage of switching via an interconnection\nnetwork over switching via memory and switching via bus?\nR11. What is the role of a packet scheduler at the output port of a router?\na. What is a drop-tail policy?\nb. What are AQM algorithms?\nc. Name one of the most widely studied and implemented AQM\nalgorithms and explain how it works.\nR13. What is HOL blocking? Does it occur in input ports or output ports?\nR14. In Section 4.2, we studied FIFO, Priority, Round Robin (RR), and\nWeighted Fair Queuing (WFQ) packet scheduling disciplines? Which\nof these queuing disciplines ensure that all packets depart in the order\nin which they arrived?\nR15. Give an example showing why a network operator might want one\nclass of packets to be given priority over another class of packets.\nR16. What is an essential different between RR and WFQ packet\nscheduling? Is there a case (Hint: Consider the WFQ weights) where\nRR and WFQ will behave exactly the same?\nSECTION 4.3\nR17. Suppose Host A sends Host B a TCP segment encapsulated in an IP\ndatagram. When Host B receives the datagram, how does the network\nlayer in Host B know it should pass the segment (that is, the payload\nof the datagram) to TCP rather than to UDP or to some other upper-\nlayer protocol?\nR18. What field in the IP header can be used to ensure that a packet is\nforwarded through no more than N routers?\nR19. Recall that we saw the Internet checksum being used in both\ntransport-layer segment (in UDP and TCP headers, Figures 3.7 and\n3.29 respectively) and in network-layer datagrams (IP header, Figure\n4.17). Now consider a transport layer segment encapsulated in an IP\ndatagram. Are the checksums in the segment header and datagram\nheader computed over any common bytes in the IP datagram?\nExplain your answer.\nR20. When a large datagram is fragmented into multiple smaller\ndatagrams, where are these smaller datagrams reassembled into a\nsingle larger datagram?\nR21. How many IP addresses does a router have?\nR22. What is the 32-bit binary equivalent of the IP address 202.3.14.25?\nR23. Visit a host that uses DHCP to obtain its IP address, network mask,\ndefault router, and IP address of its local DNS server. List these\nR24. Suppose there are four routers between a source host and a\ndestination host. Ignoring fragmentation, an IP datagram sent from\nthe source host to the ­destination host will travel over how many\nintroduction to Chapter 4, software-defined networking (SDN) makes\na clear separation between the data and control planes, implementing\ncontrol-plane functions in a separate “controller” service that is\ndistinct, and remote, from the forwarding components of the routers it\ncontrols. We’ll cover SDN controllers in Section 5.5.\nIn Sections 5.6 and 5.7, we’ll cover some of the nuts and bolts of\nmanaging an IP network: ICMP (the Internet Control Message\nProtocol) and SNMP (the Simple Network Management Protocol).\n5.1 Introduction\nLet’s quickly set the context for our study of the network control plane by\nrecalling Figures 4.2 and 4.3. There, we saw that the forwarding table (in\nthe case of ­destination-based forwarding) and the flow table (in the case of\ngeneralized forwarding) were the principal elements that linked the network\nlayer’s data and control planes. We learned that these tables specify the\nlocal data-plane forwarding behavior of a router. We saw that in the case of\ngeneralized forwarding, the actions taken could include not only forwarding\na packet to a router’s output port, but also dropping a packet, replicating a\npacket, and/or rewriting layer 2, 3 or 4 packet-header fields.\nIn this chapter, we’ll study how those forwarding and flow tables are\ncomputed, maintained and installed. In our introduction to the network\nlayer in Section 4.1, we learned that there are two possible approaches for\nFigure 5.1 ♦Per-router control: Individual routing algorithm\ncomponents interact in the control plane\nPer-router control. Figure 5.1 illustrates the case where a routing\nalgorithm runs in each and every router; both a forwarding and a\nrouting function are contained within each router. Each router has a\nrouting component that communicates with the routing components in\nother routers to compute the values for its forwarding table. This per-\nrouter control approach has been used in the Internet for decades. The\nOSPF and BGP protocols that we’ll study in Sections 5.3 and 5.4 are\nbased on this per-router approach to control.\nLogically centralized control. Figure 5.2 illustrates the case in which a\nlogically centralized controller computes and distributes the forwarding\ntables to be used by each and every router. As we saw in Sections 4.4\nand 4.5, the generalized match-plus-action abstraction allows the router\nto perform traditional IP forwarding as well as a rich set of other\nfunctions (load sharing, firewalling, and NAT) that had been previously\nimplemented in separate middleboxes.\nFigure 5.2 ♦Logically centralized control: A distinct, typically\nremote, controller interacts with local control agents\nThe controller interacts with a control agent (CA) in each of the routers\nvia a well-defined protocol to configure and manage that router’s flow\ntable. Typically, the CA has minimum functionality; its job is to\ncommunicate with the controller, and to do as the controller commands.\nUnlike the routing algorithms in Figure 5.1, the CAs do not directly interact\nwith each other nor do they actively take part in computing the forwarding\ntable. This is a key distinction between per-router control and logically\ncentralized control.\nBy “logically centralized” control [Levin 2012] we mean that the\nrouting control service is accessed as if it were a single central service\npoint, even though the service is likely to be implemented via multiple\nservers for fault-tolerance, and performance scalability reasons. As we will\nsee in Section 5.5, SDN adopts this notion of a logically centralized\ncontroller—an approach that is finding increased use in production\ndeployments. Google uses SDN to control the routers in its internal B4\nglobal wide-area network that interconnects its data centers [Jain 2013].\nSWAN [Hong 2013], from Microsoft Research, uses a logically ­centralized\ncontroller to manage routing and forwarding between a wide area network\nand a data center network. Major ISP deployments, including COMCAST’s\nActiveCore and Deutsche Telecom’s Access 4.0 are actively integrating\nSDN into their networks. And as we’ll see in Chapter 8, SDN control is\ncentral to 4G/5G cellular networking as well. [AT&T 2019] notes, “ …\nSDN, isn’t a vision, a goal, or a promise. It’s a reality. By the end of next\nyear, 75% of our network functions will be fully virtualized and software-\ncontrolled.” China Telecom and China Unicom are using SDN both within\ndata centers and between data centers [Li 2015].\n5.2 Routing Algorithms\nIn this section, we’ll study routing algorithms, whose goal is to determine good paths\n(equivalently, routes), from senders to receivers, through the network of routers.\nTypically, a “good” path is one that has the least cost. We’ll see that in practice,\nhowever, real-world concerns such as policy issues (for example, a rule such as “router\nx, belonging to organization Y, should not forward any packets originating from the\nnetwork owned by organization Z ”) also come into play. We note that whether the\nnetwork control plane adopts a per-router control approach or a logically centralized\napproach, there must always be a well-defined sequence of routers that a packet will\ncross in traveling from sending to receiving host. Thus, the routing algorithms that\ncompute these paths are of fundamental importance, and another candidate for our top-\n10 list of fundamentally important networking concepts.\nA graph is used to formulate routing problems. Recall that a graph G = (N, E) is a\nset N of nodes and a collection E of edges, where each edge is a pair of nodes from N.\nIn the context of network-layer routing, the nodes in the graph represent routers—the\npoints at which packet-forwarding decisions are made—and the edges connecting\nthese nodes represent the physical links between these routers. Such a graph\nabstraction of a computer network is shown in Figure 5.3. When we study the BGP\ninter-domain routing protocol, we’ll see that nodes represent networks, and the edge\nconnecting two such nodes represents direction connectivity (know as peering)\nbetween the two networks. To view some graphs representing real network maps, see\n[CAIDA 2020]; for a discussion of how well different graph-based models model the\nInternet, see [Zegura 1997, Faloutsos 1999, Li 2004].\nFigure 5.3 ♦Abstract graph model of a computer network\nAs shown in Figure 5.3, an edge also has a value representing its cost. Typically,\nan edge’s cost may reflect the physical length of the corresponding link (for example,\na transoceanic link might have a higher cost than a short-haul terrestrial link), the link\nspeed, or the monetary cost associated with a link. For our purposes, we’ll simply take\nthe edge costs as a given and won’t worry about how they are determined. For any\nedge (x, y) in E, in E, we denote c(x, y) as the cost of the edge between nodes x and y.\nIf the pair (x, y) does not belong to E, we set c(x, y) = ∞. Also, we’ll only consider\nundirected graphs (i.e., graphs whose edges do not have a direction) in our discussion\nhere, so that edge (x, y) is the same as edge (y, x) and that c(x, y) = c(y, x); however, the\nalgorithms we’ll study can be easily extended to the case of directed links with a\ndifferent cost in each direction. Also, a node y is said to be a neighbor of node x if (x,\ny) belongs to E.\nGiven that costs are assigned to the various edges in the graph abstraction, a\nnatural goal of a routing algorithm is to identify the least costly paths between sources\nand destinations. To make this problem more precise, recall that a path in a graph G =\n(N, E) is a sequence of nodes (x , x , ..., x ) such that each of the pairs (x , x ), (x , x ),\n, x ) are edges in E. The cost of a path (x , x , ..., x ) is simply the sum of all the\nedge costs along the path, that is, c(x , x ) + c(x , x ) + ...+ c(x\n, x ). Given any two\nnodes x and y, there are typically many paths between the two nodes, with each path\nhaving a cost. One or more of these paths is a least-cost path. The least-cost problem\nis therefore clear: Find a path between the source and destination that has least cost. In\nFigure 5.3, for example, the least-cost path between source node u and destination\nnode w is (u, x, y, w) with a path cost of 3. Note that if all edges in the graph have the\nsame cost, the least-cost path is also the shortest path (that is, the path with the\nsmallest number of links between the source and the destination).\nAs a simple exercise, try finding the least-cost path from node u to z in Figure 5.3\nand reflect for a moment on how you calculated that path. If you are like most people,\nyou found the path from u to z by examining Figure 5.3, tracing a few routes from u to\nz, and somehow convincing yourself that the path you had chosen had the least cost\namong all possible paths. (Did you check all of the 17 possible paths between u and z?\nProbably not!) Such a calculation is an example of a centralized routing algorithm—\nthe routing algorithm was run in one location, your brain, with complete information\nabout the network. Broadly, one way in which we can classify routing algorithms is\naccording to whether they are centralized or decentralized.\nA centralized routing algorithm computes the least-cost path between a source\nand destination using complete, global knowledge about the network. That is, the\nalgorithm takes the connectivity between all nodes and all link costs as inputs.\nThis then requires that the algorithm somehow obtain this information before\nactually performing the calculation. The calculation itself can be run at one site\n(e.g., a logically centralized controller as in Figure 5.2) or could be replicated in\nthe routing component of each and every router (e.g., as in Figure 5.1). The key\ndistinguishing feature here, however, is that the algorithm has complete\ninformation about connectivity and link costs. Algorithms with global state\ninformation are often referred to as link-state (LS) algorithms, since the\nalgorithm must be aware of the cost of each link in the network. We’ll study LS\nalgorithms in Section 5.2.1.\nIn a decentralized routing algorithm, the calculation of the least-cost path is\ncarried out in an iterative, distributed manner by the routers. No node has complete\ninformation about the costs of all network links. Instead, each node begins with\nonly the knowledge of the costs of its own directly attached links. Then, through\nan iterative process of calculation and exchange of information with its\nneighboring nodes, a node gradually calculates the least-cost path to a destination\nor set of destinations. The decentralized routing algorithm we’ll study below in\nSection 5.2.2 is called a distance-vector (DV) algorithm, because each node\nmaintains a vector of estimates of the costs (distances) to all other nodes in the\nnetwork. Such decentralized algorithms, with interactive message exchange\nbetween neighboring routers is perhaps more naturally suited to control planes\nwhere the routers interact directly with each other, as in Figure 5.1.\nA second broad way to classify routing algorithms is according to whether they\nare static or dynamic. In static routing algorithms, routes change very slowly over\ntime, often as a result of human intervention (for example, a human manually editing a\nlink costs). Dynamic routing algorithms change the routing paths as the network\ntraffic loads or topology change. A dynamic algorithm can be run either periodically or\nin direct response to topology or link cost changes. While dynamic algorithms are\nmore responsive to network changes, they are also more susceptible to problems such\nas routing loops and route oscillation.\nA third way to classify routing algorithms is according to whether they are load-\nsensitive or load-insensitive. In a load-sensitive algorithm, link costs vary\ndynamically to reflect the current level of congestion in the underlying link. If a high\ncost is associated with a link that is currently congested, a routing algorithm will tend\nto choose routes around such a congested link. While early ARPAnet routing\nalgorithms were load-sensitive [McQuillan 1980], a number of difficulties were\nencountered [Huitema 1998]. Today’s Internet routing algorithms (such as RIP, OSPF,\nand BGP) are load-insensitive, as a link’s cost does not explicitly reflect its current (or\nrecent past) level of congestion.\n5.2.1 The Link-State (LS) Routing Algorithm\nRecall that in a link-state algorithm, the network topology and all link costs are\nknown, that is, available as input to the LS algorithm. In practice, this is accomplished\nby having each node broadcast link-state packets to all other nodes in the network,\nwith each link-state packet containing the identities and costs of its attached links. In\npractice (for example, with the Internet’s OSPF routing protocol, discussed in Section\n5.3), this is often accomplished by a link-state broadcast algorithm ­[Perlman 1999].\nThe result of the nodes’ broadcast is that all nodes have an identical and complete view\nof the network. Each node can then run the LS algorithm and compute the same set of\nleast-cost paths as every other node.\nThe link-state routing algorithm we present below is known as Dijkstra’s\nalgorithm, named after its inventor. A closely related algorithm is Prim’s algorithm;\nsee [Cormen 2001] for a general discussion of graph algorithms. Dijkstra’s algorithm\ncomputes the least-cost path from one node (the source, which we will refer to as u) to\nall other nodes in the network. Dijkstra’s algorithm is iterative and has the property\nthat after the kth iteration of the algorithm, the least-cost paths are known to k\ndestination nodes, and among the least-cost paths to all destination nodes, these k\npaths will have the k smallest costs. Let us define the following notation:\nD(v): cost of the least-cost path from the source node to destination v as of this\niteration of the algorithm.\np(v): previous node (neighbor of v) along the current least-cost path from the\nsource to v.\nN': subset of nodes; v is in N' if the least-cost path from the source to v is\ndefinitively known.\nThe centralized routing algorithm consists of an initialization step followed by a\nloop. The number of times the loop is executed is equal to the number of nodes in the\nnetwork. Upon termination, the algorithm will have calculated the shortest paths from\nthe source node u to every other node in the network.\nLink-State (LS) Algorithm for Source Node u\nInitialization:\nfor all nodes v\nif v is a neighbor of u\nthen D(v) = c(u,v)\nelse D(v) = ∞\nfind w not in N’ such that D(w) is a minimum\n10 add w to N’\n11 update D(v) for each neighbor v of w and not in N’:\n12  D(v) = min(D(v), D(w)+ c(w,v) )\n13  /* new cost to v is either old cost to v or known\n14  least path cost to w plus cost from w to v */\n15 until N’= N\nAs an example, let’s consider the network in Figure 5.3 and compute the least-cost\npaths from u to all possible destinations. A tabular summary of the algorithm’s\ncomputation is shown in Table 5.1, where each line in the table gives the values of the\nalgorithm’s variables at the end of the iteration. Let’s consider the few first steps in\nIn the initialization step, the currently known least-cost paths from u to its directly\nattached neighbors, v, x, and w, are initialized to 2, 1, and 5, respectively. Note in\nparticular that the cost to w is set to 5 (even though we will soon see that a lesser-\ncost path does indeed exist) since this is the cost of the direct (one hop) link from u\nto w. The costs to y and z are set to infinity because they are not directly connected\nIn the first iteration, we look among those nodes not yet added to the set N' and\nfind that node with the least cost as of the end of the previous iteration. That node\nis x, with a cost of 1, and thus x is added to the set N'. Line 12 of the LS algorithm\nis then performed to update D(v) for all nodes v, yielding the results shown in the\nsecond line (Step 1) in Table 5.1. The cost of the path to v is unchanged. The cost\nof the path to w (which was 5 at the end of the initialization) through node x is\nfound to have a cost of 4. Hence this lower-cost path is selected and w’s\npredecessor along the shortest path from u is set to x. Similarly, the cost to y\n(through x) is computed to be 2, and the table is updated accordingly.\nIn the second iteration, nodes v and y are found to have the least-cost paths (2), and\nwe break the tie arbitrarily and add y to the set N' so that N' now contains u, x, and\ny. The cost to the remaining nodes not yet in N', that is, nodes v, w, and z, are\nupdated via line 12 of the LS algorithm, yielding the results shown in the third row\nin Table 5.1.\nAnd so on . . .\nTable 5.1 ♦Running the link-state algorithm on the network in Figure 5.3\nWhen the LS algorithm terminates, we have, for each node, its predecessor along\nthe least-cost path from the source node. For each predecessor, we also have its\npredecessor, and so in this manner we can construct the entire path from the source to\nall destinations. The forwarding table in a node, say node u, can then be constructed\nfrom this information by storing, for each destination, the next-hop node on the least-\ncost path from u to the destination. Figure 5.4 shows the resulting least-cost paths and\nforwarding table in u for the network in Figure 5.3.\nFigure 5.4 ♦Least cost path and forwarding table for node u\nWhat is the computational complexity of this algorithm? That is, given n nodes\n(not counting the source), how much computation must be done in the worst case to\nfind the least-cost paths from the source to all destinations? In the first iteration, we\nneed to search through all n nodes to determine the node, w, not in N' that has the\nminimum cost. In the second iteration, we need to check n − 1 nodes to determine the\nminimum cost; in the third iteration n − 2 nodes, and so on. Overall, the total number\nof nodes we need to search through over all the iterations is n(n + 1)/2, and thus we\nsay that the preceding implementation of the LS algorithm has worst-case complexity\nof order n squared: O(n ). (A more sophisticated implementation of this algorithm,\nusing a data structure known as a heap, can find the minimum in line 9 in logarithmic\nrather than linear time, thus reducing the complexity.)\nBefore completing our discussion of the LS algorithm, let us consider a pathology\nthat can arise. Figure 5.5 shows a simple network topology where link costs are equal\nto the load carried on the link, for example, reflecting the delay that would be\nexperienced. In this example, link costs are not symmetric; that is, c(u,v) equals c(v,u)\nonly if the load carried on both directions on the link (u,v) is the same. In this example,\nnode z originates a unit of traffic destined for w, node x also originates a unit of traffic\ndestined for w, and node y injects an amount of traffic equal to e, also destined for w.\nThe initial routing is shown in Figure 5.5(a) with the link costs corresponding to the\namount of traffic carried.\nWhen the LS algorithm is next run, node y determines (based on the link costs\nshown in Figure 5.5(a)) that the clockwise path to w has a cost of 1, while the\ncounterclockwise path to w (which it had been using) has a cost of 1 + e. Hence y’s\nleast-cost path to w is now clockwise. Similarly, x determines that its new least-cost\npath to w is also clockwise, resulting in costs shown in Figure 5.5(b). When the LS\nalgorithm is run next, nodes x, y, and z all detect a zero-cost path to w in the\ncounterclockwise direction, and all route their traffic to the counterclockwise routes.\nThe next time the LS algorithm is run, x, y, and z all then route their traffic to the\nclockwise routes.\nFigure 5.5 ♦Oscillations with congestion-sensitive routing\nWhat can be done to prevent such oscillations (which can occur in any algorithm,\nnot just an LS algorithm, that uses a congestion or delay-based link metric)? One\nsolution would be to mandate that link costs not depend on the amount of traffic\ncarried—an unacceptable solution since one goal of routing is to avoid highly\ncongested (for example, high-delay) links. Another solution is to ensure that not all\nrouters run the LS algorithm at the same time. This seems a more reasonable solution,\nsince we would hope that even if routers ran the LS algorithm with the same\nperiodicity, the execution instance of the algorithm would not be the same at each\nnode. Interestingly, researchers have found that routers in the Internet can self-\nsynchronize among themselves [Floyd Synchronization 1994]. That is, even though\nthey initially execute the algorithm with the same period but at different instants of\ntime, the algorithm execution instance can eventually become, and remain,\nsynchronized at the routers. One way to avoid such self-synchronization is for each\nrouter to randomize the time it sends out a link advertisement.\nHaving studied the LS algorithm, let’s consider the other major routing algorithm\nthat is used in practice today—the distance-vector routing algorithm.\n5.2.2 The Distance-Vector (DV) Routing Algorithm\nWhereas the LS algorithm is an algorithm using global information, the distance-\nvector (DV) algorithm is iterative, asynchronous, and distributed. It is distributed in\nthat each node receives some information from one or more of its directly attached\nneighbors, performs a calculation, and then distributes the results of its calculation\nback to its neighbors. It is iterative in that this process continues on until no more\ninformation is exchanged between neighbors. (Interestingly, the algorithm is also self-\nterminating—there is no signal that the computation should stop; it just stops.) The\nalgorithm is asynchronous in that it does not require all of the nodes to operate in\nlockstep with each other. We’ll see that an asynchronous, iterative, self-terminating,\ndistributed algorithm is much more interesting and fun than a centralized algorithm!\nBefore we present the DV algorithm, it will prove beneficial to discuss an\nimportant relationship that exists among the costs of the least-cost paths. Let d (y) be\nthe cost of the least-cost path from node x to node y. Then the least costs are related by\nthe celebrated Bellman-Ford equation, namely,\nwhere the min  in the equation is taken over all of x’s neighbors. The Bellman-Ford\nequation is rather intuitive. Indeed, after traveling from x to v, if we then take the least-\ncost path from v to y, the path cost will be c(x, v) + d (y). Since we must begin by\ndx(y) = minv{c(x,  v) + dv( y)},\ntraveling to some neighbor v, the least cost from x to y is the minimum of c(x, v) +\nd (y) taken over all neighbors v.\nBut for those who might be skeptical about the validity of the equation, let’s check\nit for source node u and destination node z in Figure 5.3. The source node u has three\nneighbors: nodes v, x, and w. By walking along various paths in the graph, it is easy to\nsee that d (z) = 5, d (z) = 3, and d (z) = 3. Plugging these values into Equation 5.1,\nalong with the costs c(u, v) = 2, c(u, x) = 1, and c(u, w) = 5, gives d (z) = min{2 + 5, 5\n+ 3, 1 + 3} = 4, which is obviously true and which is exactly what the Dijskstra\nalgorithm gave us for the same network. This quick verification should help relieve\nany skepticism you may have.\nThe Bellman-Ford equation is not just an intellectual curiosity. It actually has\nsignificant practical importance: the solution to the Bellman-Ford equation provides\nthe entries in node x’s forwarding table. To see this, let v* be any neighboring node\nthat achieves the minimum in Equation 5.1. Then, if node x wants to send a packet to\nnode y along a least-cost path, it should first forward the packet to node v*. Thus, node\nx’s forwarding table would specify node v* as the next-hop router for the ultimate\ndestination y. Another important practical contribution of the Bellman-Ford equation is\nthat it suggests the form of the neighbor-to-neighbor communication that will take\nplace in the DV algorithm.\nThe basic idea is as follows. Each node x begins with D (y), an estimate of the cost\nof the least-cost path from itself to node y, for all nodes, y, in N. Let D  = [D (y): y in\nN] be node x’s distance vector, which is the vector of cost estimates from x to all other\nnodes, y, in N. With the DV algorithm, each node x maintains the following routing\ninformation:\nFor each neighbor v, the cost c(x,v) from x to directly attached neighbor, v\nNode x’s distance vector, that is, D  = [D (y): y in N], containing x’s estimate of its\ncost to all destinations, y, in N\nThe distance vectors of each of its neighbors, that is, D  = [D (y): y in N] for each\nneighbor v of x\nIn the distributed, asynchronous algorithm, from time to time, each node sends a copy\nof its distance vector to each of its neighbors. When a node x receives a new distance\nvector from any of its neighbors w, it saves w’s distance vector, and then uses the\nBellman-Ford equation to update its own distance vector as follows:\nIf node x’s distance vector has changed as a result of this update step, node x will then\nsend its updated distance vector to each of its neighbors, which can in turn update their\nown distance vectors. Miraculously enough, as long as all the nodes continue to\nexchange their distance vectors in an asynchronous fashion, each cost estimate D (y)\nconverges to d (y), the actual cost of the least-cost path from node x to node y\n[Bertsekas 1991]!\nDistance-Vector (DV) Algorithm\nAt each node, x:\nInitialization:\n for all destinations y in N:\n Dx(y)= c(x,y)/* if y is not a neighbor then c(x,y)= ∞\n for each neighbor w\n D w(y) = ? for all destinations y in N\n for each neighbor w\n send distance vector Dx = [Dx(y): y in N] to w\n10 wait (until I see a link cost change to some neighbor w\n11  until I receive a distance vector from some neighbor\n13  for each y in N:\n14  D x(y) = minv{c(x,v) + Dv(y)}\nDx(y) = minv{c(x,  v) + Dv(y)}\nfor each node y in N\n16 if Dx(y) changed for any destination y\n17  send distance vector Dx = [Dx(y): y in N] to all\nIn the DV algorithm, a node x updates its distance-vector estimate when it either\nsees a cost change in one of its directly attached links or receives a distance-vector\nupdate from some neighbor. But to update its own forwarding table for a given\ndestination y, what node x really needs to know is not the shortest-path distance to y\nbut instead the neighboring node v*(y) that is the next-hop router along the shortest\npath to y. As you might expect, the next-hop router v*(y) is the neighbor v that\nachieves the minimum in Line 14 of the DV algorithm. (If there are multiple neighbors\nv that achieve the minimum, then v*(y) can be any of the minimizing neighbors.)\nThus, in Lines 13–14, for each destination y, node x also determines v*(y) and updates\nits forwarding table for destination y.\nRecall that the LS algorithm is a centralized algorithm in the sense that it requires\neach node to first obtain a complete map of the network before running the Dijkstra\nalgorithm. The DV algorithm is decentralized and does not use such global\ninformation. Indeed, the only information a node will have is the costs of the links to\nits directly attached neighbors and information it receives from these neighbors. Each\nnode waits for an update from any neighbor (Lines 10–11), calculates its new distance\nvector when receiving an update (Line 14), and distributes its new distance vector to\nits neighbors (Lines 16–17). DV-like algorithms are used in many routing protocols in\npractice, including the Internet’s RIP and BGP, ISO IDRP, Novell IPX, and the\noriginal ARPAnet.\nFigure 5.6 illustrates the operation of the DV algorithm for the simple three-node\nnetwork shown at the top of the figure. The operation of the algorithm is illustrated in\na synchronous manner, where all nodes simultaneously receive distance vectors from\ntheir neighbors, compute their new distance vectors, and inform their neighbors if their\ndistance vectors have changed. After studying this example, you should convince\nyourself that the algorithm operates correctly in an asynchronous manner as well, with\nnode computations and update generation/reception occurring at any time.\nFigure 5.6 ♦Distance-vector (DV) algorithm in operation\nThe leftmost column of the figure displays three initial routing tables for each of\nthe three nodes. For example, the table in the upper-left corner is node x’s initial\nrouting table. Within a specific routing table, each row is a distance vector—\nspecifically, each node’s routing table includes its own distance vector and that of each\nof its neighbors. Thus, the first row in node x’s initial routing table is D  = [D (x),\nD (y), D (z)] = [0, 2, 7]. The second and third rows in this table are the most recently\nreceived distance vectors from nodes y and z, respectively. Because at initialization\nnode x has not received anything from node y or z, the entries in the second and third\nrows are initialized to infinity.\nAfter initialization, each node sends its distance vector to each of its two\nneighbors. This is illustrated in Figure 5.6 by the arrows from the first column of\ntables to the second column of tables. For example, node x sends its distance vector D\n= [0, 2, 7] to both nodes y and z. After receiving the updates, each node recomputes its\nown distance vector. For example, node x computes\nD (y) = min{c(x,y) + D (y), c(x,z) + D (y)} = min{2 + 0, 7 + 1} = 2\nD (z) = min{c(x,y) + D (z), c(x,z) + D (z)} = min{2 + 1, 7 + 0} = 3\nThe second column therefore displays, for each node, the node’s new distance vector\nalong with distance vectors just received from its neighbors. Note, for example, that\nnode x’s estimate for the least cost to node z, D (z), has changed from 7 to 3. Also note\nthat for node x, neighboring node y achieves the minimum in line 14 of the DV\nalgorithm; thus, at this stage of the algorithm, we have at node x that v (y) = y and v (z)\nAfter the nodes recompute their distance vectors, they again send their updated\ndistance vectors to their neighbors (if there has been a change). This is illustrated in\nFigure 5.6 by the arrows from the second column of tables to the third column of\ntables. Note that only nodes x and z send updates: node y’s distance vector didn’t\nchange so node y doesn’t send an update. After receiving the updates, the nodes then\nrecompute their distance vectors and update their routing tables, which are shown in\nthe third column.\nThe process of receiving updated distance vectors from neighbors, recomputing\nrouting table entries, and informing neighbors of changed costs of the least-cost path\nto a destination continues until no update messages are sent. At this point, since no\nupdate messages are sent, no further routing table calculations will occur and the\nalgorithm will enter a quiescent state; that is, all nodes will be performing the wait in\nLines 10–11 of the DV algorithm. The algorithm remains in the quiescent state until a\nlink cost changes, as discussed next.\nDistance-Vector Algorithm: Link-Cost Changes and Link Failure\nWhen a node running the DV algorithm detects a change in the link cost from itself to\na neighbor (Lines 10–11), it updates its distance vector (Lines 13–14) and, if there’s a\nchange in the cost of the least-cost path, informs its neighbors (Lines 16–17) of its new\ndistance vector. Figure 5.7(a) illustrates a scenario where the link cost from y to x\nchanges from 4 to 1. We focus here only on y’ and z’s distance table entries to\ndestination x. The DV algorithm causes the following sequence of events to occur:\nAt time t , y detects the link-cost change (the cost has changed from 4 to 1),\nupdates its distance vector, and informs its neighbors of this change since its\ndistance vector has changed.\nAt time t , z receives the update from y and updates its table. It computes a new\nleast cost to x (it has decreased from a cost of 5 to a cost of 2) and sends its new\ndistance vector to its neighbors.\nAt time t , y receives z’s update and updates its distance table. y’s least costs do not\nchange and hence y does not send any message to z. The algorithm comes to a\nquiescent state.\nThus, only two iterations are required for the DV algorithm to reach a quiescent state.\nThe good news about the decreased cost between x and y has propagated quickly\nthrough the network.\nLet’s now consider what can happen when a link cost increases. Suppose that the\nlink cost between x and y increases from 4 to 60, as shown in Figure 5.7(b).\nFigure 5.7 ♦Changes in link cost\n1. Before the link cost changes, D (x) = 4, D (z) = 1, D (y) = 1, and D (x) = 5. At time\nt , y detects the link-cost change (the cost has changed from 4 to 60). y computes\nits new minimum-cost path to x to have a cost of\nDy(x) = min{c(y, x) + Dx(x), c(y, z) + Dz(x)} = min{60 + 0, 1 + 5} = 6\nOf course, with our global view of the network, we can see that this new cost via z\nis wrong. But the only information node y has is that its direct cost to x is 60 and\nthat z has last told y that z could get to x with a cost of 5. So in order to get to x, y\nwould now route through z, fully expecting that z will be able to get to x with a cost\nof 5. As of t  we have a routing loop—in order to get to x, y routes through z, and z\nroutes through y. A routing loop is like a black hole—a packet destined for x\narriving at y or z as of t  will bounce back and forth between these two nodes\nforever (or until the forwarding tables are changed).\n2. Since node y has computed a new minimum cost to x, it informs z of its new\ndistance vector at time t .\n3. Sometime after t , z receives y’s new distance vector, which indicates that y’s\nminimum cost to x is 6. z knows it can get to y with a cost of 1 and hence computes\na new least cost to x of D (x) = min {50 + 0,1 + 6} = 7. Since z’s least cost to x has\nincreased, it then informs y of its new distance vector at t .\n4. In a similar manner, after receiving z’s new distance vector, y determines D (x) = 8\nand sends z its distance vector. z then determines D (x) = 9 and sends y its distance\nvector, and so on.\nHow long will the process continue? You should convince yourself that the loop will\npersist for 44 iterations (message exchanges between y and z)—until z eventually\ncomputes the cost of its path via y to be greater than 50. At this point, z will (finally!)\ndetermine that its least-cost path to x is via its direct connection to x. y will then route\nto x via z. The result of the bad news about the increase in link cost has indeed traveled\nslowly! What would have happened if the link cost c(y, x) had changed from 4 to\n10,000 and the cost c(z, x) had been 9,999? Because of such scenarios, the problem we\nhave seen is sometimes referred to as the count-to-infinity ­problem.\nDistance-Vector Algorithm: Adding Poisoned Reverse\nThe specific looping scenario just described can be avoided using a technique known\nas poisoned reverse. The idea is simple—if z routes through y to get to destination x,\nthen z will advertise to y that its distance to x is infinity, that is, z will advertise to y\nthat D (x) = ∞ (even though z knows D (x) = 5 in truth). z will continue telling this\nlittle white lie to y as long as it routes to x via y. Since y believes that z has no path to\nx, y will never attempt to route to x via z, as long as z continues to route to x via y (and\nlies about doing so).\nLet’s now see how poisoned reverse solves the particular looping problem we\nencountered before in Figure 5.5(b). As a result of the poisoned reverse, y’s distance\ntable indicates D (x) = ∞. When the cost of the (x, y) link changes from 4 to 60 at time\nt , y updates its table and continues to route directly to x, albeit at a higher cost of 60,\nand informs z of its new cost to x, that is, D (x) = 60. After receiving the update at t , z\nimmediately shifts its route to x to be via the direct (z, x) link at a cost of 50. Since this\nis a new least-cost path to x, and since the path no longer passes through y, z now\ninforms y that D (x) = 50 at t . After receiving the update from z, y updates its distance\ntable with D (x) = 51. Also, since z is now on y’s least-cost path to x, y poisons the\nreverse path from z to x by informing z at time t  that D (x) = ∞ (even though y knows\nthat D (x) = 51 in truth).\nDoes poisoned reverse solve the general count-to-infinity problem? It does not.\nYou should convince yourself that loops involving three or more nodes (rather than\nsimply two immediately neighboring nodes) will not be detected by the poisoned\nreverse technique.\nA Comparison of LS and DV Routing Algorithms\nThe DV and LS algorithms take complementary approaches toward computing\nrouting. In the DV algorithm, each node talks to only its directly connected neighbors,\nbut it provides its neighbors with least-cost estimates from itself to all the nodes (that\nit knows about) in the network. The LS algorithm requires global information.\nConsequently, when implemented in each and every router, for example, as in Figures\n4.2 and 5.1, each node would need to communicate with all other nodes (via\nbroadcast), but it tells them only the costs of its directly connected links. Let’s\nconclude our study of LS and DV algorithms with a quick comparison of some of their\nattributes. Recall that N is the set of nodes (routers) and E is the set of edges (links).\nMessage complexity. We have seen that LS requires each node to know the cost of\neach link in the network. This requires O(|N| |E|) messages to be sent. Also,\nwhenever a link cost changes, the new link cost must be sent to all nodes. The DV\nalgorithm requires message exchanges between directly connected neighbors at\neach iteration. We have seen that the time needed for the algorithm to converge\ncan depend on many factors. When link costs change, the DV algorithm will\npropagate the results of the changed link cost only if the new link cost results in a\nchanged least-cost path for one of the nodes attached to that link.\nSpeed of convergence. We have seen that our implementation of LS is an O(|N|2)\nalgorithm requiring O(|N| |E|)) messages. The DV algorithm can converge slowly\nand can have routing loops while the algorithm is converging. DV also suffers\nfrom the count-to-infinity problem.\nRobustness. What can happen if a router fails, misbehaves, or is sabotaged? Under\nLS, a router could broadcast an incorrect cost for one of its attached links (but no\nothers). A node could also corrupt or drop any packets it received as part of an LS\nbroadcast. But an LS node is computing only its own forwarding tables; other\nnodes are performing similar calculations for themselves. This means route\ncalculations are somewhat separated under LS, providing a degree of robustness.\nUnder DV, a node can advertise incorrect least-cost paths to any or all destinations.\n(Indeed, in 1997, a malfunctioning router in a small ISP provided national\nbackbone routers with erroneous routing information. This caused other routers to\nflood the malfunctioning router with traffic and caused large portions of the\nInternet to become disconnected for up to several hours [Neumann 1997].) More\ngenerally, we note that, at each iteration, a node’s calculation in DV is passed on to\nits neighbor and then indirectly to its neighbor’s neighbor on the next iteration. In\nthis sense, an incorrect node calculation can be diffused through the entire network\nIn the end, neither algorithm is an obvious winner over the other; indeed, both\nalgorithms are used in the Internet.\n5.3 Intra-AS Routing in the Internet: OSPF\nIn our study of routing algorithms so far, we’ve viewed the network simply\nas a collection of interconnected routers. One router was indistinguishable\nfrom another in the sense that all routers executed the same routing\nalgorithm to compute routing paths through the entire network. In practice,\nthis model and its view of a homogenous set of routers all executing the\nsame routing algorithm is simplistic for two important reasons:\nScale. As the number of routers becomes large, the overhead involved\nin communicating, computing, and storing routing information becomes\nprohibitive. Today’s Internet consists of hundreds of millions of routers.\nStoring routing information for possible destinations at each of these\nrouters would clearly require enormous amounts of memory. The\noverhead required to broadcast connectivity and link cost updates\namong all of the routers would be huge! A distance-vector algorithm\nthat iterated among such a large number of routers would surely never\nconverge. Clearly, something must be done to reduce the complexity of\nroute computation in a network as large as the Internet.\nAdministrative autonomy. As described in Section 1.3, the Internet is a\nnetwork of ISPs, with each ISP consisting of its own network of routers.\nAn ISP generally desires to operate its network as it pleases (for\nexample, to run whatever routing algorithm it chooses within its\nnetwork) or to hide aspects of its network’s internal organization from\nthe outside. Ideally, an organization should be able to operate and\nadminister its network as it wishes, while still being able to connect its\nnetwork to other outside networks.\nBoth of these problems can be solved by organizing routers into\nautonomous ­systems (ASs), with each AS consisting of a group of routers\nthat are under the same administrative control. Often the routers in an ISP,\nand the links that interconnect them, constitute a single AS. Some ISPs,\nhowever, partition their network into multiple ASs. In particular, some tier-1\nISPs use one gigantic AS for their entire network, whereas others break up\ntheir ISP into tens of interconnected ASs. An autonomous system is\nidentified by its globally unique autonomous system number (ASN) [RFC\n1930]. AS numbers, like IP addresses, are assigned by ICANN regional\nregistries [ICANN 2020].\nRouters within the same AS all run the same routing algorithm and\nhave information about each other. The routing algorithm ­running within an\nautonomous system is called an intra-autonomous system routing ­-\nOpen Shortest Path First (OSPF)\nOSPF routing and its closely related cousin, IS-IS, are widely used for\nintra-AS routing in the Internet. The Open in OSPF indicates that the\nrouting protocol specification is publicly available (for example, as opposed\nto Cisco’s EIGRP protocol, which was only recently became open [Savage\n2015], after roughly 20 years as a Cisco-proprietary protocol). The most\nrecent version of OSPF, version 2, is defined in [RFC 2328], a public\nOSPF is a link-state protocol that uses flooding of link-state\ninformation and a Dijkstra’s least-cost path algorithm. With OSPF, each\nrouter constructs a complete topological map (that is, a graph) of the entire\nautonomous system. Each router then locally runs Dijkstra’s shortest-path\nalgorithm to determine a shortest-path tree to all subnets, with itself as the\nroot node. Individual link costs are configured by the network administrator\n(see sidebar, Principles and Practice: Setting OSPF Weights). The\nadministrator might choose to set all link costs to 1, thus achieving\nminimum-hop routing, or might choose to set the link weights to be\ninversely proportional to link capacity in order to discourage traffic from\nusing low-bandwidth links. OSPF does not mandate a policy for how link\nweights are set (that is the job of the ­network administrator), but instead\nprovides the mechanisms (protocol) for determining least-cost path routing\nfor the given set of link weights.\nOur discussion of link-state routing has implicitly assumed that link weights are set, a\nrouting algorithm such as OSPF is run, and traffic flows according to the routing tables\ncomputed by the LS algorithm. In terms of cause and effect, the link weights are given (i.e.,\nthey come first) and result (via Dijkstra’s algorithm) in routing paths that minimize overall\ncost. In this viewpoint, link weights reflect the cost of using a link (for example, if link\nweights are inversely proportional to capacity, then the use of high-capacity links would\nhave smaller weight and thus be more attractive from a routing standpoint) and Dijsktra’s\nalgorithm serves to minimize overall cost.\nIn practice, the cause and effect relationship between link weights and routing paths may\nbe reversed, with network operators configuring link weights in order to obtain routing paths\nthat achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For example,\nsuppose a network operator has an estimate of traffic flow entering the network at each\ningress point and destined for each egress point. The operator may then want to put in\nplace a specific routing of ingress-to-egress flows that minimizes the maximum utilization\nover all of the network’s links. But with a routing algorithm such as OSPF, the operator’s\nmain “knobs” for tuning the routing of flows through the network are the link weights. Thus,\nin order to achieve the goal of minimizing the maximum link utilization, the operator must\nfind the set of link weights that achieves this goal. This is a reversal of the cause and effect\nrelationship—the desired routing of flows is known, and the OSPF link weights must be\nfound such that the OSPF routing algorithm results in this desired routing of flows.\nWith OSPF, a router broadcasts routing information to all other routers\nin the autonomous system, not just to its neighboring routers. A router\nbroadcasts link-state information whenever there is a change in a link’s state\n(for example, a change in cost or a change in up/down status). It also\nbroadcasts a link’s state periodically (at least once every 30 minutes), even\nif the link’s state has not changed. RFC 2328 notes that “this periodic\nupdating of link state advertisements adds robustness to the link state\nalgorithm.” OSPF advertisements are contained in OSPF messages that are\ncarried directly by IP, with an upper-layer protocol of 89 for OSPF. Thus,\nthe OSPF protocol must itself implement functionality such as reliable\nmessage transfer and link-state broadcast. The OSPF protocol also checks\nthat links are operational (via a HELLO message that is sent to an attached\nneighbor) and allows an OSPF router to obtain a neighboring router’s\ndatabase of network-wide link state.\nSome of the advances embodied in OSPF include the following:\nSecurity. Exchanges between OSPF routers (for example, link-state\nupdates) can be authenticated. With authentication, only trusted routers\ncan participate in the OSPF protocol within an AS, thus preventing\nmalicious intruders (or networking students taking their newfound\nknowledge out for a joyride) from injecting incorrect information into\nrouter tables. By default, OSPF packets between routers are  not\nauthenticated and could be forged. Two types of authentication can be\nconfigured—simple and MD5 (see Chapter 8 for a discussion on MD5\nand authentication in general). With simple authentication, the same\npassword is configured on each router. When a router sends an OSPF\npacket, it includes the password in plaintext. Clearly, simple\nauthentication is not very secure. MD5 authentication is based on\nshared secret keys that are configured in all the routers. For each OSPF\npacket that it sends, the router computes the MD5 hash of the content of\nthe OSPF packet appended with the secret key. (See the discussion of\nmessage authentication codes in Chapter 8.) Then the router includes\nthe resulting hash value in the OSPF packet. The receiving router, using\nthe preconfigured secret key, will compute an MD5 hash of the packet\nand compare it with the hash value that the packet carries, thus\nverifying the packet’s authenticity. Sequence numbers are also used\nwith MD5 authentication to protect against replay attacks.\nMultiple same-cost paths. When multiple paths to a destination have the\nsame cost, OSPF allows multiple paths to be used (that is, a single path\nneed not be chosen for carrying all traffic when multiple equal-cost\npaths exist).\nIntegrated support for unicast and multicast routing. Multicast OSPF\n(MOSPF) [RFC 1584] provides simple extensions to OSPF to provide\nfor multicast routing. MOSPF uses the existing OSPF link database and\nadds a new type of link-state advertisement to the existing OSPF link-\nstate broadcast mechanism.\nSupport for hierarchy within a single AS. An OSPF autonomous system\ncan be configured hierarchically into areas. Each area runs its own\nOSPF link-state routing algorithm, with each router in an area\nbroadcasting its link state to all other routers in that area. Within each\narea, one or more area border routers are responsible for routing packets\noutside the area. Lastly, exactly one OSPF area in the AS is configured\nto be the backbone area. The primary role of the backbone area is to\nroute traffic between the other areas in the AS. The backbone always\ncontains all area border routers in the AS and may contain non-border\nrouters as well. Inter-area routing within the AS requires that the packet\nbe first routed to an area border router (intra-area routing), then routed\nthrough the backbone to the area border router that is in the destination\narea, and then routed to the final destination.\nOSPF is a relatively complex protocol, and our coverage here has been\nnecessarily brief; [Huitema 1998; Moy 1998; RFC 2328] provide additional\n5.4 Routing Among the ISPs: BGP\nWe just learned that OSPF is an example of an intra-AS routing protocol.\nWhen routing a packet between a source and destination within the same\nAS, the route the packet follows is entirely determined by the intra-AS\nrouting protocol. However, to route a packet across multiple ASs, say from\na smartphone in Timbuktu to a server in a datacenter in Silicon Valley, we\nneed an inter-autonomous ­system routing protocol. Since an inter-AS\ncoordination \ncommunicating ASs must run the same inter-AS routing protocol. In fact, in\nthe Internet, all ASs run the same inter-AS routing protocol, called the\nBorder Gateway Protocol, more commonly known as BGP [RFC 4271;\nStewart 1999].\nGluing the Internet Together: BGP\nBGP is arguably the most important of all the Internet protocols (the\nonly other contender would be the IP protocol that we studied in Section\n4.3), as it is the protocol that glues the thousands of ISPs in the Internet\ntogether. As we will soon see, BGP is a decentralized and asynchronous\nprotocol in the vein of distance-vector routing described in Section 5.2.2.\nAlthough BGP is a complex and challenging protocol, to understand the\nInternet on a deep level, we need to become familiar with its underpinnings\nand operation. The time we devote to learning BGP will be well worth the\n5.4.1 The Role of BGP\nTo understand the responsibilities of BGP, consider an AS and an arbitrary\nrouter in that AS. Recall that every router has a forwarding table, which\nplays the central role in the process of forwarding arriving packets to\noutbound router links. As we have learned, for destinations that are within\nthe same AS, the entries in the router’s forwarding table are determined by\nthe AS’s intra-AS routing protocol. But what about destinations that are\noutside of the AS? This is precisely where BGP comes to the rescue.\nIn BGP, packets are not routed to a specific destination address, but\ninstead to CIDRized prefixes, with each prefix representing a subnet or a\ncollection of subnets. In the world of BGP, a destination may take the form\n138.16.68/22, which for this example includes 1,024 IP addresses. Thus, a\nrouter’s forwarding table will have entries of the form (x, I), where x is a\nprefix (such as 138.16.68/22) and I is an interface number for one of the\nrouter’s interfaces.\nAs an inter-AS routing protocol, BGP provides each router a means to:\n1. Obtain prefix reachability information from neighboring ASs. In\nparticular, BGP allows each subnet to advertise its existence to the rest\nof the Internet. A subnet screams, “I exist and I am here,” and BGP\nmakes sure that all the routers in the Internet know about this subnet. If\nit weren’t for BGP, each subnet would be an isolated island—alone,\nunknown and unreachable by the rest of the Internet.\n2. Determine the “best” routes to the prefixes. A router may learn about\ntwo or more different routes to a specific prefix. To determine the best\nroute, the router will locally run a BGP route-selection procedure (using\nthe prefix reachability information it obtained via neighboring routers).\nThe best route will be determined based on policy as well as the\nreachability information.\nLet us now delve into how BGP carries out these two tasks.\n5.4.2 Advertising BGP Route Information\nConsider the network shown in Figure 5.8. As we can see, this simple\nnetwork has three autonomous systems: AS1, AS2, and AS3. As shown,\nAS3 includes a subnet with prefix x. For each AS, each router is either a\ngateway router or an internal router. A gateway router is a router on the\nedge of an AS that directly connects to one or more routers in other ASs. An\ninternal router connects only to hosts and routers within its own AS. In\nAS1, for example, router 1c is a gateway router; routers 1a, 1b, and 1d are\ninternal routers.\nFigure 5.8 ♦Network with three autonomous systems. AS3 includes\na subnet with prefix x\nLet’s consider the task of advertising reachability information for prefix\nx to all of the routers shown in Figure 5.8. At a high level, this is\nstraightforward. First, AS3 sends a BGP message to AS2, saying that x\nexists and is in AS3; let’s denote this message as “AS3 x”. Then AS2 sends\na BGP message to AS1, saying that x exists and that you can get to x by\nfirst passing through AS2 and then going to AS3; let’s denote that message\nas “AS2 AS3 x”. In this manner, each of the autonomous systems will not\nonly learn about the existence of x, but also learn about a path of\nautonomous systems that leads to x.\nAlthough the discussion in the above paragraph about advertising BGP\nreachability information should get the general idea across, it is not precise\nin the sense that autonomous systems do not actually send messages to each\nother, but instead routers do. To understand this, let’s now re-examine the\nexample in Figure 5.8. In BGP, pairs of routers exchange routing\ninformation over semi-permanent TCP connections using port 179. Each\nsuch TCP connection, along with all the BGP messages sent over the\nconnection, is called a BGP connection. Furthermore, a BGP connection\nthat spans two ASs is called an external BGP (eBGP) connection, and a\nBGP session between routers in the same AS is called an internal BGP\n(iBGP) connection. Examples of BGP connections for the network in\nFigure 5.8 are shown in Figure 5.9. There is typically one eBGP connection\nfor each link that directly connects gateway routers in different ASs; thus, in\nFigure 5.9, there is an eBGP connection between gateway routers 1c and 2a\nand an eBGP connection between gateway routers 2c and 3a.\nFigure 5.9 ♦eBGP and iBGP connections\nThere are also iBGP connections between routers within each of the\nASs. In particular, Figure 5.9 displays a common configuration of one BGP\nconnection for each pair of routers internal to an AS, creating a mesh of\nTCP connections within each AS. In Figure 5.9, the eBGP connections are\nshown with the long dashes; the iBGP connections are shown with the short\ndashes. Note that iBGP connections do not always correspond to physical\nIn order to propagate the reachability information, both iBGP and\neBGP sessions are used. Consider again advertising the reachability\ninformation for prefix x to all routers in AS1 and AS2. In this process,\ngateway router 3a first sends an eBGP message “AS3 x” to gateway router\n2c. Gateway router 2c then sends the iBGP message “AS3 x” to all of the\nother routers in AS2, including to gateway router 2a. Gateway router 2a\nthen sends the eBGP message “AS2 AS3 x” to gateway router 1c. Finally,\ngateway router 1c uses iBGP to send the message “AS2 AS3 x” to all the\nrouters in AS1. After this process is complete, each router in AS1 and AS2\nis aware of the existence of x and is also aware of an AS path that leads to\nOf course, in a real network, from a given router there may be many\ndifferent paths to a given destination, each through a different sequence of\nASs. For example, consider the network in Figure 5.10, which is the\noriginal network in Figure 5.8, with an additional physical link from router\n1d to router 3d. In this case, there are two paths from AS1 to x: the path\n“AS2 AS3 x” via router 1c; and the new path “AS3 x” via the router 1d.\n5.4.3 Determining the Best Routes\nAs we have just learned, there may be many paths from a given router to a\ndestination subnet. In fact, in the Internet, routers often receive reachability\ninformation about dozens of different possible paths. How does a router\nchoose among these paths (and then configure its forwarding table\naccordingly)?\nBefore addressing this critical question, we need to introduce a little\nmore BGP terminology. When a router advertises a prefix across a BGP\nconnection, it includes with the prefix several BGP attributes. In BGP\njargon, a prefix along with its attributes is called a route. Two of the more\nimportant attributes are AS-PATH and NEXT-HOP. The AS-PATH attribute\ncontains the list of ASs through which the advertisement has passed, as\nwe’ve seen in our examples above. To generate the AS-PATH value, when a\nprefix is passed to an AS, the AS adds its ASN to the existing list in the AS-\nPATH. For example, in Figure 5.10, there are two routes from AS1 to\nsubnet x: one which uses the AS-PATH “AS2 AS3”; and another that uses\nthe AS-PATH “A3”. BGP routers also use the AS-PATH attribute to detect\nand prevent looping advertisements; specifically, if a router sees that its\nown AS is contained in the path list, it will reject the advertisement.\nFigure 5.10 ♦Network augmented with peering link between AS1\nProviding the critical link between the inter-AS and intra-AS routing\nprotocols, the NEXT-HOP attribute has a subtle but important use. The\nNEXT-HOP is the IP address of the router interface that begins the AS-\nPATH. To gain insight into this attribute, let’s again refer to Figure 5.10. As\nindicated in Figure 5.10, the NEXT-HOP attribute for the route “AS2 AS3\nx” from AS1 to x that passes through AS2 is the IP address of the left\ninterface on router 2a. The NEXT-HOP attribute for the route “AS3 x” from\nAS1 to x that bypasses AS2 is the IP address of the leftmost interface of\nrouter 3d. In summary, in this toy example, each router in AS1 becomes\naware of two BGP routes to prefix x:\nIP address of leftmost interface for router 2a; AS2 AS3; x\nIP address of leftmost interface of router 3d; AS3; x\nHere, each BGP route is written as a list with three components: NEXT-\nHOP; AS-PATH; destination prefix. In practice, a BGP route includes\nadditional attributes, which we will ignore for the time being. Note that the\nNEXT-HOP attribute is an IP address of a router that does not belong to\nAS1; however, the subnet that contains this IP address directly attaches to\nHot Potato Routing\nWe are now finally in position to talk about BGP routing algorithms in a\nprecise manner. We will begin with one of the simplest routing algorithms,\nnamely, hot potato routing.\nConsider router 1b in the network in Figure 5.10. As just described, this\nrouter will learn about two possible BGP routes to prefix x. In hot potato\nrouting, the route chosen (from among all possible routes) is that route with\nthe least cost to the NEXT-HOP router beginning that route. In this\nexample, router 1b will consult its intra-AS routing information to find the\nleast-cost intra-AS path to NEXT-HOP router 2a and the least-cost intra-AS\npath to NEXT-HOP router 3d, and then select the route with the smallest of\nthese least-cost paths. For example, suppose that cost is defined as the\nnumber of links traversed. Then the least cost from router 1b to router 2a is\n2, the least cost from router 1b to router 2d is 3, and router 2a would\ntherefore be selected. Router 1b would then consult its forwarding table\n(configured by its intra-AS algorithm) and find the interface I that is on the\nleast-cost path to router 2a. It then adds (x, I) to its forwarding table.\nThe steps for adding an outside-AS prefix in a router’s forwarding table\nfor hot potato routing are summarized in Figure 5.11. It is important to note\nthat when adding an outside-AS prefix into a forwarding table, both the\ninter-AS routing protocol (BGP) and the intra-AS routing protocol (e.g.,\nOSPF) are used.\nFigure 5.11 ♦Steps in adding outside-AS destination in a router’s\nforwarding table\nThe idea behind hot-potato routing is for router 1b to get packets out of\nits AS as quickly as possible (more specifically, with the least cost possible)\nwithout worrying about the cost of the remaining portions of the path\noutside of its AS to the destination. In the name “hot potato routing,” a\npacket is analogous to a hot potato that is burning in your hands. Because it\nis burning hot, you want to pass it off to another person (another AS) as\nquickly as possible. Hot potato routing is thus a selfish ­algorithm—it tries\nto reduce the cost in its own AS while ignoring the other components of the\nend-to-end costs outside its AS. Note that with hot potato routing, two\nrouters in the same AS may choose two different AS paths to the same\nprefix. For example, we just saw that router 1b would send packets through\nAS2 to reach x. However, router 1d would bypass AS2 and send packets\ndirectly to AS3 to reach x.\nRoute-Selection Algorithm\nIn practice, BGP uses an algorithm that is more complicated than hot potato\nrouting, but nevertheless incorporates hot potato routing. For any given\ndestination prefix, the input into BGP’s route-selection algorithm is the set\nof all routes to that prefix that have been learned and accepted by the router.\nIf there is only one such route, then BGP obviously selects that route. If\nthere are two or more routes to the same prefix, then BGP sequentially\ninvokes the following elimination rules until one route remains:\n1. A route is assigned a local preference value as one of its attributes (in\naddition to the AS-PATH and NEXT-HOP attributes). The local\npreference of a route could have been set by the router or could have\nbeen learned from another router in the same AS. The value of the local\npreference attribute is a policy decision that is left entirely up to the\nAS’s network administrator. (We will shortly discuss BGP policy issues\nin some detail.) The routes with the highest local preference values are\n2. From the remaining routes (all with the same highest local preference\nvalue), the route with the shortest AS-PATH is selected. If this rule were\nthe only rule for route selection, then BGP would be using a DV\nalgorithm for path determination, where the distance metric uses the\nnumber of AS hops rather than the number of router hops.\n3. From the remaining routes (all with the same highest local preference\nvalue and the same AS-PATH length), hot potato routing is used, that is,\nthe route with the closest NEXT-HOP router is selected.\n4. If more than one route still remains, the router uses BGP identifiers to\nselect the route; see [Stewart 1999].\nAs an example, let’s again consider router 1b in Figure 5.10. Recall that\nthere are exactly two BGP routes to prefix x, one that passes through AS2\nand one that bypasses AS2. Also recall that if hot potato routing on its own\nwere used, then BGP would route packets through AS2 to prefix x. But in\nthe above route-selection algorithm, rule 2 is applied before rule 3, causing\nBGP to select the route that bypasses AS2, since that route has a shorter AS\nPATH. So we see that with the above route-selection algorithm, BGP is no\nlonger a selfish algorithm—it first looks for routes with short AS paths\n(thereby likely reducing end-to-end delay).\nAs noted above, BGP is the de facto standard for inter-AS routing for\nthe many other access solutions described in Chapter 1. Your local ISP will\nalso provide you with an IP address range, for example, a /24 address range\nconsisting of 256 addresses. Once you have your physical connectivity and\nyour IP address range, you will assign one of the IP addresses (in your\naddress range) to your Web server, one to your mail server, one to your\nDNS server, one to your gateway router, and other IP addresses to other\nservers and ­networking devices in your company’s network.\nIn addition to contracting with an ISP, you will also need to contract\nwith an Internet registrar to obtain a domain name for your company, as\ndescribed in Chapter 2. For example, if your company’s name is, say,\nXanadu Inc., you will naturally try to obtain the domain name xanadu.com.\nYour company must also obtain presence in the DNS system. Specifically,\nbecause outsiders will want to contact your DNS server to obtain the IP\naddresses of your servers, you will also need to provide your registrar with\nthe IP address of your DNS server. Your registrar will then put an entry for\nyour DNS server (domain name and corresponding IP address) in the .com\ntop-level-domain servers, as described in Chapter 2. After this step is\ncompleted, any user who knows your domain name (e.g., xanadu.com) will\nbe able to obtain the IP address of your DNS server via the DNS system.\nSo that people can discover the IP addresses of your Web server, in\nyour DNS server you will need to include entries that map the host name of\nyour Web server (e.g., www.xanadu.com) to its IP address. You will want to\nhave similar entries for other publicly available servers in your company,\nincluding your mail server. In this manner, if Alice wants to browse your\nWeb server, the DNS system will contact your DNS server, find the IP\naddress of your Web server, and give it to Alice. Alice can then establish a\nTCP connection directly with your Web server.\nHowever, there still remains one other necessary and crucial step to\nallow outsiders from around the world to access your Web server. Consider\nwhat happens when Alice, who knows the IP address of your Web server,\nsends an IP datagram (e.g., a TCP SYN segment) to that IP address. This\ndatagram will be routed through the Internet, visiting a series of routers in\nmany different ASs, and eventually reach your Web server. When any one\nof the routers receives the datagram, it is going to look for an entry in its\nforwarding table to determine on which outgoing port it should forward the\ndatagram. Therefore, each of the routers needs to know about the existence\nof your company’s /24 prefix (or some aggregate entry). How does a router\nbecome aware of your company’s prefix? As we have just seen, it becomes\naware of it from BGP! Specifically, when your company contracts with a\nlocal ISP and gets assigned a prefix (i.e., an address range), your local ISP\nwill use BGP to advertise your prefix to the ISPs to which it connects.\nThose ISPs will then, in turn, use BGP to propagate the advertisement.\nEventually, all Internet routers will know about your prefix (or about some\naggregate that includes your prefix) and thus be able to appropriately\nforward datagrams destined to your Web and mail servers.\n5.5 The SDN Control Plane\nIn this section, we’ll dive into the SDN control plane—the network-wide\nlogic that controls packet forwarding among a network’s SDN-enabled\ndevices, as well as the configuration and management of these devices and\ntheir services. Our study here builds on our earlier discussion of generalized\nSDN forwarding in Section 4.4, so you might want to first review that\nsection, as well as Section 5.1 of this chapter, before continuing on. As in\nSection 4.4, we’ll again adopt the terminology used in the SDN literature\nand refer to the network’s forwarding devices as “packet switches” (or just\nswitches, with “packet” being understood), since forwarding decisions can\nbe made on the basis of network-layer source/destination addresses, link-\nlayer source/destination addresses, as well as many other values in\ntransport-, network-, and link-layer packet-header fields.\nFour key characteristics of an SDN architecture can be identified\n[Kreutz 2015]:\nFlow-based forwarding. Packet forwarding by SDN-controlled switches\ncan be based on any number of header field values in the transport-\nlayer, network-layer, or link-layer header. We saw in Section 4.4 that the\nOpenFlow1.0 abstraction allows forwarding based on eleven different\nheader field values. This contrasts sharply with the traditional approach\nto router-based forwarding that we studied in Sections 5.2–5.4, where\nforwarding of IP datagrams was based solely on a datagram’s\ndestination IP address. Recall from Figure 5.2 that packet forwarding\nrules are specified in a switch’s flow table; it is the job of the SDN\ncontrol plane to compute, manage and install flow table entries in all of\nthe network’s switches.\nSeparation of data plane and control plane. This separation is shown\nclearly in Figures 5.2 and 5.14. The data plane consists of the network’s\nswitches—relatively simple (but fast) devices that execute the “match\nplus action” rules in their flow tables. The control plane consists of\nservers and software that determine and manage the switches’ flow\nNetwork control functions: external to data-plane switches. Given that\nthe “S” in SDN is for “software,” it’s perhaps not surprising that the\nSDN control plane is implemented in software. Unlike traditional\nrouters, however, this software executes on servers that are both distinct\nand remote from the network’s switches. As shown in Figure 5.14, the\ncontrol plane itself consists of two components—an SDN controller (or\nnetwork operating system [Gude 2008]) and a set of network-control\napplications. The controller maintains accurate network state\ninformation (e.g., the state of remote links, switches, and hosts);\nprovides this information to the network-control applications running in\nthe control plane; and provides the means through which these\napplications can monitor, program, and control the underlying network\ndevices. Although the controller in Figure 5.14 is shown as a single\ncentral server, in practice the controller is only logically centralized; it\nis typically implemented on several servers that provide coordinated,\nscalable performance and high availability.\nA programmable network. The network is programmable through the\nnetwork-control applications running in the control plane. These\napplications represent the “brains” of the SDN control plane, using the\nAPIs provided by the SDN controller to specify and control the data\nplane in the network devices. For example, a routing network-control\napplication might determine the end-end paths between sources and\ndestinations (for example, by executing Dijkstra’s algorithm using the\nnode-state and link-state information maintained by the SDN\ncontroller). Another network application might perform access control,\nthat is, determine which packets are to be blocked at a switch, as in our\nthird example in Section 4.4.3. Yet another application might have\nswitches forward packets in a manner that performs server load\nbalancing (the second example we considered in Section 4.4.3).\nFrom this discussion, we can see that SDN represents a significant\n“unbundling” of network functionality—data plane switches, SDN\ncontrollers, and network-control applications are separate entities that may\neach be provided by different vendors and organizations. This contrasts\nwith the pre-SDN model in which a switch/router (together with its\nembedded control plane software and protocol implementations) was\nmonolithic, vertically integrated, and sold by a single vendor. This\nunbundling of network functionality in SDN has been likened to the earlier\nevolution from mainframe computers (where hardware, system software,\nand applications were provided by a single vendor) to personal computers\n(with their separate hardware, operating systems, and applications). The\nunbundling of computing hardware, system software, and applications has\nled to a rich, open ecosystem driven by innovation in all three of these\nareas; one hope for SDN is that it will continue to drive and enable such\nrich innovation.\nGiven our understanding of the SDN architecture of Figure 5.14, many\nquestions naturally arise. How and where are the flow tables actually\ncomputed? How are these tables updated in response to events at SDN-\ncontrolled devices (e.g., an attached link going up/down)? And how are the\nflow table entries at multiple switches coordinated in such a way as to result\nin orchestrated and consistent network-wide functionality (e.g., end-to-end\npaths for forwarding packets from sources to destinations, or coordinated\ndistributed firewalls)? It is the role of the SDN control plane to provide\nthese, and many other, capabilities.\nFigure 5.14 ♦Components of the SDN architecture: SDN-controlled\nswitches, the SDN controller, network-control\napplications\n5.5.1 The SDN Control Plane: SDN Controller and\nSDN Network-control Applications\nLet’s begin our discussion of the SDN control plane in the abstract, by\nconsidering the generic capabilities that the control plane must provide. As\nwe’ll see, this abstract, “first principles” approach will lead us to an overall\narchitecture that reflects how SDN control planes have been implemented\nin practice.\nAs noted above, the SDN control plane divides broadly into two\ncomponents—the \nnetwork-control\napplications. Let’s explore the controller first. Many SDN controllers have\nbeen developed since the earliest SDN controller [Gude 2008]; see [Kreutz\n2015] for an extremely thorough survey. Figure 5.15 provides a more\ndetailed view of a generic SDN controller. A controller’s functionality can\nbe broadly organized into three layers. Let’s consider these layers in an\nuncharacteristically bottom-up fashion:\nFigure 5.15 ♦Components of an SDN controller\nA communication layer: communicating between the SDN controller\nand controlled network devices. Clearly, if an SDN controller is going\nto control the operation of a remote SDN-enabled switch, host, or other\ndevice, a protocol is needed to transfer information between the\ncontroller and that device. In addition, a device must be able to\ncommunicate locally-observed events to the controller (for example, a\nmessage indicating that an attached link has gone up or down, that a\ndevice has just joined the network, or a heartbeat indicating that a\ndevice is up and operational). These events provide the SDN controller\nwith an up-to-date view of the network’s state. This protocol constitutes\nthe lowest layer of the controller architecture, as shown in Figure 5.15.\nThe communication between the controller and the controlled devices\ncross what has come to be known as the controller’s “southbound”\ninterface. In Section 5.5.2, we’ll study OpenFlow—a specific protocol\nthat provides this communication functionality. OpenFlow is\nimplemented in most, if not all, SDN controllers.\nA network-wide state-management layer. The ultimate control decisions\nmade by the SDN control plane—for example, configuring flow tables\nin all switches to achieve the desired end-end forwarding, to implement\nload balancing, or to implement a particular firewalling capability—will\nrequire that the controller have up-to-date information about state of the\nnetworks’ hosts, links, switches, and other SDN-controlled devices. A\nswitch’s flow table contains counters whose values might also be\nprofitably used by network-control applications; these values should\nthus be available to the applications. Since the ultimate aim of the\ncontrol plane is to determine flow tables for the various controlled\ndevices, a controller might also maintain a copy of these tables. These\npieces of information all constitute examples of the network-wide\n“state” maintained by the SDN controller.\nThe interface to the network-control application layer. The controller\ninteracts with network-control applications through its “northbound”\ninterface. This API allows network-control applications to read/write\nnetwork state and flow tables within the state-management layer.\nApplications can register to be notified when state-change events occur,\nso that they can take actions in response to network event notifications\nsent from SDN-controlled devices. Different types of APIs may be\nprovided; we’ll see that two popular SDN controllers communicate with\ntheir applications using a REST [Fielding 2000] request-response\nWe have noted several times that an SDN controller can be considered\nto be ­“logically centralized,” that is, that the controller may be viewed\nexternally (for example, from the point of view of SDN-controlled devices\nand external network-control applications) as a single, monolithic service.\nHowever, these services and the databases used to hold state information\nare implemented in practice by a distributed set of servers for fault\ntolerance, high availability, or for performance reasons. With controller\nfunctions being implemented by a set of servers, the semantics of the\ncontroller’s internal operations (e.g., maintaining logical time ordering of\nevents, consistency, consensus, and more) must be considered [Panda\n2013]. Such concerns are common across many different distributed\nsystems; see [Lamport 1989, Lampson 1996] for elegant solutions to these\nchallenges. Modern controllers such as OpenDaylight [OpenDaylight 2020]\nand ONOS [ONOS 2020] (see sidebar) have placed considerable emphasis\non architecting a logically centralized but physically distributed controller\nplatform that provides scalable services and high availability to the\ncontrolled devices and network-control applications alike.\nThe architecture depicted in Figure 5.15 closely resembles the\narchitecture of the originally proposed NOX controller in 2008 [Gude\n2008], as well as that of today’s OpenDaylight [OpenDaylight 2020] and\nONOS [ONOS 2020] SDN controllers (see sidebar). We’ll cover an\nexample of controller operation in Section 5.5.3. First, however, let’s\nexamine the OpenFlow protocol, the earliest and now one of several pro­-\ntocols that can be used for communication between an SDN controller and a\ncontrolled device, which lies in the controller’s communication layer.\n5.5.2 OpenFlow Protocol\nThe OpenFlow protocol [OpenFlow 2009, ONF 2020] operates between an\nSDN controller and an SDN-controlled switch or other device\nimplementing the OpenFlow API that we studied earlier in Section 4.4. The\nOpenFlow protocol operates over TCP, with a default port number of 6653.\nAmong the important messages flowing from the controller to the\ncontrolled switch are the following:\nConfiguration. This message allows the controller to query and set a\nswitch’s configuration parameters.\nModify-State. This message is used by a controller to add/delete or\nmodify entries in the switch’s flow table, and to set switch port\nproperties.\nRead-State. This message is used by a controller to collect statistics and\ncounter values from the switch’s flow table and ports.\nSend-Packet. This message is used by the controller to send a specific\npacket out of a specified port at the controlled switch. The message\nitself contains the packet to be sent in its payload.\nAmong the messages flowing from the SDN-controlled switch to the\ncontroller are the following:\nFlow-Removed. This message informs the controller that a flow table\nentry has been removed, for example by a timeout or as the result of a\nreceived modify-state message.\nPort-status. This message is used by a switch to inform the controller of\na change in port status.\nPacket-in. Recall from Section 4.4 that a packet arriving at a switch port\nand not matching any flow table entry is sent to the controller for\nadditional processing. Matched packets may also be sent to the\ncontroller, as an action to be taken on a match. The packet-in message is\nused to send such packets to the controller.\nAdditional OpenFlow messages are defined in [OpenFlow 2009, ONF\nGOOGLE’S SOFTWARE-DEFINED GLOBAL NETWORK\nRecall from the case study in Section 2.6 that Google deploys a dedicated wide-area\nnetwork (WAN) that interconnects its data centers and server clusters (in IXPs and ISPs).\nThis network, called B4, has a Google-designed SDN control plane built on OpenFlow.\nGoogle’s network is able to drive WAN links at near 70% utilization over the long run (a two\nto three fold increase over typical link utilizations) and split application flows among multiple\npaths based on application priority and existing flow demands [Jain 2013].\nThe Google B4 network is particularly it well-suited for SDN: (i) Google controls all\ndevices from the edge servers in IXPs and ISPs to routers in their network core; (ii) the\nmost bandwidth-intensive applications are large-scale data copies between sites that can\ndefer to higher-priority interactive applications during times of resource congestion; (iii) with\nonly a few dozen data centers being connected, centralized control is feasible.\nGoogle’s B4 network uses custom-built switches, each implementing a slightly extended\nversion of OpenFlow, with a local Open Flow Agent (OFA) that is similar in spirit to the\ncontrol agent we encountered in Figure 5.2. Each OFA in turn connects to an Open Flow\nController (OFC) in the network control server (NCS), using a separate “out of band”\nnetwork, distinct from the network that carries data-center traffic between data centers. The\nOFC thus provides the services used by the NCS to communicate with its controlled\nswitches, similar in spirit to the lowest layer in the SDN architecture shown in Figure 5.15. In\nB4, the OFC also performs state management functions, keeping node and link status in a\nNetwork Information Base (NIB). Google’s implementation of the OFC is based on the ONIX\nSDN controller [Koponen 2010]. Two routing protocols, BGP (for routing between the data\ncenters) and IS-IS (a close relative of OSPF, for routing within a data center), are\nimplemented. Paxos [Chandra 2007] is used to execute hot replicas of NCS components to\nprotect against failure.\nA traffic engineering network-control application, sitting logically above the set of network\ncontrol servers, interacts with these servers to provide global, network-wide bandwidth\nprovisioning for groups of application flows. With B4, SDN made an important leap forward\ninto the operational networks of a global network provider. See [Jain 2013; Hong 2018] for a\ndetailed description of B4.\n5.5.3 Data and Control Plane Interaction: An\nIn order to solidify our understanding of the interaction between SDN-\ncontrolled switches and the SDN controller, let’s consider the example\nshown in Figure 5.16, in which Dijkstra’s algorithm (which we studied in\nSection 5.2) is used to determine shortest path routes. The SDN scenario in\nFigure 5.16 has two important differences from the earlier per-router-\ncontrol scenario of Sections 5.2.1 and 5.3, where ­Dijkstra’s algorithm was\nimplemented in each and every router and link-state updates were flooded\namong all network routers:\nFigure 5.16 ♦SDN controller scenario: Link-state change\nDijkstra’s algorithm is executed as a separate application, outside of the\npacket switches.\nPacket switches send link updates to the SDN controller and not to each\nIn this example, let’s assume that the link between switch s1 and s2\ngoes down; that shortest path routing is implemented, and consequently and\nthat incoming and outgoing flow forwarding rules at s1, s3, and s4 are\naffected, but that s2’s operation is unchanged. Let’s also assume that\nOpenFlow is used as the communication layer protocol, and that the control\nplane performs no other function other than link-state routing.\n1. Switch s1, experiencing a link failure between itself and s2, notifies the\nSDN controller of the link-state change using the OpenFlow port-status\n2. The SDN controller receives the OpenFlow message indicating the link-\nstate change, and notifies the link-state manager, which updates a link-\nstate ­database.\n3. The network-control application that implements Dijkstra’s link-state\nrouting has previously registered to be notified when link state changes.\nThat application receives the notification of the link-state change.\n4. The link-state routing application interacts with the link-state manager\nto get updated link state; it might also consult other components in the\nstate-­management layer. It then computes the new least-cost paths.\n5. The link-state routing application then interacts with the flow table\nmanager, which determines the flow tables to be updated.\n6. The flow table manager then uses the OpenFlow protocol to update flow\ntable entries at affected switches—s1 (which will now route packets\ndestined to s2 via s4), s2 (which will now begin receiving packets from\ns1 via intermediate switch s4), and s4 (which must now forward packets\nfrom s1 destined to s2).\nThis example is simple but illustrates how the SDN control plane provides\ncontrol-plane services (in this case, network-layer routing) that had been\npreviously implemented with per-router control exercised in each and every\nnetwork router. One can now easily appreciate how an SDN-enabled ISP\ncould easily switch from least-cost path routing to a more hand-tailored\napproach to routing. Indeed, since the controller can tailor the flow tables as\nit pleases, it can implement any form of forwarding that it pleases—simply\nby changing its application-control software. This ease of change should be\ncontrasted to the case of a traditional per-router control plane, where\nsoftware in all routers (which might be provided to the ISP by multiple\nindependent vendors) must be changed.\n5.5.4 SDN: Past and Future\nAlthough the intense interest in SDN is a relatively recent phenomenon, the\ntechnical roots of SDN, and the separation of the data and control planes in\nparticular, go back considerably further. In 2004, [Feamster 2004,\nLakshman 2004, RFC 3746] all argued for the separation of the network’s\ndata and control planes. [van der Merwe 1998] describes a control\nframework for ATM networks [Black 1995] with multiple controllers, each\ncontrolling a number of ATM switches. The Ethane project [Casado 2007]\npioneered the notion of a network of simple flow-based Ethernet switches\nwith  match-plus-action flow tables, a centralized controller that managed\nflow admission and routing, and the forwarding of unmatched packets from\nthe switch to the controller. A network of more than 300 Ethane switches\nwas operational in 2007. Ethane quickly evolved into the OpenFlow\nproject, and the rest (as the saying goes) is history!\nNumerous research efforts are aimed at developing future SDN\narchitectures and capabilities. As we have seen, the SDN revolution is\nleading to the disruptive replacement of dedicated monolithic switches and\nrouters (with both data and control planes) by simple commodity switching\nhardware and a sophisticated software control plane. A generalization of\nSDN known as network functions virtualization (NFV) (which we\ndiscussed earlier in Section 4.5) similarly aims at disruptive replacement of\nsophisticated middleboxes (such as middleboxes with dedicated hardware\nand proprietary software for media caching/service) with simple commodity\nservers, switching, and storage. A second area of important research seeks\nto extend SDN concepts from the intra-AS setting to the inter-AS setting\n[Gupta 2014].\nSDN CONTROLLER CASE STUDIES: THE OPENDAYLIGHT AND ONOS\nIn the earliest days of SDN, there was a single SDN protocol (OpenFlow [McKeown 2008;\nOpenFlow 2009]) and a single SDN controller (NOX [Gude 2008]). Since then, the number\nof SDN controllers in particular has grown significantly [Kreutz 2015]. Some SDN controllers\nare company-specific and proprietary, particularly when used to control internal proprietary\nnetworks (e.g., within or among a company’s data centers). But many more controllers are\nopen-source and implemented in a variety of programming languages [Erickson 2013]. Most\nrecently, the OpenDaylight controller [OpenDaylight 2020] and the ONOS controller [ONOS\n2020] have found considerable industry support. They are both open-source and are being\ndeveloped in partnership with the Linux Foundation.\nThe OpenDaylight Controller\nFigure 5.17 presents a simplified view of the OpenDaylight (ODL) controller platform\n[OpenDaylight 2020, Eckel 2017].\nFigure 5.17 ♦A simplified view of the OpenDaylight controller\nODL’s Basic Network Functions are at the heart of the controller, and correspond closely\nto the network-wide state management capabilities that we encountered in Figure 5.15. The\nService Abstraction Layer (SAL) is the controller’s nerve center, allowing controller\ncomponents and applications to invoke each other’s services, access configuration and\noperational data, and to subscribe to events they generate. The SAL also provides a\nuniform abstract interface to specific protocols operating between the ODL controller and\nthe controlled devices. These protocols include OpenFlow (which we covered in Section\n4.5), and the Simple Network Management Protocol (SNMP) and the Network Configuration\n(NETCONF) protocol, both of which we’ll cover in Section 5.7. The Open vSwitch Database\nManagement Protocol (OVSDB) is used to manage data center switching, an important\napplication area for SDN technology. We’ll introduce data center networking in Chapter 6. \nNetwork Orchestrations and Applications determine how data-plane forwarding and other\nservices, such as firewalling and load balancing, are accomplished in the controlled\ndevices. ODL provides two ways in which applications can interoperate with native\ncontroller services (and hence devices) and with each other. In the API-Driven (AD-SAL)\napproach, shown in Figure 5.17, applications communicate with controller modules using a\nREST request-response API running over HTTP. Initial releases of the OpenDaylight\ncontroller provided only the AD-SAL. As ODL became increasingly used for network\nconfiguration and management, later ODL releases introduced a Model-Driven (MD-SAL)\napproach. Here, the YANG data modeling language [RFC 6020] defines models of device,\nprotocol, and network configuration and operational state data. Devices are then configured\nand managed by manipulating this data using the NETCONF protocol.\nThe ONOS Controller\nFigure 5.18 presents a simplified view of the ONOS controller ONOS 2020]. Similar to the\ncanonical controller in Figure 5.15, three layers can be identified in the ONOS ­controller:\nFigure 5.18 ♦ONOS controller architecture\nNorthbound abstractions and protocols. A unique feature of ONOS is its intent\nframework, which allows an application to request a high-level service (e.g., to setup a\nconnection between host A and Host B, or conversely to not allow Host A and host B to\ncommunicate) without having to know the details of how this service is performed. State\ninformation is provided to network-control applications across the northbound API either\nsynchronously (via query) or asynchronously (via listener callbacks, e.g., when network\nstate changes).\nDistributed core. The state of the network’s links, hosts, and devices is maintained in\nONOS’s distributed core. ONOS is deployed as a service on a set of interconnected\nservers, with each server running an identical copy of the ONOS software; an increased\nnumber of servers offers an increased service capacity. The ONOS core provides the\nmechanisms for service replication and coordination among instances, providing the\napplications above and the network devices below with the abstraction of logically\ncentralized core services.\nSouthbound abstractions and protocols. The southbound abstractions mask the\nheterogeneity of the underlying hosts, links, switches, and protocols, allowing the\ndistributed core to be both device and protocol agnostic. Because of this abstraction, the\nsouthbound interface below the distributed core is logically higher than in our canonical\ncontroller in Figure 5.14 or the ODL controller in Figure 5.17.\n5.6 ICMP: The Internet Control Message Protocol\nThe Internet Control Message Protocol (ICMP), specified in [RFC 792], is\nused by hosts and routers to communicate network-layer information to\neach other. The most typical use of ICMP is for error reporting. For\nexample, when running an HTTP session, you may have encountered an\nerror message such as “Destination network unreachable.” This message\nhad its origins in ICMP. At some point, an IP router was unable to find a\npath to the host specified in your HTTP request. That router created and\nsent an ICMP message to your host indicating the error.\nICMP is often considered part of IP, but architecturally it lies just above\nIP, as ICMP messages are carried inside IP datagrams. That is, ICMP\nmessages are carried as IP payload, just as TCP or UDP segments are\ncarried as IP payload. Similarly, when a host receives an IP datagram with\nICMP specified as the upper-layer protocol (an upper-layer protocol number\nnot a process. Chapter 11 of [Stevens 1990] provides the source code for the\nping client program. Note that the client program needs to be able to\ninstruct the operating system to generate an ICMP message of type 8 code\nAnother interesting ICMP message is the source quench message. This\nmessage is seldom used in practice. Its original purpose was to perform\ncongestion control—to allow a congested router to send an ICMP source\nquench message to a host to force that host to reduce its transmission rate.\nWe have seen in Chapter 3 that TCP has its own congestion-control\nmechanism that operates at the transport layer, and that Explicit Congestion\nNotification bits can be used by network-later devices to signal congestion.\nIn Chapter 1, we introduced the Traceroute program, which allows us to\ntrace a route from a host to any other host in the world. Interestingly,\nTraceroute is implemented with ICMP messages. To determine the names\nand addresses of the routers between source and destination, Traceroute in\nthe source sends a series of ordinary IP datagrams to the destination. Each\nof these datagrams carries a UDP segment with an unlikely UDP port\nnumber. The first of these datagrams has a TTL of 1, the second of 2, the\nthird of 3, and so on. The source also starts timers for each of the\ndatagrams. When the nth datagram arrives at the nth router, the nth router\nobserves that the TTL of the datagram has just expired. According to the\nrules of the IP protocol, the router discards the datagram and sends an\nICMP warning message to the source (type 11 code 0). This warning\nmessage includes the name of the router and its IP address. When this\nICMP message arrives back at the source, the source obtains the round-trip\ntime from the timer and the name and IP address of the nth router from the\nICMP message.\nHow does a Traceroute source know when to stop sending UDP\nsegments? Recall that the source increments the TTL field for each\ndatagram it sends. Thus, one of the datagrams will eventually make it all the\nway to the destination host. Because this datagram contains a UDP segment\nwith an unlikely port number, the destination host sends a port unreachable\nICMP message (type 3 code 3) back to the source. When the source host\nreceives this particular ICMP message, it knows it does not need to send\nadditional probe packets. (The standard Traceroute program actually sends\nsets of three packets with the same TTL; thus, the Traceroute output\nprovides three results for each TTL.)\nIn this manner, the source host learns the number and the identities of\nrouters that lie between it and the destination host and the round-trip time\nbetween the two hosts. Note that the Traceroute client program must be able\nto instruct the operating system to generate UDP datagrams with specific\nTTL values and must also be able to be notified by its operating system\nwhen ICMP messages arrive. Now that you understand how Traceroute\nworks, you may want to go back and play with it some more.\nA new version of ICMP has been defined for IPv6 in RFC 4443. In\naddition to reorganizing the existing ICMP type and code definitions,\nICMPv6 also added new types and codes required by the new IPv6\nfunctionality. These include the “Packet Too Big” type and an\n“unrecognized IPv6 options” error code.\n5.7 Network Management and SNMP,\nNETCONF/YANG\nHaving now made our way to the end of our study of the network layer,\nwith only the link-layer before us, we’re well aware that a network consists\nof many complex, interacting pieces of hardware and software—from the\nlinks, switches, routers, hosts, and other devices that comprise the physical\ncomponents of the network to the many protocols that control and\ncoordinate these devices. When hundreds or thousands of such components\nare brought together by an organization to form a network, the job of the\nnetwork administrator to keep the network “up and running” is surely a\nchallenge. We saw in Section  5.5 that the logically centralized controller\ncan help with this process in an SDN context. But the challenge of network\nmanagement has been around long before SDN, with a rich set of network\nmanagement tools and approaches that help the network administrator\nmonitor, manage, and control the network. We’ll study these tools and\ntechniques in this section, as well as new tools and techniques that have co-\nevolved along with SDN.\nAn often-asked question is “What is network management?” A well-\nconceived, single-sentence (albeit a rather long run-on sentence) definition\nof network management from [Saydam 1996] is:\nNetwork management includes the deployment, integration, and\ncoordination of the hardware, software, and human elements to\nmonitor, test, poll, configure, analyze, evaluate, and control the network\nand element resources to meet the real-time, operational performance,\nand Quality of Service requirements at a reasonable cost.\nGiven this broad definition, we’ll cover only the rudiments of network\nmanagement in this section—the architecture, protocols, and data used by a\nnetwork administrator in performing their task. We’ll not cover the\nadministrator’s decision-making processes, where topics such as fault\nidentification [Labovitz 1997; Steinder 2002; Feamster 2005; Wu 2005;\nTeixeira 2006], anomaly detection [Lakhina 2005; Barford 2009], network\ndesign/engineering to meet contracted Service Level Agreements (SLA’s)\n[Huston 1999a], and more come into consideration. Our focus is thus\nagain, we see that ­security—a topic we’ll cover in detail in Chapter 8 — is\nof critical concern, but once again a concern whose importance had been\nrealized perhaps a bit late and only then “added on.”\nThe Management Information Base (MIB)\nWe learned earlier that a managed device’s operational state data (and to\nsome extent its configuration data) in the SNMP/MIB approach to network\nmanagement are represented as objects that are gathered together into an\nMIB for that device. An MIB object might be a counter, such as the number\nof IP datagrams discarded at a router due to errors in an IP datagram header;\nor the number of carrier sense errors in an Ethernet interface card;\ndescriptive information such as the version of the software running on a\nDNS server; status information such as whether a particular device is\nfunctioning correctly; or protocol-specific information such as a routing\npath to a destination. Related MIB objects are gathered into MIB modules.\nThere are over 400 MIB modules defined in various IETC RFC’s; there are\nmany more device- and vendor-specific MIBs. [RFC 4293] specifies the\nipSystemStatsInDelivers) for managing implementations of the Internet\nProtocol (IP) and its associated Internet Control Message Protocol (ICMP).\n[RFC 4022] specifies the MIB module for TCP, and [RFC 4113] specifies\nthe MIB module for UDP.\nWhile MIB-related RFCs make for rather tedious and dry reading, it is\nnonetheless instructive (i.e., like eating vegetables, it is “good for you”) to\nStatsInDelivers object-type definition from [RFC 4293] defines a\n32-bit read-only counter that keeps track of the number of IP datagrams that\nwere received at the managed device and were successfully delivered to an\nupper-layer protocol. In the example below, Counter32 is one of the basic\ndata types defined in the SMI.\nipSystemStatsInDelivers OBJECT-TYPE\n     SYNTAX Counter32\n     MAX-ACCESS read-only\n     STATUS current\n“The total number of datagrams\nsuccessfully delivered to IPuser-\nprotocols (including ICMP).\nWhen tracking interface statistics, the\ncounter of the interface to which these\ndatagrams were addressed is\nincremented. This interface might not\nbe the same as the input interface for\nsome of the datagrams.\nDiscontinuities in the value of this\ncounter can occur at re-initialization\nof the management system, and at other\ntimes as indicated by the value of\nipSystemStatsDiscontinuityTime.”\n::= { ipSystemStatsEntry 18 }\n5.7.3 The Network Configuration Protocol\n(NETCONF) and YANG\nThe NETCONF protocol operates between the managing server and the\nmanaged network devices, providing messaging to (i) retrieve, set, and\nmodify configuration data at managed devices; (ii) to query operational data\nand statistics at managed devices; and (iii) to subscribe to notifications\ngenerated by managed devices. The managing server actively controls a\nmanaged device by sending it configurations, which are specified in a\nstructured XML document, and activating a configuration at the managed\ndevice. NETCONF uses a remote procedure call (RPC) paradigm, where\nprotocol messages are also encoded in XML and exchanged between the\nmanaging server and a managed device over a secure, connection-oriented\nsession such as the TLS (Transport Layer Security) protocol (discussed in\nChapter 8) over TCP.\nFigure 5.22 shows an example NETCONF session. First, the managing\nserver establishes a secure connection to the managed device. (In\nNETCONF parlance, the managing server is actually referred to as the\n“client” and the managed device as the “server,” since the managing server\nestablishes the connection to the managed device. But we’ll ignore that here\nfor consistency with the longer-standing network-management server/client\nterminology shown in Figure 5.20.) Once a secure connection has been\nestablished, the managing server and the managed device exchange <hello>\nmessages, declaring their “capabilities”—NETCONF functionality that\nsupplements the base NETCONF specification in [RFC 6241]. Interactions\nbetween the managing server and managed device take the form of a remote\nprocedure call, using the <rpc> and <rpc-response> messages. These\nmessages are used to retrieve, set, query and modify device configurations,\noperational data and statistics, and to subscribe to device notifications.\nDevice notifications themselves are proactively sent from managed device\nto the managing server using NETCONF <notification> messages. A\nsession is closed with the <session-close message>.\nFigure 5.22 ♦NETCONF session between managing\nserver/controller and managed device\nTable 5.3 shows a number of the important NETCONF operations that\na managing server can perform at a managed device. As in the case of\nSNMP, we see operations for retrieving operational state data (<get>), and\nfor event notification. However, the <get-config>, <edit-config>, <lock>\nand <unlock> operation demon­strate NETCONF’s particular emphasis on\ndevice configuration. Using the basic operations shown in Table 5.3, it is\nalso possible to create a set of more sophisticated network management\ntransactions that either complete atomically (i.e., as a group) and\nsuccessfully on a set of devices, or are fully reversed and leave the devices\nin their pre-transaction state. Such multi-device transactions—“enabl[ing]\noperators to concentrate on the configuration of the network as a whole\nrather than individual devices” was an important operator requirement put\nforth in [RFC 3535].\nTable 5.3 ♦Selected NETCONF operations\nA full description of NETCONF is beyond our scope here; [RFC 6241,\nRFC 5277, Claise 2019; Schonwalder 2010] provide more in-depth\nBut since this is the first time we’ve seen protocol messages formatted\nas an XML document (rather than the traditional message with header fields\nand message body, e.g., as shown in Figure 5.21 for the SNMP PDU), let’s\nconclude our brief study of NETCONF with two examples. \nIn the first example, the XML document sent from the managing server\nto the managed device is a NETCONF <get> command requesting all\ndevice configuration and operational data. With this command, the server\ncan learn about the device’s configuration.\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc message-id=”101”\n03    xmlns=”urn:ietf:params:xml:ns:netconf:base\nAlthough few people can completely parse XML directly, we see that\nthe NETCONF command is relatively human-readable, and is much more\nreminiscent of HTTP and HTML than the protocol message formats that we\nsaw for SNMP PDU format in Figure 5.21. The RPC message itself spans\nlines 02–05 (we have added line numbers here for pedagogical purposes).\nThe RPC has a message ID value of 101, declared in line 02, and contains a\nsingle NETCONF <get> command. The reply from the device contains a\nmatching ID number (101), and all of the device’s configuration data (in\nXML format, of course), starting in line 04, ultimately with a closing </rpc-\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc-reply message-id=”101”\n03    xmlns=”urn:ietf:params:xml:ns:netconf:base\n04  <!-- . . . all configuration data\nreturned... -->\n</rpc-reply>\nIn the second example below, adapted from [RFC 6241], the XML\ndocument sent from the managing server to the managed device sets the\nMaximum Transmission Unit (MTU) of an interface named “Ethernet0/0”\nto 1500 bytes:\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc message-id=”101”\n  xmlns=”urn:ietf:params:xml:ns:netconf:base:1.0\n04   <edit-config>\n05     <target>\n06       <running/>\n07     </target>\n08     <config>\n09       <top\nxmlns=”http://example.com/schema/1.2/config”>\n10          <interface>\n11             <name>Ethernet0/0</name>\n12             <mtu>1500</mtu>\n13          </interface>\n14       </top>\n15     </config>\n16   </edit-config>\nThe RPC message itself spans lines 02–17, has a message ID value of\n101, and contains a single NETCONF <edit-config> command, spanning\nlines 04–15. Line 06 indicates that the running device configuration at the\nmanaged device will be changed. Lines 11 and 12 specify the MTU size to\nbe set of the Ethernet0/0 interface.\nOnce the managed device has changed the interface’s MTU size in the\nconfiguration, it responds back to the managing server with an OK reply\n(line 04 below), again within an XML document:\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc-reply message-id=”101”\nxmlns=”urn:ietf:params:xml:ns:netconf:base:1.0”>\n05 </rpc-reply>\nYANG is the data modeling language used to precisely specify the\nstructure, syntax, and semantics of network management data used by\nNETCONF, in much the same way that the SMI is used to specify MIBs in\nSNMP. All YANG definitions are contained in modules, and an XML\ndocument describing a device and its capabilities can be generated from a\nYANG module.\nYANG features a small set of built-in data types (as in the case of SMI)\nand also allows data modelers to express constraints that must be satisfied\nby a valid NETCONF configuration—a powerful aid in helping ensure that\nNETCONF configurations satisfy specified correctness and consistency\nconstraints. YANG is also used to specify NETCONF notifications.\nA fuller discussion of YANG is beyond our scope here. For more\ninformation, we refer the interested reader to the excellent book [Claise\n5.8 Summary\nWe have now completed our two-chapter journey into the network core—a\njourney that began with our study of the network layer’s data plane in\nChapter 4 and finished here with our study of the network layer’s control\nplane. We learned that the control plane is the network-wide logic that\ncontrols not only how a datagram is forwarded among routers along an end-\nto-end path from the source host to the destination host, but also how\nnetwork-layer components and services are configured and managed.\nWe learned that there are two broad approaches towards building a\ncontrol plane: traditional per-router control (where a routing algorithm runs\nin each and every router and the routing component in the router\ncommunicates with the routing components in other routers) and software-\ndefined networking (SDN) control (where a logically centralized controller\ncomputes and distributes the forwarding tables to be used by each and every\nrouter). We studied two fundamental routing algorithms for computing least\ncost paths in a graph—link-state routing and distance-vector routing—in\nSection 5.2; these algorithms find application in both per-router control and\nin SDN control. These algorithms are the basis for two widely deployed\nInternet routing protocols, OSPF and BGP, that we covered in Sections 5.3\nand 5.4. We covered the SDN approach to the network-layer control plane\nin Section 5.5, investigating SDN network-control applications, the SDN\ncontroller, and the OpenFlow protocol for communicating between the\ncontroller and SDN-controlled devices. In Sections 5.6 and 5.7, we covered\nsome of the nuts and bolts of managing an IP network: ICMP (the Internet\nControl Message Protocol) and network management using SNMP and\nNETCONF/YANG.\nHaving completed our study of the network layer, our journey now\ntakes us one step further down the protocol stack, namely, to the link layer.\nLike the network layer, the link layer is part of each and every network-\nconnected device. But we will see in the next chapter that the link layer has\nthe much more localized task of moving packets between nodes on the\nsame link or LAN. Although this task may appear on the surface to be\nrather simple compared with that of the network layer’s tasks, we will see\nthat the link layer involves a number of important and fascinating issues\nthat can keep us busy for a long time.\nHomework Problems and Questions\nSECTION 5.1\nR1. What is meant by a control plane that is based on per-router control?\nIn such cases, when we say the network control and data planes are\nimplemented “monolithically,” what do we mean?\nR2. What is meant by a control plane that is based on logically\ncentralized control? In such cases, are the data plane and the control\nplane implemented within the same device or in separate devices?\nSECTION 5.2\nR3. Compare and contrast the properties of a centralized and a distributed\nrouting algorithm. Give an example of a routing protocol that takes a\ncentralized and a decentralized approach.\nR4. Compare and contrast static and dynamic routing algorithms.\nR5. What is the “count to infinity” problem in distance vector routing?\nR6. How is a least cost path calculated in a decentralized routing\nSECTIONS 5.3-5.4\nR7. Why are different inter-AS and intra-AS protocols used in the\nR8. True or false: When an OSPF route sends its link state information, it\nis sent only to those nodes directly attached neighbors. Explain.\nR9. What is meant by an area in an OSPF autonomous system? Why was\nthe concept of an area introduced?\nR10. Define and contrast the following terms: subnet, prefix, and BGP\nR11. How does BGP use the NEXT-HOP attribute? How does it use the\nAS-PATH attribute?\nR12. Describe how a network administrator of an upper-tier ISP can\nimplement policy when configuring BGP.\nR13. True or false: When a BGP router receives an advertised path from its\nneighbor, it must add its own identity to the received path and then\nsend that new path on to all of its neighbors. Explain.\nSECTION 5.5\nR14. Describe the main role of the communication layer, the network-wide\nstate-­management layer, and the network-control application layer in\nan SDN controller.\nR15. Suppose you wanted to implement a new routing protocol in the SDN\ncontrol plane. At which layer would you implement that protocol?\nR16. What types of messages flow across an SDN controller’s northbound\nand southbound APIs? Who is the recipient of these messages sent\nfrom the controller across the southbound interface, and who sends\nmessages to the controller across the northbound interface?\nR17. Describe the purpose of two types of OpenFlow messages (of your\nchoosing) that are sent from a controlled device to the controller.\nDescribe the purpose of two types of Openflow messages (of your\nchoosing) that are send from the controller to a controlled device.\nR18. What is the purpose of the service abstraction layer in the\nOpenDaylight SDN controller?\nSECTIONS 5.6-5.7\nR19. Names four different types of ICMP messages\nR20. What two types of ICMP messages are received at the sending host\nexecuting the Traceroute program?\nR21. Define the following terms in the context of SNMP: managing server,\nmanaged device, network management agent and MIB.\nR22. What are the purposes of the SNMP GetRequest and SetRequest\nR23. What is the purpose of the SNMP trap message?\nP1. Consider the figure below.\nEnumerate all paths from A to D that do not contain any loops\nP2. Repeat Problem P1 for paths from C to D, B to F, and C to F.\nP3. Consider the following network. With the indicated link costs, use\nDijkstra’s shortest-path algorithm to compute the shortest path from x\nto all network nodes. Show how the algorithm works by computing a\ntable similar to Table 5.1.\nDijkstra’s algorithm: discussion and example\nP4. Consider the network shown in Problem P3. Using Dijkstra’s\nalgorithm, and showing your work using a table similar to Table 5.1,\ndo the following:\na. Compute the shortest path from t to all network nodes.\nb. Compute the shortest path from u to all network nodes.\na. Compute the shortest path from v to all network nodes.\nd. Compute the shortest path from w to all network nodes.\ne. Compute the shortest path from y to all network nodes.\nf. Compute the shortest path from z to all network nodes.\nP5. Consider the network shown below. Assume that each node initially\nknows the costs to each of its neighbors. Consider the distance-\nvector ­algorithm and show the distance table entries at node z.\nP6. Consider a general topology (that is, not the specific network shown\nabove) and a synchronous version of the distance-vector algorithm.\nSuppose that at each iteration, a node exchanges its distance vectors\nwith its neighbors and receives their distance vectors. Assuming that\nthe algorithm begins with each node knowing only the costs to its\nimmediate neighbors, what is the maximum number of iterations\nrequired before the distributed algorithm converges? Justify your\nP7. Consider the network fragment shown below. x has only two\nattached ­neighbors, w and y. w has a minimum-cost path to\ndestination u ­(illustrated with the dotted line through the remaining\nnetwork) of 9, and y has a minimum-cost path to u of 11. The\ncomplete paths from w and y to u (and between w and y) are pictured\nwith dotted lines, as they are irrelevant to the solution.\na. Give x’s distance vector for destinations w, y, and u.\nb. Give a link-cost change for either c(x,w) or c(x,y) such that x will\ninform its neighbors of a new minimum-cost path to u as a result\nof executing the distance-vector algorithm.\nc. Give a link-cost change for either c(x,w) or c(x,y) such that x will\nnot inform its neighbors of a new minimum-cost path to u as a\nresult of executing the distance-vector algorithm.\nP8. Consider the three-node topology shown in Figure 5.6. Rather than\nhaving the link costs shown in Figure 5.6, the link costs are c(x,y) = 3,\nc(y,z) = 6, c(z,x) = 4. Compute the distance tables after the\ninitialization step and after each iteration of a synchronous version of\nthe distance-vector algorithm (as we did in our earlier discussion of\nFigure 5.6).\nP9. Can the poisoned reverse solve the general count-to-infinity problem?\nJustify your answer.\nP10. Argue that for the distance-vector algorithm in Figure 5.6, each value\nin the distance vector D(x) is non-increasing and will eventually\nstabilize in a finite number of steps.\nP11. Consider Figure 5.7. Suppose there is another router w, connected to\nrouter y and z. The costs of all links are given as follows: c(x,y) = 4,\nc(x,z) = 50, c(y,w) = 1, c(z,w) = 1, c(y,z) = 3. Suppose that poisoned\nreverse is used in the distance-vector routing algorithm.\na. When the distance vector routing is stabilized, router w, y, and z\ninform their distances to x to each other. What distance values do\nthey tell each other?\nb. Now suppose that the link cost between x and y increases to 60.\nWill there be a count-to-infinity problem even if poisoned\nreverse is used? Why or why not? If there is a count-to-infinity\nproblem, then how many iterations are needed for the distance-\nvector routing to reach a stable state again? Justify your answer.\nc. How do you modify c(y,z) such that there is no count-to-infinity\nproblem at all if c(y,x) changes from 4 to 60?\nP12. What is the message complexity of LS routing algorithm?\nP13. Will a BGP router always choose the loop-free route with the shortest\nASpath length? Justify your answer.\nP14. Consider the network shown below. Suppose AS3 and AS2 are\nrunning OSPF for their intra-AS routing protocol. Suppose AS1 and\nAS4 are running RIP for their intra-AS routing protocol. Suppose\neBGP and iBGP are used for the inter-AS routing protocol. Initially\nsuppose there is no physical link between AS2 and AS4.\na. Router 3c learns about prefix x from which routing protocol:\nOSPF, RIP, eBGP, or iBGP?\nb. Router 3a learns about x from which routing protocol?\nc. Router 1c learns about x from which routing protocol?\nd. Router 1d learns about x from which routing protocol?\nP15. Referring to the previous problem, once router 1d learns about x it\nwill put an entry (x, I) in its forwarding table.\na. Will I be equal to I  or I  for this entry? Explain why in one\nb. Now suppose that there is a physical link between AS2 and AS4,\nshown by the dotted line. Suppose router 1d learns that x is\naccessible via AS2 as well as via AS3. Will I be set to I  or I ?\nExplain why in one sentence.\nc. Now suppose there is another AS, called AS5, which lies on the\npath between AS2 and AS4 (not shown in diagram). Suppose\nrouter 1d learns that x is accessible via AS2 AS5 AS4 as well as\nvia AS3 AS4. Will I be set to I  or I ? Explain why in one\nP16. Consider the following network. ISP B provides national backbone\nservice to regional ISP A. ISP C provides national backbone service\nto regional ISP D. Each ISP consists of one AS. B and C peer with\neach other in two places using BGP. Consider traffic going from A to\nD. B would prefer to hand that traffic over to C on the West Coast (so\nthat C would have to absorb the cost of carrying the traffic cross-\ncountry), while C would prefer to get the traffic via its East Coast\npeering point with B (so that B would have carried the traffic across\nthe country). What BGP mechanism might C use, so that B would\nhand over A-to-D traffic at its East Coast peering point? To answer\nthis question, you will need to dig into the BGP ­specification.\nP17. In Figure 5.13, consider the path information that reaches stub\nnetworks W, X, and Y. Based on the information available at W and\nX, what are their respective views of the network topology? Justify\nyour answer. The topology view at Y is shown below.\nP18. Consider Figure 5.13. B would never forward traffic destined to Y via\nX based on BGP routing. But there are some very popular\napplications for which data packets go to X first and then flow to Y.\nIdentify one such application, and describe how data packets follow a\npath not given by BGP routing.\nP19. In Figure 5.13, suppose that there is another stub network V that is a\ncustomer of ISP A. Suppose that B and C have a peering relationship,\nand A is a customer of both B and C. Suppose that A would like to\nhave the traffic destined to W to come from B only, and the traffic\ndestined to V from either B or C. How should A advertise its routes to\nB and C? What AS routes does C receive?\nP20. Suppose ASs X and Z are not directly connected but instead are\nconnected by AS Y. Further suppose that X has a peering agreement\nwith Y, and that Y has a peering agreement with Z. Finally, suppose\nthat Z wants to transit all of Y’s traffic but does not want to transit X’s\ntraffic. Does BGP allow Z to ­implement this policy?\nP21. Consider the two ways in which communication occurs between a\nmanaging entity and a managed device: request-response mode and\ntrapping. What are the pros and cons of these two approaches, in\nterms of (1) overhead, (2) notification time when exceptional events\noccur, and (3) robustness with respect to lost messages between the\nmanaging entity and the device?\nP22. In Section 5.7, we saw that it was preferable to transport SNMP\nmessages in unreliable UDP datagrams. Why do you think the\ndesigners of SNMP chose UDP rather than TCP as the transport\nprotocol of choice for SNMP?\nSocket Programming Assignment 5: ICMP Ping\nAt the end of Chapter 2, there are four socket programming assignments.\nHere you  will find a fifth assignment which employs ICMP, a protocol\ndiscussed in this chapter.\nPing is a popular networking application used to test from a remote\nlocation whether a particular host is up and reachable. It is also often used\nto measure latency between the client host and the target host. It works by\nsending ICMP “echo request” packets (i.e., ping packets) to the target host\nand listening for ICMP “echo response” replies (i.e., pong packets). Ping\nmeasures the RRT, records packet loss, and calculates a statistical summary\nof multiple ping-pong exchanges (the minimum, mean, max, and standard\ndeviation of the round-trip times).\nIn this lab, you will write your own Ping application in Python. Your\napplication will use ICMP. But in order to keep your program simple, you\nwill not exactly follow the official specification in RFC 1739. Note that you\nwill only need to write the client side of the program, as the functionality\nneeded on the server side is built into almost all operating systems. You can\nfind full details of this assignment, as well as important snippets of the\nPython code, at the Web site http://www.pearsonglobaleditions.com.\nProgramming Assignment: Routing\nIn this programming assignment, you will be writing a “distributed” set of\nprocedures that implements a distributed asynchronous distance-vector\nrouting for the network shown below.\nYou are to write the following routines that will “execute”\nasynchronously within the emulated environment provided for this\nassignment. For node 0, you will write the routines:\nrtinit0(). This routine will be called once at the beginning of the\nemulation. rtinit0() has no arguments. It should initialize your distance\ntable in node 0 to reflect the direct costs of 1, 3, and 7 to nodes 1, 2, and\n3, respectively. In the figure above, all links are bidirectional and the\ncosts in both directions are identical. After initializing the distance table\nand any other data structures needed by your node 0 routines, it should\nthen send its directly connected neighbors (in this case, 1, 2, and 3) the\ncost of its minimum-cost paths to all other network nodes. This\nminimum-cost information is sent to neighboring nodes in a routing\nupdate packet by calling the routine tolayer2(), as described in the full\nassignment. The format of the routing update packet is also described in\nthe full assignment.\nrtupdate0(struct rtpkt *rcvdpkt). This routine will be called when node\n0 receives a routing packet that was sent to it by one of its directly\nconnected neighbors. The parameter *rcvdpkt is a pointer to the packet\nthat was received. rtupdate0() is the “heart” of the distance-vector\nalgorithm. The values it receives in a routing update packet from some\nother node i contain i’s current shortest-path costs to all other network\nnodes. rtupdate0() uses these received values to update its own distance\ntable (as specified by the distance-vector algorithm). If its own\nminimum cost to another node changes as a result of the update, node 0\ninforms its directly connected neighbors of this change in minimum\ncost by sending them a routing packet. Recall that in the distance-vector\nalgorithm, only directly connected nodes will exchange routing packets.\nThus, nodes 1 and 2 will communicate with each other, but nodes 1 and\n3 will not communicate with each other.\nSimilar routines are defined for nodes 1, 2, and 3. Thus, you will write eight\nprocedures in all: rtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(),\nrtupdate1(), rtupdate2(), and rtupdate3(). These routines will together\nimplement a distributed, asynchronous computation of the distance tables\nfor the topology and costs shown in the figure on the preceding page.\nYou can find the full details of the programming assignment, as well as\nC code that you will need to create the simulated hardware/software\nenvironment, at http://www.pearsonglobaleditions.com. A Java version of\nthe assignment is also available.\nWireshark Lab: ICMP\nIn the Web site for this textbook, www.pearsonglobaleditions.com, you’ll\nfind a Wireshark lab assignment that examines the use of the ICMP protocol\nin the ping and traceroute commands.\nAN INTERVIEW WITH…\nJennifer Rexford\nJennifer Rexford is a Professor in the Computer Science\ndepartment at Princeton University. Her research has the\nbroad goal of making computer networks easier to design and\nmanage, with particular emphasis on programmable neworks.\nFrom 1996–2004, she was a member of the Network\nManagement and Performance department at AT&T Labs–\nResearch. While at AT&T, she designed techniques and tools\nfor network measurement, traffic engineering, and router\nconfiguration that were deployed in AT&T’s backbone\nnetwork. Jennifer is co-author of the book “Web Protocols\nand Practice: Networking Protocols, Caching, and Traffic\nMeasurement,” published by Addison-Wesley in May 2001.\nShe served as the chair of ACM SIGCOMM from 2003 to\n2007. She received her BSE degree in electrical engineering\nfrom Princeton University in 1991, and her PhD degree in\nelectrical engineering and computer science from the\nUniversity of Michigan in 1996. Jennifer was the 2004 winner\nof ACM’s Grace Murray Hopper Award for outstanding young\ncomputer professional, the ACM Athena Lecturer Award\n(2016), the NCWIT Harrold and Notkin Research and\nGraduate Mentoring Award (2017), the ACM SIGCOMM\naward for lifetime contributions (2018), and the IEEE Internet\nAward (2019). She is an ACM Fellow (2008), an IEEE Fellow\n(2018), and the National Academy of Engineering (2014).\nCourtesy of Jennifer Rexford\nPlease describe one or two of the most\nexciting projects you have worked on\nduring your career. What were the biggest\nchallenges?\nWhen I was a researcher at AT&T, a group of us\ndesigned a new way to manage routing in Internet\nService Provider backbone networks. Traditionally,\nnetwork operators configure each router\nindividually, and these routers run distributed\nprotocols to compute paths through the network.\nWe believed that network management would be\nsimpler and more flexible if network operators\ncould exercise direct control over how routers\nforward traffic based on a network-wide view of\nthe topology and traffic. The Routing Control\nPlatform (RCP) we designed and built could\ncompute the routes for all of AT&T’s backbone on\na single commodity computer, and could control\nlegacy routers without modification. To me, this\nproject was exciting because we had a provocative\nidea, a working system, and ultimately a real\ndeployment in an operational network. Fast\nforward a few years, and software-defined\nnetworking (SDN) has become a mainstream\ntechnology, and standard protocols (like standard\nprotocols (like OpenFlow) and languages (like P4)\nhave made it much easier to tell the underlying\nswitches what to do.\nHow do you think software-defined\nnetworking should evolve in the future?\nIn a major break from the past, the software\ncontrolling network devices can be created by\nmany different programmers, not just at companies\nselling network equipment. Yet, unlike the\napplications running on a server or a smart phone,\nSDN applications must work together to handle the\nsame traffic. Network operators do not want to\nperform load balancing on some traffic and routing\non other traffic; instead, they want to perform load\nbalancing and routing, together, on the same\ntraffic. Future SDN platforms should offer good\nprogramming abstractions for composing\nindependently written multiple applications\ntogether. More broadly, good programming\nabstractions can make it easier to create\napplications, without having to worry about low-\nlevel details like flow table entries, traffic counters,\nbit patterns in packet headers, and so on. Also,\nwhile an SDN controller is logically centralized,\nthe network still consists of a distributed collection\nof devices. Future programmable networks should\noffer good abstractions for updating a distributed\nset of devices, so network administrators can\nreason about what happens to packets in flight\nwhile the devices are updated. Programming\nabstractions for programmable network is an\nexciting area for interdisciplinary\nresearch between computer networking, distributed\nsystems, and programming languages, with a real\nchance for practical impact in the years ahead.\nWhere do you see the future of networking\nand the Internet?\nNetworking is an exciting field because the\napplications and the underlying technologies\nchange all the time. We are always reinventing\nourselves! Who would have predicted even ten\nyears ago the dominance of smart phones, allowing\nmobile users to access existing applications as well\nas new location-based services? The emergence of\ncloud computing is fundamentally changing the\nrelationship between users and the applications\nthey run, and networked sensors and actuators (the\n“Internet of Things”) are enabling a wealth of new\napplications (and security vulnerabilities!). The\npace of innovation is truly inspiring.\nThe underlying network is a crucial\ncomponent in all of these innovations. Yet, the\nnetwork is notoriously “in the way”—limiting\nperformance, compromising reliability,\nconstraining applications, and complicating the\ndeployment and management of services. We\nshould strive to make the network of the future as\ninvisible as the air we breathe, so it never stands in\nthe way of new ideas and valuable services. To do\nthis, we need to raise the level of abstraction above\nindividual network devices and protocols (and\ntheir attendant acronyms!), so we can reason about\nthe network and the user’s high-level goals as a\nWhat people inspired you professionally?\nI’ve long been inspired by Sally Floyd who\nworked for many years at the International\nComputer Science Institute. Her research was\nalways purposeful, focusing on the important\nchallenges facing the Internet. She dug deeply into\nhard questions until she understood the problem\nand the space of solutions completely, and she\ndevoted serious energy into “making things\nhappen,” such as pushing her ideas into protocol\nstandards and network equipment. Also, she gave\nback to the community, through professional\nservice in numerous standards and research\norganizations and by creating tools (such as the\nwidely used ns-2 and ns-3 simulators) that enable\nother researchers to succeed. She retired in 2009,\nand passed away in 2019, but her influence on the\nfield will be felt for years to come.\nWhat are your recommendations for\nstudents who want careers in computer\nscience and networking?\nNetworking is an inherently interdisciplinary field.\nApplying techniques from other discipline’s\nbreakthroughs in networking come from such\ndiverse areas as queuing theory, game theory,\ncontrol theory, distributed systems, network\noptimization, programming languages, machine\nlearning, algorithms, data structures, and so on. I\nthink that becoming conversant in a related field,\nor collaborating closely with experts in those\nfields, is a wonderful way to put networking on a\nstronger foundation, so we can learn how to build\nnetworks that are worthy of society’s trust. Beyond\nthe theoretical disciplines, networking is exciting\nbecause we create real artifacts that real people\nuse. Mastering how to design and build systems—\nby gaining experience in operating systems,\ncomputer architecture, and so on—is another\nfantastic way to amplify your knowledge of\nnetworking to help make the world a better place.\nThe Link Layer and\nIn the previous two chapters, we learned that the network layer\nprovides a communication service between any two network hosts.\nBetween the two hosts, datagrams travel over a series of\ncommunication links, some wired and some wireless, starting at the\nsource host, passing through a series of packet switches (switches and\nrouters) and ending at the destination host. As we continue down the\nprotocol stack, from the network layer to the link layer, we naturally\nwonder how packets are sent across the individual links that make up\nthe end-to-end communication path. How are the network-layer\ndatagrams encapsulated in the link-layer frames for transmission over\na single link? Are different link-layer protocols used in the different\nlinks along the communication path? How are transmission conflicts in\nbroadcast links resolved? Is there addressing at the link layer and, if\nso, how does the link-layer addressing operate with the network-layer\naddressing we learned about in Chapter 4? And what exactly is the\ndifference between a switch and a router? We’ll answer these and other\nimportant questions in this chapter.\nIn discussing the link layer, we’ll see that there are two\nfundamentally ­different types of link-layer channels. The first type are\nbroadcast channels, which connect multiple hosts in wireless LANs, in\nsatellite networks, and in hybrid fiber-coaxial cable (HFC) access\nnetworks. Since many hosts are connected to the same broadcast\ncommunication channel, a so-called medium access protocol is needed\nto coordinate frame transmission. In some cases, a central controller\nmay be used to coordinate transmissions; in other cases, the hosts\nthemselves coordinate transmissions. The second type of link-layer\nchannel is the point-to-point communication link, such as that often\nfound between two routers connected by a long-distance link, or\nbetween a user’s office computer and the nearby Ethernet switch to\nwhich it is connected. Coordinating access to a point-to-point link is\nsimpler; the reference material on this book’s Web site has a detailed\ndiscussion of the Point-to-Point Protocol (PPP), which is used in\nsettings ranging from dial-up service over a telephone line to high-\nspeed point-to-point frame transport over fiber-optic links.\nWe’ll explore several important link-layer concepts and technologies in\nthis ­chapter. We’ll dive deeper into error detection and correction, a topic\nwe touched on briefly in Chapter 3. We’ll consider multiple access\nnetworks and switched LANs, including Ethernet—by far the most\nprevalent wired LAN technology. We’ll also look at virtual LANs, and data\ncenter networks. Although WiFi, and more generally wireless LANs, are\nlink-layer topics, we’ll postpone our study of these important topics until\n6.1 Introduction to the Link Layer\nLet’s begin with some important terminology. We’ll find it convenient in\nthis chapter to refer to any device that runs a link-layer (i.e., layer 2)\nprotocol as a node. Nodes include hosts, routers, switches, and WiFi access\npoints (discussed in Chapter 7). We will also refer to the communication\nchannels that connect adjacent nodes along the communication path as\nlinks. In order for a datagram to be transferred from source host to\ndestination host, it must be moved over each of the individual links in the\nend-to-end path. As an example, in the company network shown at the\nbottom of Figure 6.1, consider sending a datagram from one of the wireless\nhosts to one of the servers. This datagram will actually pass through six\nlinks: a WiFi link between sending host and WiFi access point, an Ethernet\nlink between the access point and a link-layer switch; a link between the\nlink-layer switch and the router, a link between the two routers; an Ethernet\nlink between the router and a link-layer switch; and finally an Ethernet link\nbetween the switch and the server. Over a given link, a transmitting node\nencapsulates the datagram in a link-layer frame and transmits the frame\ninto the link.\nFigure 6.1 ♦Six link-layer hops between wireless host and server\nIn order to gain further insight into the link layer and how it relates to\nthe ­network layer, let’s consider a transportation analogy. Consider a travel\nagent who is planning a trip for a tourist traveling from Princeton, New\nJersey, to Lausanne, Switzerland. The travel agent decides that it is most\nconvenient for the tourist to take a limousine from Princeton to JFK airport,\nthen a plane from JFK airport to Geneva’s airport, and finally a train from\nGeneva’s airport to Lausanne’s train station. Once the travel agent makes\nthe three reservations, it is the responsibility of the Princeton limousine\ncompany to get the tourist from Princeton to JFK; it is the responsibility of\nthe airline company to get the tourist from JFK to Geneva; and it is the\nresponsibility of the Swiss train service to get the tourist from Geneva to\nLausanne. Each of the three segments of the trip is “direct” between two\n“adjacent” locations. Note that the three transportation segments are\nmanaged by different companies and use entirely different transportation\nmodes (limousine, plane, and train). Although the transportation modes are\ndifferent, they each provide the basic service of moving passengers from\none location to an adjacent location. In this transportation analogy, the\ntourist is a datagram, each transportation segment is a link, the\ntransportation mode is a link-layer protocol, and the travel agent is a routing\n6.1.1 The Services Provided by the Link Layer\nAlthough the basic service of any link layer is to move a datagram from one\nnode to an adjacent node over a single communication link, the details of\nthe provided service can vary from one link-layer protocol to the next.\nPossible services that can be offered by a link-layer protocol include:\nFraming. Almost all link-layer protocols encapsulate each network-\nlayer datagram within a link-layer frame before transmission over the\nlink. A frame consists of a data field, in which the network-layer\ndatagram is inserted, and a number of header fields. The structure of the\nframe is specified by the link-layer protocol. We’ll see several different\nframe formats when we examine specific link-layer protocols in the\nsecond half of this chapter.\nLink access. A medium access control (MAC) protocol specifies the\nrules by which a frame is transmitted onto the link. For point-to-point\nlinks that have a single sender at one end of the link and a single\nreceiver at the other end of the link, the MAC protocol is simple (or\nnonexistent)—the sender can send a frame whenever the link is idle.\nThe more interesting case is when multiple nodes share a single\nbroadcast link—the so-called multiple access problem. Here, the MAC\nprotocol serves to coordinate the frame transmissions of the many\nReliable delivery. When a link-layer protocol provides reliable delivery\nservice, it guarantees to move each network-layer datagram across the\nlink without error. Recall that certain transport-layer protocols (such as\nTCP) also provide a reliable delivery service. Similar to a transport-\nlayer reliable delivery service, a link-layer reliable delivery service can\nwe’ll study in Chapter 7.\nFigure 6.2 ♦Network adapter: Its relationship to other host\ncomponents and to protocol stack functionality\nOn the sending side, the controller takes a datagram that has been\ncreated and stored in host memory by the higher layers of the protocol\nstack, encapsulates the datagram in a link-layer frame (filling in the frame’s\nvarious fields), and then transmits the frame into the communication link,\nfollowing the link-access protocol. On the receiving side, a controller\nreceives the entire frame, and extracts the network-layer datagram. If the\nlink layer performs error detection, then it is the sending controller that sets\nthe error-detection bits in the frame header and it is the receiving controller\nthat performs error detection.\nFigure 6.2 shows that while most of the link layer is implemented in\nhardware, part of the link layer is implemented in software that runs on the\nhost’s CPU. The software components of the link layer implement higher-\nlevel link-layer functionality such as assembling link-layer addressing\ninformation and activating the controller hardware. On the receiving side,\nlink-layer software responds to controller interrupts (for example, due to the\nreceipt of one or more frames), handling error conditions and passing a\ndatagram up to the network layer. Thus, the link layer is a combination of\nhardware and software—the place in the protocol stack where software\nmeets hardware. [Intel 2020] provides a readable overview (as well as a\ndetailed description) of the XL710 controller from a software-programming\npoint of view.\n6.2 Error-Detection and -Correction Techniques\nIn the previous section, we noted that bit-level error detection and\ncorrection—detecting and correcting the corruption of bits in a link-layer\nframe sent from one node to another physically connected neighboring node\n—are two services often ­provided by the link layer. We saw in Chapter 3\nthat error-detection and -correction services are also often offered at the\ntransport layer as well. In this section, we’ll examine a few of the simplest\ntechniques that can be used to detect and, in some cases, correct such bit\nerrors. A full treatment of the theory and implementation of this topic is\nitself the topic of many textbooks (e.g., [Schwartz 1980] or [Bertsekas\n1991]), and our treatment here is necessarily brief. Our goal here is to\ndevelop an intuitive feel for the capabilities that error-detection and -\ncorrection techniques provide and to see how a few simple techniques work\nand are used in practice in the link layer.\nFigure 6.3 illustrates the setting for our study. At the sending node,\ndata, D, to be protected against bit errors is augmented with error-detection\nand -correction bits (EDC). Typically, the data to be protected includes not\nonly the datagram passed down from the network layer for transmission\nacross the link, but also link-level addressing information, sequence\nnumbers, and other fields in the link frame header. Both D and EDC are\nsent to the receiving node in a link-level frame. At the receiving node, a\nsequence of bits, D' and EDC' is received. Note that D' and EDC' may differ\nfrom the original D and EDC as a result of in-transit bit flips.\nFigure 6.3 ♦Error-detection and -correction scenario\nThe receiver’s challenge is to determine whether or not D' is the same\nas the original D, given that it has only received D' and EDC'. The exact\nwording of the receiver’s decision in Figure 6.3 (we ask whether an error is\ndetected, not whether an error has occurred!) is important. Error-detection\nand -correction techniques allow the receiver to sometimes, but not always,\ndetect that bit errors have occurred. Even with the use of error-detection bits\nthere still may be undetected bit errors; that is, the receiver may be\nunaware that the received information contains bit errors. As a\nconsequence, the receiver might deliver a corrupted datagram to the\nlink-layer ARQ techniques similar to those we examined in Chapter 3. FEC\ntechniques are valuable because they can decrease the number of sender\nretransmissions required. Perhaps more important, they allow for immediate\ncorrection of errors at the receiver. This avoids having to wait for the round-\ntrip propagation delay needed for the sender to receive a NAK packet and\nfor the retransmitted packet to propagate back to the receiver—a potentially\nimportant advantage for real-time network applications [Rubenstein 1998]\nor links (such as deep-space links) with long propagation delays. Research\nexamining the use of FEC in error-control protocols includes [Biersack\n1992; Nonnenmacher 1998; Byers 1998; Shacham 1990].\n6.2.2 Checksumming Methods\nIn checksumming techniques, the d bits of data in Figure 6.4 are treated as a\nsequence of k-bit integers. One simple checksumming method is to simply\nsum these k-bit integers and use the resulting sum as the error-detection\nbits. The Internet checksum is based on this approach—bytes of data are\ntreated as 16-bit integers and summed. The 1s complement of this sum then\nforms the Internet checksum that is carried in the segment header. As\ndiscussed in Section 3.3, the receiver checks the checksum by taking the 1s\ncomplement of the sum of the received data (including the checksum) and\nchecking whether the result is all 0 bits. If any of the bits are 1, an error is\nindicated. RFC 1071 discusses the Internet checksum algorithm and its\nimplementation in detail. In the TCP and UDP protocols, the Internet\nchecksum is computed over all fields (header and data fields included). In\nIP, the checksum is computed over the IP header (since the UDP or TCP\nsegment has its own checksum). In other protocols, for example, XTP\n[Strayer 1992], one checksum is computed over the header and another\nchecksum is computed over the entire packet.\nChecksumming methods require relatively little packet overhead. For\nexample, the checksums in TCP and UDP use only 16 bits. However, they\nprovide relatively weak protection against errors as compared with cyclic\nredundancy check, which is discussed below and which is often used in the\nlink layer. A natural question at this point is, Why is checksumming used at\nthe transport layer and cyclic redundancy check used at the link layer?\nRecall that the transport layer is typically implemented in software in a host\nas part of the host’s operating system. Because transport-layer error\ndetection is implemented in software, it is important to have a simple and\nfast error-detection scheme such as checksumming. On the other hand, error\ndetection at the link layer is implemented in dedicated hardware in adapters,\nwhich can rapidly perform the more complex CRC operations. Feldmeier\n[Feldmeier 1995] presents fast software implementation techniques for not\nonly weighted checksum codes, but CRC (see below) and other codes as\n6.2.3 Cyclic Redundancy Check (CRC)\nAn error-detection technique used widely in today’s computer networks is\nbased on cyclic redundancy check (CRC) codes. CRC codes are also\nknown as polynomial codes, since it is possible to view the bit string to be\nsent as a polynomial whose coefficients are the 0 and 1 values in the bit\nstring, with operations on the bit string interpreted as polynomial\narithmetic.\nCRC codes operate as follows. Consider the d-bit piece of data, D, that\nthe sending node wants to send to the receiving node. The sender and\nreceiver must first agree on an r + 1 bit pattern, known as a generator,\nwhich we will denote as G. We will require that the most significant\n(leftmost) bit of G be a 1. The key idea behind CRC codes is shown in\nFigure 6.6. For a given piece of data, D, the sender will choose r additional\nbits, R, and append them to D such that the resulting d + r bit pattern\n(interpreted as a binary number) is exactly divisible by G (i.e., has no\nremainder) using modulo-2 arithmetic. The process of error checking with\nCRCs is thus simple: The receiver divides the d + r received bits by G. If\nthe remainder is nonzero, the receiver knows that an error has occurred;\notherwise the data is accepted as being correct.\nAll CRC calculations are done in modulo-2 arithmetic without carries\nin addition or borrows in subtraction. This means that addition and\nsubtraction are identical, and both are equivalent to the bitwise exclusive-or\n(XOR) of the operands. Thus, for example,\n1011 XOR 0101 = 1110\n1001 XOR 1101 = 0100\nAlso, we similarly have\n1011 − 0101 = 1110\n1001 − 1101 = 0100\nMultiplication and division are the same as in base-2 arithmetic, except that\nany required addition or subtraction is done without carries or borrows. As\nin regular binary arithmetic, multiplication by 2  left shifts a bit pattern by k\nplaces. Thus, given D and R, the quantity D · 2  XOR R yields the d + r bit\npattern shown in Figure 6.6. We’ll use this algebraic characterization of the\nd + r bit pattern from Figure 6.6 in our discussion below.\nFigure 6.6 ♦CRC\nLet us now turn to the crucial question of how the sender computes R.\nRecall that we want to find R such that there is an n such that\nD ⋅2r XOR R = nG\nThat is, we want to choose R such that G divides into D · 2  XOR R\nwithout remainder. If we XOR (that is, add modulo-2, without carry) R to\nboth sides of the above equation, we get\nD ⋅2r = nG XOR R\nThis equation tells us that if we divide D · 2  by G, the value of the\nremainder is precisely R. In other words, we can calculate R as\nR = remainder D⋅2r\nFigure 6.7 illustrates this calculation for the case of D = 101110, d = 6,\nG = 1001, and r = 3. The 9 bits transmitted in this case are 101 110 011.\nYou should check these calculations for yourself and also check that indeed\nD · 2  = 101011 · G XOR R.\nFigure 6.7 ♦A sample CRC calculation\nInternational standards have been defined for 8-, 12-, 16-, and 32-bit\ngenerators, G. The CRC-32 32-bit standard, which has been adopted in a\nnumber of link-level IEEE protocols, uses a generator of\nGCRC-32 = 100000100110000010001110110110111\nEach of the CRC standards can detect burst errors of fewer than r + 1\nbits. (This means that all consecutive bit errors of r bits or fewer will be\ndetected.) Furthermore, under appropriate assumptions, a burst of length\ngreater than r + 1 bits is detected with probability 1 − 0.5 . Also, each of the\nCRC standards can detect any odd number of bit errors. See [Williams\n1993] for a discussion of implementing CRC checks. The theory behind\nCRC codes and even more powerful codes is beyond the scope of this text.\nThe text [Schwartz 1980] provides an excellent introduction to this topic.\n6.3 Multiple Access Links and Protocols\nIn the introduction to this chapter, we noted that there are two types of\nnetwork links: point-to-point links and broadcast links. A point-to-point\nlink consists of a single sender at one end of the link and a single receiver\nat the other end of the link. Many link-layer protocols have been designed\nfor point-to-point links; the point-to-point protocol (PPP) and high-level\ndata link control (HDLC) are two such protocols. The second type of link, a\nbroadcast link, can have multiple sending and receiving nodes all\nconnected to the same, single, shared broadcast channel. The term\nbroadcast is used here because when any one node transmits a frame, the\nchannel broadcasts the frame and each of the other nodes receives a copy.\nEthernet and wireless LANs are examples of broadcast link-layer\ntechnologies. In this section, we’ll take a step back from specific link-layer\nprotocols and first examine a problem of central importance to the link\nlayer: how to coordinate the access of multiple sending and receiving nodes\nto a shared broadcast channel—the multiple access problem. Broadcast\nchannels are often used in LANs, networks that are geographically\nconcentrated in a single building (or on a corporate or university campus).\nThus, we’ll look at how multiple access channels are used in LANs at the\nend of this section.\nWe are all familiar with the notion of broadcasting—television has been\nusing it since its invention. But traditional television is a one-way broadcast\n(that is, one fixed node transmitting to many receiving nodes), while nodes\non a computer network broadcast channel can both send and receive.\nPerhaps a more apt human analogy for a broadcast channel is a cocktail\nparty, where many people gather in a large room (the air providing the\nbroadcast medium) to talk and listen. A second good analogy is something\nmany readers will be familiar with—a classroom—where teacher(s) and\nstudent(s) similarly share the same, single, broadcast medium. A central\nproblem in both scenarios is that of determining who gets to talk (that is,\ntransmit into the channel) and when. As humans, we’ve evolved an\nelaborate set of protocols for sharing the broadcast channel:\n“Give everyone a chance to speak.”\n“Don’t speak until you are spoken to.”\n“Don’t monopolize the conversation.”\n“Raise your hand if you have a question.”\n“Don’t interrupt when someone is speaking.”\n“Don’t fall asleep when someone is talking.”\nComputer networks similarly have protocols—so-called multiple\naccess ­protocols—by which nodes regulate their transmission into the\nshared broadcast channel. As shown in Figure 6.8, multiple access protocols\nare needed in a wide variety of network settings, including both wired and\nwireless access networks, and satellite networks. Although technically each\nnode accesses the broadcast channel through its adapter, in this section, we\nwill refer to the node as the sending and receiving device. In practice,\nhundreds or even thousands of nodes can directly communicate over a\nbroadcast channel.\nFigure 6.8 ♦Various multiple access channels\nBecause all nodes are capable of transmitting frames, more than two\nnodes can transmit frames at the same time. When this happens, all of the\nnodes receive multiple frames at the same time; that is, the transmitted\nframes collide at all of the receivers. Typically, when there is a collision,\nnone of the receiving nodes can make any sense of any of the frames that\nwere transmitted; in a sense, the signals of the colliding frames become\ninextricably tangled together. Thus, all the frames involved in the collision\nare lost, and the broadcast channel is wasted during the collision interval.\nClearly, if many nodes want to transmit frames frequently, many\ntransmissions will result in collisions, and much of the bandwidth of the\nbroadcast channel will be wasted.\nIn order to ensure that the broadcast channel performs useful work\nwhen multiple nodes are active, it is necessary to somehow coordinate the\ntransmissions of the active nodes. This coordination job is the responsibility\nof the multiple access protocol. Over the past 40 years, thousands of papers\nand hundreds of PhD dissertations have been written on multiple access\nprotocols; a comprehensive survey of the first 20 years of this body of work\nis [Rom 1990]. Furthermore, active research in multiple access protocols\ncontinues due to the continued emergence of new types of links, particularly\nnew wireless links.\nOver the years, dozens of multiple access protocols have been\nimplemented in a variety of link-layer technologies. Nevertheless, we can\nclassify just about any multiple access protocol as belonging to one of three\ncategories: channel partitioning protocols, random access protocols, and\ntaking-turns protocols. We’ll cover these categories of multiple access\nprotocols in the following three subsections.\nLet’s conclude this overview by noting that, ideally, a multiple access\nprotocol for a broadcast channel of rate R bits per second should have the\nfollowing desirable characteristics:\n1. When only one node has data to send, that node has a throughput of R\n2. When M nodes have data to send, each of these nodes has a throughput\nof R/M bps. This need not necessarily imply that each of the M nodes\nalways has an instantaneous rate of R/M, but rather that each node\nshould have an average transmission rate of R/M over some suitably\ndefined interval of time.\n3. The protocol is decentralized; that is, there is no master node that\nrepresents a single point of failure for the network.\n4. The protocol is simple, so that it is inexpensive to implement.\n6.3.1 Channel Partitioning Protocols\nRecall from our early discussion back in Section 1.3 that time-division ­-\nmultiplexing (TDM) and frequency-division multiplexing (FDM) are two\ntechniques that can be used to partition a broadcast channel’s bandwidth\namong all nodes sharing that channel. As an example, suppose the channel\nsupports N nodes and that the transmission rate of the channel is R bps.\nTDM divides time into time frames and further divides each time frame\ninto N time slots. (The TDM time frame should not be confused with the\nlink-layer unit of data exchanged between sending and receiving adapters,\nwhich is also called a frame. In order to reduce confusion, in this subsection\nwe’ll refer to the link-layer unit of data exchanged as a packet.) Each time\nslot is then assigned to one of the N nodes. Whenever a node has a packet to\nsend, it transmits the packet’s bits during its assigned time slot in the\nrevolving TDM frame. Typically, slot sizes are chosen so that a single\npacket can be transmitted during a slot time. Figure 6.9 shows a simple\nfour-node TDM example. Returning to our cocktail party analogy, a TDM-\nregulated cocktail party would allow one partygoer to speak for a fixed\nperiod of time, then allow another partygoer to speak for the same amount\nof time, and so on. Once everyone had had a chance to talk, the ­pattern\nwould repeat.\nFigure 6.9 ♦A four-node TDM and FDM example\nTDM is appealing because it eliminates collisions and is perfectly fair:\nEach node gets a dedicated transmission rate of R/N bps during each frame\ntime. However, it has two major drawbacks. First, a node is limited to an\naverage rate of R/N bps even when it is the only node with packets to send.\nA second drawback is that a node must always wait for its turn in the\ntransmission sequence—again, even when it is the only node with a frame\nto send. Imagine the partygoer who is the only one with anything to say\n(and imagine that this is the even rarer circumstance where everyone wants\nto hear what that one person has to say). Clearly, TDM would be a poor\nchoice for a multiple access protocol for this particular party.\nWhile TDM shares the broadcast channel in time, FDM divides the R\nbps channel into different frequencies (each with a bandwidth of R/N) and\nassigns each frequency to one of the N nodes. FDM thus creates N smaller\nchannels of R/N bps out of the single, larger R bps channel. FDM shares\nboth the advantages and drawbacks of TDM. It avoids collisions and\ndivides the bandwidth fairly among the N nodes. However, FDM also\nshares a principal disadvantage with TDM—a node is limited to a\nbandwidth of R/N, even when it is the only node with packets to send.\nA third channel partitioning protocol is code division multiple access\n(CDMA). While TDM and FDM assign time slots and frequencies,\nrespectively, to the nodes, CDMA assigns a different code to each node.\nEach node then uses its unique code to encode the data bits it sends. If the\ncodes are chosen carefully, CDMA networks have the wonderful property\nthat different nodes can transmit simultaneously and yet have their\nrespective receivers correctly receive a sender’s encoded data bits\n(assuming the receiver knows the sender’s code) in spite of interfering\ntransmissions by other nodes. CDMA has been used in military systems for\nsome time (due to its anti-jamming properties) and now has widespread\ncivilian use, particularly in cellular telephony. Because CDMA’s use is so\ntightly tied to wireless channels, we’ll save our discussion of the technical\ndetails of CDMA until Chapter 7. For now, it will suffice to know that\nCDMA codes, like time slots in TDM and frequencies in FDM, can be\nallocated to the multiple access channel users.\n6.3.2 Random Access Protocols\nThe second broad class of multiple access protocols are random access\nprotocols. In a random access protocol, a transmitting node always\ntransmits at the full rate of the channel, namely, R bps. When there is a\ncollision, each node involved in the collision repeatedly retransmits its\nframe (that is, packet) until its frame gets through without a collision. But\nwhen a node experiences a collision, it doesn’t necessarily retransmit the\nframe right away. Instead it waits a random delay before retransmitting the\nframe. Each node involved in a collision chooses independent random\ndelays. Because the random delays are independently chosen, it is possible\nthat one of the nodes will pick a delay that is sufficiently less than the\ndelays of the other colliding nodes and will therefore be able to sneak its\nframe into the channel without a collision.\nThere are dozens if not hundreds of random access protocols described\nin the literature [Rom 1990; Bertsekas 1991]. In this section we’ll describe\na few of the most commonly used random access protocols—the ALOHA\nprotocols [Abramson 1970; Abramson 1985; Abramson 2009] and the\ncarrier sense multiple access (CSMA) protocols [Kleinrock 1975b].\nEthernet [Metcalfe 1976] is a popular and widely deployed CSMA protocol.\nSlotted ALOHA\nLet’s begin our study of random access protocols with one of the simplest\nrandom access protocols, the slotted ALOHA protocol. In our description of\nslotted ALOHA, we assume the following:\nAll frames consist of exactly L bits.\nTime is divided into slots of size L/R seconds (that is, a slot equals the\ntime to transmit one frame).\nNodes start to transmit frames only at the beginnings of slots.\nThe nodes are synchronized so that each node knows when the slots\nIf two or more frames collide in a slot, then all the nodes detect the\ncollision event before the slot ends.\nLet p be a probability, that is, a number between 0 and 1. The operation of\nslotted ALOHA in each node is simple:\nWhen the node has a fresh frame to send, it waits until the beginning of\nthe next slot and transmits the entire frame in the slot.\nIf there isn’t a collision, the node has successfully transmitted its frame\nand thus need not consider retransmitting the frame. (The node can\nprepare a new frame for transmission, if it has one.)\nIf there is a collision, the node detects the collision before the end of the\nslot. The node retransmits its frame in each subsequent slot with\nprobability p until the frame is transmitted without a collision.\nBy retransmitting with probability p, we mean that the node effectively\ntosses a biased coin; the event heads corresponds to “retransmit,” which\noccurs with probability p. The event tails corresponds to “skip the slot and\ntoss the coin again in the next slot”; this occurs with probability (1 − p). All\nnodes involved in the collision toss their coins independently.\nSlotted ALOHA would appear to have many advantages. Unlike\nchannel partitioning, slotted ALOHA allows a node to transmit\ncontinuously at the full rate, R, when that node is the only active node. (A\nnode is said to be active if it has frames to send.) Slotted ALOHA is also\ndecentralized, \nindependently decides when to retransmit. (Slotted ALOHA does, however,\nrequire the slots to be synchronized in the nodes; shortly we’ll discuss an\nunslotted version of the ALOHA protocol, as well as CSMA protocols, none\nof which require such synchronization.) Slotted ALOHA is also an\nextremely simple protocol.\nSlotted ALOHA works well when there is only one active node, but\nhow ­efficient is it when there are multiple active nodes? There are two\npossible efficiency concerns here. First, as shown in Figure 6.10, when\nthere are multiple active nodes, a certain fraction of the slots will have\ncollisions and will therefore be “wasted.” The second concern is that\nanother fraction of the slots will be empty because all active nodes refrain\nfrom transmitting as a result of the probabilistic transmission policy. The\nonly “unwasted” slots will be those in which exactly one node transmits. A\nslot in which exactly one node transmits is said to be a successful slot. The\nefficiency of a slotted multiple access protocol is defined to be the long-run\nfraction of successful slots in the case when there are a large number of\nactive nodes, each always having a large number of frames to send. Note\nthat if no form of access control were used, and each node were to\nimmediately retransmit after each collision, the efficiency would be zero.\nSlotted ALOHA clearly increases the efficiency beyond zero, but by how\nFigure 6.10 ♦Nodes 1, 2, and 3 collide in the first slot. Node 2\nfinally succeeds in the fourth slot, node 1 in the eighth\nslot, and node 3 in the ninth slot\nWe now proceed to outline the derivation of the maximum efficiency of\nslotted ALOHA. To keep this derivation simple, let’s modify the protocol a\nlittle and assume that each node attempts to transmit a frame in each slot\nwith probability p. (That is, we assume that each node always has a frame\nto send and that the node transmits with probability p for a fresh frame as\nwell as for a frame that has already suffered a collision.) Suppose there are\nN nodes. Then the probability that a given slot is a successful slot is the\nprobability that one of the nodes transmits and that the remaining N − 1\nnodes do not transmit. The probability that a given node transmits is p; the\nprobability that the remaining nodes do not transmit is (1 − p)\n. Therefore,\nthe probability a given node has a success is p(1 − p)\n. Because there are\nN nodes, the probability that any one of the N nodes has a success is Np(1 −\nThus, when there are N active nodes, the efficiency of slotted ALOHA\nis Np(1 − p)\n. To obtain the maximum efficiency for N active nodes, we\nhave to find the p* that maximizes this expression. (See the homework\nproblems for a general outline of this derivation.) And to obtain the\nmaximum efficiency for a large number of active nodes, we take the limit of\nNp*(1 − p*)\n as N approaches infinity. (Again, see the homework\nproblems.) After performing these calculations, we’ll find that the\nmaximum efficiency of the protocol is given by 1/e = 0.37. That is, when a\nlarge number of nodes have many frames to transmit, then (at best) only 37\npercent of the slots do useful work. Thus, the effective transmission rate of\nthe channel is not R bps but only 0.37 R bps! A similar analysis also shows\nthat 37 percent of the slots go empty and 26 percent of slots have collisions.\nImagine the poor network administrator who has purchased a 100-Mbps\nslotted ALOHA system, expecting to be able to use the network to transmit\ndata among a large number of users at an aggregate rate of, say, 80 Mbps!\nAlthough the channel is capable of transmitting a given frame at the full\nchannel rate of 100 Mbps, in the long run, the successful throughput of this\nchannel will be less than 37 Mbps.\nThe slotted ALOHA protocol required that all nodes synchronize their\ntransmissions to start at the beginning of a slot. The first ALOHA protocol\n[Abramson 1970] was actually an unslotted, fully decentralized protocol. In\npure ALOHA, when a frame first arrives (that is, a network-layer datagram\nis passed down from the network layer at the sending node), the node\nimmediately transmits the frame in its entirety into the broadcast channel. If\na transmitted frame experiences a collision with one or more other\ntransmissions, the node will then immediately (after completely\ntransmitting its collided frame) retransmit the frame with probability p.\nOtherwise, the node waits for a frame transmission time. After this wait, it\nthen transmits the frame with probability p, or waits (remaining idle) for\nanother frame time with probability 1 – p.\nTo determine the maximum efficiency of pure ALOHA, we focus on an\nindividual node. We’ll make the same assumptions as in our slotted\nALOHA analysis and take the frame transmission time to be the unit of\ntime. At any given time, the probability that a node is transmitting a frame\nis p. Suppose this frame begins transmission at time t . As shown in Figure\n6.11, in order for this frame to be successfully transmitted, no other nodes\ncan begin their transmission in the interval of time [t  − 1, t ]. Such a\ntransmission would overlap with the beginning of the transmission of node\ni’s frame. The probability that all other nodes do not begin a transmission in\nthis interval is (1 − p)\n. Similarly, no other node can begin a transmission\nwhile node i is transmitting, as such a transmission would overlap with the\nlatter part of node i’s transmission. The probability that all other nodes do\nnot begin a transmission in this interval is also (1 − p)\n. Thus, the\nprobability that a given node has a successful transmission is p(1 − p)\nBy taking limits as in the slotted ALOHA case, we find that the maximum\nefficiency of the pure ALOHA protocol is only 1/(2e)—exactly half that of\nslotted ALOHA. This then is the price to be paid for a fully decentralized\nALOHA protocol.\nFigure 6.11 ♦Interfering transmissions in pure ALOHA\nCarrier Sense Multiple Access (CSMA)\nIn both slotted and pure ALOHA, a node’s decision to transmit is made\nindependently of the activity of the other nodes attached to the broadcast\nchannel. In particular, a node neither pays attention to whether another node\nhappens to be transmitting when it begins to transmit, nor stops transmitting\nif another node begins to interfere with its transmission. In our cocktail\nparty analogy, ALOHA protocols are quite like a boorish partygoer who\ncontinues to chatter away regardless of whether other people are talking. As\nhumans, we have human protocols that allow us not only to behave with\nmore civility, but also to decrease the amount of time spent “colliding” with\neach other in conversation and, consequently, to increase the amount of data\nwe exchange in our conversations. Specifically, there are two important\nrules for polite human conversation:\nListen before speaking. If someone else is speaking, wait until they are\nfinished. In the networking world, this is called carrier sensing—a\nnode listens to the channel before transmitting. If a frame from another\nnode is currently being transmitted into the channel, a node then waits\nuntil it detects no transmissions for a short amount of time and then\nbegins transmission.\nIf someone else begins talking at the same time, stop talking. In the\nnetworking world, this is called collision detection—a transmitting\nnode listens to the channel while it is transmitting. If it detects that\nanother node is transmitting an interfering frame, it stops transmitting\nand waits a random amount of time before repeating the sense-and-\ntransmit-when-idle cycle.\nThese two rules are embodied in the family of carrier sense multiple\naccess (CSMA) and CSMA with collision detection (CSMA/CD)\nprotocols [Kleinrock 1975b; Metcalfe 1976; Lam 1980; Rom 1990]. Many\nvariations on CSMA and CSMA/CD have been proposed. Here, we’ll\nconsider a few of the most important, and fundamental, characteristics of\nCSMA and CSMA/CD.\nNorm Abramson, a PhD engineer, had a passion for surfing and an interest in packet\nswitching. This combination of interests brought him to the University of Hawaii in\n1969. Hawaii consists of many mountainous islands, making it difficult to install and\noperate land-based networks. When not surfing, Abramson thought about how to\ndesign a network that does packet switching over radio. The network he designed had\none central host and several secondary nodes scattered over the Hawaiian Islands.\nThe network had two channels, each using a different frequency band. The downlink\nchannel broadcasted packets from the central host to the secondary hosts; and the\nupstream channel sent packets from the secondary hosts to the central host. In\naddition to sending informational packets, the central host also sent on the downstream\nsurprising, recalling from Chapter 4 that hosts and routers have network-\nlayer addresses as well. You might be asking, why in the world do we need\nto have addresses at both the network and link layers? In addition to\ndescribing the syntax and function of the link-layer addresses, in this\nsection we hope to shed some light on why the two layers of addresses are\nuseful and, in fact, indispensable. We’ll also cover the Address Resolution\nProtocol (ARP), which provides a mechanism to translate IP addresses to\nlink-layer addresses.\nMAC Addresses\nIn truth, it is not hosts and routers that have link-layer addresses but rather\ntheir adapters (that is, network interfaces) that have link-layer addresses. A\nhost or router with multiple network interfaces will thus have multiple link-\nlayer addresses associated with it, just as it would also have multiple IP\naddresses associated with it. It’s important to note, however, that link-layer\nswitches do not have link-layer addresses associated with their interfaces\nthat connect to hosts and routers. This is because the job of the link-layer\nswitch is to carry datagrams between hosts and routers; a switch does this\njob transparently, that is, without the host or router having to explicitly\naddress the frame to the intervening switch. This is illustrated in Figure\n6.16. A link-layer address is variously called a LAN address, a physical\naddress, or a MAC address. Because MAC address seems to be the most\npopular term, we’ll henceforth refer to link-layer addresses as MAC\naddresses. For most LANs (including Ethernet and 802.11 wireless LANs),\nthe MAC address is 6 bytes long, giving 2  possible MAC addresses. As\nshown in Figure 6.16, these 6-byte addresses are typically expressed in\nhexadecimal notation, with each byte of the address expressed as a pair of\nhexadecimal numbers. Although MAC addresses were designed to be\npermanent, it is now possible to change an adapter’s MAC address via\nsoftware. For the rest of this section, however, we’ll assume that an\nadapter’s MAC address is fixed.\nOne interesting property of MAC addresses is that no two adapters\nhave the same address. This might seem surprising given that adapters are\nmanufactured in many countries by many companies. How does a company\nmanufacturing adapters in Taiwan make sure that it is using different\naddresses from a company manufacturing adapters in Belgium? The answer\nis that the IEEE manages the MAC address space. In particular, when a\ncompany wants to manufacture adapters, it purchases a chunk of the\naddress space consisting of 2  addresses for a nominal fee. IEEE allocates\nthe chunk of 2  addresses by fixing the first 24 bits of a MAC address and\nletting the company create unique combinations of the last 24 bits for each\nAn adapter’s MAC address has a flat structure (as opposed to a\nhierarchical structure) and doesn’t change no matter where the adapter goes.\nA laptop with an Ethernet interface always has the same MAC address, no\nmatter where the computer goes. A smartphone with an 802.11 interface\nalways has the same MAC address, no matter where the smartphone goes.\nRecall that, in contrast, IP addresses have a hierarchical structure (that is, a\nnetwork part and a host part), and a host’s IP addresses needs to be changed\nwhen the host moves, i.e., changes the network to which it is attached. An\nadapter’s MAC address is analogous to a person’s social security number,\nwhich also has a flat addressing structure and which doesn’t change no\nmatter where the person goes. An IP address is analogous to a person’s\npostal address, which is hierarchical and which must be changed whenever\na person moves. Just as a person may find it useful to have both a postal\naddress and a social security number, it is useful for a host and router\ninterfaces to have both a network-layer address and a MAC address.\nFigure 6.16 ♦Each interface connected to a LAN has a unique\nMAC address\nWhen an adapter wants to send a frame to some destination adapter, the\nsending adapter inserts the destination adapter’s MAC address into the\nframe and then sends the frame into the LAN. As we will soon see, a switch\noccasionally broadcasts an incoming frame onto all of its interfaces. We’ll\nsee in Chapter 7 that 802.11 also broadcasts frames. Thus, an adapter may\nreceive a frame that isn’t addressed to it. Thus, when an adapter receives a\nframe, it will check to see whether the destination MAC address in the\nframe matches its own MAC address. If there is a match, the adapter\nextracts the enclosed datagram and passes the datagram up the protocol\nstack. If there isn’t a match, the adapter discards the frame, without passing\nthe network-layer datagram up. Thus, the destination only will be\ninterrupted when the frame is received.\nHowever, sometimes a sending adapter does want all the other adapters\non the LAN to receive and process the frame it is about to send. In this case,\nthe sending adapter inserts a special MAC broadcast address into the\ndestination address field of the frame. For LANs that use 6-byte addresses\n(such as Ethernet and 802.11), the broadcast address is a string of 48\nconsecutive 1s (that is, FF-FF-FF-FF-FF-FF in hexadecimal notation).\nAddress Resolution Protocol (ARP)\nBecause there are both network-layer addresses (for example, Internet IP\naddresses) and link-layer addresses (that is, MAC addresses), there is a need\nto translate between them. For the Internet, this is the job of the Address\nResolution Protocol (ARP) [RFC 826].\nTo understand the need for a protocol such as ARP, consider the\nnetwork shown in Figure 6.17. In this simple example, each host and router\nhas a single IP address and single MAC address. As usual, IP addresses are\nshown in dotted-decimal notation and MAC addresses are shown in\nhexadecimal notation. For the purposes of this discussion, we will assume\nin this section that the switch broadcasts all frames; that is, whenever a\nswitch receives a frame on one interface, it forwards the frame on all of its\nother interfaces. In the next section, we will provide a more accurate\nexplanation of how switches operate.\nFigure 6.17 ♦Each interface on a LAN has an IP address and a\nMAC address\nThere are several reasons why hosts and router interfaces have MAC addresses in ­addition\nto network-layer addresses. First, LANs are designed for arbitrary network-layer protocols,\nnot just for IP and the Internet. If adapters were assigned IP addresses rather than “neutral”\nMAC addresses, then adapters would not easily be able to support other network-layer\nprotocols (for example, IPX or DECnet). Second, if adapters were to use network-layer\naddresses instead of MAC addresses, the network-layer address would have to be stored in\nthe adapter RAM and reconfigured every time the adapter was moved (or powered up).\nAnother option is to not use any addresses in the adapters and have each adapter pass the\ndata (typically, an IP datagram) of each frame it receives up the protocol stack. The network\nlayer could then check for a matching network-layer address. One problem with this option\nis that the host would be interrupted by every frame sent on the LAN, including by frames\nthat were destined for other hosts on the same broadcast LAN. In summary, in order for the\nlayers to be largely independent building blocks in a network architecture, different layers\nneed to have their own addressing scheme. We have now seen three types of addresses:\nhost names for the application layer, IP addresses for the network layer, and MAC\naddresses for the link layer.\nNow suppose that the host with IP address 222.222.222.220 wants to\nsend an IP datagram to host 222.222.222.222. In this example, both the\nsource and destination are in the same subnet, in the addressing sense of\nSection 4.3.3. To send a datagram, the source must give its adapter not only\nthe IP datagram but also the MAC address for destination 222.222.222.222.\nThe sending adapter will then construct a link-layer frame containing the\ndestination’s MAC address and send the frame into the LAN.\nThe important question addressed in this section is, How does the\nsending host determine the MAC address for the destination host with IP\naddress 222.222.222.222? As you might have guessed, it uses ARP. An ARP\nmodule in the sending host takes any IP address on the same LAN as input,\nand returns the corresponding MAC address. In the example at hand,\nsending host 222.222.222.220 provides its ARP module the IP address\n222.222.222.222, and the ARP module returns the corresponding MAC\naddress 49-BD-D2-C7-56-2A.\nSo we see that ARP resolves an IP address to a MAC address. In many\nways it is analogous to DNS (studied in Section 2.5), which resolves host\nnames to IP addresses. However, one important difference between the two\nresolvers is that DNS resolves host names for hosts anywhere in the\nInternet, whereas ARP resolves IP addresses only for hosts and router\ninterfaces on the same subnet. If a node in California were to try to use ARP\nto resolve the IP address for a node in Mississippi, ARP would return with\nNow that we have explained what ARP does, let’s look at how it works.\nEach host and router has an ARP table in its memory, which contains\nmappings of IP addresses to MAC addresses. Figure 6.18 shows what an\nARP table in host 222.222.222.220 might look like. The ARP table also\ncontains a time-to-live (TTL) value, which indicates when each mapping\nwill be deleted from the table. Note that a table does not necessarily contain\nan entry for every host and router on the subnet; some may have never been\nentered into the table, and others may have expired. A typical expiration\ntime for an entry is 20 minutes from when an entry is placed in an ARP\nFigure 6.18 ♦A possible ARP table in 222.222.222.220\nNow suppose that host 222.222.222.220 wants to send a datagram that\nis IP-addressed to another host or router on that subnet. The sending host\nneeds to obtain the MAC address of the destination given the IP address.\nThis task is easy if the sender’s ARP table has an entry for the destination\nnode. But what if the ARP table doesn’t currently have an entry for the\ndestination? In particular, suppose 222.222.222.220 wants to send a\ndatagram to 222.222.222.222. In this case, the sender uses the ARP protocol\nto resolve the address. First, the sender constructs a special packet called an\nARP packet. An ARP packet has several fields, including the sending and\nreceiving IP and MAC addresses. Both ARP query and response packets\nhave the same format. The purpose of the ARP query packet is to query all\nthe other hosts and routers on the subnet to determine the MAC address\ncorresponding to the IP address that is being resolved.\nReturning to our example, 222.222.222.220 passes an ARP query\npacket to the adapter along with an indication that the adapter should send\nthe packet to the MAC broadcast address, namely, FF-FF-FF-FF-FF-FF.\nThe adapter encapsulates the ARP packet in a link-layer frame, uses the\nbroadcast address for the frame’s destination address, and transmits the\nframe into the subnet. Recalling our social security ­number/postal address\nanalogy, an ARP query is equivalent to a person shouting out in a crowded\nroom of cubicles in some company (say, AnyCorp): “What is the social\nsecurity number of the person whose postal address is Cubicle 13, Room\n112, AnyCorp, Palo Alto, California?” The frame containing the ARP query\nis received by all the other adapters on the subnet, and (because of the\nbroadcast address) each adapter passes the ARP packet within the frame up\nto its ARP module. Each of these ARP modules checks to see if its IP\naddress matches the destination IP address in the ARP packet. The one with\na match sends back to the querying host a response ARP packet with the\ndesired mapping. The querying host 222.222.222.220 can then update its\nARP table and send its IP datagram, encapsulated in a link-layer frame\nwhose destination MAC is that of the host or router responding to the\nearlier ARP query.\nThere are a couple of interesting things to note about the ARP protocol.\nFirst, the query ARP message is sent within a broadcast frame, whereas the\nresponse ARP message is sent within a standard frame. Before reading on\nyou should think about why this is so. Second, ARP is plug-and-play; that\nis, an ARP table gets built ­automatically—it doesn’t have to be configured\nby a system administrator. And if a host becomes disconnected from the\nsubnet, its entry is eventually deleted from the other ARP tables in the\nStudents often wonder if ARP is a link-layer protocol or a network-\nlayer protocol. As we’ve seen, an ARP packet is encapsulated within a link-\nlayer frame and thus lies architecturally above the link layer. However, an\nARP packet has fields containing link-layer addresses and thus is arguably a\nlink-layer protocol, but it also contains network-layer addresses and thus is\nalso arguably a network-layer protocol. In the end, ARP is probably best\nconsidered a protocol that straddles the boundary between the link and\nnetwork layers—not fitting neatly into the simple layered protocol stack we\nstudied in Chapter 1. Such are the complexities of real-world protocols!\nSending a Datagram off the Subnet\nIt should now be clear how ARP operates when a host wants to send a\ndatagram to another host on the same subnet. But now let’s look at the more\ncomplicated situation when a host on a subnet wants to send a network-\nlayer datagram to a host off the subnet (that is, across a router onto another\nsubnet). Let’s discuss this issue in the context of Figure 6.19, which shows\na simple network consisting of two subnets interconnected by a router.\nThere are several interesting things to note about Figure 6.19. Each host\nhas exactly one IP address and one adapter. But, as discussed in Chapter 4,\na router has an IP address for each of its interfaces. For each router interface\nthere is also an ARP module (in the router) and an adapter. Because the\nrouter in Figure 6.19 has two interfaces, it has two IP addresses, two ARP\nmodules, and two adapters. Of course, each adapter in the network has its\nown MAC address.\nFigure 6.19 ♦Two subnets interconnected by a router\nAlso note that Subnet 1 has the network address 111.111.111/24 and\nthat Subnet 2 has the network address 222.222.222/24. Thus, all of the\ninterfaces connected to Subnet 1 have addresses of the form\n111.111.111.xxx and all of the interfaces connected to Subnet 2 have\naddresses of the form 222.222.222.xxx.\nNow let’s examine how a host on Subnet 1 would send a datagram to a\nhost on Subnet 2. Specifically, suppose that host 111.111.111.111 wants to\nsend an IP datagram to a host 222.222.222.222. The sending host passes the\ndatagram to its adapter, as usual. But the sending host must also indicate to\nits adapter an appropriate destination MAC address. What MAC address\nshould the adapter use? One might be tempted to guess that the appropriate\nMAC address is that of the adapter for host 222.222.222.222, namely, 49-\nBD-D2-C7-56-2A. This guess, however, would be wrong! If the sending\nadapter were to use that MAC address, then none of the ­adapters on Subnet\n1 would bother to pass the IP datagram up to its network layer, since the\nframe’s destination address would not match the MAC address of any\nadapter on Subnet 1. The datagram would just die and go to datagram\nIf we look carefully at Figure 6.19, we see that in order for a datagram\nto go from 111.111.111.111 to a host on Subnet 2, the datagram must first be\nsent to the router interface 111.111.111.110, which is the IP address of the\nfirst-hop router on the path to the final destination. Thus, the appropriate\nMAC address for the frame is the address of the adapter for router interface\n111.111.111.110, namely, E6-E9-00-17-BB-4B. How does the sending host\nacquire the MAC address for 111.111.111.110? By using ARP, of course!\nOnce the sending adapter has this MAC address, it creates a frame\n(containing the datagram addressed to 222.222.222.222) and sends the\nframe into Subnet 1. The router adapter on Subnet 1 sees that the link-layer\nframe is addressed to it, and therefore passes the frame to the network layer\nof the router. Hooray—the IP datagram has successfully been moved from\nsource host to the router! But we are not finished. We still have to move the\ndatagram from the router to the destination. The router now has to\ndetermine the correct interface on which the datagram is to be forwarded.\nAs discussed in Chapter 4, this is done by consulting a forwarding table in\nthe router. The forwarding table tells the router that the datagram is to be\nforwarded via router interface 222.222.222.220. This interface then passes\nthe datagram to its adapter, which encapsulates the datagram in a new frame\nand sends the frame into Subnet 2. This time, the destination MAC address\nof the frame is indeed the MAC address of the ultimate destination. And\nhow does the router obtain this destination MAC address? From ARP, of\nARP for Ethernet is defined in RFC 826. A nice introduction to ARP is\ngiven in the TCP/IP tutorial, RFC 1180. We’ll explore ARP in more detail in\nthe homework problems.\n6.4.2 Ethernet\nEthernet has pretty much taken over the wired LAN market. In the 1980s\nand the early 1990s, Ethernet faced many challenges from other LAN\ntechnologies, ­including token ring, FDDI, and ATM. Some of these other\ntechnologies succeeded in capturing a part of the LAN market for a few\nyears. But since its invention in the mid-1970s, Ethernet has continued to\nevolve and grow and has held on to its dominant position. Today, Ethernet\nis by far the most prevalent wired LAN technology, and it is likely to\nremain so for the foreseeable future. One might say that Ethernet has been\nto local area networking what the Internet has been to global networking.\nThere are many reasons for Ethernet’s success. First, Ethernet was the\nfirst widely deployed high-speed LAN. Because it was deployed early,\nnetwork administrators became intimately familiar with Ethernet—its\nwonders and its quirks—and were reluctant to switch over to other LAN\ntechnologies when they came on the scene. Second, token ring, FDDI, and\nATM were more complex and expensive than Ethernet, which further\ndiscouraged network administrators from switching over. Third, the most\ncompelling reason to switch to another LAN technology (such as FDDI or\nATM) was usually the higher data rate of the new technology; however,\nEthernet always fought back, producing versions that operated at equal data\nrates or higher. Switched Ethernet was also introduced in the early 1990s,\nwhich further increased its effective data rates. Finally, because Ethernet\nhas been so popular, Ethernet hardware (in particular, adapters and\nswitches) has become a commodity and is remarkably cheap.\nThe original Ethernet LAN was invented in the mid-1970s by Bob\nMetcalfe and David Boggs. The original Ethernet LAN used a coaxial bus\nto interconnect the nodes. Bus topologies for Ethernet actually persisted\nthroughout the 1980s and into the mid-1990s. Ethernet with a bus topology\nis a broadcast LAN—all transmitted frames travel to and are processed by\nall adapters connected to the bus. Recall that we covered Ethernet’s\nCSMA/CD multiple access protocol with binary exponential backoff in\nSection 6.3.2.\nBy the late 1990s, most companies and universities had replaced their\nLANs with Ethernet installations using a hub-based star topology. In such\nan installation the hosts (and routers) are directly connected to a hub with\ntwisted-pair copper wire. A hub is a physical-layer device that acts on\nindividual bits rather than frames. When a bit, representing a zero or a one,\narrives from one interface, the hub simply re-creates the bit, boosts its\nenergy strength, and transmits the bit onto all the other interfaces. Thus,\nEthernet with a hub-based star topology is also a broadcast LAN—\nwhenever a hub receives a bit from one of its interfaces, it sends a copy out\non all of its other interfaces. In particular, if a hub receives frames from two\ndifferent interfaces at the same time, a collision occurs and the nodes that\ncreated the frames must retransmit.\nIn the early 2000s, Ethernet experienced yet another major evolutionary\nchange. Ethernet installations continued to use a star topology, but the hub\nat the center was replaced with a switch. We’ll be examining switched\nEthernet in depth later in this chapter. For now, we only mention that a\nswitch is not only “collision-less” but is also a bona-fide store-and-forward\npacket switch; but unlike routers, which operate up through layer 3, a\nswitch operates only up through layer 2.\nEthernet Frame Structure\nWe can learn a lot about Ethernet by examining the Ethernet frame, which\nis shown in Figure 6.20. To give this discussion about Ethernet frames a\ntangible context, let’s consider sending an IP datagram from one host to\nanother host, with both hosts on the same Ethernet LAN (for example, the\nEthernet LAN in Figure 6.17.) (Although the payload of our Ethernet frame\nis an IP datagram, we note that an Ethernet frame can carry other network-\nlayer packets as well.) Let the sending adapter, adapter A, have the MAC\naddress AA-AA-AA-AA-AA-AA and the receiving adapter, adapter B, have\nBB-BB-BB-BB-BB-BB. \nencapsulates the IP datagram within an Ethernet frame and passes the frame\nto the physical layer. The receiving adapter receives the frame from the\nphysical layer, extracts the IP datagram, and passes the IP datagram to the\nnetwork layer. In this context, let’s now examine the six fields of the\nEthernet frame, as shown in Figure 6.20.\nFigure 6.20 ♦Ethernet frame structure\nData field (46 to 1,500 bytes). This field carries the IP datagram. The\nmaximum transmission unit (MTU) of Ethernet is 1,500 bytes. This\nmeans that if the IP datagram exceeds 1,500 bytes, then the host has to\nfragment the datagram, as discussed in Section 4.3.2. The minimum size\nof the data field is 46 bytes. This means that if the IP datagram is less\nthan 46 bytes, the data field has to be “stuffed” to fill it out to 46 bytes.\nWhen stuffing is used, the data passed to the network layer contains the\nstuffing as well as an IP datagram. The network layer uses the length\nfield in the IP datagram header to remove the stuffing.\nDestination address (6 bytes). This field contains the MAC address of\nthe destination adapter, BB-BB-BB-BB-BB-BB. When adapter B\nreceives an Ethernet frame whose destination address is either BB-BB-\nat Host B see gaps as well? As we learned in Chapter 3, this depends on\nwhether the application is using UDP or TCP. If the application is using\nUDP, then the application in Host B will indeed see gaps in the data. On the\nother hand, if the application is using TCP, then TCP in Host B will not\nacknowledge the data contained in discarded frames, causing TCP in Host A\nto retransmit. Note that when TCP retransmits data, the data will eventually\nreturn to the Ethernet adapter at which it was discarded. Thus, in this sense,\nEthernet does retransmit data, although Ethernet is unaware of whether it is\ntransmitting a brand-new datagram with brand-new data, or a datagram that\ncontains data that has already been transmitted at least once.\nEthernet Technologies\nIn our discussion above, we’ve referred to Ethernet as if it were a single\nprotocol standard. But in fact, Ethernet comes in many different flavors,\nwith somewhat bewildering acronyms such as 10BASE-T, 10BASE-2,\n100BASE-T, 1000BASE-LX, 10GBASE-T and 40GBASE-T. These and\nmany other Ethernet technologies have been standardized over the years by\nthe IEEE 802.3 CSMA/CD (Ethernet) working group [IEEE 802.3 2020].\nWhile these acronyms may appear bewildering, there is actually\nconsiderable order here. The first part of the acronym refers to the speed of\nthe standard: 10, 100, 1000, or 10G, for 10 Megabit (per second), 100\nMegabit, Gigabit, 10 Gigabit and 40 Gigibit Ethernet, respectively. “BASE”\nrefers to baseband Ethernet, meaning that the physical media only carries\nEthernet traffic; almost all of the 802.3 standards are for baseband Ethernet.\nThe final part of the acronym refers to the physical media itself; Ethernet is\nboth a link-layer and a physical-layer specification and is carried over a\nvariety of physical media including coaxial cable, copper wire, and fiber.\nGenerally, a “T” refers to twisted-pair copper wires.\nHistorically, an Ethernet was initially conceived of as a segment of\ncoaxial cable. The early 10BASE-2 and 10BASE-5 standards specify 10\nMbps Ethernet over two types of coaxial cable, each limited in length to\n500 meters. Longer runs could be obtained by using a repeater—a\nphysical-layer device that receives a signal on the input side, and\nregenerates the signal on the output side. A coaxial cable corresponds nicely\nto our view of Ethernet as a broadcast medium—all frames transmitted by\none interface are received at other interfaces, and Ethernet’s CDMA/CD\nprotocol nicely solves the multiple access problem. Nodes simply attach to\nthe cable, and voila, we have a local area network!\nEthernet has passed through a series of evolutionary steps over the\nyears, and today’s Ethernet is very different from the original bus-topology\ndesigns using coaxial cable. In most installations today, nodes are\nconnected to a switch via point-to-point segments made of twisted-pair\ncopper wires or fiber-optic cables, as shown in Figures 6.15–6.17.\nIn the mid-1990s, Ethernet was standardized at 100 Mbps, 10 times\nfaster than 10 Mbps Ethernet. The original Ethernet MAC protocol and\nframe format were preserved, but higher-speed physical layers were defined\nfor copper wire (100BASE-T) and fiber (100BASE-FX, 100BASE-SX,\n100BASE-BX). Figure 6.21 shows these different standards and the\ncommon Ethernet MAC protocol and frame format. 100 Mbps Ethernet is\nlimited to a 100-meter distance over twisted pair, and to several kilometers\nover fiber, allowing Ethernet switches in different buildings to be\nFigure 6.21 ♦100 Mbps Ethernet standards: A common link layer,\ndifferent physical layers\nGigabit Ethernet is an extension to the highly successful 10 Mbps and\n100 Mbps Ethernet standards. Offering a raw data rate of 40,000 Mbps, 40\nGigabit Ethernet maintains full compatibility with the huge installed base of\nEthernet equipment. The standard for Gigabit Ethernet, referred to as IEEE\n802.3z, does the following:\nUses the standard Ethernet frame format (Figure 6.20) and is backward\ncompatible with 10BASE-T and 100BASE-T technologies. This allows\nfor easy integration of Gigabit Ethernet with the existing installed base\nof Ethernet equipment.\nAllows for point-to-point links as well as shared broadcast channels.\nPoint-to-point links use switches while broadcast channels use hubs, as\ndescribed earlier. In Gigabit Ethernet jargon, hubs are called buffered\ndistributors.\nUses CSMA/CD for shared broadcast channels. In order to have\nacceptable efficiency, the maximum distance between nodes must be\nseverely restricted.\nAllows for full-duplex operation at 40 Gbps in both directions for point-\nto-point channels.\nInitially operating over optical fiber, Gigabit Ethernet is now able to run\nover category 5 UTP cabling (for 1000BASE-T and 10GBASE-T).\nLet’s conclude our discussion of Ethernet technology by posing a\nquestion that may have begun troubling you. In the days of bus topologies\nand hub-based star topologies, Ethernet was clearly a broadcast link (as\ndefined in Section 6.3) in which frame collisions occurred when nodes\ntransmitted at the same time. To deal with these collisions, the Ethernet\nstandard included the CSMA/CD protocol, which is particularly effective\nfor a wired broadcast LAN spanning a small geographical region. But if the\nprevalent use of Ethernet today is a switch-based star topology, using store-\nand-forward packet switching, is there really a need anymore for an\nEthernet MAC protocol? As we’ll see shortly, a switch coordinates its\ntransmissions and never forwards more than one frame onto the same\ninterface at any time. Furthermore, modern switches are full-duplex, so that\na switch and a node can each send frames to each other at the same time\nwithout interference. In other words, in a switch-based Ethernet LAN there\nare no collisions and, therefore, there is no need for a MAC protocol!\nAs we’ve seen, today’s Ethernets are very different from the original\nEthernet conceived by Metcalfe and Boggs more than 40 years ago—speeds\nhave increased by three orders of magnitude, Ethernet frames are carried\nover a variety of media, switched-Ethernets have become dominant, and\nnow even the MAC protocol is often unnecessary! Is all of this really still\nEthernet? The answer, of course, is “yes, by definition.” It is interesting to\nnote, however, that through all of these changes, there has indeed been one\nenduring constant that has remained unchanged over 30 years—Ethernet’s\nframe format. Perhaps this then is the one true and timeless centerpiece of\nthe Ethernet standard.\n6.4.3 Link-Layer Switches\nUp until this point, we have been purposefully vague about what a switch\nactually does and how it works. The role of the switch is to receive\nincoming link-layer frames and forward them onto outgoing links; we’ll\nstudy this forwarding function in detail in this subsection. We’ll see that the\nswitch itself is transparent to the hosts and routers in the subnet; that is, a\nhost/router addresses a frame to another host/router (rather than addressing\nthe frame to the switch) and happily sends the frame into the LAN, unaware\nthat a switch will be receiving the frame and forwarding it. The rate at\nwhich frames arrive to any one of the switch’s output interfaces may\ntemporarily exceed the link capacity of that interface. To accommodate this\nproblem, switch output interfaces have buffers, in much the same way that\nrouter output interfaces have buffers for datagrams. Let’s now take a closer\nlook at how switches operate.\nForwarding and Filtering\nFiltering is the switch function that determines whether a frame should be\nforwarded to some interface or should just be dropped. Forwarding is the\nswitch function that determines the interfaces to which a frame should be\ndirected, and then moves the frame to those interfaces. Switch filtering and\nforwarding are done with a switch table. The switch table contains entries\nfor some, but not necessarily all, of the hosts and routers on a LAN. An\nentry in the switch table contains (1)  a  MAC address, (2) the switch\ninterface that leads toward that MAC address, and (3) the time at which the\nentry was placed in the table. An example switch table for the uppermost\nswitch in Figure 6.15 is shown in Figure 6.22. This description of frame\nforwarding may sound similar to our discussion of datagram forwarding in\nChapter 4. Indeed, in our discussion of generalized forwarding in Section\n4.4, we learned that many modern packet switches can be configured to\nforward on the basis of layer-2 destination MAC addresses (i.e., function as\na layer-2 switch) or layer-3 IP destination addresses (i.e., function as a\nlayer-3 router). Nonetheless, we’ll make the important distinction that\nswitches forward packets based on MAC addresses rather than on IP\naddresses. We will also see that a traditional (i.e., in a non-SDN context)\nswitch table is constructed in a very different manner from a router’s\nforwarding table.\nFigure 6.22 ♦Portion of a switch table for the uppermost switch in\nFigure 6.15\nTo understand how switch filtering and forwarding work, suppose a\nframe with destination address DD-DD-DD-DD-DD-DD arrives at the\nAs we learned in Chapter 4, routers are store-and-forward packet switches\nthat forward packets using network-layer addresses. Although a switch is\nalso a store-and-forward packet switch, it is fundamentally different from a\nrouter in that it forwards packets using MAC addresses. Whereas a router is\na layer-3 packet switch, a switch is a layer-2 packet switch. Recall,\nhowever, that we learned in Section 4.4 that modern switches using the\n“match plus action” operation can be used to forward a layer-2 frame based\non the frame's destination MAC address, as well as a layer-3 datagram\nusing the datagram's destination IP address. Indeed, we saw that switches\nusing the OpenFlow standard can perform generalized packet forwarding\nbased on any of eleven different frame, datagram, and transport-layer\nheader fields.\nEven though switches and routers are fundamentally different, network\nadministrators must often choose between them when installing an\ninterconnection device. For example, for the network in Figure 6.15, the\nnetwork administrator could just as easily have used a router instead of a\nswitch to connect the department LANs, servers, and internet gateway\nrouter. Indeed, a router would permit interdepartmental communication\nwithout creating collisions. Given that both switches and routers are\ncandidates for interconnection devices, what are the pros and cons of the\ntwo approaches?\nFirst consider the pros and cons of switches. As mentioned above,\nswitches are plug-and-play, a property that is cherished by all the\noverworked network administrators of the world. Switches can also have\nrelatively high filtering and forwarding rates—as shown in Figure 6.24,\nswitches have to process frames only up through layer 2, whereas routers\nhave to process datagrams up through layer 3. On the other hand, to prevent\nthe cycling of broadcast frames, the active topology of a switched network\nis restricted to a spanning tree. Also, a large switched network would\nrequire large ARP tables in the hosts and routers and would generate\nsubstantial ARP traffic and processing. Furthermore, switches are\nsusceptible to broadcast storms—if one host goes haywire and transmits an\nendless stream of Ethernet broadcast frames, the switches will forward all\nof these frames, causing the entire network to collapse.\nFigure 6.24 ♦Packet processing in switches, routers, and hosts\nNow consider the pros and cons of routers. Because network addressing\nis often hierarchical (and not flat, as is MAC addressing), packets do not\nnormally cycle through routers even when the network has redundant paths.\n(However, packets can cycle when router tables are misconfigured; but as\nwe learned in Chapter 4, IP uses a special datagram header field to limit the\ncycling.) Thus, packets are not restricted to a spanning tree and can use the\nbest path between source and destination. Because routers do not have the\nspanning tree restriction, they have allowed the Internet to be built with a\nrich topology that includes, for example, multiple active links between\nEurope and North America. Another feature of routers is that they provide\nfirewall protection against layer-2 broadcast storms. Perhaps the most\nsignificant drawback of routers, though, is that they are not plug-and-play—\nthey and the hosts that connect to them need their IP addresses to be\nconfigured. Also, routers often have a larger per-packet processing time\nthan switches, because they have to process up through the layer-3 fields.\nFinally, there are two different ways to pronounce the word router, either as\n“rootor” or as “rowter,” and people waste a lot of time arguing over the\nproper pronunciation [Perlman 1999].\nGiven that both switches and routers have their pros and cons (as\nsummarized in Table 6.1), when should an institutional network (for\nexample, a university campus network or a corporate campus network) use\nswitches, and when should it use routers? Typically, small networks\nconsisting of a few hundred hosts have a few LAN segments. Switches\nsuffice for these small networks, as they localize traffic and increase\naggregate throughput without requiring any configuration of IP addresses.\nBut larger networks consisting of thousands of hosts typically include\nrouters within the network (in addition to switches). The routers provide a\nmore robust isolation of traffic, control broadcast storms, and use more\n“intelligent” routes among the hosts in the network.\nTable 6.1 ♦Comparison of the typical features of popular\ninterconnection devices\nFor more discussion of the pros and cons of switched versus routed\nnetworks, as well as a discussion of how switched LAN technology can be\nextended to accommodate two orders of magnitude more hosts than today’s\nEthernets, see [Meyers 2004; Kim 2008].\n6.4.4 Virtual Local Area Networks (VLANs)\nIn our earlier discussion of Figure 6.15, we noted that modern institutional\nLANs are often configured hierarchically, with each workgroup\n(department) having its own switched LAN connected to the switched\nLANs of other groups via a switch hierarchy. While such a configuration\nworks well in an ideal world, the real world is often far from ideal. Three\ndrawbacks can be identified in the configuration in Figure 6.15:\nLack of traffic isolation. Although the hierarchy localizes group traffic\nto within a single switch, broadcast traffic (e.g., frames carrying ARP\nand DHCP messages or frames whose destination has not yet been\nlearned by a self-learning switch) must still traverse the entire\ninstitutional network. Limiting the scope of such broadcast traffic would\nimprove LAN performance. Perhaps more importantly, it also may be\ndesirable to limit LAN broadcast traffic for security/privacy reasons.\nFor example, if one group contains the company’s executive\nmanagement team and another group contains disgruntled employees\nrunning Wireshark packet sniffers, the network manager may well\nprefer that the executives’ traffic never even reaches employee hosts.\nThis type of isolation could be provided by replacing the center switch\nin Figure 6.15 with a router. We’ll see shortly that this isolation also can\nbe achieved via a switched (layer 2) solution.\nInefficient use of switches. If instead of three groups, the institution had\n10 groups, then 10 first-level switches would be required. If each group\nwere small, say less than 10 people, then a single 96-port switch would\nlikely be large enough to accommodate everyone, but this single switch\nwould not provide traffic isolation.\nManaging users. If an employee moves between groups, the physical\ncabling must be changed to connect the employee to a different switch\nin Figure 6.15. Employees belonging to two groups make the problem\neven harder.\nFortunately, each of these difficulties can be handled by a switch that\nsupports virtual local area networks (VLANs). As the name suggests, a\nswitch that supports VLANs allows multiple virtual local area networks to\nbe defined over a single physical local area network infrastructure. Hosts\nwithin a VLAN communicate with each other as if they (and no other hosts)\nwere connected to the switch. In a port-based VLAN, the switch’s ports\n(interfaces) are divided into groups by the network manager. Each group\nconstitutes a VLAN, with the ports in each VLAN forming a broadcast\ndomain (i.e., broadcast traffic from one port can only reach other ports in\nthe group). Figure 6.25 shows a single switch with 16 ports. Ports 2 to 8\nbelong to the EE VLAN, while ports 9 to 15 belong to the CS VLAN (ports\n1 and 16 are unassigned). This VLAN solves all of the difficulties noted\nabove—EE and CS VLAN frames are isolated from each other, the two\nswitches in Figure 6.15 have been replaced by a single switch, and if the\nuser at switch port 8 joins the CS Department, the network operator simply\nreconfigures the VLAN software so that port 8 is now associated with the\nCS VLAN. One can easily imagine how the VLAN switch is configured\nand operates—the network manager declares a port to belong to a given\nVLAN (with undeclared ports belonging to a default VLAN) using switch\nmanagement software, a table of port-to-VLAN mappings is maintained\nwithin the switch; and switch hardware only delivers frames between ports\nbelonging to the same VLAN.\nFigure 6.25 ♦A single switch with two configured VLANs\nBut by completely isolating the two VLANs, we have introduced a new\ndifficulty! How can traffic from the EE Department be sent to the CS\nDepartment? One way to handle this would be to connect a VLAN switch\nport (e.g., port 1 in Figure 6.25) to an external router and configure that port\nto belong both the EE and CS VLANs. In this case, even though the EE and\nCS departments share the same physical switch, the logical configuration\nwould look as if the EE and CS departments had separate switches\nconnected via a router. An IP datagram going from the EE to the CS\ndepartment would first cross the EE VLAN to reach the router and then be\nforwarded by the router back over the CS VLAN to the CS host.\nFortunately, switch vendors make such configurations easy for the network\nmanager by building a single device that contains both a VLAN switch and\na router, so a separate external router is not needed. A homework problem at\nthe end of the chapter explores this scenario in more detail.\nReturning again to Figure 6.15, let’s now suppose that rather than\nhaving a separate Computer Engineering department, some EE and CS\nfaculty are housed in a separate building, where (of course!) they need\nnetwork access, and (of course!) they’d like to be part of their department’s\nVLAN. Figure 6.26 shows a second 8-port switch, where the switch ports\nhave been defined as belonging to the EE or the CS VLAN, as needed. But\nhow should these two switches be interconnected? One easy solution would\nbe to define a port belonging to the CS VLAN on each switch (similarly for\nthe EE VLAN) and to connect these ports to each other, as shown in Figure\n6.26(a). This solution doesn’t scale, however, since N VLANS would\nrequire N ports on each switch simply to interconnect the two switches.\nFigure 6.26 ♦Connecting two VLAN switches with two VLANs: (a)\ntwo cables (b) trunked\nFigure 6.27 ♦Original Ethernet frame (top), 802.1Q-tagged\nEthernet VLAN frame (below)\nA more scalable approach to interconnecting VLAN switches is known\nas VLAN trunking. In the VLAN trunking approach shown in Figure\n6.26(b), a special port on each switch (port 16 on the left switch and port 1\non the right switch) is configured as a trunk port to interconnect the two\nVLAN switches. The trunk port belongs to all VLANs, and frames sent to\nany VLAN are forwarded over the trunk link to the other switch. But this\nraises yet another question: How does a switch know that a frame arriving\non a trunk port belongs to a particular VLAN? The IEEE has defined an\nextended Ethernet frame format, 802.1Q, for frames crossing a VLAN\ntrunk. As shown in Figure 6.27, the 802.1Q frame consists of the standard\nEthernet frame with a four-byte VLAN tag added into the header that\ncarries the identity of the VLAN to which the frame belongs. The VLAN\ntag is added into a frame by the switch at the sending side of a VLAN trunk,\nparsed, and removed by the switch at the receiving side of the trunk. The\nVLAN tag itself consists of a 2-byte Tag Protocol Identifier (TPID) field\n(with a fixed hexadecimal value of 81-00), a 2-byte Tag Control\nInformation field that contains a 12-bit VLAN identifier field, and a 3-bit\npriority field that is similar in intent to the IP datagram TOS field.\nIn this discussion, we’ve only briefly touched on VLANs and have\nfocused on port-based VLANs. We should also mention that VLANs can be\ndefined in several other ways. In MAC-based VLANs, the network manager\nspecifies the set of MAC addresses that belong to each VLAN; whenever a\ndevice attaches to a port, the port is connected into the appropriate VLAN\nbased on the MAC address of the device. VLANs can also be defined based\non network-layer protocols (e.g., IPv4, IPv6, or Appletalk) and other\ncriteria. It is also possible for VLANs to be extended across IP routers,\nallowing islands of LANs to be connected together to form a single VLAN\nthat could span the globe [Yu 2011]. See the 802.1Q standard [IEEE 802.1q\n2005] for more details.\n6.5 Link Virtualization: A Network as a Link Layer\nBecause this chapter concerns link-layer protocols, and given that we’re\nnow nearing the chapter’s end, let’s reflect on how our understanding of the\nterm link has evolved. We began this chapter by viewing the link as a\nphysical wire connecting two communicating hosts. In studying multiple\naccess protocols, we saw that multiple hosts could be connected by a shared\nwire and that the “wire” connecting the hosts could be radio spectra or other\nmedia. This led us to consider the link a bit more abstractly as a channel,\nrather than as a wire. In our study of Ethernet LANs (Figure 6.15), we saw\nthat the interconnecting media could actually be a rather complex switched\ninfrastructure. Throughout this evolution, however, the hosts themselves\nmaintained the view that the interconnecting medium was simply a link-\nlayer channel connecting two or more hosts. We saw, for example, that an\nEthernet host can be blissfully unaware of whether it is connected to other\nLAN hosts by a single short LAN segment (Figure 6.17) or by a\ngeographically dispersed switched LAN (Figure 6.15) or by a VLAN\n(Figure 6.26).\nIn the case of a dialup modem connection between two hosts, the link\nconnecting the two hosts is actually the telephone network—a logically\nseparate, global telecommunications network with its own switches, links,\nand protocol stacks for data transfer and signaling. From the Internet link-\nlayer point of view, however, the dial-up connection through the telephone\nnetwork is viewed as a simple “wire.” In this sense, the Internet virtualizes\nthe telephone network, viewing the telephone network as a link-layer\ntechnology providing link-layer connectivity between two Internet hosts.\nYou may recall from our discussion of overlay networks in Chapter 2 that\nan overlay network similarly views the Internet as a means for providing\nconnectivity between overlay nodes, seeking to overlay the Internet in the\nsame way that the Internet overlays the telephone network.\nIn this section, we’ll consider Multiprotocol Label Switching (MPLS)\nnetworks. Unlike the circuit-switched telephone network, MPLS is a\npacket-switched, virtual-circuit network in its own right. It has its own\npacket formats and forwarding behaviors. Thus, from a pedagogical\nviewpoint, a discussion of MPLS fits well into a study of either the network\nlayer or the link layer. From an Internet viewpoint, however, we can\nconsider MPLS, like the telephone network and switched-­Ethernets, as a\nlink-layer technology that serves to interconnect IP devices. Thus, we’ll\nconsider MPLS in our discussion of the link layer. Frame-relay and ATM\nnetworks can also be used to interconnect IP devices, though they represent\na slightly older (but still deployed) technology and will not be covered here;\nsee the very readable book [Goralski 1999] for details. Our treatment of\nMPLS will be necessarily brief, as entire books could be (and have been)\nwritten on these networks. We recommend [Davie 2000] for details on\nMPLS. We’ll focus here primarily on how MPLS ­servers interconnect to IP\ndevices, although we’ll dive a bit deeper into the underlying technologies as\n6.5.1 Multiprotocol Label Switching (MPLS)\nMultiprotocol Label Switching (MPLS) evolved from a number of industry\nefforts in the mid-to-late 1990s to improve the forwarding speed of IP\nrouters by adopting a key concept from the world of virtual-circuit\nnetworks: a fixed-length label. The goal was not to abandon the destination-\nbased IP datagram-forwarding infrastructure for one based on fixed-length\nlabels and virtual circuits, but to augment it by selectively labeling\ndatagrams and allowing routers to forward datagrams based on fixed-length\nlabels (rather than destination IP addresses) when possible. Importantly,\nthese techniques work hand-in-hand with IP, using IP addressing and\nrouting. The IETF unified these efforts in the MPLS protocol [RFC 3031,\nRFC 3032], effectively blending VC techniques into a routed datagram\nLet’s begin our study of MPLS by considering the format of a link-\nlayer frame that is handled by an MPLS-capable router. Figure 6.28 shows\nthat a link-layer frame transmitted between MPLS-capable devices has a\nsmall MPLS header added between the layer-2 (e.g., Ethernet) header and\nlayer-3 (i.e., IP) header. RFC 3032 defines the format of the MPLS header\nfor such links; headers are defined for ATM and frame-relayed networks as\nwell in other RFCs. Among the fields in the MPLS header are the label, 3\nbits reserved for experimental use, a single S bit, which is used to indicate\nthe end of a series of “stacked” MPLS headers (an advanced topic that we’ll\nnot cover here), and a time-to-live field.\nFigure 6.28 ♦MPLS header: Located between link- and network-\nlayer headers\nIt’s immediately evident from Figure 6.28 that an MPLS-enhanced\nframe can only be sent between routers that are both MPLS capable (since a\nnon-MPLS-capable router would be quite confused when it found an MPLS\nheader where it had expected to find the IP header!). An MPLS-capable\nrouter is often referred to as a label-switched router, since it forwards an\nMPLS frame by looking up the MPLS label in its forwarding table and then\nimmediately passing the datagram to the appropriate output interface. Thus,\nthe MPLS-capable router need not extract the destination IP address and\nperform a lookup of the longest prefix match in the forwarding table. But\nhow does a router know if its neighbor is indeed MPLS capable, and how\ndoes a router know what label to associate with the given IP destination? To\nanswer these questions, we’ll need to take a look at the interaction among a\ngroup of MPLS-capable routers.\nIn the example in Figure 6.29, routers R1 through R4 are MPLS\ncapable. R5 and R6 are standard IP routers. R1 has advertised to R2 and R3\nthat it (R1) can route to destination A, and that a received frame with MPLS\nlabel 6 will be forwarded to destination A. Router R3 has advertised to\nrouter R4 that it can route to destinations A and D, and that incoming\nframes with MPLS labels 10 and 12, respectively, will be switched toward\nthose destinations. Router R2 has also advertised to router R4 that it (R2)\ncan reach destination A, and that a received frame with MPLS label 8 will\nbe switched toward A. Note that router R4 is now in the interesting position\nof having two MPLS paths to reach A: via interface 0 with outbound MPLS\nlabel 10, and via interface 1 with an MPLS label of 8. The broad picture\npainted in Figure 6.29 is that IP devices R5, R6, A, and D are connected\ntogether via an MPLS infrastructure (MPLS-capable routers R1, R2, R3,\nand R4) in much the same way that a switched LAN or an ATM network\ncan connect together IP devices. And like a switched LAN or ATM network,\nthe MPLS-capable routers R1 through R4 do so without ever touching the\nIP header of a packet.\nFigure 6.29 ♦MPLS-enhanced forwarding\nIn our discussion above, we’ve not specified the specific protocol used\nto distribute labels among the MPLS-capable routers, as the details of this\nsignaling are well beyond the scope of this book. We note, however, that the\nIETF working group on MPLS has specified in [RFC 3468] that an\nextension of the RSVP protocol, known as RSVP-TE [RFC 3209], will be\nthe focus of its efforts for MPLS signaling. We’ve also not discussed how\nMPLS actually computes the paths for packets among MPLS capable\nrouters, nor how it gathers link-state information (e.g., amount of link\nbandwidth unreserved by MPLS) to use in these path computations.\nExisting link-state routing algorithms (e.g., OSPF) have been extended to\nflood this information to MPLS-capable routers. Interestingly, the actual\npath computation algorithms are not standardized, and are currently vendor-\nThus far, the emphasis of our discussion of MPLS has been on the fact\nthat MPLS performs switching based on labels, without needing to consider\nthe IP address of a packet. The true advantages of MPLS and the reason for\ncurrent interest in MPLS, however, lie not in the potential increases in\nswitching speeds, but rather in the new traffic management capabilities that\nMPLS enables. As noted above, R4 has two MPLS paths to A. If forwarding\nwere performed up at the IP layer on the basis of IP address, the IP routing\nprotocols we studied in Chapter 5 would specify only a single, least-cost\npath to A. Thus, MPLS provides the ability to forward packets along routes\nthat would not be possible using standard IP routing protocols. This is one\nsimple form of traffic engineering using MPLS [RFC 3346; RFC 3272;\nRFC 2702; Xiao 2000], in which a network operator can override normal IP\nrouting and force some of the traffic headed toward a given destination\nalong one path, and other traffic destined toward the same destination along\nanother path (whether for policy, performance, or some other reason).\nIt is also possible to use MPLS for many other purposes as well. It can\nbe used to perform fast restoration of MPLS forwarding paths, e.g., to\nreroute traffic over a precomputed failover path in response to link failure\n[Kar 2000; Huang 2002; RFC 3469]. Finally, we note that MPLS can, and\nhas, been used to implement so-called ­virtual private networks (VPNs). In\nimplementing a VPN for a customer, an ISP uses its MPLS-enabled\nnetwork to connect together the customer’s various networks. MPLS can be\nused to isolate both the resources and addressing used by the customer’s\nVPN from that of other users crossing the ISP’s network; see [DeClercq\n2002] for details.\nOur discussion of MPLS has been brief, and we encourage you to\nwe studied in Chapter 5, and that many of MPLS’ traffic engineering\ncapabilities can also be achieved via SDN and the generalized forwarding\nparadigm we studied in Chapter 4. Only the future will tell whether MPLS\nand SDN will continue to co-exist, or whether newer technologies (such as\nSDN) will eventually replace MPLS.\n6.6 Data Center Networking\nInternet companies such as Google, Microsoft, Amazon, and Alibaba have\nbuilt massive data centers, each housing tens to hundreds of thousands of\nhosts. As briefly discussed in the sidebar in Section 1.2, data centers are not\nonly connected to the Internet, but also internally include complex\ncomputer networks, called data center networks, which interconnect their\ninternal hosts. In this section, we provide a brief introduction to data center\nnetworking for cloud applications.\nBroadly speaking, data centers serve three purposes. First, they provide\ncontent such as Web pages, search results, e-mail, or streaming video to\nusers. Second, they serve as massively-parallel computing infrastructures\npresented in Chapter 4 are actually necessary.)\n6.7.2 Still Getting Started: DNS and ARP\nWhen Bob types the URL for www.google.com into his Web browser, he\nbegins the long chain of events that will eventually result in Google’s home\npage being displayed by his Web browser. Bob’s Web browser begins the\nprocess by creating a TCP socket (Section 2.7) that will be used to send the\nHTTP request (Section 2.2) to  www.google.com. In order to create the\nsocket, Bob’s laptop will need to know the IP address of www.google.com.\nWe learned in Section 2.5, that the DNS ­protocol is used to provide this\nname-to-IP-address translation service.\n8. The operating system on Bob’s laptop thus creates a DNS query\nmessage (Section 2.5.3), putting the string “www.google.com” in the\nquestion section of the DNS message. This DNS message is then placed\nwithin a UDP segment with a destination port of 53 (DNS server). The\nUDP segment is then placed within an IP datagram with an IP\ndestination address of 68.87.71.226 (the address of the DNS server\nreturned in the DHCP ACK in step 5) and a source IP address of\n68.85.2.101.\n9. Bob’s laptop then places the datagram containing the DNS query\nmessage in an Ethernet frame. This frame will be sent (addressed, at the\nlink layer) to the gateway router in Bob’s school’s network. However,\neven though Bob’s laptop knows the IP address of the school’s gateway\nrouter (68.85.2.1) via the DHCP ACK message in step 5 above, it\ndoesn’t know the gateway router’s MAC address. In order to obtain the\nMAC address of the gateway router, Bob’s ­laptop will need to use the\nARP protocol (Section 6.4.1).\n10. Bob’s laptop creates an ARP query message with a target IP address of\n68.85.2.1 (the default gateway), places the ARP message within an\nEthernet frame with a broadcast destination address\n(FF:FF:FF:FF:FF:FF) and sends the Ethernet frame to the switch, which\ndelivers the frame to all connected devices, including the gateway router.\n11. The gateway router receives the frame containing the ARP query\nmessage on the interface to the school network, and finds that the target\nIP address of 68.85.2.1 in the ARP message matches the IP address of its\ninterface. The gateway router thus prepares an ARP reply, indicating that\nits MAC address of 00:22:6B:45:1F:1B corresponds to IP address\n68.85.2.1. It places the ARP reply message in an Ethernet frame, with a\ndestination address of 00:16:D3:23:68:8A (Bob’s laptop) and sends the\nframe to the switch, which delivers the frame to Bob’s laptop.\n12. Bob’s laptop receives the frame containing the ARP reply message and\nextracts the MAC address of the gateway router (00:22:6B:45:1F:1B)\nfrom the ARP reply message.\n13. Bob’s laptop can now (finally!) address the Ethernet frame containing\nthe DNS query to the gateway router’s MAC address. Note that the IP\ndatagram in this frame has an IP destination address of 68.87.71.226 (the\nDNS server), while the frame has a destination address of\n00:22:6B:45:1F:1B (the gateway router). Bob’s laptop sends this frame to\nthe switch, which delivers the frame to the gateway router.\n6.7.3 Still Getting Started: Intra-Domain Routing to\nthe DNS Server\n14. The gateway router receives the frame and extracts the IP datagram\ncontaining the DNS query. The router looks up the destination address of\nthis datagram (68.87.71.226) and determines from its forwarding table\nthat the datagram should be sent to the leftmost router in the Comcast\nnetwork in Figure 6.32. The IP datagram is placed inside a link-layer\nframe appropriate for the link connecting the school’s router to the\nleftmost Comcast router and the frame is sent over this link.\n15. The leftmost router in the Comcast network receives the frame, extracts\nthe IP datagram, examines the datagram’s destination address\n(68.87.71.226) and determines the outgoing interface on which to\nforward the datagram toward the DNS server from its forwarding table,\nwhich has been filled in by ­Comcast’s intra-domain protocol (such as\nRIP, OSPF or IS-IS, Section 5.3) as well as the Internet’s inter-\ndomain protocol, BGP (Section 5.4).\n16. Eventually the IP datagram containing the DNS query arrives at the\nDNS server. The DNS server extracts the DNS query message, looks up\nthe name www.google.com in its DNS database (Section 2.5), and finds\nthe DNS resource record that contains the IP address (64.233.169.105)\nfor www.google.com. (assuming that it is currently cached in the DNS\nserver). Recall that this cached data originated in the authoritative DNS\nserver (Section 2.5) for google.com. The DNS server forms a DNS reply\nmessage containing this hostname-to-IP-address mapping, and places the\nDNS reply message in a UDP segment, and the segment within an IP\ndatagram addressed to Bob’s laptop (68.85.2.101). This datagram will be\nforwarded back through the Comcast network to the school’s router and\nfrom there, via the Ethernet switch to Bob’s laptop.\n17. Bob’s laptop extracts the IP address of the server www.google.com from\nthe DNS message. Finally, after a lot of work, Bob’s laptop is now ready\nto contact the www.google.com server!\n6.7.4 Web Client-Server Interaction: TCP and HTTP\n18. Now that Bob’s laptop has the IP address of www.google.com, it can\ncreate the TCP socket (Section 2.7) that will be used to send the HTTP\nGET message (Section 2.2.3) to www.google.com. When Bob creates the\nTCP socket, the TCP in Bob’s laptop must first perform a three-way\nhandshake (Section 3.5.6) with the TCP in www.google.com. Bob’s\nlaptop thus first creates a TCP SYN segment with destination port 80\n(for HTTP), places the TCP segment inside an IP datagram with a\ndestination IP address of 64.233.169.105 (www.google.com), places the\ndatagram inside a frame with a destination MAC address of\n00:22:6B:45:1F:1B (the gateway router) and sends the frame to the\n19. The routers in the school network, Comcast’s network, and Google’s\nnetwork forward the datagram containing the TCP SYN toward\nwww.google.com, using the forwarding table in each router, as in steps\n14–16 above. Recall that the router forwarding table entries governing\nforwarding of packets over the inter-domain link between the Comcast\nand Google networks are determined by the BGP protocol (Chapter 5).\n20. Eventually, the datagram containing the TCP SYN arrives at\nwww.google.com. The TCP SYN message is extracted from the datagram\nand demultiplexed to the welcome socket associated with port 80. A\nconnection socket (Section 2.7) is created for the TCP connection\nbetween the Google HTTP server and Bob’s laptop. A TCP SYNACK\n(Section 3.5.6) segment is generated, placed inside a datagram addressed\nto Bob’s laptop, and finally placed inside a link-layer frame appropriate\nfor the link connecting www.google.com to its first-hop router.\n21. The datagram containing the TCP SYNACK segment is forwarded\nthrough the Google, Comcast, and school networks, eventually arriving\nat the Ethernet controller in Bob’s laptop. The datagram is demultiplexed\nwithin the operating system to the TCP socket created in step 18, which\nenters the connected state.\n22. With the socket on Bob’s laptop now (finally!) ready to send bytes to\nwww.google.com, Bob’s browser creates the HTTP GET message\n(Section 2.2.3) containing the URL to be fetched. The HTTP GET\nmessage is then written into the socket, with the GET message becoming\nthe payload of a TCP segment. The TCP segment is placed in a datagram\nand sent and delivered to www.google.com as in steps 18–20 above.\n23. The HTTP server at www.google.com reads the HTTP GET message\nfrom the TCP socket, creates an HTTP response message (Section 2.2),\nplaces the requested Web page content in the body of the HTTP response\nmessage, and sends the message into the TCP socket.\n24. The datagram containing the HTTP reply message is forwarded through\nthe Google, Comcast, and school networks, and arrives at Bob’s laptop.\nBob’s Web browser program reads the HTTP response from the socket,\nextracts the html for the Web page from the body of the HTTP response,\nand finally (finally!) displays the Web page!\nOur scenario above has covered a lot of networking ground! If you’ve\nunderstood most or all of the above example, then you’ve also covered a lot\nof ground since you first read Section 1.1, where we wrote “much of this\nbook is concerned with computer network protocols” and you may have\nwondered what a protocol actually was! As detailed as the above example\nmight seem, we’ve omitted a number of possible additional protocols (e.g.,\nNAT running in the school’s gateway router, wireless access to the school’s\nnetwork, security protocols for accessing the school network or encrypting\nprotocols), \nconsiderations (Web caching, the DNS hierarchy) that one would encounter\nin the public ­Internet. We’ll cover a number of these topics and more in the\nsecond part of this book.\nLastly, we note that our example above was an integrated and holistic,\nbut also very “nuts and bolts,” view of many of the protocols that we’ve\nstudied in the first part of this book. The example focused more on the\n“how” than the “why.” For a broader, more reflective view on the design of\nnetwork protocols in general, you might want to re-read the “Architectural\nand in Chapter 1 (our discussion of physical media in Section 1.2). We’ll\nconsider the physical layer again when we study wireless link\ncharacteristics in the next chapter.\nAlthough our journey down the protocol stack is over, our study of\ncomputer networking is not yet at an end. In the following three chapters,\nwe cover wireless networking, network security, and multimedia\nnetworking. These four topics do not fit conveniently into any one layer;\nindeed, each topic crosscuts many layers. Understanding these topics (billed\nas advanced topics in some networking texts) thus requires a firm\nfoundation in all layers of the protocol stack—a foundation that our study\nof the link layer has now completed!\nHomework Problems and Questions\nChapter 6 Review Questions\nSECTION 6.1-6.2\nR1. What is framing in link layer?\nR2. If all the links in the Internet were to provide reliable delivery\nservice, would the TCP reliable delivery service be redundant? Why\nor why not?\nR3. Name three error-detection strategies employed by link layer.\nSECTION 6.3\nR4. Suppose two nodes start to transmit at the same time a packet of\nlength L over a broadcast channel of rate R. Denote the propagation\ndelay between the two nodes as d\n. Will there be a collision if d\nL/R? Why or why not?\nR5. In Section 6.3, we listed four desirable characteristics of a broadcast\nchannel. Which of these characteristics does slotted ALOHA have?\nWhich of these characteristics does token passing have?\nR6. In CSMA/CD, after the fifth collision, what is the probability that a\nnode chooses K = 4? The result K = 4 corresponds to a delay of how\nmany ­seconds on a 10 Mbps Ethernet?\nR7. While TDM and FDM assign time slots and frequencies, CDMA\nassigns a ­different code to each node. Explain the basic principle in\nwhich CDMA works.\nR8. Why does collision occur in CSMA, if all nodes perform carrier\nsensing before transmission?\nSECTION 6.4\nR9. How big is the MAC address space? The IPv4 address space? The\nIPv6 address space?\nR10. Suppose nodes A, B, and C each attach to the same broadcast LAN\n(through their adapters). If A sends thousands of IP datagrams to B\nwith each encapsulating frame addressed to the MAC address of B,\nwill C’s adapter process these frames? If so, will C’s adapter pass the\nIP datagrams in these frames to the network layer C? How would\nyour answers change if A sends frames with the MAC broadcast\nR11. IEEE manages the MAC address space, allocating chunks of it to\ncompanies manufacturing network adapters. The first half of the bits\nof the addresses in these chunks are fixed, ensuring that the address\nspace is unique. How long will a chunk last for a company\nmanufacturing 1,000,000 network adapters per year?\nR12. For the network in Figure 6.19, the router has two ARP modules,\neach with its own ARP table. Is it possible that the same MAC\naddress appears in both tables?\nR13. What is a hub used for?\nR14. Consider Figure 6.15. How many subnetworks are there, in the\naddressing sense of Section 4.3?\nR15. Each host and router has an ARP table in its memory. What are the\nRecall from Chapter 6 that when hosts communicate over a shared medium,\na protocol is needed so that the signals sent by multiple senders do not\ninterfere at the receivers. In Chapter 6, we described three classes of\nmedium access protocols: channel partitioning, random access, and taking\nturns. Code division multiple access (CDMA) belongs to the family of\nchannel partitioning protocols. It is prevalent in wireless LAN and cellular\ntechnologies. Because CDMA is so important in the wireless world, we’ll\ntake a quick look at CDMA now, before getting into specific wireless access\ntechnologies in the subsequent sections.\nIn a CDMA protocol, each bit being sent is encoded by multiplying the\nbit by a signal (the code) that changes at a much faster rate (known as the\nchipping rate) than the original sequence of data bits. Figure 7.5 shows a\nsimple, idealized CDMA encoding/decoding scenario. Suppose that the rate\nat which original data bits reach the CDMA encoder defines the unit of\ntime; that is, each original data bit to be transmitted requires a one-bit slot\ntime. Let d  be the value of the data bit for the ith bit slot. For mathematical\nconvenience, we represent a data bit with a 0 value as −1. Each bit slot is\nfurther subdivided into M mini-slots; in Figure 7.5, M = 8, although in\npractice M is much larger. The CDMA code used by the sender consists of a\nsequence of M values, c , m = 1, . . . , M, each taking a +1 or −1 value. In\nthe example in Figure 7.5, the M-bit CDMA code being used by the sender\nis (1, 1, 1, −1, 1, −1, −1, −1).\nFigure 7.5 ♦A simple CDMA example: Sender encoding, receiver\nTo illustrate how CDMA works, let us focus on the ith data bit, d . For\nthe mth mini-slot of the bit-transmission time of d , the output of the CDMA\nencoder, Z , is the value of d  multiplied by the mth bit in the assigned\nCDMA code, c :\nZi,m = di ⋅cm\nIn a simple world, with no interfering senders, the receiver would receive\nthe encoded bits, Z , and recover the original data bit, d , by computing:\nThe reader might want to work through the details of the example in Figure\n7.5 to see that the original data bits are indeed correctly recovered at the\nreceiver using Equation 7.2.\nThe world is far from ideal, however, and as noted above, CDMA must\nwork in the presence of interfering senders that are encoding and\ntransmitting their data using a different assigned code. But how can a\nCDMA receiver recover a sender’s original data bits when those data bits\nare being tangled with bits being transmitted by other senders? CDMA\nworks under the assumption that the interfering transmitted bit signals are\nadditive. This means, for example, that if three senders send a 1 value, and a\nfourth sender sends a −1 value during the same mini-slot, then the received\nsignal at all receivers during that mini-slot is a 2 (since 1 + 1 + 1 − 1 = 2).\nIn the presence of multiple senders, sender s computes its encoded\ntransmissions, Z s\ni,m, in exactly the same manner as in Equation 7.1. The\nvalue received at a receiver during the\nmth mini-slot of the ith bit slot, however, is now the sum of the transmitted\nbits from all N senders during that mini-slot:\nAmazingly, if the senders’ codes are chosen carefully, each receiver can\nrecover the data sent by a given sender out of the aggregate signal simply\nby using the sender’s code in exactly the same manner as in Equation 7.2:\nas shown in Figure 7.6, for a two-sender CDMA example. The M-bit\nCDMA code being used by the upper sender is (1, 1, 1, −1, 1, −1, −1, −1),\nwhile the CDMA code being used by the lower sender is (1, −1, 1, 1, 1, −1,\n1, 1). Figure 7.6 illustrates a receiver recovering the original data bits from\nthe upper sender. Note that the receiver is able to extract the data from\nsender 1 in spite of the interfering transmission from sender 2.\nFigure 7.6 ♦A two-sender CDMA example\nRecall our cocktail analogy from Chapter 6. A CDMA protocol is\nsimilar to having partygoers speaking in multiple languages; in such\ncircumstances humans are actually quite good at locking into the\nconversation in the language they understand, while filtering out the\nremaining conversations. We see here that CDMA is a partitioning protocol\nin that it partitions the codespace (as opposed to time or frequency) and\nassigns each node a dedicated piece of the codespace.\nOur discussion here of CDMA is necessarily brief; in practice a number\nof difficult issues must be addressed. First, in order for the CDMA receivers\nto be able to extract a particular sender’s signal, the CDMA codes must be\ncarefully chosen. ­Second, our discussion has assumed that the received\nsignal strengths from various senders are the same; in reality, this can be\ndifficult to achieve. There is a considerable body of literature addressing\nthese and other issues related to CDMA; see ­[Pickholtz 1982; Viterbi 1995]\nfor details.\n7.3 WiFi: 802.11 Wireless LANs\nPervasive in the workplace, the home, educational institutions, cafés,\nairports, and street corners, wireless LANs are now one of the most\nimportant access network technologies in the Internet today. Although many\ntechnologies and standards for wireless LANs were developed in the 1990s,\none particular class of standards has clearly emerged as the winner: the\nIEEE 802.11 wireless LAN, also known as WiFi. In this section, we’ll take\na close look at 802.11 wireless LANs, examining its frame structure, its\nmedium access protocol, and its internetworking of 802.11 LANs with\nwired Ethernet LANs.\nAs summarized in Table 7.1, there are several 802.11 standards [IEEE\n802.11 2020]. The 802.11 b, g, n, ac, ax are successive generations of\n802.11 technology aimed for wireless local area networks (WLANs),\ntypically less than 70 m range in a home office, workplace, or business\nsetting. The 802.11 n, ac, and ax standards have recently been branded as\nWiFi 4, 5 and 6, respectively—no doubt competing with 4G and 5G cellular\nnetwork branding. The 802.11 af, ah standards operate over longer distances\nand are aimed at Internet of Things, sensor networks, and metering\napplications.\nThe different 802.11 b, g, n, ac, ax standards all share some common\ncharacteristics, including the 802.11 frame format that we will study shortly,\nand are backward compatible, meaning, for example, that a mobile capable\nonly of 802.11 g may still interact with a newer 802.11 ac or 802.11 ax base\nstation. They also all use the same medium access protocol, CSMA/CA,\nwhich we’ll also discuss shortly, while also 802.11 ax also supports\ncentralized scheduling by the base station of transmissions from associated\nwireless devices.\nHowever, as shown in Table 7.1, the standards have some major\ndifferences at the physical layer. 802.11 devices operate in two different\nfrequency ranges: 2.4–2.485 GHz (referred to as the 2.4 GHz range) and\n5.1–5.8 GHz (referred to as the 5 GHz range). The 2.4 GHz range is an\nunlicensed frequency band, where 802.11 devices may compete for\nfrequency spectrum with 2.4 GHz phones and appliances such as\nmicrowave ovens. At 5 GHz, 802.11 LANs have a shorter transmission\ndistance for a given power level and suffer more from multipath\npropagation. The 802.11n, 802.11ac, and 802.11ax standards use multiple\ninput multiple-output (MIMO) antennas; that is, two or more antennas on\nthe sending side and two or more antennas on the receiving side that are\ntransmitting/receiving different signals [Diggavi 2004]. 802.11ac and\n802.11 ax base stations may transmit to multiple stations simultaneously,\nand use “smart” antennas to adaptively beamform to target transmissions in\nthe direction of a receiver. This decreases interference and increases the\ndistance reached at a given data rate. The data rates shown in Table 7.1 are\nfor an idealized environment, for example, a receiver close to the base\nstation, with no ­interference—a scenario that we’re unlikely to experience\nin practice! So as the saying goes, YMMV: Your Mileage (or in this case\nyour wireless data rate) May Vary.\nTable 7.1 ♦Summary of IEEE 802.11 standards\n7.3.1 The 802.11 Wireless LAN Architecture\nFigure 7.7 illustrates the principal components of the 802.11 wireless LAN\narchitecture. The fundamental building block of the 802.11 architecture is\nthe basic service set (BSS). A BSS contains one or more wireless stations\nand a central base station, known as an access point (AP) in 802.11\nparlance. Figure 7.7 shows the AP in each of two BSSs connecting to an\ninterconnection device (such as a switch or router), which in turn leads to\nthe Internet. In a typical home network, there is one AP and one router\n(typically integrated together as one unit) that connects the BSS to the\nFigure 7.7 ♦IEEE 802.11 LAN architecture\nAs with Ethernet devices, each 802.11 wireless station has a 6-byte\nMAC address that is stored in the firmware of the station’s adapter (that is,\n802.11 network interface card). Each AP also has a MAC address for its\nwireless interface. As with Ethernet, these MAC addresses are administered\nby IEEE and are (in theory) ­globally unique.\nAs noted in Section 7.1, wireless LANs that deploy APs are often\nreferred to as infrastructure wireless LANs, with the “infrastructure”\nbeing the APs along with the wired Ethernet infrastructure that\ninterconnects the APs and a router. Figure 7.8 shows that IEEE 802.11\nstations can also group themselves together to form an ad hoc network—a\nnetwork with no central control and with no connections to the ­“outside\nworld.” Here, the network is formed “on the fly,” by mobile devices that\nhave found themselves in proximity to each other, that have a need to\ncommunicate, and that find no preexisting network infrastructure in their\nlocation. An ad hoc network might be formed when people with laptops get\ntogether (e.g., in a conference room, a train, or a car) and want to exchange\ndata in the absence of a centralized AP. There has been tremendous interest\nin ad hoc networking, as communicating portable devices continue to\nproliferate. In this section, though, we’ll focus our attention on\ninfrastructure wireless LANs.\nFigure 7.8 ♦An IEEE 802.11 ad hoc network\nChannels and Association\nIn 802.11, each wireless station needs to associate with an AP before it can\nsend or receive network-layer data. Although all of the 802.11 standards use\nassociation, we’ll discuss this topic specifically in the context of IEEE\n802.11b, g, n, ac, ax.\nWhen a network administrator installs an AP, the administrator assigns\na one- or two-word Service Set Identifier (SSID) to the access point.\n(When you choose Wi-Fi under Setting on your iPhone, for example, a list\nis displayed showing the SSID of each AP in range.) The administrator must\nalso assign a channel number to the AP. To understand channel numbers,\nrecall that 802.11 operates in the frequency range of 2.4 GHz to 2.4835\nGHz. Within this 85 MHz band, 802.11 defines 11 partially overlapping\nchannels. Any two channels are non-overlapping if and only if they are\nseparated by four or more channels. In particular, the set of channels 1, 6,\nand 11 is the only set of three non-overlapping channels. This means that an\nadministrator could create a wireless LAN with an aggregate maximum\ntransmission rate of three times the maximum transmission rate shown in\nTable 7.1 by installing three 802.11 APs at the same physical location,\nassigning channels 1, 6, and 11 to the APs, and interconnecting each of the\nAPs with a switch.\nNow that we have a basic understanding of 802.11 channels, let’s\ndescribe an interesting (and not completely uncommon) situation—that of a\nWiFi jungle. A WiFi jungle is any physical location where a wireless\nstation receives a sufficiently strong signal from two or more APs. For\nexample, in many cafés in New York City, a wireless station can pick up a\nsignal from numerous nearby APs. One of the APs might be managed by\nthe café, while the other APs might be in residential apartments near the\ncafé. Each of these APs would likely be located in a different IP subnet and\nwould have been independently assigned a channel.\nNow suppose you enter such a WiFi jungle with your smartphone,\ntablet, or ­laptop, seeking wireless Internet access and a blueberry muffin.\nSuppose there are five APs in the WiFi jungle. To gain Internet access, your\nwireless device needs to join exactly one of the subnets and hence needs to\nassociate with exactly one of the APs. ­Associating means the wireless\ndevice creates a virtual wire between itself and the AP. Specifically, only the\nassociated AP will send data frames (that is, frames containing data, such as\na datagram) to your wireless device, and your wireless device will send data\nframes into the Internet only through the associated AP. But how does your\nwireless device associate with a particular AP? And more fundamentally,\nhow does your wireless device know which APs, if any, are out there in the\nThe 802.11 standard requires that an AP periodically send beacon\nframes, each of which includes the AP’s SSID and MAC address. Your\nwireless device, knowing that APs are sending out beacon frames, scans the\n11 channels, seeking beacon frames from any APs that may be out there\n(some of which may be transmitting on the same channel—it’s a jungle out\nthere!). Having learned about available APs from the beacon frames, you\n(or your wireless device) select one of the APs for association.\nThe 802.11 standard does not specify an algorithm for selecting which\nof the available APs to associate with; that algorithm is left up to the\ndesigners of the 802.11 firmware and software in your wireless device.\nTypically, the device chooses the AP whose beacon frame is received with\nthe highest signal strength. While a high signal strength is good (see, e.g.,\nFigure 7.3), signal strength is not the only AP characteristic that will\ndetermine the performance a device receives. In particular, it’s possible that\nthe selected AP may have a strong signal, but may be overloaded with other\naffiliated devices (that will need to share the wireless bandwidth at that AP),\nwhile an unloaded AP is not selected due to a slightly weaker signal. A\nnumber of alternative ways of choosing APs have thus recently been\nproposed [Vasudevan 2005; Nicholson 2006; Sundaresan 2006]. For an\ninteresting and down-to-earth discussion of how signal strength is\nmeasured, see [Bardwell 2004].\nThe process of scanning channels and listening for beacon frames is\nknown as passive scanning (see Figure 7.9a). A wireless device can also\nperform active scanning, by broadcasting a probe frame that will be\nreceived by all APs within the wireless device’s range, as shown in Figure\n7.9b. APs respond to the probe request frame with a probe response frame.\nThe wireless device can then choose the AP with which to associate from\namong the responding APs.\nFigure 7.9 ♦Active and passive scanning for access points\nAfter selecting the AP with which to associate, the wireless device\nsends an association request frame to the AP, and the AP responds with an\nassociation response frame. Note that this second request/response\nhandshake is needed with active scanning, since an AP responding to the\ninitial probe request frame doesn’t know which of the (possibly many)\nresponding APs the device will choose to associate with, in much the same\nway that a DHCP client can choose from among multiple DHCP servers\n(see Figure 4.21). Once associated with an AP, the device will want to join\nthe subnet (in the IP addressing sense of Section 4.3.3) to which the AP\nbelongs. Thus, the device will typically send a DHCP discovery message\n(see Figure 4.21) into the subnet via the AP in order to obtain an IP address\non the subnet. Once the address is obtained, the rest of the world then views\nthat device simply as another host with an IP address in that subnet.\nIn order to create an association with a particular AP, the wireless\ndevice may be required to authenticate itself to the AP. 802.11 wireless\nLANs provide a number of alternatives for authentication and access. One\napproach, used by many companies, is to permit access to a wireless\nnetwork based on a device’s MAC address. A second approach, used by\nmany Internet cafés, employs usernames and passwords. In both cases, the\nAP typically communicates with an authentication server, relaying\ninformation between the wireless device and the authentication server using\na protocol such as RADIUS [RFC 2865] or DIAMETER [RFC 6733].\nSeparating the authentication server from the AP allows one authentication\nserver to serve many APs, centralizing the (often sensitive) decisions of\nauthentication and access within the single server, and keeping AP costs and\ncomplexity low. We’ll see in Chapter 8 that the new IEEE 802.11i protocol\ndefining security aspects of the 802.11 protocol family takes precisely this\n7.3.2 The 802.11 MAC Protocol\nOnce a wireless device is associated with an AP, it can start sending and\nreceiving data frames to and from the access point. But because multiple\nwireless devices, or the AP itself may want to transmit data frames at the\nsame time over the same channel, a multiple access protocol is needed to\ncoordinate the transmissions. In the following, we'll refer to the devices or\nthe AP as wireless “stations” that share the multiple access channel. As\ndiscussed in Chapter 6 and Section 7.2.1, broadly speaking there are three\nclasses of multiple access protocols: channel partitioning (including\nCDMA), random access, and taking turns. Inspired by the huge success of\nEthernet and its random access protocol, the designers of 802.11 chose a\nrandom access protocol for 802.11 wireless LANs. This random access\nprotocol is referred to as CSMA with collision avoidance, or more\nsuccinctly as CSMA/CA. As with Ethernet’s CSMA/CD, the “CSMA” in\nCSMA/CA stands for “carrier sense multiple access,” meaning that each\nstation senses the channel before transmitting, and refrains from\ntransmitting when the channel is sensed busy. Although both ­Ethernet and\n802.11 use carrier-sensing random access, the two MAC protocols have\nimportant differences. First, instead of using collision detection, 802.11 uses\ncollision-avoidance techniques. Second, because of the relatively high bit\nerror rates of wireless channels, 802.11 (unlike Ethernet) uses a link-layer\nin Chapter 3.\nRecall that the 802.11 protocol allows a transmitting station to reserve\nthe channel for a period of time that includes the time to transmit its data\nused or not (WEP is discussed in Chapter 8).\n7.3.4 Mobility in the Same IP Subnet\nIn order to increase the physical range of a wireless LAN, companies and\nuniversities will often deploy multiple BSSs within the same IP subnet. This\nnaturally raises the issue of mobility among the BSSs—how do wireless\nstations seamlessly move from one BSS to another while maintaining\nongoing TCP sessions? As we’ll see in this subsection, mobility can be\nhandled in a relatively straightforward manner when the BSSs are part of\nthe subnet. When stations move between subnets, more sophisticated\nmobility management protocols will be needed, such as those we’ll study in\nSections 7.5 and 7.6.\nLet’s now look at a specific example of mobility between BSSs in the\nsame subnet. Figure 7.15 shows two interconnected BSSs with a host, H1,\nmoving from BSS1 to BSS2. Because in this example the interconnection\ndevice that connects the two BSSs is not a router, all of the stations in the\ntwo BSSs, including the APs, belong to the same IP subnet. Thus, when H1\nmoves from BSS1 to BSS2, it may keep its IP address and all of its ongoing\nTCP connections. If the interconnection device were a router, then H1\nwould have to obtain a new IP address in the subnet in which it was\nmoving. This address change would disrupt (and eventually terminate) any\non-going TCP connections at H1. In Section 7.6, we’ll see how a network-\nlayer mobility protocol, such as mobile IP, can be used to avoid this\nFigure 7.15 ♦Mobility in the same subnet\nBut what specifically happens when H1 moves from BSS1 to BSS2? As\nH1 wanders away from AP1, H1 detects a weakening signal from AP1 and\nstarts to scan for a stronger signal. H1 receives beacon frames from AP2\n(which in many corporate and university settings will have the same SSID\nas AP1). H1 then disassociates with AP1 and associates with AP2, while\nkeeping its IP address and maintaining its ongoing TCP sessions.\nThis addresses the handover problem from the host and AP viewpoint.\nBut what about the switch in Figure 7.15? How does it know that the host\nhas moved from one AP to another? As you may recall from Chapter 6,\nswitches are “self-learning” and automatically build their forwarding tables.\nThis self-learning feature nicely handles occasional moves (for example,\nwhen an employee gets transferred from one department to another);\nhowever, switches were not designed to support highly mobile users who\nwant to maintain TCP connections while moving between BSSs. To\nappreciate the problem here, recall that before the move, the switch has an\nentry in its forwarding table that pairs H1’s MAC address with the outgoing\nswitch interface through which H1 can be reached. If H1 is initially in\nBSS1, then a datagram destined to H1 will be directed to H1 via AP1. Once\nH1 associates with BSS2, however, its frames should be directed to AP2.\nOne solution (a bit of a hack, really) is for AP2 to send a broadcast Ethernet\nframe with H1’s source address to the switch just after the new association.\nWhen the switch receives the frame, it updates its forwarding table,\nallowing H1 to be reached via AP2. The 802.11f standards group is\ndeveloping an inter-AP protocol to handle these and related issues.\nOur discussion above has focused on mobility with the same LAN\nsubnet. Recall that VLANs, which we studied in Section 6.4.4, can be used\nto connect together islands of LANs into a large virtual LAN that can span a\nlarge geographical region. Mobility among base stations within such a\nVLAN can be handled in exactly the same manner as above [Yu 2011].\nLOCATION DISCOVERY: GPS AND WIFI POSITIONING\nMany of the most useful and important smartphone apps today are location-based\nmobile apps, including Foursquare, Yelp, Uber, Pokémon Go, and Waze. These ­-\nsoftware apps all make use of an API that allows them to extract their current\ngeographical position directly from the smartphone. Have you ever wondered how your\nsmartphone obtains its geographical position? Today, it is done by combining two\nsystems, the Global Positioning System (GPS) and the WiFi Positioning System\nThe GPS, with a constellation of 30+ satellites, broadcasts satellite location and\ntiming information, which in turn is used by each GPS receiver to estimate its\ngeolocation. The United States government created the system, maintains it, and\nmakes it freely accessible to anyone with a GPS receiver. The satellites have very\nstable atomic clocks that are synchronized with one another and with ground clocks.\nThe satellites also know their locations with great precision. Each GPS satellite\ncontinuously broadcasts a radio signal containing its current time and position. If a\nGPS receiver obtains this information from at least four satellites, it can solve\ntriangulation equations to estimate its position.\nGPS, however, cannot always provide accurate geolocations if it does not have line-\nof-sight with at least four GPS satellites or when there is interference from other high-\nfrequency communication systems. This is particularly true in urban environments,\nwhere tall buildings frequently block GPS signals. This is where WiFi positioning ­-\nsystems come to the rescue. WiFi positioning systems make use of databases of WiFi\naccess points, which are independently maintained by various Internet companies,\nincluding Google, Apple, and Microsoft. Each database contains information about\nmillions of WiFi access points, including each access point’s SSID and an estimate of\nits geographic location. To understand how a WiFi positioning system makes use of\nsuch a database, consider an Android smartphone along with the Google location\nservice. From each nearby access point, the smartphone receives and measures the\nsignal strength of beacon signals (see Section 7.3.1), which contain the access point’s\nSSID. The smartphone can therefore continually send messages to the Google\nlocation service (in the cloud) that include the SSIDs of nearby access points and the\ncorresponding signal strengths. It will also send its GPS position (obtained via the\nsatellite broadcast signals, as described above) when available. Using the signal-\nstrength information, Google will estimate the distance between the smartphone and\neach of the WiFi access points. Leveraging these estimated distances, it can then\nsolve triangulation equations to estimate the smartphone’s geolocation. Finally, this\nWiFi-based estimate is combined with the GPS satellite-based estimate to form an\naggregate estimate, which is then sent back to the smartphone and used by the\nlocation-based mobile apps.\nBut you may still be wondering how Google (and Apple, Microsoft, and so on) obtain\nand maintain the database of access points, and in particular, the access point’s\ngeographic location? Recall that for a given access point, every nearby Android\nsmartphone will send to the Google location service the strength of the ­signal received\nfrom the access point as well as the smartphone’s estimated location. Given that\nthousands of smartphones may be passing by the access point during any single day,\nGoogle’s location service will have lots of data at its disposition to use in estimating the\naccess point’s position, again by solving triangulation equations. Thus, the access\npoints help the smartphones determine their locations, and in turn the smartphones\nhelp the access points determine their locations!\n7.3.5 Advanced Features in 802.11\nWe’ll wrap up our coverage of 802.11 with a short discussion of two\nadvanced capabilities found in 802.11 networks. As we’ll see, these\ncapabilities are not completely specified in the 802.11 standard, but rather\nare made possible by mechanisms specified in the standard. This allows\ndifferent vendors to implement these capabilities using their own\n(proprietary) approaches, presumably giving them an edge over the\ncompetition.\n802.11 Rate Adaptation\nWe saw earlier in Figure 7.3 that different modulation techniques (with the\ndifferent transmission rates that they provide) are appropriate for different\nSNR scenarios. Consider, for example, a mobile 802.11 user who is initially\n20 meters away from the base station, with a high signal-to-noise ratio.\nGiven the high SNR, the user can communicate with the base station using\na physical-layer modulation technique that provides high transmission rates\nwhile maintaining a low BER. This is one happy user! Suppose now that the\nuser becomes mobile, walking away from the base station, with the SNR\nfalling as the distance from the base station increases. In this case, if the\nmodulation technique used in the 802.11 protocol operating between the\nbase station and the user does not change, the BER will become\nunacceptably high as the SNR decreases, and eventually no transmitted\nframes will be received correctly.\nFor this reason, some 802.11 implementations have a rate adaptation\ncapability that adaptively selects the underlying physical-layer modulation\ntechnique to use based on current or recent channel characteristics. If a node\nIP protocol we studied in Chapter 4. As with earlier 2G and 3G networks, 4G LTE is full\nof rather obtuse acronyms and element names. We’ll try to cut through that jumble by\nfirst focusing on element functions and how the various elements of a 4G LTE network\ninteract with each other in both the data and the control planes:\nMobile Device. This is a smartphone, tablet, laptop, or IoT device that connects into\na cellular carrier’s network. This is where applications such as web browsers, map\napps, voice and videoconference apps, mobile payment apps, and so much more are\nrun. The mobile device typically implements the full 5-layer Internet protocol stack,\nincluding the transport and application layers, as we saw with hosts at the Internet’s\nnetwork edge. The mobile device is a network endpoint, with an IP address (obtained\nthrough NAT, as we’ll see). The mobile device also has a globally unique 64-bit\nidentifier called the International Mobile ­Subscriber Identity (IMSI), which is\nstored on its SIM (Subscriber Identity Module) card. The IMSI identifies the\nsubscriber in the worldwide cellular carrier network system, including the country\nand home cellular carrier network to which the subscriber belongs. In some ways, the\nIMSI is analogous to a MAC address. The SIM card also stores information about the\nservices that the subscriber is able to access and encryption key information for that\nsubscriber. In the official 4G LTE jargon, the mobile device is referred to as User\nEquipment (UE). However, in this textbook, we’ll use the more reader-friendly term\n“mobile device” throughout. We also note here that a mobile device is not always\nmobile; for example, the device might be a fixed temperature sensor or a surveillance\nBase Station. The base station sits at the “edge” of the carrier’s network and is\nresponsible for managing the wireless radio resources and the mobile devices with its\ncoverage area (shown as a hexagonal cell in Figure 7.17). As we’ll see, a mobile\ndevice will interact with a base station to attach to the carrier’s network. The base\nstation coordinates device authentication and allocation of resources (channel access)\nin the radio access network. In this sense, cellular base station functions are\ncomparable (but by no means identical) to those of APs in wireless LANs. But\ncellular base stations have several other important roles not found in wireless LANs.\nIn particular, base stations create device-specific IP tunnels from the mobile device to\ngateways and interact among themselves to handle device mobility among cells.\nNearby base stations also coordinate among themselves to manage the radio\nspectrum to minimize interference between cells. In the official 4G LTE terminology,\nthe base station is referred to as an “eNode-B,” which is rather opaque and non-\ndescriptive. In this textbook, we will instead use the reader-friendlier term “base\nstation” throughout.\nAs an aside, if you find LTE terminology a bit opaque, you aren’t alone! The\netymology of “eNode-B” is rooted in earlier 3G terminology, where network function\npoints were referred to as “nodes,” with “B” harkening back to earlier “Base Station\n(BS)” 1G terminology or “Base Transceiver Station (BTS)” in 2G terminology. 4G\nLTE is an “e”volution over 3G, and hence, an “e” now precedes “Node-B” in 4G LTE\nterminology. This name opaqueness shows no signs in stopping! In 5G systems,\neNode-B functions are now referred to as “ng-eNB”; perhaps you can guess what that\nacronym stands for!\nFigure 7.17 ♦Elements of the 4G LTE architecture\nHome Subscriber Server (HSS). As shown in Figure 7.18, the HSS is a ­control-\nplane element. The HSS is a database, storing information about the mobile devices\nfor which the HSS’s network is their home network. It is used in conjunction with the\nMME (discussed below) for device authentication.\nServing Gateway (S-GW), Packet Data Network Gateway (P-GW), and\nother  network routers. As shown in Figure 7.18, the Serving Gateway and the\nPacket Data Network Gateway are two routers (often collocated in practice) that lie\non the data path between the mobile device and the Internet. The PDN Gateway also\nprovides NAT IP addresses to mobile devices and performs NAT functions (see\nSection 4.3.4). The PDN Gateway is the last LTE element that a datagram originating\nat a mobile device encounters before entering the larger Internet. To the outside\nworld, the P-GW looks like any other gateway router; the mobility of the mobile\nnodes within the cellular carrier’s LTE network is hidden from the outside world\nbehind the P-GW. In addition to these gateway routers, a cellular carrier’s all-IP core\nwill have additional routers whose role is similar to that of traditional IP routers—to\nforward IP datagrams among themselves along paths that will typically terminate at\nelements of the LTE core network.\nMobility Management Entity (MME). The MME is also a control-plane element,\nas shown in Figure 7.18. Along with the HSS, it plays an important role in\nauthenticating a device wanting to connect into its network. It also sets up the tunnels\non the data path from/to the device and the PDN Internet gateway router, and\nmaintains information about an active mobile device’s cell location within the\ncarrier’s cellular network. But, as shown in Figure 7.18, it is not in the forwarding\npath for the mobile device’s datagrams being sent to and from the Internet.\nAuthentication. It is important for the network and the mobile device attaching to\nthe network to mutually authenticate each other—for the network to know that\nthe attaching device is indeed the device associated with a given IMSI, and for\nthe mobile device to know that the network to which it is attaching is also a\nlegitimate cellular carrier network. We will cover authentication in Chapter 8 and\ncover 4G authentication in Section 8.8. Here, we simply note that the MME plays\na middleman role between the mobile and Home Subscriber Service (HSS) in the\nmobile’s home network. Specifically, after receiving an attach request from\nmobile device, the local MME contacts the HSS in the mobile’s home network.\nThe mobile’s home HSS then returns enough encrypted information to the local\nMME to prove to the mobile device that the home HSS is performing\nauthentication through this MME, and for the mobile device to prove to the MME\nthat it is indeed the mobile associated with that IMSI. When a mobile device is\nattached to its home network, the HSS to be contacted during authentication is\nlocated within that same home network. However, when a mobile device is\nroaming on a visited network operated by a different cellular network carrier, the\nMME in that roaming network will need to contact the HSS in the mobile\ndevice’s home network.\nPath setup. As shown in the bottom half of Figure 7.18, the data path from the\nmobile device to the carrier’s gateway router consists of a wireless first hop\nbetween the mobile device and the base station, and concatenated IP tunnels\nbetween the base station and the Serving Gateway, and the Serving Gateway and\nthe PDN Gateway. Tunnels are setup under the control of the MME and used for\ndata forwarding (rather than direct forwarding among network routers) to\nfacilitate device mobility—when a device moves, only the tunnel endpoint\nterminating at the base station needs to be changed, while other tunnel endpoints,\nand the Quality of Service associated with a tunnel, remain unchanged.\nCell location tracking. As the device moves between cells, the base stations will\nupdate the MME on the device’s location. If the mobile device is in a sleep mode\nbut nonetheless moving between cells, the base stations can no longer track the\ndevice’s location. In this case, it will be the responsibility of the MME to locate\nthe device for wakeup, through a process known as paging.\nFigure 7.18 ♦LTE data-plane and control-plane elements\nTable 7.2 summarizes the key LTE architectural elements that we have discussed\nabove and compares these functions with those we encountered in our study of WiFi\nwireless LANs (WLANs).\nTable 7.2 ♦LTE Elements, and similar WLAN (WiFi) functions\nTHE ARCHITECTURAL EVOLUTION FROM 2G TO 3G TO 4G\nIn a relatively short span of 20 years, cellular carrier networks have undergone an astonishing\ntransition from being almost exclusively circuit-switched telephone networks to being all-IP packet-\nswitched data networks which include voice as just one of many applications. How did this transition\nhappen from an architectural standpoint? Was there a “flag day,” when the previous telephony-oriented\nnetworks were turned “off” and the all-IP cellular network was turned “on”? Or did elements in the\nprevious telephony-oriented networks begin taking on dual circuit (legacy) and packet (new)\nfunctionality, as we saw with the IPv4-to-IPv6 transition in Section 4.3.4?\nFigure 7.19 is taken from the earlier 7th edition of this textbook, which covered both 2G and 3G\ncellular networks. (We have retired this historical material, which is still available on this book’s website,\nin favor of a deeper coverage of 4G LTE in this 8th edition). Although the 2G network is a circuit-\nswitched mobile telephone network, a comparison of Figures 7.17 and 7.19 illustrates a similar\nconceptual structure, albeit for voice rather than for data services—a wireless edge controlled by a\nbase station, a gateway from the carrier’s network to the outside world, and aggregation points\nbetween the base stations and the gateway.\nFigure 7.19 ♦Elements of the 2G cellular architecture, supporting circuit-\nswitched voice service with the carrier’s core network\nFigure 7.20 (also taken from the 7th edition of this textbook) shows the main architectural\ncomponents of the 3G cellular architecture, which supports both circuit-switched voice service and\npacket-switched data services. Here, the transition from a voice-only network to a combined voice and\ndata network is clear: the existing core 2G cellular voice network elements remained untouched.\nHowever, additional cellular data functionality was added in parallel to, and functioned independently\nfrom, the existing core voice network at that time. As shown in Figure 7.20, the splitting point into these\ntwo separate core voice and data networks happened at the network edge, at the base station in the\nradio access network. The alternative—integrating new data services directly into the core elements of\nthe existing cellular voice network—would have raised the same challenges encountered in integrating\nnew (IPv6) and legacy (IPv4) technologies in the Internet. The carriers also wanted to leverage and\nexploit their considerable investment of existing infrastructure (and profitable services!) in their existing\ncellular voice network.\nFigure 7.20 ♦3G system architecture: supporting separate circuit-switched\nvoice service and packet-switched data service with the\ncarrier’s core network\n7.4.2 LTE Protocols Stacks\nSince the 4G LTE architecture is an all-IP architecture, we’re already very familiar with\nthe higher-layer protocols in the LTE protocol stack, in particular IP, TCP, UDP, and\nvarious application layer protocols, from our studies in Chapters 2 through 5.\nConsequently, the new LTE protocols that we’ll focus on here are primarily at the link\nand physical layers, and in mobility management.\nFigure 7.21 shows the user-plane protocol stacks at the LTE mobile node, the base\nstation, and the serving gateway. We’ll touch on several of LTE’s control-plane protocols\nlater when we study LTE mobility management (Section 7.6) and security (Section 8.8).\nAs we can see from Figure 7.21, most of the new and interesting user-plane protocol\nactivity is happening at the wireless radio link between the mobile device and the base\nFigure 7.21 ♦LTE data-plane protocol stacks\nLTE divides the mobile device’s link layer into three sublayers:\nPacket Data Convergence. This uppermost sublayer of the link layer sits just below\nIP. The Packet Data Convergence Protocol (PDCP) [3GPP PDCP 2019] performs IP\nheader/compression in order to decrease the number of bits sent over the wireless\nlink, and encryption/decryption of the IP datagram using keys that were established\nvia signaling messages between the LTE mobile device and the Mobility\nManagement Entity (MME) when the mobile device first attached to the network;\nwe’ll cover aspects of LTE security in Section 8.8.2.\nRadio Link Control. The Radio Link Control (RLC) Protocol [3GPP RLCP\n2018] performs two important functions: (i) fragmenting (on the sending side) and\nreassembly (on the receiving) of IP datagrams that are too large to fit into the\nunderlying link-layer frames, and (ii) link-layer reliable data transfer at the through\nthe use of an ACK/NAK-based ARQ protocol. Recall the we’ve studied the basic\nelements of ARQ protocols in Section 3.4.1.\nMedium Access Control (MAC). The MAC layer performs transmission scheduling,\nthat is, the requesting and use of the radio transmission slots described in Section\n7.4.4. The MAC sublayer also performs additional error detection/­correction\nfunctions, including the use of redundant bit transmission as a forward error-\ncorrection technique. The amount of redundancy can be adapted to channel\nconditions.\nFigure 7.21 also shows the use of tunnels in the user data path. As discussed above,\nthese tunnels are established, under MME control, when the mobile device first attaches\nto the network. Each tunnel between two endpoints has a unique tunnel endpoint\nidentifier (TEID). When the base station receives datagrams from the mobile device, it\nencapsulates them using the GPRS Tunneling Protocol [3GPP GTPv1-U 2019], including\nthe TEID, and sends them in UDP segments to the Serving Gateway at the other end of\nthe tunnel. On the receiving side, the base station decapsulates tunneled UDP datagrams,\nextracts the encapsulated IP datagram destined for the mobile device, and forwards that\nIP datagram over the wireless hop to the mobile device.\n7.4.3 LTE Radio Access Network\nLTE uses a combination of frequency division multiplexing and time division\nmultiplexing on the downstream channel, known as orthogonal frequency division\nmultiplexing (OFDM) [Hwang 2009]. (The term “orthogonal” comes from the fact the\nsignals being sent on different frequency channels are created so that they interfere very\nlittle with each other, even when channel frequencies are tightly spaced). In LTE, each\nactive mobile device is allocated one or more 0.5 ms time slots in one or more of the\nchannel frequencies. Figure 7.22 shows an allocation of eight time slots over four\nfrequencies. By being allocated increasingly more time slots (whether on the same\nfrequency or on different frequencies), a mobile device is able to achieve increasingly\nhigher transmission rates. Slot (re)allocation among mobile devices can be performed as\noften as once every millisecond. Different modulation schemes can also be used to\nchange the transmission rate; see our earlier discussion of Figure 7.3 and dynamic\nselection of modulation schemes in WiFi networks.\nFigure 7.22 ♦Twenty 0.5-ms slots organized into 10 ms frames at each\nfrequency. An eight-slot allocation is shown shaded.\nThe particular allocation of time slots to mobile devices is not mandated by the LTE\nstandard. Instead, the decision of which mobile devices will be allowed to transmit in a\ngiven time slot on a given frequency is determined by the scheduling algorithms\nprovided by the LTE equipment vendor and/or the network operator. With opportunistic\nscheduling [Bender 2000; Kolding 2003; Kulkarni 2005], matching the physical-layer\nprotocol to the channel conditions between the sender and receiver and choosing the\nreceivers to which packets will be sent based on channel conditions allow the base station\nto make best use of the wireless medium. In addition, user priorities and contracted levels\nof service (e.g., silver, gold, or platinum) can be used in scheduling downstream packet\ntransmissions. In addition to the LTE capabilities described above, LTE-Advanced allows\nfor downstream bandwidths of hundreds of Mbps by allocating aggregated channels to a\nmobile device [Akyildiz 2010].\n7.4.4 Additional LTE Functions: Network Attachment and\nPower Management\nLet’s conclude or study of 4G LTE here by considering two additional important LTE\nfunctions: (i) the process with which a mobile device first attaches to the network and (ii)\nthe techniques used by the mobile device, in conjunction with core network elements, to\nmanage its power use.\nNetwork Attachment\nThe process by which a mobile device attaches to the cellular carrier’s network divides\nbroadly into three phases:\nAttachment to a Base Station. This first phase of device attachment is similar in\npurpose to, but quite different in practice from, the 802.11 association protocol that\nwe studied in Section 7.3.1. A mobile device wishing to attach to a cellular carrier\nnetwork will begin a bootstrap process to learn about, and then associate with, a\nnearby base station. The mobile device initially searches all channels in all frequency\nbands for a primary synchronization signal that is periodically broadcast every 5 ms\nby a base station. Once this signal is found, the mobile device remains on this\nfrequency and locates the secondary synchronization signal. With information found\nin this second signal, the device can locate (following several further steps)\nadditional information such as channel bandwidth, channel configurations, and the\ncellular carrier information of that base station. Armed with this information, the\nmobile device can select a base station to associate with (preferentially attaching to\nits home network, if available) and establish a control-plane signaling connection\nacross the wireless hop with that base station. This mobile-to-base-station channel\nwill be used through the remainder of the network attachment process.\nMutual Authentication. In our earlier description of the Mobility Management Entity\n(MME) in Section 7.4.1, we noted that the base station contacts the local MME to\nperform mutual authentication—a process that we’ll study in further detail in Section\n8.8.2. This is the second phase of network attachment, allowing the network to know\nthat the attaching device is indeed the device associated with a given IMSI, and the\nmobile device to know that the network to which it is attaching is also a legitimate\ncellular carrier network. Once this second phase of network attachment is complete,\nthe MME and mobile device have mutually authenticated each other, and the MME\nalso knows the identity of the base station to which the mobile is attached. Armed\nwith this information, the MME is now ready to configure the Mobile-device-to-\nPDN-gateway data path.\nMobile-device-to-PDN-gateway Data Path Configuration. The MME contacts the\nPDN gateway (which also provides a NAT address for the mobile device), the\nServing gateway, and the base station to establish the two tunnels shown in Figure\n7.21. Once this phase is complete, the mobile device is able to send/receive IP\ndatagrams via the base station through these tunnels to and from the Internet!\nPower Management: Sleep Modes\nRecall in our earlier discussion of advanced features in 802.11 (Section 7.3.5) and\nBluetooth (Section 7.3.6) that a radio in a wireless device may enter a sleep state to save\npower when it is not transmitting or receiving in order to minimize the amount of time\nthat the mobile device’s circuitry needs to be “on” for sending/receiving data, and for\nchannel sensing. In 4G LTE, a sleeping mobile device can be in one of two different\nsleep states. In the discontinuous reception state, which is typically entered after several\nhundred milliseconds of inactivity [Sauter 2014], the mobile device and the base station\nwill schedule periodic times in advance (typically several hundred milliseconds apart) at\nwhich the mobile device will wake up and actively monitor the channel for downstream\n(base station to mobile device) transmissions; apart from these scheduled times, however,\nthe mobile device’s radio will be sleeping.\nIf the discontinuous reception state might be considered a “light sleep,” the second\nsleep state—the Idle state—which follows even longer periods of 5 to 10 seconds of\ninactivity, might be thought of as a “deep sleep.” While in this deep sleep, the mobile\ndevice’s radio wakes up and monitors the channel even less frequently. Indeed, this sleep\nis so deep that if the mobile device moves into a new cell in the carrier’s network while\nsleeping, it need not inform the base station with which it was previous associated. Thus,\nwhen waking up periodically from this deep sleep, the mobile device will need to re-\nestablish an association with a (potentially new) base station in order to check for paging\nmessages broadcast by the MME to base stations nearby the base station with which the\nmobile was last associated. These control-plane paging messages, which are broadcast by\nthese base stations to all mobile devices in their cells, indicate which mobile devices\nshould fully wake up and re-establish a new data-plane connection to a base station (see\nFigure 7.18) in order to receive incoming packets.\n7.4.5 The Global Cellular Network: A Network of Networks\nHaving now studied the 4G cellular network architecture, let’s take a step back at take a\nlook at how the global cellular network—itself a “network of networks” like the Internet\n—is organized.\nFigure 7.23 shows a user’s mobile smartphone connected via a 4G base station into\nits home network. The user’s home mobile network is operated by a cellular carrier such\nas Verizon, AT&T, T-Mobile, or Sprint in the United States; Orange in France; or SK\nTelecom in Korea. The user’s home network, in turn, is connected to the networks of\nother cellular carriers and to the global Internet, though one or more gateway routers in\nthe home network, as shown in Figure 7.23. The mobile networks themselves\ninterconnect with each other either via the public Internet or via an Internet Protocol\nPacket eXchange (IPX) Network [GSMA 2018a]. An IPX is a managed network\nspecifically for interconnecting cellular carriers, similar to Internet eXchange Points (see\nFigure 1.15) for peering among ISPs. From Figure 7.23, we can see that the global\ncellular network is indeed a “network of networks”—just like the Internet (recall Figure\n1.15 and Section 5.4). 4G networks can also peer with 3G cellular voice/data networks\nand earlier voice-only networks.\nFigure 7.23 ♦The global cellular data network: a network of networks.\nWe’ll return shortly to additional 4G LTE topics—mobility management in Section\n7.6, and 4G security in Section 8.8.2—later, after developing the basic principles needed\nfor these topics. Let’s now take a quick look at the emerging 5G networks.\n7.4.6 5G Cellular Networks\nThe ultimate wide-area data service would be one with ubiquitous gigabit connection\nspeeds, extremely low latency, and unrestricted limitations on the number of users and\ndevices that could be supported in any region. Such a service would open the door to all\nkinds of new applications, including pervasive augmented reality and virtual reality,\ncontrol of autonomous vehicles via wireless connections, control of robots in factories\nvia wireless connections, and replacement of residential access technologies, such as\nDSL and cable, with fixed wireless Internet services (that is, residential wireless\nconnections from base stations to modems in homes).\nIt is expected that 5G, for which progressively improved versions are likely to be\nrolled out in the 2020 decade, will make a big step towards achieving the goals of the\nultimate wide-area data service. It is predicted that 5G will provide roughly a 10x\nincrease in peak bitrate, a 10x decrease in latency, and a 100x increase in traffic capacity\nover 4G [Qualcomm 2019].\nPrincipally, 5G refers to “5G NR (New Radio),” which is the standard adopted by\n3GPP. Other 5G technologies besides NR do exist, however. For example, Verizon’s\nproprietary 5G TF network operates on 28 and 39 GHz frequencies and is used only for\nfixed wireless Internet service, not in smartphones.\n5G standards divide frequencies into two groups: FR1 (450 MHz–6 GHz) and FR2\n(24 GHz–52 GHz). Most early deployments will be in the FR1 space, although there are\nearly deployments as of 2020 in the FR2 space for fixed Internet residential access as\nmentioned just above. Importantly, the physical layer (that is, wireless) aspects of 5G are\nnot backward-compatible with 4G mobile communications systems such as LTE: in\nparticular, it can’t be delivered to existing smartphones by deploying base station\nupgrades or software updates. Therefore, in the transition to 5G, wireless carriers will\nneed to make substantial investments in physical infrastructure.\nFR2 frequencies are also known as millimeter wave frequencies. While millimeter\nwave frequencies allow for much faster data speeds, they come with two major\nMillimeter wave frequencies have much shorter range from base station to receivers.\nThis makes millimeter wave technology unsuitable in rural areas and requires denser\ndeployments of base stations in urban areas.\nMillimeter wave communication is highly susceptible to atmospheric interference.\nNearby foliage and rain can cause problems for outdoor use.\n5G is not one cohesive standard, but instead consists of three co-existing standards\n[Dahlman 2018]:\neMBB (Enhanced Mobile Broadband). Initial deployments of 5G NR have focused\non eMBB, which provides for increased bandwidth for higher download and upload\nspeeds, as well as a moderate reduction in latency when compared to 4G LTE. eMBB\nenables rich media applications, such as mobile augmented reality and virtual reality,\nas well as mobile 4K resolution and 360° video streaming.\nURLLC (Ultra Reliable Low-Latency Communications). URLLC is targeted towards\napplications that are highly latency-sensitive, such as factory automation and\nautonomous driving. URLLC is targeting latencies of  1msec. As of this writing,\ntechnologies that enable URLLC are still being standardized.\nmMTC (Massive Machine Type Communications). mMTC is a narrowband access\ntype for sensing, metering, and monitoring applications. One priority for the design\nof 5G networks is to lower barriers for network connectivity for IoT devices. In\naddition to lowering latency, emerging technologies for 5G networks are focusing on\nreducing power requirements, making the use of IoT devices more pervasive than has\nbeen with 4G LTE.\n5G and Millimeter Wave Frequencies\nMany 5G innovations will be a direct result of working in the millimeter wave\nfrequencies in the 24 GHz–52 GHz band. For example, these frequencies offer the\npotential of achieving 100x increase in capacity over 4G. To get some insight into this,\ncapacity can be defined as the product of three terms [Björnson 2017]:\ncapacity = cell density  ×  available spectrum  ×  spectral efficiency\nwhere cell density is in units of cells/km2, available spectrum is in units of Hertz, and\nspectral efficiency is a measure of how efficiently each base station can communicate\nwith users and is in units of bps/Hz/cell. By multiplying these units out, it is easy to see\nthat capacity is in units of bps/km2. For each of these three terms, the values will be\nlarger for 5G than for 4G:\nBecause millimeter frequencies have much shorter range than 4G LTE frequencies,\nmore base stations are required, which in turn increases the cell density.\nBecause 5G FR2 operates in a much larger frequency band (52 − 24 = 28 GHz) than\n4G LTE (up to about 2 GHz), it has more available spectrum.\nWith regard to spectral efficiency, information theory says that if you want to double\nspectral efficiency, a 17-fold increase in power is needed [Björnson 2017]. Instead of\nincreasing power, 5G uses MIMO-technology (the same technology we encountered\nin our study of 802.11 networks in Section 7.3), which uses multiple antennas at each\nbase station. Rather than broadcasting signals in all directions, each MIMO antenna\nemploys beam forming and directs the signal at the user. MIMO technology allows a\nbase station to send to 10–20 users at the same time in the same frequency band.\nBy increasing all three terms in the capacity equation, 5G is expected to provide a\n100x increase in capacity in urban areas. Similarly, owing to the much wider frequency\nband, 5G is expected to provide peak download rates of 1 Gbps or higher.\nMillimeter wave signals are, however, easily blocked by buildings and trees. Small\ncell stations are needed to fill in coverage gaps between base stations and users. In a\nhighly populous region, the distance between two small cells could vary from 10 to 100\nmeters [Dahlman 2018].\n5G Core Network\nThe 5G Core network is the data network that manages all of the 5G mobile voice, data\nand Internet connections. The 5G Core network is being redesigned to better integrate\nwith the Internet and cloud-based services, and also includes distributed servers and\ncaches across the network, thereby reducing latency. Network function virtualization (as\ndiscussed in Chapters 4 and 5), and network slicing for different applications and\nservices, will be managed in the core.\nThe new 5G Core specification introduces major changes in the way mobile\nnetworks support a wide variety of services with varied performance. As in the case of\nthe 4G core network (recall Figures 7.17 and 7.18), the 5G core relays data traffic from\nend devices, authenticates devices, and manages device mobility. The 5G core also\ncontains all of the network elements that we encountered in Section 7.4.2—the mobile\ndevices, the cells, the base stations, and the Mobility Management Entity (now divided\ninto two sub-elements, as discussed below), the HSS, and the Serving and PDN\nAlthough the 4G and 5G core networks perform similar functions, there are some\nmajor differences in that the new 5G core architecture. The 5G Core is designed for\ncomplete control and user-plane separation (see Chapter 5). The 5G Core consists purely\nof virtualized software-based network functions. This new architecture will give\noperators the flexibility to meet the diverse requirements of the different 5G applications.\nSome of the new 5G core network functions include [Rommer 2019]:\nUser-Plane Function (UPF). Control and user-plane separation (see Chapter 5)\nallows packet processing to be distributed and pushed to the network edge.\nAccess and Mobility Management Function (AMF). The 5G Core essentially\ndecomposes the 4G Mobility Management Entity (MME) into two functional\nelements: AMF and SMF. The AMF receives all the connection and session\ninformation from end-user equipment but only handles connection and mobility\nmanagement tasks.\nSession Management Function (SMF). Session management is handled by the\nSession Management Function (SMF). The SMF is responsible for interacting with\nthe decoupled data plane. The SMF also performs IP address management and plays\nthe role of DHCP.\nAs of this writing (2020), 5G is in its early stages of deployment, and many 5G\nstandards have yet to be finalized. Only time will tell whether 5G will become a\npervasive broadband wireless service, whether it will successfully compete with WiFi for\nindoor wireless service, whether it will become a critical component of factory\nautomation and the autonomous vehicle infrastructure, and whether it will take us a big\nstep forward toward the ultimate wide-area wireless service.\n7.5 Mobility Management: Principles\nHaving covered the wireless nature of the communication links in a\nwireless network, it’s now time to turn our attention to the mobility that\nthese wireless links enable. In the broadest sense, a mobile device is one\nthat changes its point of attachment into the network over time. Because the\nterm mobility has taken on many meanings in both the computer and\ntelephony worlds, it will serve us well first to carefully consider forms of\n7.5.1 Device Mobility: a Network-layer Perspective\nFrom the network layer’s standpoint, a physically mobile device will\npresent a very different set of challenges to the network layer, depending on\nhow active the device is as it moves between points of attachment to the\nnetwork. At the one end of the spectrum, scenario (a) in Figure 7.24 is the\nmobile user who himself/herself physically moves between networks, but\npowers down the mobile device when moving. For example, a student\nmight disconnect from a wireless classroom network and power down\nhis/her device, head to the dining commons and connect to the wireless\naccess network there while eating, and then disconnect and power down\nfrom the dining commons network, walk to the library, and connect to the\nlibrary’s wireless network while studying. From a networking perspective,\nthis device is not mobile—it attaches to an access network and remains in\nthat access network while on. In this case, the device serially associates\nwith, and later disassociates from, each wireless access network\nencountered. This case of device (non-)mobility can be completely handled\nusing the networking mechanisms we’ve already studied in Sections 7.3 and\nIn scenario (b) in Figure 7.24, the device is physically mobile but\nremains attached to the same access network. This device is also not mobile\nfrom a network-layer perspective. Additionally, if the device remains\nassociated with the same 802.11 AP or LTE base station, the device is not\neven mobile from a link-layer perspective.\nFigure 7.24 ♦Various degrees of mobility, from a network-layer\nperspective\nFrom a network standpoint, our interest in device mobility really starts\nwith case (c), where a device changes its access network (e.g., 802.11\nWLAN or LTE cell) while continuing to send and receiving IP datagrams,\nand while maintaining higher-level (e.g., TCP) connections. Here, the\nnetwork will need to provide handover—a transfer of responsibility for\nforwarding datagrams to/from one AP or base station to the mobile device\n—as the device moves among WLANs or among LTE cells. We’ll cover\nhandover in detail in Section 7.6. If the handover occurs within access\nnetworks belonging to a single network provider, that provider can\norchestrate handover on its own. When a mobile device roams between\nmultiple provider networks, as in scenario (d), the providers must\norchestrate handover together, which considerably complicates the\nhandover process.\n7.5.2 Home Networks and Roaming on Visited\nAs we learned in our discussions of cellular 4G LTE networks in Section\n7.4.1, every subscriber has a “home” with some cellular provider. We\nlearned that the Home Subscriber Service (HSS) stores information about\neach of its subscribers, including a globally unique device ID (embedded in\na subscriber’s SIM card), information about services that the subscriber\nmay access, cryptographic keys to be used for communication, and\nbilling/charging information. When a device is connected to a cellular\nnetwork, other than its home network, that device is said to be roaming on\na visited network. When a mobile device attaches to, and roams on, a\nvisited network, coordination will be required between the home network\nand the visited network.\nThe Internet does not have a similarly strong notion of a home network\nor a visited network. In practice, a student’s home network might be the\nnetwork operated by his/her school; for mobile professionals, their home\nnetwork might be their company network. The visited network might be the\nnetwork of a school or a company they are visiting. But there is no notion\nof a home/visited network deeply embedded in the Internet’s architecture.\nThe Mobile IP protocol [Perkins 1998, RFC 5944], which we will cover\nbriefly in Section 7.6, was a proposal that strongly incorporated the notion\nof home/visited networks. But Mobile IP has seen limited deployment/use\nin practice. There are also activities underway that are built on top of the\nexisting IP infrastructure to provide authenticated network access across\nvisited IP networks. Eduroam [Eduroam 2020] is one such activity.\nThe notion of a mobile device having a home network provides two\nimportant advantages: the home network provides a single location where\ninformation about that device can be found, and (as we will see) it can serve\nas a coordination point for communication to/from a roaming mobile\nTo appreciate the potential value of the central point of information and\ncoordination, consider the human analogy of a 20-something adult Bob\nmoving out of the family home. Bob becomes mobile, living in a series of\ndormitories and apartments, and often changing addresses. If an old friend\nAlice wants to get in touch, how can Alice find the current address of Bob?\nOne common way is to contact the family, since a mobile 20-something\nadult will often register his or her current address with the family (if for no\nother reason than so that the parents can send money to help pay the rent!).\nThe family home becomes that unique location that others can go to as a\nfirst step in communicating with Bob. Additionally, later postal\ncommunication from Alice may be either indirect (e.g., with mail being sent\nfirst to Bob’s family home and then forwarded to Bob) or direct (e.g., with\nAlice using the address obtained from Bob’s parents to send mail directly to\n7.5.3 Direct and Indirect Routing to/from a Mobile\nLet us now consider the conundrum faced by the Internet-connected host\n(that we will refer to as a correspondent) in Figure 7.25 wishing to\ncommunicate with a mobile device that might be located within that mobile\ndevice’s cellular home network, or might be roaming in a visited network.\nIn our development below, we’ll adopt a 4G/5G cellular network\nperspective, since these networks have such a long history of supporting\ndevice mobility. But as we’ll see, the fundamental challenges and basic\nsolution approaches for supporting device mobility are equally applicable in\nboth cellular networks and in the Internet.\nAs shown in Figure 7.25, we’ll assume that the mobile device has a\nglobally unique identifier associated with it. In 4G, LTE cellular networks\n(see Section 7.4), this would be the International Mobile Subscriber Identity\n(IMSI) and an associated phone number, stored on a mobile device’s SIM\ncard. For mobile Internet users, this would be a permanent IP address in the\nIP address range of its home network, as in the case of the Mobile IP\narchitecture.\nFigure 7.25 ♦Elements of a mobile network architecture\nWhat approaches might be used in a mobile network architecture that\nwould allow a datagram sent by the correspondent to reach that mobile\ndevice? Three basic approaches can be identified and are discussed below.\nAs we will see, the latter two of these are adopted in practice.\nLeveraging the Existing IP Address Infrastructure\nPerhaps the simplest approach to routing to a mobile device in a visited\nnetwork is to simply use the existing IP addressing infrastructure—to add\nnothing new to the architecture. What could be easier!\nRecall from our discussion of Figure 4.21 that an ISP uses BGP to\nadvertise routes to destination networks by enumerating the CIDRized\naddress ranges of reachable networks. A visited network could thus\nadvertise to all other networks that a particular mobile device is resident in\nits network simply by advertising a highly specific address—the mobile\ndevice’s full 32-bit IP permanent address—essentially informing other\nnetworks that it has the path to be used to forward datagrams to that mobile\ndevice. These neighboring networks would then propagate this routing\ninformation throughout the network as part of the normal BGP procedure of\nupdating routing information and forwarding tables. Since datagrams will\nalways be forwarded to the router advertising the most specific destination\nfor that address (see Section 4.3), all datagrams addressed to that mobile\ndevice will be forwarded to the visited network. If the mobile device leaves\none visited network and joins another, the new visited network can\nadvertise a new, highly specific route to the mobile device, and the old\nvisited network can withdraw its routing information regarding the mobile\nThis solves two problems at once, and does so without making changes\nto the network-layer infrastructure! Other networks know the location of the\nmobile device, and it is easy to route datagrams to the mobile device, since\nthe forwarding tables will direct datagrams to the visited network. The killer\ndrawback, however, is that of scalability—network routers would have to\nmaintain forwarding table entries for potentially billions of mobile devices,\nand update a device’s entry each time it roams to a different network.\nClearly, this approach would not work in practice. Some additional\ndrawbacks are explored in the problems at the end of this chapter.\nAn alternative, more practical, approach (and one that has been adopted\nin practice) is to push mobility functionality from the network core to the\nnetwork edge—a recurring theme in our study of Internet architecture. A\nnatural way to do this is via the mobile device’s home network. In much the\nsame way that parents of the mobile 20-something adult track their child’s\nlocation, a mobility management entity (MME) in the mobile device’s home\nnetwork could track the visited network in which the mobile device resides.\nThis information might reside in a database, shown as the HSS database in\nFigure 7.25. A protocol operating between the visited network and the home\nnetwork will be needed to update the network in which the mobile device\nresides. You might recall that we encountered the MME and HSS elements\nin our study of 4G LTE. We’ll reuse their element names here, since they\nare so descriptive, and also because they are pervasively deployed in 4G\nLet’s next consider the visited network elements shown in Figure 7.25\nin more detail. The mobile device will clearly need an IP address in the\nvisited network. The possibilities here include using a permanent address\nassociated with the mobile device’s home network, allocating a new address\nin the address range of the visited network, or providing an IP address via\nNAT (see Section 4.3.4). In the latter two cases, a mobile device has a\ntransient identifier (a newly allocated IP address) in addition to its\npermanent identifiers stored in the HSS in its home network. These cases\nare analogous to a writer addressing a letter to the address of the house in\nwhich our mobile 20-something adult is currently living. In the case of a\nNAT address, datagrams destined to the mobile device would eventually\nreach the NAT gateway router in the visited network, which would then\nperform NAT address translation and forward the datagram to the mobile\nWe have now seen a number of elements of a solution to the\ncorrespondent’s dilemma in Figure 7.24: home and visited networks, the\nMME and HSS, and mobile device addressing. But how should datagrams\nbe addressed and forwarded to the mobile device? Since only the HSS (and\nnot network-wide routers) knows the location of the mobile device, the\ncorrespondent cannot simply address a datagram to the mobile device’s\npermanent address and send it into the network. Something more must be\ndone. Two approaches can be identified: indirect and direct routing.\nIndirect Routing to a Mobile Device\nLet’s again consider the correspondent that wants to send a datagram to a\nmobile device. In the indirect routing approach, the correspondent simply\naddresses the datagram to the mobile device’s permanent address and sends\nthe datagram into the network, blissfully unaware of whether the mobile\ndevice is resident in its home network or in a visited network; mobility is\nthus completely transparent to the correspondent. Such datagrams are first\nrouted, as usual, to the mobile device’s home network. This is illustrated in\nstep 1 in Figure 7.26.\nFigure 7.26 ♦Indirect routing to a mobile device\nLet’s now turn our attention to the HSS, which is responsible for\ninteracting with visited networks to track the mobile device’s location, and\nthe home network’s gateway router. One job of this gateway router is to be\non the lookout for an arriving datagram addressed to a device whose home\nis in that network, but that currently resides in a visited network. The home\nnetwork gateway intercepts this datagram, consults with the HSS to\ndetermine the visited network where the mobile device is resident, and\nforwards the datagram toward the visited network gateway router—step 2\nin Figure 7.26. The visited network gateway router then forwards the\ndatagram toward the mobile device—step 3 in Figure 7.26. If NAT\ntranslation is used, as in Figure 7.26, the visited network gateway router\nperforms NAT translation.\nIt is instructive to consider the rerouting at the home network in bit\nmore detail. Clearly, the home network gateway will need to forward the\narriving datagram to the gateway router in the visited network. On the other\nhand, it is desirable to leave the correspondent’s datagram intact, since the\napplication receiving the datagram should be unaware that the datagram\nwas forwarded via the home network. Both goals can be satisfied by having\nthe home gateway encapsulate the correspondent’s original complete\ndatagram within a new (larger) datagram. This larger datagram is then\naddressed and delivered to the visited network’s gateway router, which will\ndecapsulate the datagram—that is, remove the correspondent’s original\ndatagram from within the larger encapsulating datagram—and forward (step\n3 in Figure 7.26) the original datagram to the mobile device. The sharp\nreader will note that the encapsulation/decapsulation described here is\nprecisely the notion of tunneling, discussed in Section 4.3 in the context of\nIPv6; indeed, we also discussed the use of tunneling in the context of Figure\n7.18, when we introduced the 4G LTE data plane.\nFinally, let’s consider how the mobile device sends datagrams to the\ncorrespondent. In the context of Figure 7.26, the mobile device will clearly\nneed to forward the datagram through the visited gateway router, in order to\nperform NAT translation. But how then should the visited gateway router\nforward the datagram to the correspondent? As shown in Figure 7.26, there\nare two options here: (4a) the datagram could be tunneled back to the home\ngateway router, and sent to the correspondent from there, or (4b) the\ndatagram could be transmitted from the visited network directly to the\ncorrespondent—an approach known as local breakout [GSMA 2019a] in\nLet’s summarize our discussion of indirect routing by reviewing the\nnew network-layer functionality required to support mobility.\nA mobile-device–to–visited-network association protocol. The mobile\ndevice will need to associate with the visited network, and will similarly\nneed to disassociate when leaving the visited network.\nA visited-network–to–home-network-HSS registration protocol. The\nvisited network will need to register the mobile device’s location with\nthe HSS in the home network, and perhaps use information obtained\nfrom the HSS in performing device authentication.\nA datagram tunneling protocol between in the home network gateway\nand the visited network gateway router. The sending side performs\nencapsulation and forwarding of the correspondent’s original datagram;\non the receiving side, the gateway router performs decapsulation, NAT\ntranslation, and forwarding of the original datagram to the mobile\nThe previous discussion provides all the needed elements for a mobile\ndevice to maintain an ongoing connection with a correspondent as the\ndevice moves among networks. When a device roams from one visited\nnetwork to another, the new visited network information needs to be\nupdated in the home network HSS, and the home-gateway-router-to-visited-\ngateway-router tunnel endpoint needs to be moved. But will the mobile\ndevice see an interrupted flow of datagrams as it moves between networks?\nAs long as the time between the mobile device disconnection from one\nvisited network and its attachment to the next visited network is small, few\ndatagrams will be lost. Recall from Chapter 3 that end-to-end connections\ncan experience datagram loss due to network congestion. Hence, occasional\ndatagram loss within a connection when a device moves between networks\nis by no means a catastrophic problem. If loss-free communication is\nrequired, upper-layer mechanisms will recover from datagram loss, whether\nsuch loss results from network congestion or from device mobility.\nOur discussion above has been purposefully somewhat generic. An\nindirect routing approach is used in the mobile IP standard [RFC 5944], as\nwell as in 4G LTE networks [Sauter 2014]. Their details, in particular the\ntunneling procedures employed, differ just a bit from our generic discussion\nDirect Routing to a Mobile Device\nThe indirect routing approach illustrated in Figure 7.26 suffers from an\ninefficiency known as the triangle routing problem—datagrams addressed\nto the mobile device must be forwarded first to the home network and then\nto the visited network, even when a much more efficient route exists\nbetween the correspondent and the roaming mobile device. In the worst\ncase, imagine a mobile user who is roaming on the same network that is the\nhome network for an overseas colleague who our mobile user is visiting.\nThe two are sitting side-by-side and exchanging data. Datagrams between\nthe mobile user and his overseas colleague will be forwarded to the mobile\nuser’s home network and then back again to the visited network!\nDirect routing overcomes the inefficiency of triangle routing, but does\nso at the cost of additional complexity. In the direct routing approach,\nshown in Figure 7.27, the correspondent first discovers the visited network\nin which the mobile is resident. This is done by querying the HSS in the\nmobile device’s home network, assuming (as in the case of indirect routing)\nthat the mobile device’s visited network is registered in the HSS. This is\nshown as steps 1 and 2 in Figure 7.27. The correspondent then tunnels\ndatagrams from its network directly to the gateway router in the mobile\ndevice’s visited network.\nFigure 7.27 ♦Direct routing to a mobile device\nWhile direct routing overcomes the triangle routing problem, it\nintroduces two important additional challenges:\nA mobile-user location protocol is needed for the correspondent to\nquery the HSS to obtain the mobile device’s visited network (steps 1\nand 2 in Figure 7.27). This is in addition to the protocol needed for the\nmobile device to register its location with its HSS.\nWhen the mobile device moves from one visited network to another,\nhow will the correspondent know to now forward datagrams to the new\nvisited network? In the case of indirect routing, this problem was easily\nsolved by updating the HSS in the home network, and changing the\ntunnel endpoint to terminate at the gateway router of the new visited\nnetwork. However, with direct routing, this change in visited networks\nis not so easily handled, as the HSS is queried by the correspondent\nonly at the beginning of the session. Thus, additional protocol\nmechanisms would be required to proactively update the correspondent\neach time the mobile device moves. Two problems at the end of this\nchapter explore solutions to this problem.\n7.6 Mobility Management in Practice\nIn the previous section, we identified key fundamental challenges and\npotential solutions in developing a network architecture to support device\nmobility: the notions of home and visited networks; the home network’s\nrole as a central point of information and control for mobile devices\nsubscribed to that home network; control-plane functions needed by a home\nnetwork’s mobility management entity to track a mobile device roaming\namong visited networks; and data-plane approaches of direct and indirect\nrouting to enable a correspondent and a mobile device to exchange\ndatagrams. Let’s now look at how these principles are put into practice! In\nSection 7.6.1, we’ll study mobility management in 4G/5G networks; in\nSection 7.6.2, we’ll look at Mobile IP, which has been proposed for the\n7.6.1 Mobility Management in 4G/5G Networks\nOur earlier study of 4G and emerging 5G architectures in Section 7.4\nacquainted us with all of the network elements that play a central role in\n4G/5G mobility management. Let’s now illustrate how those elements\ninteroperate with each other to provide mobility services in today’s 4G/5G\nnetworks [Sauter 2014; GSMA 2019b], which have their roots in earlier 3G\ncellular voice and data networks [Sauter 2014], and even earlier 2G voice-\nonly networks [Mouly 1992]. This will help us synthesize what we’ve\nlearned so far, allow us to introduce a few more advanced topics as well,\nand provide a lens into what might be in store for 5G mobility management.\nLet’s consider a simple scenario in which a mobile user (e.g., a\npassenger in a car), with a smartphone attaches to a visited 4G/5G network,\nbegins streaming a HD video from a remote server, and then moves from\nthe cell coverage of one 4G/5G base station to another. The four major steps\nin this scenario are shown in Figure 7.28:\n1. Mobile device and base station association. The mobile device\nassociates with a base station in the visited network.\n2. Control-plane configuration of network elements for the mobile device.\nThe visited and home networks establish control-plane state indicating\nthat the mobile device is resident in the visited network.\n3. Data-plane configuration of forwarding tunnels for the mobile device.\nThe visited network and the home network establish tunnels through\nwhich the mobile device and streaming server can send/receive IP\ndatagrams, using indirect routing through the home network’s Packet\nData Network gateway (P-GW).\n4. Mobile device handover from one base station to another. The mobile\ndevice changes its point of attachment to the visited network, via\nhandover from one base station to another.\nLet’s now consider each of these four steps in more detail.\nFigure 7.28 ♦An example 4G/5G mobility scenario\n1. Base station association. Recall that in Section 7.4.2, we studied the\nprocedures by which a mobile device associates with a base station. We\nlearned that the mobile device listens on all frequencies for primary\nsignals being transmitted by base stations in its area. The mobile device\nacquires progressively more information about these base stations,\nultimately selecting the base station with which to associate, and\nbootstrapping a control-­signaling channel with that base station. As part\nof this association, the mobile device provides the base station with its\nInternational Mobile Subscriber Identity (IMSI), which uniquely\nidentifies the mobile device as well as its home network and other\nadditional subscriber information.\n2. Control-plane configuration of LTE network elements for the\nmobile device.Once the mobile-device-to-base-station signaling channel\nhas been established, the base station can contact the MME in the visited\nnetwork. The MME will consult and configure a number of 4G/5G\nelements in both the home and visited networks to establish state on\nbehalf of the mobile node:\nThe MME will use to the IMSI and other information provided by\nthe mobile device to retrieve authentication, encryption, and\navailable network service information for that subscriber. That\ninformation might be in the MME’s local cache, retrieved from\nanother MME that the mobile device had recently contacted, or\nretrieved from the HSS in the mobile device’s home network. The\nmutual authentication process (which we will cover in more detail in\nSection 8.8) ensures that the visited network is sure about the\nidentity of the mobile device and that the device can authenticate the\nnetwork to which it is attaching.\nThe MME informs the HSS in the mobile device’s home network\nthat the mobile device is now resident in the visited network, and the\nHSS updates its database.\nThe base station and the mobile device select parameters for the\ndata-plane channel to be established between the mobile device and\nthe base station (recall that a control plane signaling channel is\nalready in operation).\n3. Data-plane configuration of forwarding tunnels for the mobile\ndevice. The MME next configures the data plane for the mobile device,\nas shown in Figure 7.29. Two tunnels are established. One tunnel is\nbetween the base station and a Serving Gateway in the visited network.\nThe second tunnel is between that Serving Gateway and the PDN\nGateway router in the mobile device’s home network. 4G LTE\nimplements this form of symmetric indirect routing—all traffic to/from\nthe mobile device will be tunneled through the device’s home network.\n4G/5G tunnels use the GPRS Tunneling Protocol (GTP), specified in\n[3GPP GTPv1-U 2019]. The Tunnel Endpoint ID (TEID) in the GTP\nheader indicates which tunnel a datagram belongs, allowing multiple\nflows to be multiplexed and de-multiplexed by GTP between tunnel\nIt is instructive to compare the configuration of tunnels in Figure\n7.29 (the case of mobile roaming in a visited network) with that of\nFigure 7.18 (the case of mobility only within the mobile device’s home\nnetwork). We see that in both cases, the Serving Gateway is co-resident\nin the same network as the mobile device, but PDN Gateway (which is\nalways the PDN Gateway in the mobile device’s home network) may be\nin a different network than the mobile device. This is precisely indirect\nrouting. An alternative to indirect routing, known as local breakout\n[GSMA 2019a] has been specified in which the Serving Gateway\nestablishes a tunnel to the PDN Gateway in the local, visited network. In\npractice, however, local breakout is not widely used [Sauter 2014].\nFigure 7.29 ♦Tunneling in 4G/5G networks between the Serving\nGateway in the visited network and the PDN\ngateway in the home network\nOnce the tunnels have been configured and activated, the mobile\ndevice can now forward packets to/from the Internet via the PDN\ngateway in its home network!\n4. Handover management. A handover occurs when a mobile device\nchanges its association from one base station to another. The handover\nprocess described below is the same, regardless of whether the mobile\ndevice is resident in its home network, or is roaming in a visited\nAs shown in Figure 7.30, datagrams to/from the device are initially\n(before handover) forwarded to the mobile through one base station\n(which we’ll refer to as the source base station), and after handover are\nrouted to the mobile device through another base station (which we’ll\nrefer to as the target base station). As we will see, a handover between\nbase stations results not only in the mobile device transmitting/receiving\nto/from a new base station but also in a change of the base-station side\nof the Serving-Gateway-to-base-station tunnel in Figure 7.29. In the\nsimplest case of handover, when the two base stations are near each\nother and in the same network, all changes occurring as a result of\nhandover are thus relatively local. In particular, the PDN gateway being\nused by the Serving Gateway remains blissfully unaware of device\nmobility. Of course, more complicated handoff scenarios will require the\nuse of more complex mechanisms [Sauter 2014; GSMA 2019a].\nThere may be several reasons for handover to occur. For example,\nthe signal between the current base station and the mobile may have\ndeteriorated to such an extent that communication is severely impaired.\nOr a cell may have become overloaded, handling a large amount of\ntraffic; handing over mobile devices to less congested nearby cells may\nalleviate this congestion. A mobile device periodically measures\ncharacteristics of a beacon signal from its current base station as well as\nsignals from nearby base stations that it can “hear.” These measurements\nare reported once or twice a second to the mobile device’s current\n(source) base station. Based on these measurements, the current loads of\nmobiles in nearby cells, and other factors, the source base station may\nchoose to initiate a handover. The 4G/5G standards do not specify a\nspecific algorithm to be used by a base station to determine whether or\nnot to perform handover, or which target base station to choose; this is\nan active area of research [Zheng 2008; Alexandris 2016].\nFigure 7.30 illustrates the steps involved when a source base station\ndecides to hand over a mobile device to the target base station.\nFigure 7.30 ♦Steps in handing over a mobile device from the\nsource base station to the target base station\n1. The current (source) base station selects the target base station, and\nsends a Handover Request message to the target base station.\n2. The target base station checks whether it has the resources to support the\nmobile device and its quality of service requirements. If so, it pre-\nallocates channel resources (e.g., time slots) on its radio access network\nand other resources for that device. This pre-allocation of resources\nfrees the mobile device from having to go through the time-consuming\nbase-station association protocol discussed earlier, allowing handover to\nbe executed as fast as possible. The target base station replies to the\nsource base station with a Handover Request Acknowledge message,\ncontaining all the information at the target base station that the mobile\ndevice will need to associate with the new base station.\n3. The source base station receives the Handover Request\nAcknowledgement message and informs the mobile device of the target\nbase station’s identity and channel access information. At this point, the\nmobile device can begin sending/receiving datagrams to/from the new\ntarget base station. From the mobile device’s point of view, handover is\nnow complete! However, there is still a bit of work to be done within the\n4. The source base station will also stop forwarding datagrams to the\nmobile device and instead forward any tunneled datagrams it receives to\nthe target base station, which will later forward these datagrams to the\nmobile device.\n5. The target base station informs the MME that it (the target base station)\nwill be the new base station servicing the mobile device. The MME, in\nturn, signals to the Serving Gateway and the target base station to\nreconfigure the Serving-Gateway-to-base-station tunnel to terminate at\nthe target base station, rather than at the source base station.\n6. The target base station confirms back to the source base station that the\ntunnel has been reconfigured, allowing the source base station to release\nresources associated with that mobile device.\n7. At this point, the target base station can also begin delivering datagrams\nto the mobile device, including datagrams forwarded to the target base\nstation by the source base station during handover, as well as datagrams\nnewly arriving on the reconfigured tunnel from the Serving Gateway. It\ncan also forward outgoing datagrams received from the mobile device\ninto the tunnel to the Serving Gateway.\nThe roaming configurations in today’s 4G LTE networks, such as that\ndiscussed above, will also be used in future emerging 5G networks [GSMA\n2019c]. Recall, however, from our discussion in Section 7.4.6 that the 5G\nnetworks will be denser, with significantly smaller cell sizes. This will\nmake handover an even more critically important network function. In\naddition, low handover latency will be critical for many real-time 5G\napplications. The migration of the cellular network control plane to the\nSDN framework that we studied earlier in Chapter 5 [GSMA 2018b;\nCondoluci 2018] promises to enable implementations of a higher-capacity,\nlower-latency 5G cellular network control plane. The application of SDN in\na 5G context is the subject of considerable research [Giust 2015; Ordonez-\nLucena 2017; Nguyen 2016].\n7.6.2 Mobile IP\nToday’s Internet does not have any widely deployed infrastructure that\nprovides the type of services for “on the go” mobile users that we\nencountered for 4G/5G cellular networks. But this is certainly not due to the\nlack of technical solutions for providing such services in an Internet setting!\nIndeed, the Mobile IP architecture and protocols [RFC 5944] that we will\nbriefly discuss below have been standardized by Internet RFCs for more\nthan 20 years, and research has continued on new, more secure and more\ngeneralized mobility solutions [Venkataramani 2014].\nInstead, it has perhaps been the lack of motivating business and use\ncases [Arkko 2012] and the timely development and deployment of\nalternative mobility solutions in cellular networks that has blunted the\ndeployment of Mobile IP. Recall that 20 years ago, 2G cellular networks\nhad already provided a solution for mobile voice services (the “killer app”\nfor mobile users); additionally, next generation 3G networks supporting\nvoice and data were on the horizon. Perhaps the dual technology solution—\nmobile services via cellular networks when we are truly mobile and “on the\ngo” (i.e., the rightmost side of the mobility spectrum in Figure 7.24) and\nInternet services via 802.11 networks or wireline networks when we are\nstationary or moving locally (i.e., the leftmost side of the mobility spectrum\nin Figure 7.24)—that we had 20 years ago and still have today will persist\ninto the future.\nIt will nonetheless be instructive to briefly overview the Mobile IP\nstandard here, as it provides many of the same services as cellular networks\nand implements many of the same basic mobility principles. Earlier editions\nof this textbook have provided a more in-depth study of Mobile IP than we\nwill provide here; the interested reader can find this retired material on this\ntextbook’s website. The Internet architecture and protocols for supporting\nmobility, collectively known as Mobile IP, are defined primarily in RFC\n5944 for IPv4. Mobile IP, like 4G/5G, is a complex standard, and would\nrequire an entire book to describe in detail; indeed one such book is\n[Perkins 1998b]. Our modest goal here is to provide an overview of the\nmost important aspects of Mobile IP.\nThe overall architecture and elements of Mobile IP are strikingly\nsimilar to that of cellular provider networks. There is a strong notion of a\nhome network, in which a mobile device has a permanent IP address, and\nvisited networks (known as “foreign” networks in Mobile IP), where the\nmobile device will be allocated a care-of-address. The home agent in\nMobile IP has a similar function to the LTE HSS: it tracks the location of a\nmobile device by receiving updates from foreign agents in foreign networks\nvisited by that mobile device, just as the HSS receives updates from\nMobility Management Entities (MMEs) in visited networks in which a 4G\nmobile device resides. And both 4G/5G and Mobile IP use indirect routing\nto a mobile node, using tunnels to connect the gateway routers in the home\nand visited/foreign networks. Table 7.3 summarizes the elements of the\nMobile IP architecture, along with a comparison with similar elements in\n4G/5G networks\nTable 7.3 ♦Commonalities between 4G/5G and Mobile IP\narchitectures\nThe mobile IP standard consists of three main pieces:\nAgent discovery. Mobile IP defines the protocols used by a foreign\nagent to advertise its mobility services to a mobile device that wishes to\nattach to its network. Those services will include providing a care-of-\naddress to the mobile device for use in the foreign network, registration\nof the mobile device with the home agent in the mobile device’s home\nnetwork, and forwarding of datagrams to/from the mobile device,\namong other services.\nRegistration with the home agent. Mobile IP defines the protocols used\nby the mobile device and/or foreign agent to register and deregister a\ncare-of-address with a mobile device’s home agent.\nIndirect routing of datagrams. Mobile IP also defines the manner in\nwhich datagrams are forwarded to mobile devices by a home agent,\nincluding rules for forwarding datagrams and handling error conditions,\nand several forms of tunneling [RFC 2003, RFC 2004].\nAgain, our coverage here of Mobile IP has been intentionally brief. The\nChapter 7 Review Questions\nSECTION 7.1\nR1. What does it mean for a wireless network to be operating in\n“infrastructure mode”? If the network is not in infrastructure mode,\nwhat mode of operation is it in, and what is the difference between\nthat mode of operation and infrastructure mode?\nR2. Both MANET and VANET are multi-hop infrastructure-less wireless\nnetworks. What is the difference between them?\nSECTION 7.2\nR3. What are the differences between the following types of wireless\nchannel impairments: path loss, multipath propagation, interference\nfrom other sources?\nR4. As a mobile node gets farther and farther away from a base station,\nwhat are two actions that a base station could take to ensure that the\nloss probability of a transmitted frame does not increase?\nSECTION 7.3\nR5. Describe the role of the beacon frames in 802.11.\nR6. An access point periodically sends beacon frames. What are the\nmanagement applications, a topic we examined in Chapter 5). An intruder\nthat could actively interfere with DNS lookups (as discussed in Section\n2.4), routing computations ­(Sections 5.3 and 5.4), or network management\nfunctions (Sections 5.5 and 5.7) could wreak havoc in the Internet.\nHaving now established the framework, a few of the most important\ndefinitions, and the need for network security, let us next delve into\ncryptography. While the use of cryptography in providing confidentiality is\nself-evident, we’ll see shortly that it is also central to providing end-point\nauthentication and message integrity—making cryptography a cornerstone\nof network security.\n8.2 Principles of Cryptography\nAlthough cryptography has a long history dating back at least as far as Julius ­Caesar,\nmodern cryptographic techniques, including many of those used in the Internet, are based on\nadvances made in the past 30 years. Kahn’s book, The ­Codebreakers [Kahn 1967], and\nSingh’s book, The Code Book: The Science of Secrecy from Ancient Egypt to Quantum\nCryptography [Singh 1999], provide a fascinating look at the long history of cryptography.\nA complete discussion of cryptography itself requires a complete book [Bishop 2003;\nKaufman 2002; Schneier 2015] and so we only touch on the essential aspects of\ncryptography, particularly as they are practiced on the Internet. We also note that while our\nfocus in this section will be on the use of cryptography for confidentiality, we’ll see shortly\nthat cryptographic techniques are inextricably woven into authentication, message integrity,\nnonrepudiation, and more.\nCryptographic techniques allow a sender to disguise data so that an intruder can gain no\ninformation from the intercepted data. The receiver, of course, must be able to recover the\noriginal data from the disguised data. Figure 8.2 illustrates some of the important\nterminology.\nFigure 8.2 ♦Cryptographic components\nSuppose now that Alice wants to send a message to Bob. Alice’s message in its original\nform (e.g., “Bob, I love you. Alice”) is known as ­plaintext, or cleartext. Alice\nencrypts her plaintext message using an encryption algorithm so that the encrypted\nmessage, known as ciphertext, looks unintelligible to any intruder. Interestingly, in many\nmodern cryptographic systems, including those used in the Internet, the encryption\ntechnique itself is known—published, standardized, and available to everyone (e.g., [RFC\n1321; RFC 3447; RFC 2420; NIST 2001]), even a potential intruder! Clearly, if everyone\nknows the method for encoding data, then there must be some secret information that\nprevents an intruder from decrypting the transmitted data. This is where keys come in.\nIn Figure 8.2, Alice provides a key, K , a string of numbers or characters, as input to the\nencryption algorithm. The encryption algorithm takes the key and the plaintext message, m,\nas input and produces ciphertext as output. The notation K (m) refers to the ciphertext form\n(encrypted using the key K ) of the plaintext message, m. The actual encryption algorithm\nthat uses key K  will be evident from the context. Similarly, Bob will provide a key, K , to\nthe decryption algorithm that takes the ciphertext and Bob’s key as input and produces the\noriginal plaintext as output. That is, if Bob receives an encrypted message K (m), he\ndecrypts it by computing K (K (m)) = m. In symmetric key systems, Alice’s and Bob’s keys\nare identical and are secret. In public key systems, a pair of keys is used. One of the keys is\nknown to both Bob and Alice (indeed, it is known to the whole world). The other key is\nknown only by either Bob or Alice (but not both). In the following two subsections, we\nconsider symmetric key and public key systems in more detail.\n8.2.1 Symmetric Key Cryptography\nAll cryptographic algorithms involve substituting one thing for another, for example, taking\na piece of plaintext and then computing and substituting the appropriate ciphertext to create\nthe encrypted message. Before studying a modern key-based cryptographic system, let us\nfirst get our feet wet by studying a very old, very simple symmetric key algorithm attributed\nto Julius Caesar, known as the Caesar cipher (a cipher is a method for encrypting data).\nFor English text, the Caesar cipher would work by taking each letter in the plaintext\nmessage and substituting the letter that is k letters later (allowing wraparound; that is, having\nthe letter z followed by the letter a) in the alphabet. For example, if k = 3, then the letter a in\nplaintext becomes d in ciphertext; b in plaintext becomes e in ciphertext, and so on. Here,\nthe value of k serves as the key. As an example, the plaintext message “bob, i love\nyou. Alice” becomes “ere, l oryh brx. dolfh” in ciphertext. While the\nciphertext does indeed look like gibberish, it wouldn’t take long to break the code if you\nknew that the Caesar cipher was being used, as there are only 25 possible key values.\nAn improvement on the Caesar cipher is the monoalphabetic cipher, which also\nsubstitutes one letter of the alphabet with another letter of the alphabet. ­However, rather than\nsubstituting according to a regular pattern (e.g., substitution with an offset of k for all\nletters), any letter can be substituted for any other letter, as long as each letter has a unique\nsubstitute letter, and vice versa. The substitution rule in Figure 8.3 shows one possible rule\nfor encoding plaintext.\nFigure 8.3 ♦A monoalphabetic cipher\nThe plaintext message “bob, i love you. Alice” becomes “nkn, s gktc\nwky. Mgsbc.” Thus, as in the case of the Caesar cipher, this looks like gibberish. A\nmonoalphabetic cipher would also appear to be better than the Caesar cipher in that there are\n26! (on the order of 10 ) possible pairings of letters rather than 25 possible pairings. A\nbrute-force approach of trying all 10  possible pairings would require far too much work to\nbe a feasible way of breaking the encryption algorithm and decoding the message. However,\nby statistical analysis of the plaintext language, for example, knowing that the letters e and t\nare the most frequently occurring letters in typical English text (accounting for 13 percent\nand 9 percent of letter occurrences), and knowing that particular two-and three-letter\noccurrences of letters appear quite often together (for example, “in,” “it,” “the,” “ion,”\n“ing,” and so forth) make it relatively easy to break this code. If the intruder has some\nof routers in the network (see Chapter 5). In a link-state algorithm, each\nrouter needs to broadcast a link-state message to all other routers in the\nnetwork. A router’s link-state message includes a list of its directly\nconnected neighbors and the direct costs to these neighbors. Once a router\nreceives link-state messages from all of the other routers, it can create a\ncomplete map of the network, run its least-cost routing algorithm, and\nconfigure its forwarding table. One relatively easy attack on the routing\nalgorithm is for Trudy to distribute bogus link-state messages with incorrect\nlink-state information. Thus, the need for message integrity—when router B\nreceives a link-state message from router A, router B should verify that\nrouter A actually created the message and, further, that no one tampered\nwith the message in transit.\nIn this section, we describe a popular message integrity technique that\nis used by many secure networking protocols. But before doing so, we need\nto cover another important topic in cryptography—cryptographic hash\n8.3.1 Cryptographic Hash Functions\nAs shown in Figure 8.7, a hash function takes an input, m, and computes a\nfixed-size string H(m) known as a hash. The Internet checksum (Chapter 3)\nand CRCs (Chapter 6) meet this definition. A cryptographic hash function\nis required to have the following additional property:\nFigure 8.7 ♦Hash functions\nIt is computationally infeasible to find any two different messages x and\ny such that H(x) = H(y).\nInformally, this property means that it is computationally infeasible for\nan intruder to substitute one message for another message that is protected\nby the hash function. That is, if (m, H(m)) are the message and the hash of\nthe message created by the sender, then an intruder cannot forge the\nprotocol in Chapter 3, we will find it instructive here to develop various\nversions of an authentication protocol, which we will call ap (authentication\nprotocol), and poke holes in each version as we proceed. (If you enjoy this\nstepwise evolution of a design, you might also enjoy [Bryant 1988], which\nrecounts a fictitious narrative between designers of an open-\nnetwork authentication system, and their discovery of the many subtle\nissues involved.)\nLet’s assume that Alice needs to authenticate herself to Bob.\nPerhaps the simplest authentication protocol we can imagine is one\nwhere Alice simply sends a message to Bob saying she is Alice. This\nprotocol is shown in Figure 8.15. The flaw here is obvious—there is no way\nfor Bob actually to know that the person sending the message “I am Alice”\nis indeed Alice. For example, Trudy (the intruder) could just as well send\nsuch a message.\nFigure 8.15 ♦Protocol ap1.0 and a failure scenario\nAuthentication Protocol ap2.0\nIf Alice has a well-known network address (e.g., an IP address) from which\nshe always communicates, Bob could attempt to authenticate Alice by\nverifying that the source address on the IP datagram carrying the\nauthentication message matches Alice’s well-known address. In this case,\nAlice would be authenticated. This might stop a very network-naive\nintruder from impersonating Alice, but it wouldn’t stop the determined\nstudent studying this book, or many others!\nFrom our study of the network and data link layers, we know that it is\nnot that hard (for example, if one had access to the operating system code\nand could build one’s own operating system kernel, as is the case with\nLinux and several other freely available operating systems) to create an IP\ndatagram, put whatever IP source address we want (for example, Alice’s\nwell-known IP address) into the IP datagram, and send the datagram over\nthe link-layer protocol to the first-hop router. From then on, the incorrectly\nsource-addressed datagram would be dutifully forwarded to Bob. This\napproach, shown in Figure 8.16, is a form of IP spoofing. IP spoofing can\nbe avoided if Trudy’s first-hop router is configured to forward only\ndatagrams containing Trudy’s IP source address [RFC 2827]. However, this\ncapability is not universally deployed or enforced. Bob would thus be\nfoolish to assume that Trudy’s network manager (who might be Trudy\nherself) had configured Trudy’s first-hop router to forward only\nappropriately addressed datagrams.\nAuthentication Protocol ap3.0\nOne classic approach to authentication is to use a secret password. The\npassword is a shared secret between the authenticator and the person being\nauthenticated. Gmail, Facebook, telnet, FTP, and many other services use\npassword authentication. In protocol ap3.0, Alice thus sends her secret\npassword to Bob, as shown in Figure 8.17.\nSince passwords are so widely used, we might suspect that protocol\nap3.0 is fairly secure. If so, we’d be wrong! The security flaw here is clear.\nIf Trudy eavesdrops on Alice’s communication, then she can learn Alice’s\npassword. Lest you think this is unlikely, consider the fact that when you\nTelnet to another machine and log in, the login password is sent\nunencrypted to the Telnet server. Someone connected to the Telnet client or\nserver’s LAN can possibly sniff (read and store) all packets transmitted on\nthe LAN and thus steal the login password. In fact, this is a well-known\napproach for stealing passwords (see, for example, [Jimenez 1997]). Such a\nthreat is obviously very real, so ap3.0 clearly won’t do.\nFigure 8.16 ♦Protocol ap2.0 and a failure scenario\nAuthentication Protocol ap3.1\nOur next idea for fixing ap3.0 is naturally to encrypt the password. By\nencrypting the password, we can prevent Trudy from learning Alice’s\npassword. If we assume that Alice and Bob share a symmetric secret key,\n, then Alice can encrypt the password and send her identification\nmessage, “I am Alice,” and her encrypted password to Bob. Bob then\ndecrypts the password and, assuming the password is correct, authenticates\nAlice. Bob feels comfortable in authenticating Alice since Alice not only\nknows the password, but also knows the shared secret key value needed to\nencrypt the password. Let’s call this protocol ap3.1.\nWhile it is true that ap3.1 prevents Trudy from learning Alice’s\npassword, the use of cryptography here does not solve the authentication\nproblem. Bob is subject to a playback attack: Trudy need only eavesdrop\non Alice’s communication, record the encrypted version of the password,\nand play back the encrypted version of the password to Bob to pretend that\nshe is Alice. The use of an encrypted password in ap3.1 doesn’t make the\nsituation manifestly different from that of protocol ap3.0 in Figure 8.17.\nFigure 8.17 ♦Protocol ap3.0 and a failure scenario\nAuthentication Protocol ap4.0\nThe failure scenario in Figure 8.17 resulted from the fact that Bob could not\ndistinguish between the original authentication of Alice and the later\nplayback of Alice’s original authentication. That is, Bob could not tell if\nAlice was live (that is, was currently really on the other end of the\nconnection) or whether the messages he was receiving were a recorded\nplayback of a previous authentication of Alice. The very (very) observant\nreader will recall that the three-way TCP handshake protocol needed to\naddress the same problem—the server side of a TCP connection did not\nwant to accept a connection if the received SYN segment was an old copy\n(retransmission) of a SYN segment from an earlier connection. How did the\nTCP server side solve the problem of determining whether the client was\nreally live? It chose an initial sequence number that had not been used in a\nvery long time, sent that number to the client, and then waited for the client\nto respond with an ACK segment containing that number. We can adopt the\nsame idea here for authentication purposes.\nA nonce is a number that a protocol will use only once in a lifetime.\nThat is, once a protocol uses a nonce, it will never use that number again.\nOur ap4.0 protocol uses a nonce as follows:\n1. Alice sends the message “I am Alice” to Bob.\n2. Bob chooses a nonce, R, and sends it to Alice.\n3. Alice encrypts the nonce using Alice and Bob’s symmetric secret key,\n, and sends the encrypted nonce, K\n (R), back to Bob. As in\nprotocol ap3.1, it is the fact that Alice knows K\n and uses it to encrypt\na value that lets Bob know that the message he receives was generated\nby Alice. The nonce is used to ensure that Alice is live.\n4. Bob decrypts the received message. If the decrypted nonce equals the\nnonce he sent Alice, then Alice is authenticated.\nProtocol ap4.0 is illustrated in Figure 8.18. By using the once-in-a-\nlifetime value, R, and then checking the returned value, K\n (R), Bob can\nbe sure that Alice is both who she says she is (since she knows the secret\nkey value needed to encrypt R) and live (since she has encrypted the nonce,\nR, that Bob just created).\nFigure 8.18 ♦Protocol ap4.0 and a failure scenario\nThe use of a nonce and symmetric key cryptography forms the basis of\nap4.0. A natural question is whether we can use a nonce and public key\ncryptography (rather than symmetric key cryptography) to solve the\nauthentication problem. This issue is explored in the problems at the end of\nthe chapter.\n8.5 Securing E-Mail\nIn previous sections, we examined fundamental issues in network security,\nincluding symmetric key and public key cryptography, end-point\nauthentication, key distribution, message integrity, and digital signatures.\nWe are now going to examine how these tools are being used to provide\nsecurity in the Internet.\nInterestingly, it is possible to provide security services in any of the top\nfour layers of the Internet protocol stack. When security is provided for a\nspecific application-layer protocol, the application using the protocol will\nenjoy one or more security services, such as confidentiality, authentication,\nor integrity. When security is provided for a transport-layer protocol, all\napplications that use that protocol enjoy the security services of the\ntransport protocol. When security is provided at the network layer on a\nhost-to-host basis, all transport-layer segments (and hence all application-\nlayer data) enjoy the security services of the network layer. When security\nis provided on a link basis, then the data in all frames traveling over the link\nreceive the security services of the link.\nIn Sections 8.5 through 8.8, we examine how security tools are being\nused in the application, transport, network, and link layers. Being consistent\nwith the general structure of this book, we begin at the top of the protocol\nstack and discuss security at the application layer. Our approach is to use a\nspecific application, e-mail, as a case study for application-layer security.\nWe then move down the protocol stack. We’ll examine the TLS protocol\n(which provides security at the transport layer), IPsec (which provides\nsecurity at the network layer), and the security of the IEEE 802.11 wireless\nLAN protocol.\nYou might be wondering why security functionality is being provided\nat more than one layer in the Internet. Wouldn’t it suffice simply to provide\nthe security functionality at the network layer and be done with it? There\nare two answers to this question. First, although security at the network\nlayer can offer “blanket coverage” by encrypting all the data in the\ndatagrams (that is, all the transport-layer segments) and by authenticating\nall the source IP addresses, it can’t provide user-level security. For example,\na commerce site cannot rely on IP-layer security to authenticate a customer\nwho is purchasing goods at the commerce site. Thus, there is a need for\nsecurity functionality at higher layers as well as blanket coverage at lower\nlayers. Second, it is generally easier to deploy new Internet services,\nincluding security services, at the higher layers of the protocol stack. While\nwaiting for security to be broadly deployed at the network layer, which is\nprobably still many years in the future, many application developers “just\ndo it” and introduce security functionality into their favorite applications. A\nclassic example is Pretty Good Privacy (PGP), which provides secure e-\nmail (discussed later in this section). Requiring only client and server\napplication code, PGP was one of the first security technologies to be\nbroadly used in the Internet.\n8.5.1 Secure E-Mail\nWe now use the cryptographic principles of Sections 8.2 through 8.3 to\ncreate a secure e-mail system. We create this high-level design in an\nincremental manner, at each step introducing new security services. When\ndesigning a secure e-mail system, let us keep in mind the racy example\nintroduced in Section 8.1—the love affair between Alice and Bob. Imagine\nthat Alice wants to send an e-mail message to Bob, and Trudy wants to\nBefore plowing ahead and designing a secure e-mail system for Alice\nand Bob, we should consider which security features would be most\ndesirable for them. First and foremost is confidentiality. As discussed in\nSection 8.1, neither Alice nor Bob wants Trudy to read Alice’s e-mail\nmessage. The second feature that Alice and Bob would most likely want to\nsee in the secure e-mail system is sender authentication. In particular, when\nBob receives the message “I don’t love you anymore. I\nnever want to see you again. Formerly yours,\nAlice,” he would naturally want to be sure that the message came from\nAlice and not from Trudy. Another feature that the two lovers would\nappreciate is message integrity, that is, assurance that the message Alice\nsends is not modified while en route to Bob. Finally, the e-mail system\nshould provide receiver authentication; that is, Alice wants to make sure\nthat she is indeed sending the letter to Bob and not to someone else (for\nexample, Trudy) who is impersonating Bob.\nSo let’s begin by addressing the foremost concern, confidentiality. The\nmost straightforward way to provide confidentiality is for Alice to encrypt\nthe message with symmetric key technology (such as DES or AES) and for\nBob to decrypt the message on receipt. As discussed in Section 8.2, if the\nsymmetric key is long enough, and if only Alice and Bob have the key, then\nit is extremely difficult for anyone else (including Trudy) to read the\nmessage. Although this approach is straightforward, it has the fundamental\ndifficulty that we discussed in Section 8.2—distributing a symmetric key so\nthat only Alice and Bob have copies of it. So we naturally consider an\nalternative approach—public key cryptography (using, for example, RSA).\nIn the public key approach, Bob makes his public key publicly available\n(e.g., in a public key server or on his personal Web page), Alice encrypts\nher message with Bob’s public key, and she sends the encrypted message to\nBob’s e-mail address. When Bob receives the message, he simply decrypts\nit with his private key. Assuming that Alice knows for sure that the public\nkey is Bob’s public key, this approach is an excellent means to provide the\ndesired confidentiality. One problem, however, is that public key encryption\nis relatively inefficient, particularly for long messages.\nTo overcome the efficiency problem, let’s make use of a session key\n(discussed in Section 8.2.2). In particular, Alice (1) selects a random\nsymmetric session key, K , (2) encrypts her message, m, with the symmetric\nkey, (3) encrypts the symmetric key with Bob’s public key, K , (4)\nconcatenates the encrypted message and the encrypted symmetric key to\nform a “package,” and (5) sends the package to Bob’s e-mail address. The\nsteps are illustrated in Figure 8.19. (In this and the subsequent figures, the\ncircled “+” represents concatenation and the circled “−” represents\ndeconcatenation.) When Bob receives the package, he (1) uses his private\nkey, K , to obtain the symmetric key, K , and (2) uses the symmetric key K\nto decrypt the message m.\nHaving designed a secure e-mail system that provides confidentiality,\nlet’s now design another system that provides both sender authentication\nand message integrity. We’ll suppose, for the moment, that Alice and Bob\nare no longer concerned with confidentiality (they want to share their\nfeelings with everyone!), and are concerned only about sender\nauthentication and message integrity. To accomplish this task, we use digital\nsignatures and message digests, as described in Section 8.3. Specifically,\nAlice (1) applies a hash function, H (e.g., MD5), to her message, m, to\nobtain a message digest, (2) signs the result of the hash function with her\nprivate key, K −\nA , to create a digital signature, (3) concatenates the original\n(unencrypted) message with the signature to create a package, and (4) sends\nthe package to Bob’s e-mail address. When Bob receives the package, he\n(1) applies Alice’s public key, K +\nA , to the signed message digest and (2)\ncompares the result of this operation with his own hash, H, of the message.\nThe steps are illustrated in Figure 8.20. As discussed in Section 8.3, if the\ntwo results are the same, Bob can be pretty confident that the message came\nfrom Alice and is unaltered.\nFigure 8.19 ♦Alice used a symmetric session key, K , to send a\nsecret e-mail to Bob\nFigure 8.20 ♦Using hash functions and digital signatures to provide\nsender authentication and message integrity\nNow let’s consider designing an e-mail system that provides\nconfidentiality, sender authentication, and message integrity. This can be\ndone by combining the procedures in Figures 8.19 and 8.20. Alice first\ncreates a preliminary package, exactly as in Figure 8.20, that consists of her\noriginal message along with a digitally signed hash of the message. She\nthen treats this preliminary package as a message in itself and sends this\nnew message through the sender steps in Figure 8.19, creating a new\npackage that is sent to Bob. The steps applied by Alice are shown in Figure\n8.21. When Bob receives the package, he first applies his side of Figure\n8.19 and then his side of Figure 8.20. It should be clear that this design\nachieves the goal of providing confidentiality, sender authentication, and\nmessage integrity. Note that, in this scheme, Alice uses public key\ncryptography twice: once with her own private key and once with Bob’s\npublic key. Similarly, Bob also uses public key cryptography twice—once\nwith his private key and once with Alice’s public key.\nThe secure e-mail design outlined in Figure 8.21 probably provides\nsatisfactory security for most e-mail users for most occasions. However,\nthere is still one important issue that remains to be addressed. The design in\nFigure 8.21 requires Alice to obtain Bob’s public key, and requires Bob to\nobtain Alice’s public key. The distribution of these public keys is a\nnontrivial problem. For example, Trudy might masquerade as Bob and give\nAlice her own public key while saying that it is Bob’s public key, enabling\nher to receive the message meant for Bob. As we learned in Section 8.3, a\npopular approach for securely distributing public keys is to certify the\npublic keys using a CA.\nFigure 8.21 ♦Alice uses symmetric key cyptography, public key\ncryptography, a hash function, and a digital signature\nto provide secrecy, sender authentication, and\nmessage integrity\nWritten by Phil Zimmermann in 1991, Pretty Good Privacy (PGP) is a\nnice example of an e-mail encryption scheme [PGP 2020]. The PGP design\nis, in essence, the same as the design shown in Figure 8.21. Depending on\nthe version, the PGP software uses MD5 or SHA for calculating the\nmessage digest; CAST, triple-DES, or IDEA for symmetric key encryption;\nand RSA for the public key encryption.\nWhen PGP is installed, the software creates a public key pair for the\nuser. The public key can be posted on the user’s Web site or placed in a\npublic key server. The private key is protected by the use of a password.\nThe password has to be entered every time the user accesses the private key.\nPGP gives the user the option of digitally signing the message, encrypting\nthe message, or both digitally signing and encrypting. Figure 8.22 shows a\nPGP signed message. This message appears after the MIME header. The\nencoded data in the message is K −\nA (H(m)), that is, the digitally signed\nmessage digest. As we discussed above, in order for Bob to verify the\nintegrity of the message, he needs to have access to Alice’s public key.\nFigure 8.22 ♦A PGP signed message\nFigure 8.23 shows a secret PGP message. This message also appears\nafter the MIME header. Of course, the plaintext message is not included\nwithin the secret e-mail message. When a sender (such as Alice) wants both\nconfidentiality and integrity, PGP contains a message like that of Figure\n8.23 within the message of Figure 8.22.\nFigure 8.23 ♦A secret PGP message\nPGP also provides a mechanism for public key certification, but the\nmechanism is quite different from the more conventional CA. PGP public\nkeys are certified by a web of trust. Alice herself can certify any\nkey/username pair when she believes the pair really belong together. In\naddition, PGP permits Alice to say that she trusts another user to vouch for\nthe authenticity of more keys. Some PGP users sign each other’s keys by\nholding key-signing parties. Users physically gather, exchange ­public keys,\nand certify each other’s keys by signing them with their private keys.\n8.6 Securing TCP Connections: TLS\nIn the previous section, we saw how cryptographic techniques can provide\nconfidentiality, data integrity, and end-point authentication to a specific\napplication, namely, e-mail. In this section, we’ll drop down a layer in the\nprotocol stack and examine how cryptography can enhance TCP with\nsecurity services, including confidentiality, data integrity, and end-point\nauthentication. This enhanced version of TCP is commonly known as\nTransport Layer Security (TLS), which has been standardized by the\nIETF [RFC 4346]. An earlier and similar version of this protocol is SSL\nThe SSL protocol was originally designed by Netscape, but the basic\nideas behind securing TCP had predated Netscape’s work (for example, see\nWoo [Woo 1994]). Since its inception, SSL and its successor TLS have\nenjoyed broad deployment. TLS is supported by all popular Web browsers\nand Web servers, and it is used by Gmail and essentially all Internet\ncommerce sites (including Amazon, eBay, and TaoBao). Hundreds of\nbillions of dollars are spent over TLS every year. In fact, if you have ever\npurchased anything over the Internet with your credit card, the\ncommunication between your browser and the server for this purchase\nalmost certainly went over TLS. (You can identify that TLS is being used\nby your browser when the URL begins with https: rather than http.)\nTo understand the need for TLS, let’s walk through a typical Internet\ncommerce scenario. Bob is surfing the Web and arrives at the Alice\nIncorporated site, which is selling perfume. The Alice Incorporated site\ndisplays a form in which Bob is supposed to enter the type of perfume and\nquantity desired, his address, and his payment card number. Bob enters this\ninformation, clicks on Submit, and expects to receive (via ordinary postal\nmail) the purchased perfumes; he also expects to receive a charge for his\norder in his next payment card statement. This all sounds good, but if no\nsecurity measures are taken, Bob could be in for a few surprises.\nIf no confidentiality (encryption) is used, an intruder could intercept\nBob’s order and obtain his payment card information. The intruder\ncould then make purchases at Bob’s expense.\nIf no data integrity is used, an intruder could modify Bob’s order,\nhaving him purchase ten times more bottles of perfume than desired.\nFinally, if no server authentication is used, a server could display Alice\nIncorporated’s famous logo when in actuality the site maintained by\nTrudy, who is masquerading as Alice Incorporated. After receiving\nBob’s order, Trudy could take Bob’s money and run. Or Trudy could\ncarry out an identity theft by collecting Bob’s name, address, and credit\ncard number.\nTLS addresses these issues by enhancing TCP with confidentiality, data\nintegrity, server authentication, and client authentication.\nTLS is often used to provide security to transactions that take place\nover HTTP. However, because TLS secures TCP, it can be employed by any\napplication that runs over TCP. TLS provides a simple Application\nProgrammer Interface (API) with sockets, which is similar and analogous to\nTCP’s API. When an application wants to employ TLS, the application\nincludes SSL classes/libraries. As shown in Figure 8.24, although TLS\ntechnically resides in the application layer, from the developer’s ­perspective\nit is a transport protocol that provides TCP’s services enhanced with\nsecurity services.\nFigure 8.24 ♦Although TLS technically resides in the application\nlayer, from the developer’s perspective it is a transport-\nlayer protocol\n8.6.1 The Big Picture\nWe begin by describing a simplified version of TLS, one that will allow us\nto get a big-picture understanding of the why and how of TLS. We will refer\nto this simplified version of TLS as “almost-TLS.” After describing almost-\nTLS, in the next subsection we’ll then describe the real TLS, filling in the\ndetails. Almost-TLS (and TLS) has three phases: handshake, key derivation,\nand data transfer. We now describe these three phases for a communication\nsession between a client (Bob) and a server (Alice), with Alice having a\nprivate/public key pair and a certificate that binds her identity to her public\nDuring the handshake phase, Bob needs to (a) establish a TCP connection\nwith Alice, (b) verify that Alice is really Alice, and (c) send Alice a master\nsecret key, which will be used by both Alice and Bob to generate all the\nsymmetric keys they need for the TLS session. These three steps are shown\nin Figure 8.25. Note that once the TCP connection is established, Bob sends\nAlice a hello message. Alice then responds with her certificate, which\ncontains her public key. As discussed in Section 8.3, because the certificate\nhas been certified by a CA, Bob knows for sure that the public key in the\ncertificate belongs to Alice. Bob then generates a Master Secret (MS)\n(which will only be used for this TLS session), encrypts the MS with\nAlice’s public key to create the Encrypted Master Secret (EMS), and sends\nthe EMS to Alice. Alice decrypts the EMS with her private key to get the\nMS. After this phase, both Bob and Alice (and no one else) know the master\nsecret for this TLS session.\nFigure 8.25 ♦The almost-TLS handshake, beginning with a TCP\nKey Derivation\nIn principle, the MS, now shared by Bob and Alice, could be used as the\nsymmetric session key for all subsequent encryption and data integrity\nchecking. It is, however, generally considered safer for Alice and Bob to\neach use different cryptographic keys, and also to use different keys for\nencryption and integrity checking. Thus, both Alice and Bob use the MS to\ngenerate four keys:\nE  = session encryption key for data sent from Bob to Alice\nM  = session HMAC key for data sent from Bob to Alice, where\nHMAC [RFC 2104] is a standardized hashed message authentication\ncode (MAC) that we encountered in section 8.3.2\nE  = session encryption key for data sent from Alice to Bob\nM  = session HMAC key for data sent from Alice to Bob\nAlice and Bob each generate the four keys from the MS. This could be done\nby simply slicing the MS into four keys. (But in reality TLS it is a little\nmore complicated, as we’ll see.) At the end of the key derivation phase,\nboth Alice and Bob have all four keys. The two encryption keys will be\nused to encrypt data; the two HMAC keys will be used to verify the\nintegrity of the data.\nData Transfer\nNow that Alice and Bob share the same four session keys (E , M , E , and\nM ), they can start to send secured data to each other over the TCP\nconnection. Since TCP is a byte-stream protocol, a natural approach would\nbe for TLS to encrypt application data on the fly and then pass the\nencrypted data on the fly to TCP. But if we were to do this, where would we\nput the HMAC for the integrity check? We certainly do not want to wait\nuntil the end of the TCP session to verify the integrity of all of Bob’s data\nthat was sent over the entire session! To address this issue, TLS breaks the\ndata stream into records, appends an HMAC to each record for integrity\nchecking, and then encrypts the record+HMAC. To create the HMAC, Bob\ninputs the record data along with the key M  into a hash function, as\ndiscussed in Section 8.3. To encrypt the package record+HMAC, Bob uses\nhis session encryption key E . This encrypted package is then passed to\nTCP for transport over the Internet.\nAlthough this approach goes a long way, it still isn’t bullet-proof when\nit comes to providing data integrity for the entire message stream. In\nparticular, suppose Trudy is a woman-in-the-middle and has the ability to\ninsert, delete, and replace segments in the stream of TCP segments sent\nbetween Alice and Bob. Trudy, for example, could capture two segments\nsent by Bob, reverse the order of the segments, adjust the TCP sequence\nnumbers (which are not encrypted), and then send the two reverse-ordered\nsegments to Alice. Assuming that each TCP segment encapsulates exactly\none record, let’s now take a look at how Alice would process these\n1. TCP running in Alice would think everything is fine and pass the two\nrecords to the TLS sublayer.\n2. TLS in Alice would decrypt the two records.\n3. TLS in Alice would use the HMAC in each record to verify the data\nintegrity of the two records.\n4. TLS would then pass the decrypted byte streams of the two records to\nthe application layer; but the complete byte stream received by Alice\nwould not be in the correct order due to reversal of the records!\nYou are encouraged to walk through similar scenarios for when Trudy\nremoves segments or when Trudy replays segments.\nThe solution to this problem, as you probably guessed, is to use\nsequence numbers. TLS does this as follows. Bob maintains a sequence\nnumber counter, which begins at zero and is incremented for each TLS\nrecord he sends. Bob doesn’t actually include a sequence number in the\nrecord itself, but when he calculates the HMAC, he includes the sequence\nnumber in the HMAC calculation. Thus, the HMAC is now a hash of the\ndata plus the HMAC key M  plus the current sequence number. Alice\ntracks Bob’s sequence numbers, allowing her to verify the data integrity of\na record by including the appropriate sequence number in the HMAC\ncalculation. This use of TLS sequence numbers prevents Trudy from\ncarrying out a woman-in-the-middle attack, such as reordering or replaying\nsegments. (Why?)\nThe TLS record (as well as the almost-TLS record) is shown in Figure\n8.26. The record consists of a type field, version field, length field, data\nfield, and HMAC field. Note that the first three fields are not encrypted.\nThe type field indicates whether the record is a handshake message or a\nmessage that contains application data. It is also used to close the TLS\nconnection, as discussed below. TLS at the receiving end uses the length\nfield to extract the TLS records out of the incoming TCP byte stream. The\nversion field is self-explanatory.\nFigure 8.26 ♦Record format for TLS\n8.6.2 A More Complete Picture\nThe previous subsection covered the almost-TLS protocol; it served to give\nus a basic understanding of the why and how of TLS. Now that we have a\nbasic understanding, we can dig a little deeper and examine the essentials of\nthe actual TLS protocol. In parallel to reading this description of the TLS\nprotocol, you are encouraged to complete the Wireshark TLS lab, available\nat the textbook’s Web site.\nTLS Handshake\nSSL does not mandate that Alice and Bob use a specific symmetric key\nalgorithm or a specific public-key algorithm. Instead, TLS allows Alice and\nBob to agree on the cryptographic algorithms at the beginning of the TLS\nsession, during the handshake phase. Additionally, during the handshake\nphase, Alice and Bob send nonces to each other, which are used in the\ncreation of the session keys (E , M , E , and M ). The steps of the real TLS\nhandshake are as follows:\n1. The client sends a list of cryptographic algorithms it supports, along\nwith a ­client nonce.\n2. From the list, the server chooses a symmetric algorithm (for example,\nAES) and a public key algorithm (for example, RSA with a specific key\nlength), and HMAC algorithm (MD5 or SHA-1) along with the HMAC\nkeys. It sends back to the client its choices, as well as a certificate and a\nserver nonce.\n3. The client verifies the certificate, extracts the server’s public key,\ngenerates a Pre-Master Secret (PMS), encrypts the PMS with the\nserver’s public key, and sends the encrypted PMS to the server.\n4. Using the same key derivation function (as specified by the TLS\nstandard), the client and server independently compute the Master\nSecret (MS) from the PMS and nonces. The MS is then sliced up to\ngenerate the two encryption and two HMAC keys. Furthermore, when\nthe chosen symmetric cipher employs CBC (such as 3DES or AES),\nthen two Initialization Vectors (IVs)—one for each side of the\nconnection—are also obtained from the MS. Henceforth, all ­messages\nsent between client and server are encrypted and authenticated (with the\n5. The client sends the HMAC of all the handshake messages.\n6. The server sends the HMAC of all the handshake messages.\nThe last two steps protect the handshake from tampering. To see this,\nobserve that in step 1, the client typically offers a list of algorithms—some\nstrong, some weak. This list of algorithms is sent in cleartext, since the\nencryption algorithms and keys have not yet been agreed upon. Trudy, as a\nwoman-in-the-middle, could delete the stronger algorithms from the list,\nforcing the client to select a weak algorithm. To prevent such a tampering\nattack, in step 5, the client sends the HMAC of the concatenation of all the\nhandshake messages it sent and received. The server can compare this\nHMAC with the HMAC of the handshake messages it received and sent. If\nthere is an inconsistency, the server can terminate the connection. Similarly,\nthe server sends the HMAC of the handshake messages it has seen,\nallowing the client to check for inconsistencies.\nYou may be wondering why there are nonces in steps 1 and 2. Don’t\nsequence numbers suffice for preventing the segment replay attack? The\nanswer is yes, but they don’t alone prevent the “connection replay attack.”\nConsider the following connection replay attack. Suppose Trudy sniffs all\nmessages between Alice and Bob. The next day, Trudy masquerades as Bob\nand sends to Alice exactly the same sequence of messages that Bob sent to\nAlice on the previous day. If Alice doesn’t use nonces, she will respond\nwith exactly the same sequence of messages she sent the previous day.\nAlice will not suspect any funny business, as each message she receives\nwill pass the integrity check. If Alice is an e-commerce server, she will\nthink that Bob is placing a second order (for exactly the same thing). On the\nother hand, by including a nonce in the protocol, Alice will send different\nnonces for each TCP session, causing the encryption keys to be different on\nthe two days. Therefore, when Alice receives played-back TLS records\nfrom Trudy, the records will fail the integrity checks, and the bogus e-\ncommerce transaction will not succeed. In summary, in TLS, nonces are\nused to defend against the “connection replay attack” and sequence\nnumbers are used to defend against replaying individual packets during an\nongoing session.\nConnection Closure\nAt some point, either Bob or Alice will want to end the TLS session. One\napproach would be to let Bob end the TLS session by simply terminating\nthe underlying TCP connection—that is, by having Bob send a TCP FIN\nsegment to Alice. But such a naive design sets the stage for the truncation\nattack whereby Trudy once again gets in the middle of an ongoing TLS\nsession and ends the session early with a TCP FIN. If Trudy were to do this,\nAlice would think she received all of Bob’s data when ­actuality she only\nreceived a portion of it. The solution to this problem is to indicate in the\ntype field whether the record serves to terminate the TLS session.\n(Although the TLS type is sent in the clear, it is authenticated at the receiver\nusing the record’s HMAC.) By including such a field, if Alice were to\nreceive a TCP FIN before ­receiving a closure TLS record, she would know\nthat something funny was going on.\nThis completes our introduction to TLS. We’ve seen that it uses many\nof the cryptography principles discussed in Sections 8.2 and 8.3. Readers\nwho want to explore TLS on yet a deeper level can read Rescorla’s highly\nreadable book on SSL/TLS [Rescorla 2001].\n8.7 Network-Layer Security: IPsec and Virtual\nPrivate Networks\nThe IP security protocol, more commonly known as IPsec, provides\nsecurity at the network layer. IPsec secures IP datagrams between any two\nnetwork-layer entities, including hosts and routers. As we will soon\ndescribe, many institutions (corporations, government branches, non-profit\norganizations, and so on) use IPsec to create virtual private networks\n(VPNs) that run over the public Internet.\nBefore getting into the specifics of IPsec, let’s step back and consider\nwhat it means to provide confidentiality at the network layer. With network-\nlayer confidentiality between a pair of network entities (for example,\nbetween two routers, between two hosts, or between a router and a host),\nthe sending entity encrypts the payloads of all the datagrams it sends to the\nreceiving entity. The encrypted payload could be a TCP segment, a UDP\nsegment, an ICMP message, and so on. If such a network-layer service were\nin place, all data sent from one entity to the other—including e-mail, Web\npages, TCP handshake messages, and management messages (such as\nICMP and SNMP)—would be hidden from any third party that might be\nsniffing the network. For this reason, network-layer security is said to\nprovide “blanket coverage.”\nIn addition to confidentiality, a network-layer security protocol could\npotentially provide other security services. For example, it could provide\nsource authentication, so that the receiving entity can verify the source of\nthe secured datagram. A network-layer security protocol could provide data\nintegrity, so that the receiving entity can check for any tampering of the\ndatagram that may have occurred while the datagram was in transit. A\nnetwork-layer security service could also provide replay-attack prevention,\nmeaning that Bob could detect any duplicate datagrams that an attacker\nmight insert. We will soon see that IPsec indeed provides mechanisms for\nall these security services, that is, for confidentiality, source authentication,\ndata ­integrity, and replay-attack prevention.\n8.7.1 IPsec and Virtual Private Networks (VPNs)\nAn institution that extends over multiple geographical regions often desires\nits own IP network, so that its hosts and servers can send data to each other\nin a secure and confidential manner. To achieve this goal, the institution\ncould actually deploy a stand-alone physical network—including routers,\nlinks, and a DNS ­infrastructure—that is completely separate from the public\nInternet. Such a disjoint network, dedicated to a particular institution, is\ncalled a private network. Not surprisingly, a private network can be very\ncostly, as the institution needs to purchase, install, and maintain its own\nphysical network infrastructure.\nInstead of deploying and maintaining a private network, many\ninstitutions today create VPNs over the existing public Internet. With a\nVPN, the institution’s inter-office traffic is sent over the public Internet\nrather than over a physically independent network. But to provide\nconfidentiality, the inter-office traffic is encrypted before it enters the public\nInternet. A simple example of a VPN is shown in Figure 8.27. Here the\ninstitution consists of a headquarters, a branch office, and traveling\nsalespersons that typically access the Internet from their hotel rooms.\n(There is only one salesperson shown in the figure.) In this VPN, whenever\ntwo hosts within headquarters send IP datagrams to each other or whenever\ntwo hosts within the branch office want to communicate, they use good-old\nvanilla IPv4 (that is, without IPsec services). However, when two of the\ninstitution’s hosts communicate over a path that traverses the public\nInternet, the traffic is encrypted before it enters the Internet.\nFigure 8.27 ♦Virtual private network (VPN)\nTo get a feel for how a VPN works, let’s walk through a simple\nexample in the context of Figure 8.27. When a host in headquarters sends\nan IP datagram to a salesperson in a hotel, the gateway router in\nheadquarters converts the vanilla IPv4 datagram into an IPsec datagram and\nthen forwards this IPsec datagram into the Internet. This IPsec datagram\nactually has a traditional IPv4 header, so that the routers in the public\nInternet process the datagram as if it were an ordinary IPv4 datagram—to\nthem, the datagram is a perfectly ordinary datagram. But, as shown Figure\n8.27, the payload of the IPsec datagram includes an IPsec header, which is\nused for IPsec processing; furthermore, the payload of the IPsec datagram is\nencrypted. When the IPsec datagram arrives at the salesperson’s laptop, the\nOS in the laptop decrypts the payload (and provides other security services,\nsuch as verifying data integrity) and passes the unencrypted payload to the\nupper-layer protocol (for example, to TCP or UDP).\nWe have just given a high-level overview of how an institution can\nemploy IPsec to create a VPN. To see the forest through the trees, we have\nbrushed aside many important details. Let’s now take a closer look.\n8.7.2 The AH and ESP Protocols\nIPsec is a rather complex animal—it is defined in more than a dozen RFCs.\nTwo important RFCs are RFC 4301, which describes the overall IP security\narchitecture, and RFC 6071, which provides an overview of the IPsec\nprotocol suite. Our goal in this textbook, as usual, is not simply to re-hash\nthe dry and arcane RFCs, but instead take a more operational and pedagogic\napproach to describing the protocols.\nIn the IPsec protocol suite, there are two principal protocols: the\nAuthentication Header (AH) protocol and the Encapsulation Security\nPayload (ESP) protocol. When a source IPsec entity (typically a host or a\nrouter) sends secure datagrams to a destination entity (also a host or a\nrouter), it does so with either the AH protocol or the ESP protocol. The AH\nprotocol provides source authentication and data integrity but does not\nprovide confidentiality. The ESP protocol provides source authentication,\ndata integrity, and confidentiality. Because confidentiality is often critical\nfor VPNs and other IPsec applications, the ESP protocol is much more\nwidely used than the AH protocol. In order to de-mystify IPsec and avoid\nmuch of its complication, we will henceforth focus exclusively on the ESP\nprotocol. Readers wanting to learn also about the AH protocol are\nencouraged to explore the RFCs and other online resources.\n8.7.3 Security Associations\nIPsec datagrams are sent between pairs of network entities, such as between\ntwo hosts, between two routers, or between a host and router. Before\nsending IPsec datagrams from source entity to destination entity, the source\nand destination entities create a network-layer logical connection. This\nlogical connection is called a security association (SA). An SA is a simplex\nlogical connection; that is, it is unidirectional from source to destination. If\nboth entities want to send secure datagrams to each other, then two SAs\n(that is, two logical connections) need to be established, one in each\nFor example, consider once again the institutional VPN in Figure 8.27.\nThis institution consists of a headquarters office, a branch office and, say, n\ntraveling salespersons. For the sake of example, let’s suppose that there is\nbi-directional IPsec traffic between headquarters and the branch office and\nbi-directional IPsec traffic between headquarters and the salespersons. In\nthis VPN, how many SAs are there? To answer this question, note that there\nare two SAs between the headquarters gateway router and the branch-office\ngateway router (one in each direction); for each salesperson’s laptop, there\nare two SAs between the headquarters gateway router and the laptop (again,\none in each direction). So, in total, there are (2 + 2n) SAs. Keep in mind,\nhowever, that not all traffic sent into the Internet by the gateway routers or\nby the laptops will be IPsec secured. For example, a host in headquarters\nmay want to access a Web server (such as Amazon or Google) in the public\nInternet. Thus, the gateway router (and the laptops) will emit into the\nInternet both vanilla IPv4 ­datagrams and secured IPsec datagrams.\nFigure 8.28 ♦Security association (SA) from R1 to R2\nLet’s now take a look “inside” an SA. To make the discussion tangible\nand ­concrete, let’s do this in the context of an SA from router R1 to router\nR2 in ­Figure 8.28. (You can think of Router R1 as the headquarters gateway\nrouter and Router R2 as the branch office gateway router from Figure 8.27.)\nRouter R1 will maintain state information about this SA, which will\n4.4.3 of Chapter 4. Indeed, we provided an example there of how\ngeneralized forwarding rules can be used to build a packet-filtering firewall.\nTable 8.6 ♦An access control list for a router interface\nStateful Packet Filters\nIn a traditional packet filter, filtering decisions are made on each packet in\nisolation. Stateful filters actually track TCP connections, and use this\nknowledge to make ­filtering decisions.\nTo understand stateful filters, let’s reexamine the access control list in\nTable 8.6. Although rather restrictive, the access control list in Table 8.6\nnevertheless allows any packet arriving from the outside with ACK = 1 and\nsource port 80 to get through the filter. Such packets could be used by\nattackers in attempts to crash internal systems with malformed packets,\ncarry out denial-of-service attacks, or map the internal network. The naive\nsolution is to block TCP ACK packets as well, but such an approach would\nprevent the organization’s internal users from surfing the Web.\nStateful filters solve this problem by tracking all ongoing TCP\nconnections in a connection table. This is possible because the firewall can\nobserve the beginning of a new connection by observing a three-way\nhandshake (SYN, SYNACK, and ACK); and it can observe the end of a\nconnection when it sees a FIN packet for the connection. The firewall can\nalso (conservatively) assume that the connection is over when it hasn’t seen\nany activity over the connection for, say, 60 seconds. An example\nconnection table for a firewall is shown in Table 8.7. This connection table\nindicates that there are currently three ongoing TCP connections, all of\nwhich have been initiated from within the organization. Additionally, the\nstateful filter includes a new column, “check connection,” in its access\ncontrol list, as shown in Table 8.8. Note that Table 8.8 is identical to the\naccess control list in Table 8.6, except now it indicates that the connection\nshould be checked for two of the rules.\nTable 8.7 ♦Connection table for stateful filter\nLet’s walk through some examples to see how the connection table and\nthe extended access control list work hand-in-hand. Suppose an attacker\nattempts to send a malformed packet into the organization’s network by\nsending a datagram with TCP source port 80 and with the ACK flag set.\nFurther suppose that this packet has source port number 12543 and source\nIP address 150.23.23.155. When this packet reaches the firewall, the\nfirewall checks the access control list in Table 8.7, which indicates that the\nconnection table must also be checked before permitting this packet to enter\nthe organization’s network. The firewall duly checks the connection table,\nsees that this packet is not part of an ongoing TCP connection, and rejects\nthe packet. As a second example, suppose that an internal user wants to surf\nan external Web site. Because this user first sends a TCP SYN segment, the\nuser’s TCP connection gets recorded in the connection table. When the Web\nserver sends back packets (with the ACK bit necessarily set), the firewall\nchecks the table and sees that a corresponding connection is in progress.\nThe firewall will thus let these packets pass, thereby not interfering with the\ninternal user’s Web surfing activity.\nTable 8.8 ♦Access control list for stateful filter\nApplication Gateway\nIn the examples above, we have seen that packet-level filtering allows an\nChapter 8 Review Questions\nSECTION 8.1\nR1. Operational devices such as firewalls and intrusion detection systems\nare used to counter attacks against an organization’s network. What is\nthe basic difference between a firewall and an intrusion detection\nR2. Internet entities (routers, switches, DNS servers, Web servers, user\nend systems, and so on) often need to communicate securely. Give\nthree specific example pairs of Internet entities that may want secure\ncommunication.\nSECTION 8.2\nR3. The encryption technique itself is known—published, standardized,\nand available to everyone, even a potential intruder. Then where does\nthe security of an encryption technique come from?\nR4. What is the difference between known plaintext attack and chosen\nplaintext attack?\nR5. Consider a 16-block cipher. How many possible input blocks does\nthis cipher have? How many possible mappings are there? If we view\neach mapping as a key, then how many possible keys does this cipher\nR6. Suppose N people want to communicate with each of N − 1 other\npeople using symmetric key encryption. All communication between\nany two people, i and j, is visible to all other people in this group of\nN, and no other person in this group should be able to decode their\ncommunication. How many keys are required in the system as a\nwhole? Now suppose that public key encryption is used. How many\nkeys are required in this case?\nR7. Suppose n = 1,000, a = 1,017, and b = 1,006. Use an identity of\nmodular arithmetic to calculate in your head (a · b) mod n.\nR8. Suppose you want to encrypt the message 10010111 by encrypting\nthe decimal number that corresponds to the message. What is the\ndecimal number?\nSECTIONS 8.3-8.4\nR9. In what way does a hash provide a better message integrity check\nthan a checksum (such as the Internet checksum)?\nR10. Can you “decrypt” a hash of a message to get the original message?\nExplain your answer.\nR11. Consider a variation of the MAC algorithm (Figure 8.9) where the\nsender sends (m, H(m) + s), where H(m) + s is the concatenation of\nH(m) and s. Is this variation flawed? Why or why not?\nR12. What does it mean for a signed document to be verifiable and\nnonforgeable?\nR13. In the link-state routing algorithm, we would somehow need to\ndistribute the secret authentication key to each of the routers in the\nautonomous system. How do we distribute the shared authentication\nkey to the communicating entities?\nR14. Name two popular secure networking protocols in which public key\ncertification is used.\nR15. Suppose Alice has a message that she is ready to send to anyone who\nasks. Thousands of people want to obtain Alice’s message, but each\nwants to be sure of the integrity of the message. In this context, do\nyou think a MAC-based or a digital-signature-based integrity scheme\nis more suitable? Why?\nR16. What is the purpose of a nonce in an end-point authentication\nR17. What does it mean to say that a nonce is a once-in-a-lifetime value?\nIn whose lifetime?\nR18. Is the message integrity scheme based on HMAC susceptible to\nplayback attacks? If so, how can a nonce be incorporated into the\nscheme to remove this susceptibility?\nSECTIONS 8.5-8.8\nR19. What is the de facto e-mail encryption scheme? What does it use for\nauthentication and message integrity?\nR20. In the TLS record, there is a field for TLS sequence numbers. True or\nR21. What is the purpose of the random nonces in the TLS handshake?\nR22. Suppose an TLS session employs a block cipher with CBC. True or\nfalse: The server sends to the client the IV in the clear.\nR23. Suppose Bob initiates a TCP connection to Trudy who is pretending\nto be Alice. During the handshake, Trudy sends Bob Alice’s\ncertificate. In what step of the TLS handshake algorithm will Bob\ndiscover that he is not communicating with Alice?\nR24. Consider sending a stream of packets from Host A to Host B using\nIPsec. Typically, a new SA will be established for each packet sent in\nthe stream. True or false?\nR25. Suppose that TCP is being run over IPsec between headquarters and\nthe branch office in Figure 8.28. If TCP retransmits the same packet,\nthen the two corresponding packets sent by R1 packets will have the\nsame sequence number in the ESP header. True or false?\nR26. Is there a fixed encryption algorithm in SSL?\nR27. Consider WEP for 802.11. Suppose that the data is 10001101 and the\nkeystream is 01101010. What is the resulting ciphertext?\nSECTION 8.9\nR28. Stateful packet filters maintain two data structures. Name them and\nbriefly describe what they do.\nR29. Consider a traditional (stateless) packet filter. This packet filter may\nfilter packets based on TCP flag bits as well as other header fields.\nTrue or false?\nR30. In a traditional packet filter, each interface can have its own access\ncontrol list. True or false?\nR31. Why must an application gateway work in conjunction with a router\nfilter to be effective?\nR32. Signature-based IDSs and IPSs inspect into the payloads of TCP and\nUDP segments. True or false?\nP1. Using the monoalphabetic cipher in Figure 8.3, encode the message\n“This is a secret message.”\nP2. Show that Trudy’s known-plaintext attack, in which she knows the\n(ciphertext, plaintext) translation pairs for seven letters, reduces the\nnumber of possible substitutions to be checked in the example in\nSection 8.2.1 by approximately 10 .\nP3. Consider the polyalphabetic system shown in Figure 8.4. Will a\nchosen-plaintext attack that is able to get the plaintext encoding of the\nmessage “The quick brown fox jumps over the lazy dog.” be\nsufficient to decode all messages? Why or why not?\nP4. Consider the block cipher in Figure 8.5. Suppose that each block\ncipher T  simply reverses the order of the eight input bits (so that, for\nexample, 11110000 becomes 00001111). Further suppose that the 64-\nbit scrambler does not modify any bits (so that the output value of the\nmth bit is equal to the input value of the mth bit). (a) With n = 3 and\nthe original 64-bit input equal to 10100000 repeated eight times, what\nis the value of the output? (b) Repeat part (a) but now change the last\nbit of the original 64-bit input from a 0 to a 1. (c) Repeat parts (a) and\n(b) but now suppose that the 64-bit scrambler inverses the order of the\nP5. Encode the plaintext 000001011111 with the 3-bit block cipher in\nTable 8.1 and IV = c(0) = 001. Then show that the receiver can\ndecode the ciphertext, ­knowing IV and K .\nP6. The ciphertext for the 3-bit block cipher in Table 8.1 with plaintext\n010010010 and IV = c(0) = 001 becomes:\nc(1) = K (m(1) ⊕ c(0)) = K (010 ⊕ 001) = K (011) = 100,\nc(2) = K (m(2) ⊕ c(1)) = K (010 ⊕ 100) = K (110) = 000, and\nc(3) = K (m(3) ⊕ c(2)) = K (010 ⊕ 000) = K (010) = 101.\na. Using RSA, choose p = 5 and q = 7, and encode the numbers 12,\n19, and 27 separately. Apply the decryption algorithm to the\nencrypted version to recover the original plaintext message.\nb. Choose p and q of your own and encrypt 1834 as one message\nP8. Consider RSA with p = 7 and q = 13.\na. What are n and z?\nb. Let e be 17. Why is this an acceptable choice for e?\nc. Find d such that de = 1 (mod z).\nd. Encrypt the message m = 9 using the key (n, e). Let c denote the\ncorresponding ciphertext. Show all work.\nP9. In this problem, we explore the Diffie-Hellman (DH) public-key\nencryption algorithm, which allows two entities to agree on a shared\nkey. The DH algorithm makes use of a large prime number p and\nanother large number g less than p. Both p and g are made public (so\nthat an attacker would know them). In DH, Alice and Bob each\nindependently choose secret keys, S  and S , respectively. Alice then\ncomputes her public key, T , by raising g to S  and then taking mod p.\nBob similarly computes his own public key T  by raising g to S  and\nthen taking mod p. Alice and Bob then exchange their public keys\nover the Internet. Alice then calculates the shared secret key S by\nraising T  to S  and then taking mod p. Similarly, Bob calculates the\nshared key S' by raising T  to S  and then taking mod p.\na. Prove that, in general, Alice and Bob obtain the same symmetric\nkey, that is, prove S = S'.\nb. With p = 11 and g = 2, suppose Alice and Bob choose private\nkeys S  = 5 and S  = 12, respectively. Calculate Alice’s and\nBob’s public keys, T  and T . Show all work.\nc. Following up on part (b), now calculate S as the shared\nsymmetric key. Show all work.\nd. Provide a timing diagram that shows how Diffie-Hellman can be\nattacked by a man-in-the-middle. The timing diagram should\nhave three vertical lines, one for Alice, one for Bob, and one for\nthe attacker Trudy.\nP10. Suppose Alice wants to communicate with Bob using symmetric key\ncryptography using a session key K . In Section 8.2, we learned how\npublic-key cryptography can be used to distribute the session key\nfrom Alice to Bob. In this problem, we explore how the session key\ncan be distributed—without public key cryptography—using a key\ndistribution center (KDC). The KDC is a server that shares a unique\nsecret symmetric key with each registered user. For Alice and Bob,\ndenote these keys by K\n. Design a scheme that uses the\nKDC to distribute K  to Alice and Bob. Your scheme should use three\nmessages to distribute the session key: a message from Alice to the\nKDC; a message from the KDC to Alice; and finally a message from\nAlice to Bob. The first message is K\n (A, B). Using the notation,\n, S, A, and B answer the following questions.\na. What is the second message?\nb. What is the third message?\nP11. Compute a third message, different from the two messages in Figure\n8.8, that has the same checksum as the messages in Figure 8.8.\nP12. The sender can mix some randomness into the ciphertext so that\nidentical plaintext blocks produce different ciphertext blocks. But for\neach cipher bit, the sender must now also send a random bit, doubling\nthe required ­bandwidth. Is there any way around this?\nP13. In the BitTorrent P2P file distribution protocol (see Chapter 2), the\nseed breaks the file into blocks, and the peers redistribute the blocks\nto each other. Without any protection, an attacker can easily wreak\nhavoc in a torrent by masquerading as a benevolent peer and sending\nbogus blocks to a small subset of peers in the torrent. These\nunsuspecting peers then redistribute the bogus blocks to other peers,\nwhich in turn redistribute the bogus blocks to even more peers. Thus,\nit is critical for BitTorrent to have a mechanism that allows a peer to\nverify the integrity of a block, so that it doesn’t redistribute bogus\nblocks. Assume that when a peer joins a torrent, it initially gets a\n.torrent file from a fully trusted source. Describe a simple scheme\nthat allows peers to verify the integrity of blocks.\nP14. Solving factorization in polynomial time implies breaking the RSA ­-\ncryptosystem. Is the converse true?\nP15. Consider our authentication protocol in Figure 8.18 in which Alice\nauthenticates herself to Bob, which we saw works well (i.e., we found\nno flaws in it). Now suppose that while Alice is authenticating herself\nto Bob, Bob must authenticate himself to Alice. Give a scenario by\nwhich Trudy, pretending to be Alice, can now authenticate herself to\nBob as Alice. (Hint: Consider that the sequence of operations of the\nprotocol, one with Trudy initiating and one with Bob initiating, can be\narbitrarily interleaved. Pay particular attention to the fact that both\nBob and Alice will use a nonce, and that if care is not taken, the same\nnonce can be used maliciously.)\nP16. A natural question is whether we can use a nonce and public key\ncryptography to solve the end-point authentication problem in Section\n8.4. Consider the following natural protocol: (1) Alice sends the\nmessage “I am Alice” to Bob. (2) Bob chooses a nonce, R, and\nsends it to Alice. (3) Alice uses her private key to encrypt the nonce\nand sends the resulting value to Bob. (4) Bob applies Alice’s public\nkey to the received message. Thus, Bob computes R and authenticates\na. Diagram this protocol, using the notation for public and private\nkeys employed in the textbook.\nb. Suppose that certificates are not used. Describe how Trudy can\nbecome a “woman-in-the-middle” by intercepting Alice’s\nmessages and then ­pretending to be Alice to Bob.\nP17. Figure 8.21 shows the operations that Alice must perform with PGP to\nprovide confidentiality, authentication, and integrity. Diagram the\ncorresponding operations that Bob must perform on the package\nreceived from Alice.\nP18. Suppose Alice wants to send an e-mail to Bob. Bob has a public-\nprivate key pair (K +\nB ), and Alice has Bob’s certificate. But Alice\ndoes not have a public, private key pair. Alice and Bob (and the entire\nworld) share the same hash function H(·).\na. In this situation, is it possible to design a scheme so that Bob can\nverify that Alice created the message? If so, show how with a\nblock diagram for Alice and Bob.\nb. Is it possible to design a scheme that provides confidentiality for\nsending the message from Alice to Bob? If so, show how with a\nblock diagram for Alice and Bob.\nP19. Consider the Wireshark output below for a portion of an SSL session.\na. Is Wireshark packet 112 sent by the client or server?\nb. What is the server’s IP address and port number?\nc. Assuming no loss and no retransmissions, what will be the\nsequence number of the next TCP segment sent by the client?\nd. How many SSL records does Wireshark packet 112 contain?\ne. Does packet 112 contain a Master Secret or an Encrypted Master\nSecret or neither?\nf. Assuming that the handshake type field is 1 byte and each length\nfield is 3 bytes, what are the values of the first and last bytes of\nthe Master Secret (or Encrypted Master Secret)?\ng. The client encrypted handshake message takes into account how\nmany SSL records?\nh. The server encrypted handshake message takes into account how\nmany SSL records?\nP20. In Section 8.6.1, it is shown that without sequence numbers, Trudy (a\nwoman-in-the middle) can wreak havoc in a TLS session by\ninterchanging TCP segments. Can Trudy do something similar by\ndeleting a TCP segment? What does she need to do to succeed at the\ndeletion attack? What effect will it have?\n(Wireshark screenshot reprinted by permission of the\nWireshark Foundation.)\nP21. A router’s link-state message includes a list of its directly connected\nneighbors and the direct costs to these neighbors. Once a router\nreceives link-state messages from all of the other routers, it can create\na complete map of the network, run its least-cost routing algorithm,\nand configure its forwarding table. One relatively easy attack on the\nrouting algorithm is for the attacker to distribute bogus linkstate\nmessages with incorrect link-state information. How can this be\nP22. The following true/false questions pertain to Figure 8.28.\na. When a host in 172.16.1/24 sends a datagram to an Amazon.com\nserver, the router R1 will encrypt the datagram using IPsec.\nb. When a host in 172.16.1/24 sends a datagram to a host in\n172.16.2/24, the router R1 will change the source and destination\naddress of the IP datagram.\nc. Suppose a host in 172.16.1/24 initiates a TCP connection to a\nWeb server in 172.16.2/24. As part of this connection, all\ndatagrams sent by R1 will have protocol number 50 in the left-\nmost IPv4 header field.\nd. Consider sending a TCP segment from a host in 172.16.1/24 to a",
    "unit": "Unit 2",
    "source_type": "textbook",
    "book_priority": 1,
    "source_file": "Computer Networking A Top-Down Approach",
    "chunk_id": "Computer Networking A Top-Down Approach_chunk_0"
  },
  {
    "text": "Eighth Edition\nWilliam Stallings\nUpper Saddle River, New Jersey 07458\nLibrary of Congress Cataloging-in-Publication Data on File\nVice President and Editorial Director, ECS:\nMarcia J. Horton\nExecutive Editor: Tracy Dunkelberger\nAssistant Editor: Carole Snyder\nEditorial Assistant: Christianna Lee\nExecutive Managing Editor: Vince O’Brien\nManaging Editor: Camille Trentacoste\nProduction Editor: Rose Kernan\nDirector of Creative Services: Paul Belfanti\nCreative Director: Juan Lopez\nCover Designer: Bruce Kenselaar\nManaging Editor,AV Management and Production:\nPatricia Burns\nArt Editor: Gregory Dulles\nDirector, Image Resource Center: Melinda Reo\nManager, Rights and Permissions: Zina Arabia\nManager,Visual Research: Beth Brenzel\nManager, Cover Visual Research and Permissions:\nKaren Sanatar\nManufacturing Manager, ESM: Alexis Heydt-Long\nManufacturing Buyer: Lisa McDowell\nExecutive Marketing Manager: Robin O’Brien\nMarketing Assistant: Mack Patterson\n©2007 Pearson Education, Inc.\nPearson Prentice Hall\nPearson Education, Inc.\nUpper Saddle River, NJ 07458\nAll rights reserved. No part of this book may be reproduced in any form or by any means, without permission\nin writing from the publisher.\nPearson Prentice Hall™ is a trademark of Pearson Education, Inc.\nAll other tradmarks or product names are the property of their respective owners.\nThe author and publisher of this book have used their best efforts in preparing this book.These efforts include the\ndevelopment,research,and testing of the theories and programs to determine their effectiveness.The author and\npublisher make no warranty of any kind,expressed or implied,with regard to these programs or the documentation\ncontained in this book.The author and publisher shall not be liable in any event for incidental or consequential\ndamages in connection with,or arising out of,the furnishing,performance,or use of these programs.\nPrinted in the United States of America\n10 9 8 7 6 5 4 3 2 1\nReader’s and Instructor’s Guide\nOutline of the Book\nInternet and Web Resources\nData Communications, Data Networking, and the Internet\nData Communications and Networking for Today’s Enterprise\nA Communications Model\nData Communications\nThe Internet\nAn Example Configuration\nProtocol Architecture,TCP/IP, and Internet-Based Applications\nThe Need for a Protocol Architecture\nThe TCP/IP Protocol Architecture\nThe OSI Model\nStandardization within a Protocol Architecture\nTraditional Internet-Based Applications\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nData Transmission\nConcepts and Terminology\nAnalog and Digital Data Transmission\nTransmission Impairments\nChannel Capacity\nRecommended Reading and Web Site\nKey Terms, Review Questions, and Problems\nTransmission Media\nGuided Transmission Media\nWireless Transmission\nWireless Propagation\nSignal Encoding Techniques\nDigital Data, Digital Signals\nDigital Data,Analog Signals\nAnalog Data, Digital Signals\nAnalog Data,Analog Signals\nRecommended Reading\nKey Terms, Review Questions, and Problems\nDigital Data Communication Techniques\nAsynchronous and Synchronous Transmission\nTypes of Errors\nError Detection\nError Correction\nLine Configurations\nRecommended Reading\nKey Terms, Review Questions, and Problems\nData Link Control Protocols\nFlow Control\nError Control\nHigh-Level Data Link Control (HDLC)\nRecommended Reading\nKey Terms, Review Questions, and Problems\nMultiplexing\nFrequency-Division Multiplexing\nSynchronous Time-Division Multiplexing\nStatistical Time-Division Multiplexing\nAsymmetric Digital Subscriber Line\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nSpread Spectrum 274\nThe Concept of Spread Spectrum\nFrequency Hopping Spread Spectrum\nDirect Sequence Spread Spectrum\nCode-Division Multiple Access\nRecommended Reading and Web Site\nKey Terms, Review Questions, and Problems\nChapter 10 Circuit Switching and Packet Switching\nSwitched Communications Networks\nCircuit Switching Networks\nCircuit Switching Concepts\nSoftswitch Architecture\nPacket-Switching Principles\nFrame Relay\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 11 Asynchronous Transfer Mode\nProtocol Architecture\nATM Logical Connections\nTransmission of ATM Cells\nATM Service Categories\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 12 Routing in Switched Networks\nRouting in Packet-Switching Networks\nExamples: Routing in ARPANET\nLeast-Cost Algorithms\nRecommended Reading\nKey Terms, Review Questions, and Problems\nChapter 13 Congestion Control in Data Networks\nEffects of Congestion\nCongestion Control\nTraffic Management\nCongestion Control in Packet-Switching Networks\nFrame Relay Congestion Control\nATM Traffic Management\nATM-GFR Traffic Management\nRecommended Reading\nKey Terms, Review Questions, and Problems\nChapter 14 Cellular Wireless Networks\nPrinciples of Cellular Networks\nFirst Generation Analog\nSecond Generation CDMA\nThird Generation Systems\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 15 Local Area Network Overview\nTopologies and Transmission Media\nLAN Protocol Architecture\nLayer 2 and Layer 3 Switches\nRecommended Reading and Web Site\nKey Terms, Review Questions, and Problems\nChapter 16 High-Speed LANs\nThe Emergence of High-Speed LANs\nFibre Channel\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 17 Wireless LANs\nWireless LAN Technology\nIEEE 802.11 Architecture and Services\nIEEE 802.11 Medium Access Control\nIEEE 802.11Physical Layer\nIEEE 802.11 Security Considerations\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 18 Internetwork Protocols\nBasic Protocol Functions\nPrinciples of Internetworking\nInternet Protocol Operation\nInternet Protocol\nVirtual Private Networks and IP Security\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 19 Internetwork Operation\nMulticasting\nRouting Protocols\nIntegrated Services Architecture\nDifferentiated Services\nChapter 20 Transport Protocols\nConnection-Oriented Transport Protocol Mechanisms\nTCP Congestion Control\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 21 Network Security\nSecurity Requirements and Attacks\nConfidentiality with Conventional Encryption\nMessage Authentication and Hash Functions\nPublic-Key Encryption and Digital Signatures\nSecure Socket Layer and Transport Layer Security\nIPv4 and IPv6 Security\nWi-Fi Protected Access\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 22 Internet Applications—Electronic Mail and Network Management\nElectronic Mail: SMTP and MIME\nNetwork Management: SNMP\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 23 Internet Applications—Internet Directory Service and World Wide Web\nInternet Directory Service: DNS\nWeb Access: HTTP\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nChapter 24 Internet Applications—Multimedia\nAudio and Video Compression\nReal-Time Traffic\nVoice Over IP and Multimedia Support—SIP\nReal-Time Transport Protocol (RTP)\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nThe book is divided into six parts (see Chapter 0):\n• Data Communications\n• Wide Area Networks\nibility in the design of courses. See Chapter 0 for a number of detailed suggestions for both\ntop-down and bottom-up course strategies.\nTo support instructors, the following materials are provided:\n• Solutions Manual: Solutions to all end-of-chapter Review Questions and\n• PowerPoint Slides: A set of slides covering all chapters, suitable for use in\n• PDF files: Reproductions of all figures and tables from the book.\n• Projects Manual: Suggested project assignments for all of the project cate-\ngories listed below.\nInstructors may contact their Pearson Education or Prentice Hall representative for\naccess to these materials.\nIn addition, the book’s Web site supports instructors with:\n• Links to Webs sites for other courses being taught using this book\n• Sign up information for an Internet mailing list for instructors\nThere is a Web site for this book that provides support for students and instructors.\nThe site includes links to other relevant sites, transparency masters of figures in the book,\nand sign-up information for the book’s Internet mailing list. The Web page is at\nWilliamStallings.com/DCC/DCC8e.html; see the section, Web Site for Data and Computer\n• Expanded coverage of security: Chapter 21 is more detailed; other chapters\nprovide overview of security for the relevant topic. Among the new topics are\nWi-Fi Protected Access (WPA) and the secure hash algorithm SHA-512.\n• Domain Name System (DNS): This important scheme is now covered.\n• New coverage of multimedia: Introductory section in Chapter 2; detailed cov-\nerage in Chapter 24. Topics covered include video compression, SIP, and RTP.\n• Online appendices: Fourteen online appendices provide additional detail on\nimportant topics in the text, including Sockets programming, queuing models,\nthe Internet checksum, a detailed example of TCP/IP operation, and the BNF\nIn addition, throughout the book, virtually every topic has been updated to reflect the\ndevelopments in standards and technology that have occurred since the publication of the\nseventh edition.\nCHAPTER 0 / READER’S AND INSTRUCTOR’S GUIDE\n“In the meanwhile, then,” demanded Li-loe,“relate to me the story to which reference \nhas been made, thereby proving the truth of your assertion, and at the same time\naffording an entertainment of a somewhat exceptional kind.”\n“The shadows lengthen,” replied Kai Lung,“but as the narrative in \nquestion is of an inconspicuous span I will raise no barrier against your flattering\nrequest, especially as it indicates an awakening taste \nhitherto unexpected.”\n—Kai Lung’s Golden Hours, Earnest Bramah\nThis book, with its accompanying Web site, covers a lot of material. Here we give\nthe reader some basic background information.\nThe book is organized into five parts:\nPart One. Overview: Provides an introduction to the range of topics covered in\nthe book.This part includes a general overview of data communications and net-\nworking and a discussion of protocols, OSI, and the TCP/IP protocol suite.\nPart Two. Data Communications: Concerned primarily with the exchange of\ndata between two directly connected devices.Within this restricted scope,the key\naspects of transmission, interfacing, link control, and multiplexing are examined.\nPart Three. Wide Area Networks: Examines the internal mechanisms and \nuser-network interfaces that have been developed to support voice, data, and \nmultimedia communications over long-distance networks. The traditional tech-\nnologies of packet switching and circuit switching are examined, as well as the\nmore recent ATM and wireless WANs. Separate chapters are devoted to routing\nand congestion control issues that are relevant both to switched data networks\nand to the Internet.\nPart Four. Local Area Networks: Explores the technologies and architectures\nthat have been developed for networking over shorter distances. The transmis-\nsion media, topologies, and medium access control protocols that are the key\ningredients of a LAN design are explored and specific standardized LAN sys-\ntems examined.\nPart Five. Networking Protocols: Explores both the architectural principles and\nthe mechanisms required for the exchange of data among computers, worksta-\ntions, servers, and other data processing devices. Much of the material in this part\nrelates to the TCP/IP protocol suite.\nPart Six. Internet Applications: Looks at a range of applications that operate\nover the Internet.\nA more detailed, chapter-by-chapter summary of each part appears at the\nbeginning of that part.\n0.2 / ROADMAP\nCourse Emphasis\nThe material in this book is organized into four broad categories: data transmission\nand communication; communications networks; network protocols; and applica-\ntions and security. The chapters and parts of the book are sufficiently modular to\nprovide a great deal of flexibility in the design of courses. The following are\nsuggestions for three different course designs:\n• Fundamentals of Data Communications: Parts One (overview) and Two (data\ncommunications) and Chapters 10 and 11 (circuit switching, packet switching,\n• Communications Networks: If the student has a basic background in data\ncommunications, then this course could cover Parts One (overview), Three\n(WAN), and Four (LAN).\n• Computer Networks: If the student has a basic background in data communi-\ncations, then this course could cover Part One (overview), Chapters 6 and 7\n(data communication techniques and data link control), Part Five (protocols),\nand part or all of Part Six (applications).\nIn addition, a more streamlined course that covers the entire book is possible\nby eliminating certain chapters that are not essential on a first reading. Chapters\nthat could be optional are Chapters 3 (data transmission) and 4 (transmission\nmedia), if the student has a basic understanding of these topics; Chapter 8 (multi-\nplexing); Chapter 9 (spread spectrum); Chapters 12 through 14 (routing, congestion\ncontrol, cellular networks); Chapter 18 (internetworking); and Chapter 21 (network\nBottom-Up versus Top-Down\nThe book is organized in a modular fashion.After reading Part One, the other parts\ncan be read in a number of possible sequences. Figure 0.1a shows the bottom-up\napproach provided by reading the book from front to back.With this approach, each\npart builds on the material in the previous part, so that it is always clear how a given\nlayer of functionality is supported from below. There is more material than can be\ncomfortably covered in a single semester, but the book’s organization makes it easy\nto eliminate some chapters and maintain the bottom-up sequence. Figure 0.1b\nsuggests one approach to a survey course.\nSome readers, and some instructors, are more comfortable with a top-down\napproach. After the background material (Part One), the reader continues at the\napplication level and works down through the protocol layers. This has the advan-\ntage of immediately focusing on the most visible part of the material, the applica-\ntions, and then seeing, progressively, how each layer is supported by the next layer\ndown. Figure 0.1c is an example of a comprehensive treatment and Figure 0.1d is an\nexample of a survey treatment.\nCHAPTER 0 / READER’S AND INSTRUCTOR’S GUIDE\nFinally, it is possible to select chapters to reflect specific teaching objectives by\nnot sticking to a strict chapter ordering. We give two examples used in courses\ntaught with the seventh edition. One course used the sequence Part One\n(Overview); Chapter 3 (Data Transmission); Chapter 6 (Digital Data Communica-\ntions Techniques); Chapter 7 (Data Link Control); Chapter 15 (LAN Overview);\nChapter 16 (High-Speed LANs); Chapter 10 (Circuit and Packet Switching);\nChapter 12 (Routing); Chapter 18 (Internet Protocols); and Chapter 19 (Internet\nOperation). The other course used the sequence Part One (Overview); Chapter 3\n(Data Transmission); Chapter 4 (Guided and Wireless Transmission); Chapter 5\n(Signal Encoding Techniques); Chapter 8 (Multiplexing); Chapter 15 (LAN\nSuggested Reading Orders\nData Communications\nWide Area Networks\nLocal Area Networks\nInternet and Transport Protocols\nInternet Applications\n(a) A bottom-up approach\nOverview (1, 2)\nData Communications (3, 6, 7, 8)\nWANs (10, 12)\n(b) Another bottom-up approach\nThe Internet Protocol\nInternet Applications\nData Communications\n(c) A top-down approach\nTCP/IP (18, 20)\nThe Internet Protocol\nInternet Applications\nWANs (10, 12)\n(d) Another top-down approach\n0.3 / INTERNET AND WEB RESOURCES\nOverview); Chapter 16 (High-Speed LANs); Chapter 10 (Circuit and Packet\nSwitching); Chapter 20 (Transport Protocols); Chapter 18 (Internet Protocols); and\nChapter 19 (Internet Operation).\nThere are a number of resources available on the Internet and the Web to support\nthis book and to help one keep up with developments in this field.\nWeb Sites for This Book\nA special Web page has been set up for this book at WilliamStallings.com/DCC/\nDCC8e.html. See the two-page layout at the beginning of this book for a detailed\ndescription of that site.\nAs soon as any typos or other errors are discovered, an errata list for this book\nwill be available at the Web site. Please report any errors that you spot. Errata\nsheets for my other books are at WilliamStallings.com.\nI also maintain the Computer Science Student Resource Site,\nWilliamStallings.com/StudentSupport.html. The purpose of this site is to provide docu-\nments, information, and links for computer science students and professionals. Links\nand documents are organized into four categories:\n• Math: Includes a basic math refresher, a queuing analysis primer, a number\nsystem primer, and links to numerous math sites\n• How-to: Advice and guidance for solving homework problems, writing techni-\ncal reports, and preparing technical presentations\n• Research resources: Links to important collections of papers, technical\nreports, and bibliographies\n• Miscellaneous: A variety of useful documents and links\nOther Web Sites\nThere are numerous Web sites that provide information related to the topics of this\nbook. In subsequent chapters, pointers to specific Web sites can be found in the\nRecommended Reading and Web Sites section. Because the addresses for Web sites\ntend to change frequently, I have not included URLs in the book. For all of the Web\nsites listed in the book, the appropriate link can be found at this book’s Web site.\nOther links not mentioned in this book will be added to the Web site over time.\nThe following are Web sites of general interest related to data and computer\ncommunications:\n• Network World: Information and links to resources about data communica-\ntions and networking.\n• IETF: Maintains archives that relate to the Internet and IETF activities.\nCHAPTER 0 / READER’S AND INSTRUCTOR’S GUIDE\n• Vendors: Links to thousands of hardware and software vendors who currently\nhave Web sites, as well as a list of thousands of computer and networking com-\npanies in a phone directory.\n• IEEE Communications Society: Good way to keep up on conferences, publi-\ncations, and so on.\n• ACM Special Interest Group on Communications (SIGCOMM): Good way\nto keep up on conferences, publications, and so on.\n• International Telecommunications Union: Contains a listing of ITU-T recom-\nmendations, plus information on obtaining ITU-T documents in hard copy or\n• International Organization for Standardization: Contains a listing of ISO\nstandards, plus information on obtaining ISO documents in hard copy or on\n• CommWeb: Links to vendors, tutorials, and other useful information.\n• CommsDesign: Lot of useful articles, tutorials, and product information.A bit\nhard to navigate, but worthwhile.\nUSENET Newsgroups\nA number of USENET newsgroups are devoted to some aspect of data communi-\ncations, networks, and protocols. As with virtually all USENET groups, there is a\nhigh noise-to-signal ratio, but it is worth experimenting to see if any meet your\nneeds.The most relevant are as follows:\n• comp.dcom.lans, comp.dcom.lans.misc: General discussions of LANs\n• comp.dcom.lans.ethernet: Covers Ethernet, Ethernet-like systems, and the IEEE\n802.3 CSMA/CD standards\n• comp.std.wireless: General discussion of wireless networks, including wireless\n• comp.security.misc: Computer security and encryption\n• comp.dcom.cell-relay: Covers ATM and ATM LANs\n• comp.dcom.frame-relay: Covers frame relay networks\n• comp.dcom.net-management: Discussion of network management applications,\nprotocols, and standards\n• comp.protocols.tcp-ip: The TCP/IP protocol suite\nIt has long been accepted in the telecommunications industry that standards are\nrequired to govern the physical, electrical, and procedural characteristics of com-\nmunication equipment. In the past, this view has not been embraced by the com-\nputer industry. Whereas communication equipment vendors recognize that their\n0.4 / STANDARDS\nequipment will generally interface to and communicate with other vendors’ equip-\nment, computer vendors have traditionally attempted to monopolize their cus-\ntomers.The proliferation of computers and distributed processing has made that an\nuntenable position. Computers from different vendors must communicate with\neach other and, with the ongoing evolution of protocol standards, customers will no\nlonger accept special-purpose protocol conversion software development. The\nresult is that standards now permeate all of the areas of technology discussed in\nThere are a number of advantages and disadvantages to the standards-making\nprocess. We list here the most striking ones. The principal advantages of standards\nare as follows:\n• A standard assures that there will be a large market for a particular piece of\nequipment or software. This encourages mass production and, in some cases,\nthe use of large-scale-integration (LSI) or very-large-scale-integration (VLSI)\ntechniques, resulting in lower costs.\n• A standard allows products from multiple vendors to communicate, giving the\npurchaser more flexibility in equipment selection and use.\nThe principal disadvantages are as follows:\n• A standard tends to freeze the technology. By the time a standard is devel-\noped, subjected to review and compromise, and promulgated, more efficient\ntechniques are possible.\n• There are multiple standards for the same thing. This is not a disadvantage of\nstandards per se, but of the current way things are done. Fortunately, in recent\nyears the various standards-making organizations have begun to cooperate\nmore closely. Nevertheless, there are still areas where multiple conflicting\nstandards exist.\nThroughout this book, we describe the most important standards in use or\nbeing developed for various aspects of data and computer communications.Various\norganizations have been involved in the development or promotion of these stan-\ndards. The following are the most important (in the current context) of these orga-\n• Internet Society: The Internet SOCiety (ISOC) is a professional member-\nship society with more than 150 organizational and 6000 individual mem-\nbers in over 100 countries. It provides leadership in addressing issues that\nconfront the future of the Internet and is the organization home for the\ngroups responsible for Internet infrastructure standards, including the\nInternet Engineering Task Force (IETF) and the Internet Architecture\nBoard (IAB). All of the RFCs and Internet standards are developed\nthrough these organizations.\n• IEEE 802: The IEEE (Institute of Electrical and Electronics Engineers) 802\nLAN/MAN Standards Committee develops local area network standards and\nmetropolitan area network standards. The most widely used standards are for\nthe Ethernet family, wireless LAN, bridging, and virtual bridged LANs. An\nindividual working group provides the focus for each area.\nCHAPTER 0 / READER’S AND INSTRUCTOR’S GUIDE\n1ISO is not an acronym (in which case it would be IOS), but a word, derived from the Greek, meaning\n• ITU-T: The International Telecommunication Union (ITU) is an interna-\ntional organization within the United Nations System where governments and\nthe private sector coordinate global telecom networks and services. The ITU\nTelecommunication Standardization Sector (ITU-T) is one of the three sec-\ntors of the ITU. ITU-T’s mission is the production of standards covering all\nfields of telecommunications.\n• ATM Forum: The ATM Forum is an international nonprofit organization\nformed with the objective of accelerating the use of ATM (asynchronous\ntransfer mode) products and services through a rapid convergence of interop-\nerability specifications. In addition, the Forum promotes industry cooperation\nand awareness.\n• ISO: The International Organization for Standardization (ISO)1 is a world-\nwide federation of national standards bodies from more than 140 countries,\none from each country. ISO is a nongovernmental organization that promotes\nthe development of standardization and related activities with a view to facili-\ntating the international exchange of goods and services, and to developing\ncooperation in the spheres of intellectual, scientific, technological, and eco-\nnomic activity. ISO’s work results in international agreements that are pub-\nlished as International Standards.\nChapter 1 Data Communications, Data Networks, and\nThe Internet\nChapter 1 provides an overview of Parts Two through Four of the book, giving the\n“big picture.” In essence, the book deals with four topics: data communications\nover a transmission link; wide area networks; local area networks; and protocols\nand the TCP/IP protocol architecture. Chapter 1 provides a preview of the first\nthree of these topics.\nChapter 2 Protocol Architecture,TCP/IP, and \nInternet-Based Applications\nChapter 2 discusses the concept protocol architectures. This chapter can be read\nimmediately following Chapter 1 or deferred until the beginning of Part Three,\nFour, or Five. After a general introduction, the chapter deals with the two most\nimportant protocol architectures: the Open Systems Interconnection (OSI) model\nand TCP/IP.Although the OSI model is often used as the framework for discourse in\nthis area, it is the TCP/IP protocol suite that is the basis for most commercially avail-\nable interoperable products and that is the focus of Parts Five and Six of this book.\nDATA COMMUNICATIONS, DATA\nNETWORKS, AND THE INTERNET\nData Communications and Networking for Today’s Enterprise\nA Communications Model\nData Communications\nThe Internet\nAn Example Configuration\nThe 1970s and 1980s saw a merger of the fields of computer science and data\ncommunications that profoundly changed the technology, products, and compa-\nnies of the now combined computer-communications industry. The computer-\ncommunications revolution has produced several remarkable facts:\n• There is no fundamental difference between data processing (computers)\nand data communications (transmission and switching equipment).\n• There are no fundamental differences among data, voice, and video com-\nmunications.\n• The distinction among single-processor computer,multiprocessor computer,\nlocal network, metropolitan network, and long-haul network has blurred.\nOne effect of these trends has been a growing overlap of the computer and\ncommunications industries, from component fabrication to system integration.\nAnother result is the development of integrated systems that transmit and process\nall types of data and information.Both the technology and the technical standards\norganizations are driving toward integrated public systems that make virtually all\ndata and information sources around the world easily and uniformly accessible.\nThis book aims to provide a unified view of the broad field of data and\ncomputer communications.The organization of the book reflects an attempt to\nbreak this massive subject into comprehensible parts and to build, piece by\npiece, a survey of the state of the art. This introductory chapter begins with a\ngeneral model of communications. Then a brief discussion introduces each of\nthe Parts Two through Four of this book. Chapter 2 provides an overview to\nParts Five and Six\nThe scope of this book is broad, covering three general areas: data\ncommunications, networking, and protocols; the first two are intro-\nduced in this chapter.\nData communications deals with the transmission of signals in a reli-\nable and efficient manner.Topics covered include signal transmission,\ntransmission media, signal encoding, interfacing, data link control, and\nmultiplexing.\nNetworking deals with the technology and architecture of the com-\nmunications networks used to interconnect communicating devices.\nThis field is generally divided into the topics of local area networks\n(LANs) and wide area networks (WANs).\nThe fundamental problem of communication is that of reproducing at one \npoint either exactly or approximately a message selected at another point.\n—The Mathematical Theory of Communication, Claude Shannon\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nTODAY’S ENTERPRISE\nEffective and efficient data communication and networking facilities are vital to any\nenterprise. In this section, we first look at trends that are increasing the challenge for\nthe business manager in planning and managing such facilities.Then we look specif-\nically at the requirement for ever-greater transmission speeds and network capacity.\nThree different forces have consistently driven the architecture and evolution of\ndata communications and networking facilities: traffic growth, development of new\nservices, and advances in technology.\nCommunication traffic, both local (within a building or building complex) and\nlong distance, both voice and data, has been growing at a high and steady rate for\ndecades. The increasing emphasis on office automation, remote access, online\ntransactions, and other productivity measures means that this trend is likely to con-\ntinue. Thus, managers are constantly struggling to maximize capacity and minimize\ntransmission costs.\nAs businesses rely more and more on information technology, the range of\nservices expands.This increases the demand for high-capacity networking and trans-\nmission facilities. In turn, the continuing growth in high-speed network offerings\nwith the continuing drop in prices encourages the expansion of services. Thus,\ngrowth in services and growth in traffic capacity go hand in hand. Figure 1.1 gives\nsome examples of information-based services and the data rates needed to support\nthem [ELSA02].\nFinally, trends in technology enable the provision of increasing traffic capacity\nand the support of a wide range of services. Four technology trends are particularly\n1. The trend toward faster and cheaper, both in computing and communications,\ncontinues. In terms of computing, this means more powerful computers and\nclusters of computers capable of supporting more demanding applications,\nsuch as multimedia applications. In terms of communications, the increasing\nuse of optical fiber has brought transmission prices down and greatly\nincreased capacity. For example, for long-distance telecommunication and\ndata network links, recent offerings of dense wavelength division multiplexing\n(DWDM) enable capacities of many terabits per second. For local area net-\nworks (LANs) many enterprises now have Gigabit Ethernet backbone net-\nworks and some are beginning to deploy 10-Gbps Ethernet.\n2. Both voice-oriented telecommunications networks, such as the public switched\ntelephone network (PSTN), and data networks, including the Internet, are more\n“intelligent” than ever. Two areas of intelligence are noteworthy. First, today’s\nnetworks can offer differing levels of quality of service (QoS), which include\nspecifications for maximum delay, minimum throughput, and so on. Second,\ntoday’s networks provide a variety of customizable services in the areas of net-\nwork management and security.\n1.1 / DATA COMMUNICATIONS AND NETWORKING FOR TODAY’S ENTERPRISE\nSpeed (kbps)\nTransaction processing\nMessaging/text apps\nLocation services\nStill image transfers\nInternet/VPN access\nDatabase access\nEnhanced Web surfing\nLow-quality video\nLarge file transfer\nModerate video\nInteractive entertainment\nHigh-quality video\nPerformance:\nVPN: virtual private\nServices versus Throughput Rates\n1Briefly, an intranet uses Internet and Web technology in an isolated facility internal to an enterprise; an\nextranet extends a company’s intranet out onto the Internet to allow selected customers, suppliers, and\nmobile workers to access the company’s private data and applications.\n3. The Internet, the Web, and associated applications have emerged as dominant\nfeatures of both the business and personal world,opening up many opportunities\nand challenges for managers. In addition to exploiting the Internet and the Web\nto reach customers,suppliers,and partners,enterprises have formed intranets and\nextranets1 to isolate their proprietary information free from unwanted access.\n4. There has been a trend toward ever-increasing mobility for decades, liberating\nworkers from the confines of the physical enterprise. Innovations include\nvoice mail, remote data access, pagers, fax, e-mail, cordless phones, cell phones\nand cellular networks, and Internet portals.The result is the ability of employ-\nees to take their business context with them as they move about. We are now\nseeing the growth of high-speed wireless access, which further enhances the\nability to use enterprise information resources and services anywhere.\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nData Transmission and Network Capacity Requirements\nMomentous changes in the way organizations do business and process information\nhave been driven by changes in networking technology and at the same time have\ndriven those changes. It is hard to separate chicken and egg in this field. Similarly,\nthe use of the Internet by both businesses and individuals reflects this cyclic depen-\ndency: the availability of new image-based services on the Internet (i.e., the Web)\nhas resulted in an increase in the total number of users and the traffic volume gen-\nerated by each user. This, in turn, has resulted in a need to increase the speed and\nefficiency of the Internet. On the other hand, it is only such increased speed that\nmakes the use of Web-based applications palatable to the end user.\nIn this section, we survey some of the end-user factors that fit into this equa-\ntion. We begin with the need for high-speed LANs in the business environment,\nbecause this need has appeared first and has forced the pace of networking develop-\nment. Then we look at business WAN requirements. Finally we offer a few words\nabout the effect of changes in commercial electronics on network requirements.\nThe Emergence of High-Speed LANs Personal computers and microcom-\nputer workstations began to achieve widespread acceptance in business computing\nin the early 1980s and have now achieved virtually the status of the telephone: an\nessential tool for office workers. Until relatively recently, office LANs provided\nbasic connectivity services—connecting personal computers and terminals to main-\nframes and midrange systems that ran corporate applications, and providing work-\ngroup connectivity at the departmental or divisional level. In both cases, traffic\npatterns were relatively light, with an emphasis on file transfer and electronic mail.\nThe LANs that were available for this type of workload, primarily Ethernet and\ntoken ring, are well suited to this environment.\nIn the 1990s, two significant trends altered the role of the personal computer\nand therefore the requirements on the LAN:\n1. The speed and computing power of personal computers continued to enjoy explo-\nsive growth. These more powerful platforms support graphics-intensive applica-\ntions and ever more elaborate graphical user interfaces to the operating system.\n2. MIS (management information systems) organizations have recognized the LAN\nas a viable and essential computing platform, resulting in the focus on network\ncomputing. This trend began with client/server computing, which has become a\ndominant architecture in the business environment and the more recent Web-\nfocused intranet trend. Both of these approaches involve the frequent transfer of\npotentially large volumes of data in a transaction-oriented environment.\nThe effect of these trends has been to increase the volume of data to be han-\ndled over LANs and, because applications are more interactive, to reduce the\nacceptable delay on data transfers.The earlier generation of 10-Mbps Ethernets and\n16-Mbps token rings was simply not up to the job of supporting these requirements.\nThe following are examples of requirements that call for higher-speed LANs:\n• Centralized server farms: In many applications, there is a need for user, or\nclient, systems to be able to draw huge amounts of data from multiple central-\nized servers, called server farms.An example is a color publishing operation, in\n1.1 / DATA COMMUNICATIONS AND NETWORKING FOR TODAY’S ENTERPRISE\nwhich servers typically contain tens of gigabytes of image data that must be\ndownloaded to imaging workstations.As the performance of the servers them-\nselves has increased, the bottleneck has shifted to the network.\n• Power workgroups: These groups typically consist of a small number of cooper-\nating users who need to draw massive data files across the network. Examples\nare a software development group that runs tests on a new software version, or\na computer-aided design (CAD) company that regularly runs simulations of\nnew designs. In such cases, large amounts of data are distributed to several\nworkstations, processed, and updated at very high speed for multiple iterations.\n• High-speed local backbone: As processing demand grows, LANs proliferate at\na site, and high-speed interconnection is necessary.\nCorporate Wide Area Networking Needs As recently as the early 1990s,there\nwas an emphasis in many organizations on a centralized data processing model. In a\ntypical environment, there might be significant computing facilities at a few regional\noffices, consisting of mainframes or well-equipped midrange systems.These centralized\nfacilities could handle most corporate applications, including basic finance, accounting,\nand personnel programs, as well as many of the business-specific applications. Smaller,\noutlying offices (e.g.,a bank branch) could be equipped with terminals or basic personal\ncomputers linked to one of the regional centers in a transaction-oriented environment.\nThis model began to change in the early 1990s, and the change accelerated\nthrough the mid-1990s. Many organizations have dispersed their employees into multi-\nple smaller offices. There is a growing use of telecommuting. Most significant, the\nnature of the application structure has changed. First client/server computing and,\nmore recently, intranet computing have fundamentally restructured the organizational\ndata processing environment.There is now much more reliance on personal computers,\nworkstations, and servers and much less use of centralized mainframe and midrange\nsystems.Furthermore,the virtually universal deployment of graphical user interfaces to\nthe desktop enables the end user to exploit graphic applications,multimedia,and other\ndata-intensive applications. In addition, most organizations require access to the Inter-\nnet. When a few clicks of the mouse can trigger huge volumes of data, traffic patterns\nhave become more unpredictable while the average load has risen.\nAll of these trends means that more data must be transported off premises and\ninto the wide area. It has long been accepted that in the typical business environ-\nment, about 80% of the traffic remains local and about 20% traverses wide area\nlinks. But this rule no longer applies to most companies, with a greater percentage of\nthe traffic going into the WAN environment [COHE96].This traffic flow shift places\na greater burden on LAN backbones and, of course, on the WAN facilities used by a\ncorporation.Thus, just as in the local area, changes in corporate data traffic patterns\nare driving the creation of high-speed WANs.\nDigital Electronics The rapid conversion of consumer electronics to digital\ntechnology is having an impact on both the Internet and corporate intranets. As\nthese new gadgets come into view and proliferate, they dramatically increase the\namount of image and video traffic carried by networks.\nTwo noteworthy examples of this trend are digital versatile disks (DVDs) and\ndigital still cameras. With the capacious DVD, the electronics industry has at last\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nfound an acceptable replacement for the analog VHS videotape. The DVD has\nreplaced the videotape used in videocassette recorders (VCRs) and replaced the\nCD-ROM in personal computers and servers. The DVD takes video into the digital\nage. It delivers movies with picture quality that outshines laser disks, and it can be\nrandomly accessed like audio CDs, which DVD machines can also play. Vast vol-\numes of data can be crammed onto the disk, currently seven times as much as a CD-\nROM.With DVD’s huge storage capacity and vivid quality, PC games have become\nmore realistic and educational software incorporates more video. Following in the\nwake of these developments is a new crest of traffic over the Internet and corporate\nintranets, as this material is incorporated into Web sites.\nA related product development is the digital camcorder. This product has\nmade it easier for individuals and companies to make digital video files to be placed\non corporate and Internet Web sites, again adding to the traffic burden.\nThis section introduces a simple model of communications, illustrated by the block\ndiagram in Figure 1.2a.\nThe fundamental purpose of a communications system is the exchange of data\nbetween two parties. Figure 1.2b presents one particular example, which is commu-\nnication between a workstation and a server over a public telephone network.\nAnother example is the exchange of voice signals between two telephones over the\nsame network.The key elements of the model are as follows:\n• Source. This device generates the data to be transmitted; examples are tele-\nphones and personal computers.\nPublic telephone network\nWorkstation\nDestination\nSource system\nDestination system\n(a) General block diagram\n(b) Example\nSimplified Communications Model\n1.2 / A COMMUNICATIONS MODEL\n• Transmitter: Usually, the data generated by a source system are not transmit-\nted directly in the form in which they were generated. Rather, a transmitter\ntransforms and encodes the information in such a way as to produce electro-\nmagnetic signals that can be transmitted across some sort of transmission sys-\ntem. For example, a modem takes a digital bit stream from an attached device\nsuch as a personal computer and transforms that bit stream into an analog sig-\nnal that can be handled by the telephone network.\n• Transmission system: This can be a single transmission line or a complex net-\nwork connecting source and destination.\n• Receiver: The receiver accepts the signal from the transmission system and\nconverts it into a form that can be handled by the destination device. For\nexample, a modem will accept an analog signal coming from a network or\ntransmission line and convert it into a digital bit stream.\n• Destination: Takes the incoming data from the receiver.\nThis simple narrative conceals a wealth of technical complexity. To get some\nidea of the scope of this complexity, Table 1.1 lists some of the key tasks that must be\nperformed in a data communications system. The list is somewhat arbitrary: Ele-\nments could be added; items on the list could be merged; and some items represent\nseveral tasks that are performed at different “levels” of the system. However, the list\nas it stands is suggestive of the scope of this book.\nThe first item, transmission system utilization, refers to the need to make\nefficient use of transmission facilities that are typically shared among a number of\ncommunicating devices.Various techniques (referred to as multiplexing) are used to\nallocate the total capacity of a transmission medium among a number of users.\nCongestion control techniques may be required to assure that the system is not\noverwhelmed by excessive demand for transmission services.\nTo communicate, a device must interface with the transmission system. All the\nforms of communication discussed in this book depend on the use of electromagnetic\nsignals propagated over a transmission medium. Thus, once an interface is estab-\nlished, signal generation is required for communication.The properties of the signal,\nsuch as form and intensity, must be such that the signal is (1) capable of being propa-\ngated through the transmission system, and (2) interpretable as data at the receiver.\nNot only must the signals be generated to conform to the requirements of the\ntransmission system and receiver, but also there must be some form of synchronization\nCommunications Tasks\nTransmission system utilization\nInterfacing\nSignal generation\nSynchronization\nMessage formatting\nExchange management\nError detection and correction\nNetwork management\nFlow control\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nbetween transmitter and receiver.The receiver must be able to determine when a signal\nbegins to arrive and when it ends.It must also know the duration of each signal element.\nBeyond the basic matter of deciding on the nature and timing of signals, there is\na variety of requirements for communication between two parties that might be col-\nlected under the term exchange management. If data are to be exchanged in both\ndirections over a period of time, the two parties must cooperate. For example, for two\nparties to engage in a telephone conversation, one party must dial the number of the\nother,causing signals to be generated that result in the ringing of the called phone.The\ncalled party completes a connection by lifting the receiver. For data processing\ndevices, more will be needed than simply establishing a connection; certain conven-\ntions must be decided on. These conventions may include whether both devices may\ntransmit simultaneously or must take turns, the amount of data to be sent at one time,\nthe format of the data, and what to do if certain contingencies such as an error arise.\nThe next two items might have been included under exchange management,\nbut they seem important enough to list separately. In all communications systems,\nthere is a potential for error; transmitted signals are distorted to some extent before\nreaching their destination. Error detection and correction are required in circum-\nstances where errors cannot be tolerated. This is usually the case with data process-\ning systems. For example, in transferring a file from one computer to another, it is\ntransmission; specific alternatives will be described in Chapter 5.\nThe transmitted signal s(t) presented to the medium is subject to a number\nof impairments, discussed in Chapter 3, before it reaches the receiver. Thus, the\nreceived signal r(t) may differ from s(t). The receiver will attempt to estimate\nthe original s(t), based on r(t) and its knowledge of the medium, producing a\nsequence of bits \nThese bits are sent to the output personal computer, where\nthey are briefly buffered in memory as a block of bits \nIn many cases, the\ndestination system will attempt to determine if an error has occurred and, if so,\ncooperate with the source system to eventually obtain a complete, error-free block\nof data. These data are then presented to the user via an output device, such as a\ninformation\nTransmitted\n Output data\ninformation\nDestination\nDigital bit\nDigital bit\nSimplified Data Communications Model\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nprinter or screen. The message \nas viewed by the user will usually be an exact\ncopy of the original message (m).\nNow consider a telephone conversation. In this case the input to the telephone\nis a message (m) in the form of sound waves.The sound waves are converted by the\ntelephone into electrical signals of the same frequency.These signals are transmitted\nwithout modification over the telephone line. Hence the input signal g(t) and the\ntransmitted signal s(t) are identical. The signals (t) will suffer some distortion over\nthe medium, so that r(t) will not be identical to s(t). Nevertheless, the signal r(t) is\nconverted back into a sound wave with no attempt at correction or improvement of\nsignal quality. Thus,\nis not an exact replica of m. However, the received sound\nmessage is generally comprehensible to the listener.\nThe discussion so far does not touch on other key aspects of data communica-\ntions,including data link control techniques for controlling the flow of data and detect-\ning and correcting errors, and multiplexing techniques for transmission efficiency.\nThe Transmission of Information\nThe basic building block of any communications facility is the transmission line.\nMuch of the technical detail of how information is encoded and transmitted across a\nline is of no real interest to the business manager. The manager is concerned with\nwhether the particular facility provides the required capacity, with acceptable relia-\nbility, at minimum cost. However, there are certain aspects of transmission technol-\nogy that a manager must understand to be able to ask the right questions and make\ninformed decisions.\nOne of the basic choices facing a business user is the transmission medium. For\nuse within the business premises, this choice is generally completely up to the busi-\nness. For long-distance communications, the choice is generally but not always made\nby the long-distance carrier. In either case, changes in technology are rapidly chang-\ning the mix of media used. Of particular note are fiber optic transmission and\nwireless transmission (e.g., satellite and radio).These two media are now driving the\nevolution of data communications transmission.\nThe ever-increasing capacity of fiber optic channels is making channel capac-\nity a virtually free resource. The growth of the market for optical fiber transmission\nsystems since the beginning of the 1980s is without precedent. During the past \n10 years, the cost of fiber optic transmission has dropped by more than an order of\nmagnitude, and the capacity of such systems has grown at almost as rapid a rate.\nLong-distance telephone communications trunks within the United States will soon\nconsist almost completely of fiber optic cable. Because of its high capacity and\nbecause of its security characteristics—fiber is almost impossible to tap—it is\nbecoming increasingly used within office buildings to carry the growing load of busi-\nness information. However, switching is now becoming the bottleneck.This problem\nis causing radical changes in communications architecture, including asynchronous\ntransfer mode (ATM) switching, highly parallel processing in switches, and inte-\ngrated network management schemes.\nThe second medium—wireless transmission—is a result of the trend toward\nuniversal personal telecommunications and universal access to communications.\nThe first concept refers to the ability of a person to identify himself or herself easily\n1.3 / DATA COMMUNICATIONS\nand to use conveniently any communication system in a large area (e.g., globally,\nover a continent, or in an entire country) in terms of a single account. The second\nrefers to the capability of using one’s terminal in a wide variety of environments to\nconnect to information services (e.g., to have a portable terminal that will work in\nthe office, on the street, and on airplanes equally well). This revolution in personal\ncomputing obviously involves wireless communication in a fundamental way.\nDespite the growth in the capacity and the drop in cost of transmission facili-\nties, transmission services remain the most costly component of a communications\nbudget for most businesses.Thus, the manager needs to be aware of techniques that\nincrease the efficiency of the use of these facilities. The two major approaches to\ngreater efficiency are multiplexing and compression. Multiplexing refers to the abil-\nity of a number of devices to share a transmission facility. If each device needs the\nfacility only a fraction of the time, then a sharing arrangement allows the cost of the\nfacility to be spread over many users. Compression, as the name indicates, involves\nsqueezing the data down so that a lower-capacity, cheaper transmission facility can\nbe used to meet a given demand. These two techniques show up separately and in\ncombination in a number of types of communications equipment. The manager\nneeds to understand these technologies to be able to assess the appropriateness and\ncost-effectiveness of the various products on the market.\nTransmission and Transmission Media Information can be communicated\nby converting it into an electromagnetic signal and transmitting that signal over some\nmedium, such as a twisted-pair telephone line. The most commonly used transmis-\nsion media are twisted-pair lines, coaxial cable, optical fiber cable, and terrestrial and\nsatellite microwave.The data rates that can be achieved and the rate at which errors\ncan occur depend on the nature of the signal and the type of medium. Chapters 3 and\n4 examine the significant properties of electromagnetic signals and compare the var-\nious transmission media in terms of cost, performance, and applications.\nCommunication Techniques The transmission of information across a trans-\nmission medium involves more than simply inserting a signal on the medium. The\ntechnique used to encode the information into an electromagnetic signal must be\ndetermined. There are various ways in which the encoding can be done, and the\nchoice affects performance and reliability. Furthermore, the successful transmission\nof information involves a high degree of cooperation between the various compo-\nnents.The interface between a device and the transmission medium must be agreed\non. Some means of controlling the flow of information and recovering from its loss\nor corruption must be used.These latter functions are performed by a data link con-\ntrol protocol.All these issues are examined in Chapters 5 through 7.\nTransmission Efficiency A major cost in any computer/communications facility\nis transmission cost. Because of this, it is important to maximize the amount of infor-\nmation that can be carried over a given resource or, alternatively, to minimize the\ntransmission capacity needed to satisfy a given information communications require-\nment.Two ways of achieving this objective are multiplexing and compression.The two\ntechniques can be used separately or in combination. Chapter 8 examines the three\nmost common multiplexing techniques—frequency division, synchronous time divi-\nsion, and statistical time division—as well as the important compression techniques.\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nThe number of computers in use worldwide is in the hundreds of millions. More-\nover, the expanding memory and processing power of these computers means that\nusers can put the machines to work on new kinds of applications and functions.\nAccordingly, the pressure from the users of these systems for ways to communicate\namong all these machines is irresistible. It is changing the way vendors think and the\nway all automation products and services are sold. This demand for connectivity is\nmanifested in two specific requirements: the need for communications software,\nwhich is previewed in the next section, and the need for networks.\nOne type of network that has become ubiquitous is the local area network\n(LAN). Indeed, the LAN is to be found in virtually all medium- and large-size office\nbuildings. As the number and power of computing devices have grown, so have the\nnumber and capacity of LANs to be found in an office. Although standards have\nbeen developed that reduce somewhat the number of types of LANs, there are still\nhalf a dozen general types of local area networks to choose from. Furthermore, many\noffices need more than one such network, with the attendant problems of intercon-\nnecting and managing a diverse collection of networks, computers, and terminals.\nBeyond the confines of a single office building, networks for voice, data, image,\nand video are equally important to business. Here, too, there are rapid changes.\nAdvances in technology have led to greatly increased capacity and the concept of\nintegration. Integration means that the customer equipment and networks can deal\nsimultaneously with voice, data, image, and even video. Thus, a memo or report can\nbe accompanied by voice commentary, presentation graphics, and perhaps even a\nshort video introduction or summary. Image and video services impose large\ndemands on wide area network transmission. Moreover, as LANs become ubiqui-\ntous and as their transmission rates increase, the demands on the wide area networks\nto support LAN interconnection have increased the demands on wide area network\ncapacity and switching. On the other hand, fortunately, the enormous and ever-\nincreasing capacity of fiber optic transmission provides ample resources to meet\nthese demands. However, developing switching systems with the capacity and rapid\nresponse to support these increased requirements is a challenge not yet conquered.\nThe opportunities for using networks as an aggressive competitive tool and as\na means of enhancing productivity and slashing costs are great. The manager who\nunderstands the technology and can deal effectively with vendors of service and\nequipment is able to enhance a company’s competitive position.\nIn the remainder of this section, we provide a brief overview of various net-\nworks. Parts Three and Four cover these topics in depth.\nWide Area Networks\nWide area networks generally cover a large geographical area, require the crossing\nof public right-of-ways, and rely at least in part on circuits provided by a common\ncarrier.Typically, a WAN consists of a number of interconnected switching nodes.A\ntransmission from any one device is routed through these internal nodes to the\nspecified destination device. These nodes (including the boundary nodes) are not\n1.4 / NETWORKS\nconcerned with the content of the data; rather, their purpose is to provide a\nswitching facility that will move the data from node to node until they reach their\ndestination.\nTraditionally, WANs have been implemented using one of two technologies:\ncircuit switching and packet switching. More recently, frame relay and ATM net-\nworks have assumed major roles.\nCircuit Switching In a circuit-switching network, a dedicated communications\npath is established between two stations through the nodes of the network. That\npath is a connected sequence of physical links between nodes. On each link, a logi-\ncal channel is dedicated to the connection. Data generated by the source station are\ntransmitted along the dedicated path as rapidly as possible. At each node, incoming\ndata are routed or switched to the appropriate outgoing channel without delay.The\nmost common example of circuit switching is the telephone network.\nPacket Switching A quite different approach is used in a packet-switching net-\nwork. In this case, it is not necessary to dedicate transmission capacity along a path\nthrough the network. Rather, data are sent out in a sequence of small chunks,\ncalled packets. Each packet is passed through the network from node to node along\nsome path leading from source to destination. At each node, the entire packet is\nreceived, stored briefly, and then transmitted to the next node. Packet-switching\nnetworks are commonly used for terminal-to-computer and computer-to-computer\ncommunications.\nFrame Relay Packet switching was developed at a time when digital long-\ndistance transmission facilities exhibited a relatively high error rate compared to\ntoday’s facilities. As a result, there is a considerable amount of overhead built into\npacket-switching schemes to compensate for errors. The overhead includes addi-\ntional bits added to each packet to introduce redundancy and additional processing\nat the end stations and the intermediate switching nodes to detect and recover from\nWith modern high-speed telecommunications systems, this overhead is unnec-\nessary and counterproductive. It is unnecessary because the rate of errors has been\ndramatically lowered and any remaining errors can easily be caught in the end sys-\ntems by logic that operates above the level of the packet-switching logic. It is coun-\nterproductive because the overhead involved soaks up a significant fraction of the\nhigh capacity provided by the network.\nFrame relay was developed to take advantage of these high data rates and low\nerror rates. Whereas the original packet-switching networks were designed with a\ndata rate to the end user of about 64 kbps, frame relay networks are designed to\noperate efficiently at user data rates of up to 2 Mbps. The key to achieving these\nhigh data rates is to strip out most of the overhead involved with error control.\nATM Asynchronous transfer mode (ATM), sometimes referred to as cell relay,\nis a culmination of developments in circuit switching and packet switching. ATM\ncan be viewed as an evolution from frame relay. The most obvious difference\nbetween frame relay and ATM is that frame relay uses variable-length packets,\ncalled frames, and ATM uses fixed-length packets, called cells. As with frame\nrelay, ATM provides little overhead for error control, depending on the inherent\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nreliability of the transmission system and on higher layers of logic in the end sys-\ntems to catch and correct errors. By using a fixed packet length, the processing\noverhead is reduced even further for ATM compared to frame relay. The result is\nthat ATM is designed to work in the range of 10s and 100s of Mbps, and in the\nGbps range.\nATM can also be viewed as an evolution from circuit switching. With circuit\nswitching, only fixed-data-rate circuits are available to the end system. ATM\nallows the definition of multiple virtual channels with data rates that are dynami-\ncally defined at the time the virtual channel is created. By using small, fixed-size\ncells, ATM is so efficient that it can offer a constant-data-rate channel even\nthough it is using a packet-switching technique. Thus, ATM extends circuit switch-\ning to allow multiple channels with the data rate on each channel dynamically set\nLocal Area Networks\nAs with WANs, a LAN is a communications network that interconnects a variety of\ndevices and provides a means for information exchange among those devices.There\nare several key distinctions between LANs and WANs:\n1. The scope of the LAN is small, typically a single building or a cluster of build-\nings. This difference in geographic scope leads to different technical solutions,\nas we shall see.\n2. It is usually the case that the LAN is owned by the same organization that owns\nthe attached devices. For WANs, this is less often the case, or at least a significant\nfraction of the network assets is not owned.This has two implications. First, care\nmust be taken in the choice of LAN, because there may be a substantial capital\ninvestment (compared to dial-up or leased charges for WANs) for both purchase\nand maintenance. Second, the network management responsibility for a LAN\nfalls solely on the user.\n3. The internal data rates of LANs are typically much greater than those of\nLANs come in a number of different configurations. The most common are\nswitched LANs and wireless LANs. The most common switched LAN is a switched\nEthernet LAN, which may consist of a single switch with a number of attached\ndevices, or a number of interconnected switches.Two other prominent examples are\nATM LANs, which simply use an ATM network in a local area, and Fibre Channel.\nWireless LANs use a variety of wireless transmission technologies and organiza-\ntions. LANs are examined in depth in Part Four.\nWireless Networks\nAs was just mentioned, wireless LANs are common are widely used in business\nenvironments. Wireless technology is also common for both wide area voice and\ndata networks. Wireless networks provide advantages in the areas of mobility and\nease of installation and configuration. Chapters 14 and 17 deal with wireless WANs\nand LANs, respectively.\n1.5 / THE INTERNET\nOrigins of the Internet\nThe Internet evolved from the ARPANET, which was developed in 1969 by the\nAdvanced Research Projects Agency (ARPA) of the U.S. Department of Defense.\nIt was the first operational packet-switching network.ARPANET began operations\nin four locations.Today the number of hosts is in the hundreds of millions, the num-\nber of users in the billions, and the number of countries participating nearing 200.\nThe number of connections to the Internet continues to grow exponentially.\nThe network was so successful that ARPA applied the same packet-switching\ntechnology to tactical radio communication (packet radio) and to satellite com-\nmunication (SATNET). Because the three networks operated in very different\ncommunication environments, the appropriate values for certain parameters, such\nas maximum packet size, were different in each case. Faced with the dilemma of\nintegrating these networks, Vint Cerf and Bob Kahn of ARPA started to develop\nmethods and protocols for internetworking; that is, communicating across arbi-\ntrary, multiple, packet-switched networks. They published a very influential paper\nin May of 1974 [CERF74] outlining their approach to a Transmission Control Pro-\ntocol.The proposal was refined and details filled in by the ARPANET community,\nwith major contributions from participants from European networks, such as\nCyclades (France), and EIN, eventually leading to the TCP (Transmission Control\nProtocol) and IP (Internet Protocol) protocols, which, in turn, formed the basis for\nwhat eventually became the TCP/IP protocol suite. This provided the foundation\nfor the Internet.\nKey Elements\nFigure 1.4 illustrates the key elements that comprise the Internet. The purpose of\nthe Internet, of course, is to interconnect end systems, called hosts; these include\nPCs, workstations, servers, mainframes, and so on. Most hosts that use the Internet\nare connected to a network, such as a local area network (LAN) or a wide area net-\nwork (WAN). These networks are in turn connected by routers. Each router\nattaches to two or more networks. Some hosts, such as mainframes or servers, con-\nnect directly to a router rather than through a network.\nIn essence, the Internet operates as follows. A host may send data to another\nhost anywhere on the Internet. The source host breaks the data to be sent into a\nsequence of packets, called IP datagrams or IP packets. Each packet includes a\nunique numeric address of the destination host. This address is referred to as an IP\naddress, because the address is carried in an IP packet. Based on this destination\naddress, each packet travels through a series of routers and networks from source to\ndestination. Each router, as it receives a packet, makes a routing decision and for-\nwards the packet along its way to the destination.\nInternet Architecture\nThe Internet today is made up of thousands of overlapping hierarchical networks.\nBecause of this, it is not practical to attempt a detailed description of the exact\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nWide area network\n(e.g., ATM)\nWide area network\n(e.g., ATM)\nInformation\nand workstations\nKey Elements of the Internet\narchitecture or topology of the Internet. However, an overview of the common, gen-\neral characteristics can be made. Figure 1.5 illustrates the discussion and Table 1.2\nsummarizes the terminology.\nA key element of the Internet is the set of hosts attached to it. Simply put, a\nhost is a computer.Today, computers come in many forms, including mobile phones\nand even cars. All of these forms can be hosts on the Internet. Hosts are sometimes\ngrouped together in a LAN.This is the typical configuration in a corporate environ-\nment. Individual hosts and LANs are connected to an Internet service provider\n(ISP) through a point of presence (POP).The connection is made in a series of steps\nstarting with the customer premises equipment (CPE).The CPE is the communica-\ntions equipment located onsite with the host.\nFor many home users, the CPE is a 56-kbps modem.This is perfectly adequate\nfor e-mail and related services but marginal for graphics-intensive Web surfing.\nNewer CPE offerings provide greater capacity and guaranteed service in some\ncases. A sample of these new access technologies includes DSL, cable modem, and\nsatellite. Users who connect to the Internet through their work often use worksta-\ntions or PCs connected to their employer-owned LANs, which in turn connect\nthrough shared organizational trunks to an ISP. In these cases the shared circuit is\noften a T-1 connection (1.544 Mbps), while for very large organizations T-3 connec-\ntions (44.736 Mbps) are sometimes found. Alternatively, an organization’s LAN\n1.5 / THE INTERNET\nResidential\nsubscribers\nPrivate peering\nOpen circle \u0001 NAP\nFilled circle \u0001 POP\nSimplified View of Portion of Internet\nmay be hooked to a wide area network (WAN), such as a frame relay network,\nwhich in turn connects to an ISP.\nThe CPE is physically attached to the “local loop”or “last mile.”This is the infra-\nstructure between a provider’s installation and the site where the host is located. For\nexample, a home user with a 56K modem attaches the modem to the telephone line.\nThe telephone line is typically a pair of copper wires that runs from the house to a\ncentral office (CO) owned and operated by the telephone company. In this instance\nthe local loop is the pair of copper wires running between the home and the CO. If the\nhome user has a cable modem, the local loop is the coaxial cable that runs from\nthe home to the cable company facilities.The preceding examples are a bit of an over-\nsimplification, but they suffice for this discussion. In many cases the wires that leave a\nhome are aggregated with wires from other homes and then converted to a different\nmedia such as fiber. In these cases the term local loop still refers to the path from the\nhome to the CO or cable facility.The local loop provider is not necessarily the ISP. In\nmany cases the local loop provider is the telephone company and the ISP is a large,\nnational service organization. Often, however, the local loop provider is also the ISP.\nThe ISP provides access to its larger network through a POP.A POP is simply\na facility where customers can connect to the ISP network.The facility is sometimes\nowned by the ISP, but often the ISP leases space from the local loop carrier.A POP\ncan be as simple as a bank of modems and an access server installed in a rack at the\nCO. The POPs are usually spread out over the geographic area where the provider\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nInternet Terminology\nCentral Office (CO)\nThe place where telephone companies terminate customer lines and locate switching equipment to interconnect\nthose lines with other networks.\nCustomer Premises Equipment (CPE)\nTelecommunications equipment that is located on the customer’s premises (physical location) rather than on\nthe provider’s premises or in between.Telephone handsets, modems, cable TV set-top boxes, and digital\nsubscriber line routers are examples. Historically, this term referred to equipment placed at the customer’s end\nof the telephone line and usually owned by the telephone company.Today, almost any end-user equipment can\nbe called customer premises equipment and it can be owned by the customer or by the provider.\nInternet Service Provider (ISP)\nA company that provides other companies or individuals with access to, or presence on, the Internet.An ISP\nhas the equipment and the telecommunication line access required to have a POP on the Internet for the\ngeographic area served.The larger ISPs have their own high-speed leased lines so that they are less dependent\non the telecommunication providers and can provide better service to their customers.\nNetwork Access Point (NAP)\nIn the United States, a network access point (NAP) is one of several major Internet interconnection points that\nserve to tie all the ISPs together. Originally, four NAPs—in New York,Washington, D.C., Chicago, and San\nFrancisco—were created and supported by the National Science Foundation as part of the transition from the\noriginal U.S. government—financed Internet to a commercially operated Internet. Since that time, several new\nNAPs have arrived, including WorldCom’s “MAE West” site in San Jose, California and ICS Network Systems’\n“Big East.”\nThe NAPs provide major switching facilities that serve the public in general. Companies apply to use the\nNAP facilities. Much Internet traffic is handled without involving NAPs, using peering arrangements and\ninterconnections within geographic regions.\nNetwork Service Provider (NSP)\nA company that provides backbone services to an Internet service provider (ISP).Typically, an ISP connects at\na point called an Internet exchange (IX) to a regional ISP that in turn connects to an NSP backbone.\nPoint of Presence (POP)\nA site that has a collection of telecommunications equipment, usually refers to ISP or telephone company\nsites.An ISP POP is the edge of the ISP’s network; connections from users are accepted and authenticated\nhere.An Internet access provider may operate several POPs distributed throughout its area of operation to\nincrease the chance that their subscribers will be able to reach one with a local telephone call.The largest\nnational ISPs have POPs all over the country.\noffers service. The ISP acts as a gateway to the Internet, providing many important\nservices. For most home users, the ISP provides the unique numeric IP address\nneeded to communicate with other Internet hosts. Most ISPs also provide name res-\nolution and other essential network services.The most important service an ISP pro-\nvides, though, is access to other ISP networks.Access is facilitated by formal peering\nagreements between providers. Physical access can be implemented by connecting\nPOPs from different ISPs. This can be done directly with a local connection if the\nPOPs are collocated or with leased lines when the POPs are not collocated.A more\ncommonly used mechanism is the network access point (NAP).\nA NAP is a physical facility that provides the infrastructure to move data\nbetween connected networks. In the United States, the National Science Foundation\n(NSF) privatization plan called for the creation of four NAPs.The NAPs were built\nand are operated by the private sector.The number of NAPs has grown significantly\n1.6 / AN EXAMPLE CONFIGURATION\nover the years, and the technology employed has shifted from Fiber Distributed\nData Interface (FDDI) and Ethernet to ATM and Gigabit Ethernet. Most NAPs\ntoday have an ATM core. The networks connected at a NAP are owned and oper-\nated by network service providers (NSPs). A NSP can also be an ISP but this is not\nalways the case. Peering agreements are between NSPs and do not include the NAP\noperator. The NSPs install routers at the NAP and connect them to the NAP infra-\nstructure.The NSP equipment is responsible for routing, and the NAP infrastructure\nprovides the physical access paths between routers.\nA small hypothetical example can help make the picture clearer. In this exam-\nple there are two companies, one named A, Inc. and the other B, Inc. and they are\nboth NSPs.A, Inc. and B, Inc. have a peering agreement and they both install routers\nin two NAPs, one located on the east coast of the United States and the other on the\nwest coast.There are also two other companies known as Y, Inc. and Z, Inc. and they\nare both ISPs. Finally, there is a home user named Bob and a small company named\nSmall, Inc.\nSmall, Inc. has four hosts connected together into a LAN. Each of the four\nhosts can communicate and share resources with the other three. Small, Inc. would\nlike access to a broader set of services so they contract with ISP Y, Inc. for a connec-\ntion. Small, Inc. installs a CPE to drive a leased T-1 line into a Y, Inc. POP. Once the\nCPE is connected, software automatically assigns a numeric address to each Small,\nInc. host. The Small, Inc. hosts can now communicate and share resources with any\nother host connected to the larger ISP network. On the other side of the country,\nBob decides to contract with ISP Z, Inc. He installs a modem on his phone line to\ndial into a Z, Inc. POP. Once the modem connects, a numeric address is automati-\ncally assigned to his home computer. His computer can now communicate and share\nresources with any other computer connected to the larger ISP network.\nBob’s home machine and the hosts owned by Small, Inc. cannot yet communi-\ncate.This becomes possible when their respective ISPs contract with NSPs that have\na peering agreement. In this example, the ISP Y, Inc. decides to expand its service\ncoverage to the opposite coast and contracts with the NSP A, Inc. A, Inc. sells band-\nwidth on its high-speed coast-to-coast network.The ISP Z, Inc. also wishes to expand\nits service coverage and contracts with the NSP B, Inc. Like A, Inc., B, Inc. also sells\nbandwidth on a high-speed coast-to-coast network. Because A, Inc. and B, Inc. have a\npeering agreement and have implemented the agreement at two NAPs, Bob’s home\nmachine and the hosts of Small, Inc. can now communicate and share resources.\nAlthough this example is contrived, in principle this is what the Internet is. The dif-\nferences are that the Internet has millions of hosts and many thousands of networks\nusing dozens of access technologies, including satellite, radio, leased T-1, and DSL.\nTo give some feel for the scope of concerns of Parts Two through Four, Figure 1.6\nillustrates some of the typical communications and network elements in use today.\nIn the upper-left-hand portion of the figure, we see an individual residential user\nconnected to an Internet service provider (ISP) through some sort of subscriber\nconnection. Common examples of such a connection are the public telephone\nCHAPTER 1 / DATA COMMUNICATIONS, DATA NETWORKS, AND THE INTERNET\nInformation\nHigh-speed link\n(e.g., SONET)\nand workstations\nATM network\nResidential\nInternet service\nprovider (ISP)\nA Networking Configuration\nnetwork, for which the user requires a dial-up modem (e.g. a 56-kbps modem); a dig-\nital subscriber line (DSL), which provides a high-speed link over telephone lines\nand requires a special DSL modem; and a cable TV facility, which requires a cable\nmodem. In each case, there are separate issues concerning signal encoding, error\ncontrol, and the internal structure of the subscribe network.\nTypically, an ISP will consist of a number of interconnected servers (only a\nsingle server is shown) connected to the Internet through a high-speed link. One\nexample of such a link is a SONET (synchronous optical network) line, described in\nChapter 8. The Internet consists of a number of interconnected routers that span\nthe globe. The routers forward packets of data from source to destination through\nthe Internet.\n1.6 / AN EXAMPLE CONFIGURATION\nThe lower portion of Figure 1.6 shows a LAN implemented using a single\nEthernet switch. This is a common configuration at a small business or other small\norganization.The LAN is connected to the Internet through a firewall host that pro-\nvides security services. In this example the firewall connects to the Internet through\nan ATM network.There is also a router off of the LAN hooked into a private WAN,\nwhich might be a private ATM or frame relay network.\nA variety of design issues, such as signal encoding and error control, relate to\nthe links between adjacent elements, such as between routers on the Internet or\nbetween switches in the ATM network, or between a subscriber and an ISP. The\ninternal structure of the various networks (telephone, ATM, Ethernet) raises addi-\ntional issues.We will be occupied in Parts Two through Four with the design features\nsuggested by Figure 1.6.\nPROTOCOL ARCHITECTURE,TCP/IP,\nAND INTERNET-BASED APPLICATIONS\nThe Need for a Protocol Architecture\nThe TCP/IP Protocol Architecture\nThe OSI Model\nStandardization Within a Protocol Architecure\nTraditional Internet-Based Applications\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\n1The reader may find it helpful just to skim this chapter on a first reading and then reread it more care-\nfully just before embarking on Part Five.\nTo destroy communication completely, there must be no rules in common \nbetween transmitter and receiver—neither of alphabet nor of syntax.\n—On Human Communication, Colin Cherry\nA protocol architecture is the layered structure of hardware and soft-\nware that supports the exchange of data between systems and supports\ndistributed applications, such as electronic mail and file transfer.\nAt each layer of a protocol architecture, one or more common\nprotocols are implemented in communicating systems. Each protocol\nprovides a set of rules for the exchange of data between systems.\nThe most widely used protocol architecture is the TCP/IP protocol\nsuite, which consists of the following layers: physical, network access,\ninternet, transport, and application.\nAnother important protocol architecture is the seven-layer OSI model.\nThis chapter provides a context for the detailed material that follows. It shows\nhow the concepts of Parts Two through Five fit into the broader area of computer\nnetworks and computer communications.This chapter may be read in its proper\nsequence or it may be deferred until the beginning of Part Three, Four, or Five.1\nWe begin this chapter by introducing the concept of a layered protocol\narchitecture.We then examine the most important such architecture, the TCP/IP\nprotocol suite. TCP/IP is an Internet-based concept and is the framework for\ndeveloping a complete range of computer communications standards. Virtually\nall computer vendors now provide support for this architecture. Another well-\nknown architecture is the Open Systems Interconnection (OSI) reference model.\nOSI is a standardized architecture that is often used to describe communications\nfunctions but that is now rarely implemented. OSI is briefly introduced in this\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\neither directly or via a communication network. But more is needed.Typical tasks to\nbe performed are as follow:\n1. The source system must either activate the direct data communication path or\ninform the communication network of the identity of the desired destination\n2. The source system must ascertain that the destination system is prepared to\nreceive data.\n3. The file transfer application on the source system must ascertain that the file\nmanagement program on the destination system is prepared to accept and store\nthe file for this particular user.\n4. If the file formats used on the two systems are different, one or the other sys-\ntem must perform a format translation function.\nIt is clear that there must be a high degree of cooperation between the two com-\nputer systems. Instead of implementing the logic for this as a single module, the task is\nbroken up into subtasks, each of which is implemented separately. In a protocol archi-\ntecture, the modules are arranged in a vertical stack. Each layer in the stack performs\na related subset of the functions required to communicate with another system. It\nrelies on the next lower layer to perform more primitive functions and to conceal the\ndetails of those functions. It provides services to the next higher layer. Ideally, layers\nshould be defined so that changes in one layer do not require changes in other layers.\nOf course, it takes two to communicate, so the same set of layered functions\nmust exist in two systems. Communication is achieved by having the corresponding,\nor peer, layers in two systems communicate.The peer layers communicate by means\nof formatted blocks of data that obey a set of rules or conventions known as a\nprotocol. The key features of a protocol are as follows:\n• Syntax: Concerns the format of the data blocks\n• Semantics: Includes control information for coordination and error handling\n• Timing: Includes speed matching and sequencing\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nfor providing reliability are essentially independent of the nature of the applica-\ntions.Thus, it makes sense to collect those mechanisms in a common layer shared by\nall applications; this is referred to as the host-to-host layer, or transport layer. The\nTransmission Control Protocol (TCP) is the most commonly used protocol to pro-\nvide this functionality.\nFinally, the application layer contains the logic needed to support the various\nuser applications. For each different type of application, such as file transfer, a sepa-\nrate module is needed that is peculiar to that application.\nOperation of TCP and IP\nFigure 2.1 indicates how these protocols are configured for communications. To\nmake clear that the total communications facility may consist of multiple networks,\nthe constituent networks are usually referred to as subnetworks. Some sort of net-\nwork access protocol, such as the Ethernet logic, is used to connect a computer to a\nsubnetwork. This protocol enables the host to send data across the subnetwork to\nanother host or, if the target host is on another subnetwork, to a router that will for-\nward the data. IP is implemented in all of the end systems and the routers. It acts as\na relay to move a block of data from one host, through one or more routers, to\nanother host. TCP is implemented only in the end systems; it keeps track of the\nblocks of data to assure that all are delivered reliably to the appropriate application.\nPhysical Physical\nNetwork access\nprotocol #1\nNetwork access\nprotocol #2\nGlobal internet\nSubnetwork attachment\npoint address\nLogical connection\n(e.g., virtual circuit)\nLogical connection\n(TCP connection)\n(service access point) \nTCP/IP Concepts\n2.2 / THE TCP/IP PROTOCOL ARCHITECTURE\nFor successful communication, every entity in the overall system must have a\nunique address. Actually, two levels of addressing are needed. Each host on a\nsubnetwork must have a unique global internet address; this allows the data to be\ndelivered to the proper host. Each process with a host must have an address that is\nunique within the host; this allows the host-to-host protocol (TCP) to deliver data to\nthe proper process.These latter addresses are known as ports.\nLet us trace a simple operation. Suppose that a process, associated with port 3\nat host A, wishes to send a message to another process, associated with port 2 at host\nB. The process at A hands the message down to TCP with instructions to send it to\nhost B, port 2.TCP hands the message down to IP with instructions to send it to host\nB. Note that IP need not be told the identity of the destination port. All it needs\nto know is that the data are intended for host B. Next, IP hands the message down to\nthe network access layer (e.g., Ethernet logic) with instructions to send it to router J\n(the first hop on the way to B).\nTo control this operation, control information as well as user data must be\ntransmitted, as suggested in Figure 2.2. Let us say that the sending process generates\na block of data and passes this to TCP.TCP may break this block into smaller pieces\nto make it more manageable.To each of these pieces,TCP appends control informa-\ntion known as the TCP header, forming a TCP segment. The control information is\nto be used by the peer TCP protocol entity at host B. Examples of items in this\nheader include:\n• Destination port: When the TCP entity at B receives the segment, it must\nknow to whom the data are to be delivered.\n• Sequence number: TCP numbers the segments that it sends to a particular\ndestination port sequentially, so that if they arrive out of order, the TCP entity\nat B can reorder them.\nApplication\nbyte stream\nNetwork-level\nProtocol Data Units (PDUs) in the TCP/IP Architecture\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nin the TCP segment. Chapter 20 provides more details.\nIn addition to TCP, there is one other transport-level protocol that is in com-\nmon use as part of the TCP/IP protocol suite: the User Datagram Protocol (UDP).\nUDP does not guarantee delivery, preservation of sequence, or protection against\n2.2 / THE TCP/IP PROTOCOL ARCHITECTURE\nduplication. UDP enables a procedure to send messages to other procedures with a\nminimum of protocol mechanism. Some transaction-oriented applications make use\nof UDP; one example is SNMP (Simple Network Management Protocol), the stan-\ndard network management protocol for TCP/IP networks. Because it is connection-\nless, UDP has very little to do. Essentially, it adds a port addressing capability to IP.\nThis is best seen by examining the UDP header, shown in Figure 2.3b. UDP also\nincludes a checksum to verify that no error occurs in the data; the use of the check-\nsum is optional.\nIP and IPv6\nFor decades, the keystone of the TCP/IP protocol architecture has been IP. Figure 2.4a\nshows the IP header format, which is a minimum of 20 octets, or 160 bits. The\nheader, together with the segment from the transport layer, forms an IP-level PDU\nreferred to as an IP datagram or an IP packet.The header includes 32-bit source and\ndestination addresses. The Header Checksum field is used to detect errors in the\nheader to avoid misdelivery. The Protocol field indicates which higher-layer proto-\ncol is using IP. The ID, Flags, and Fragment Offset fields are used in the fragmenta-\ntion and reassembly process. Chapter 18 provides more detail.\nIn 1995, the Internet Engineering Task Force (IETF), which develops protocol\nstandards for the Internet, issued a specification for a next-generation IP, known\nthen as IPng. This specification was turned into a standard in 1996 known as IPv6.\nIPv6 provides a number of functional enhancements over the existing IP, designed\nSource port\nDestination port\nUrgent pointer\nSequence number\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nto accommodate the higher speeds of today’s networks and the mix of data streams,\nincluding graphic and video, that are becoming more prevalent. But the driving\nforce behind the development of the new protocol was the need for more addresses.\nThe current IP uses a 32-bit address to specify a source or destination. With the\nexplosive growth of the Internet and of private networks attached to the Internet,\nthis address length became insufficient to accommodate all systems needing\naddresses. As Figure 2.4b shows, IPv6 includes 128-bit source and destination\naddress fields.\nUltimately, all installations using TCP/IP are expected to migrate from the\ncurrent IP to IPv6, but this process will take many years, if not decades.\n(a) IPv4 header\n(b) IPv6 header\nTotal Length\nIdentification\nFragment offset\nTime to Live\nHeader checksum\nOptions \u0002 padding\nSource address\nDestination address\nDS \u0001 Differentiated services field\nECN \u0001 Explicit congestion notification field\nNote: The 8-bit DS/ECN fields were formerly\nknown as the Type of Service field in the IPv4\nheader and the Traffic Class field in the IPv6\nPayload length\nNext header\nSource address\nDestination address\n2.2 / THE TCP/IP PROTOCOL ARCHITECTURE\nTCP/IP Applications\nA number of applications have been standardized to operate on top of TCP. We\nmention three of the most common here.\nThe Simple Mail Transfer Protocol (SMTP) provides a basic electronic mail\ntransport facility. It provides a mechanism for transferring messages among sepa-\nrate hosts. Features of SMTP include mailing lists, return receipts, and forwarding.\nThe SMTP protocol does not specify the way in which messages are to be created;\nsome local editing or native electronic mail facility is required. Once a message is\ncreated, SMTP accepts the message and makes use of TCP to send it to an SMTP\nmodule on another host. The target SMTP module will make use of a local elec-\ntronic mail package to store the incoming message in a user’s mailbox.\nThe File Transfer Protocol (FTP) is used to send files from one system to\nanother under user command. Both text and binary files are accommodated, and the\nprotocol provides features for controlling user access.When a user wishes to engage\nin file transfer, FTP sets up a TCP connection to the target system for the exchange\nof control messages.This connection allows user ID and password to be transmitted\nand allows the user to specify the file and file actions desired. Once a file transfer is\napproved, a second TCP connection is set up for the data transfer. The file is trans-\nferred over the data connection, without the overhead of any headers or control\ninformation at the application level.When the transfer is complete, the control con-\nnection is used to signal the completion and to accept new file transfer commands.\nTELNET provides a remote logon capability, which enables a user at a ter-\nminal or personal computer to logon to a remote computer and function as if\ndirectly connected to that computer.The protocol was designed to work with sim-\nple scroll-mode terminals. TELNET is actually implemented in two modules:\nUser TELNET interacts with the terminal I/O module to communicate with a\nlocal terminal. It converts the characteristics of real terminals to the network\nstandard and vice versa. Server TELNET interacts with an application, acting as\na surrogate terminal handler so that remote terminals appear as local to the\napplication. Terminal traffic between User and Server TELNET is carried on a\nTCP connection.\nProtocol Interfaces Each layer in the TCP/IP protocol suite interacts with its\nimmediate adjacent layers.At the source, the application layer makes use of the ser-\nvices of the end-to-end layer and provides data down to that layer. A similar rela-\ntionship exists at the interface of the end-to-end and internet layers and at the\ninterface of the internet and network access layers. At the destination, each layer\ndelivers data up to the next higher layer.\nThis use of each individual layer is not required by the architecture. As\nFigure 2.5 suggests, it is possible to develop applications that directly invoke the\nservices of any one of the layers. Most applications require a reliable end-to-end\nprotocol and thus make use of TCP. Some special-purpose applications do not\nneed the services of TCP. Some of these applications, such as the Simple Network\nManagement Protocol (SNMP), use an alternative end-to-end protocol known as\nthe User Datagram Protocol (UDP); others may make use of IP directly. Applica-\ntions that do not involve internetworking and that do not need TCP have been\ndeveloped to invoke the network access layer directly.\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nThe Open Systems Interconnection (OSI) reference model was developed by the\nInternational Organization for Standardization (ISO)2 as a model for a computer\nprotocol architecture and as a framework for developing protocol standards. The\nOSI model consists of seven layers:\n• Application\n• Presentation\n• Transport\n• Data link\nFigure 2.6 illustrates the OSI model and provides a brief definition of the functions\nperformed at each layer.The intent of the OSI model is that protocols be developed\nto perform the functions of each layer.\n2ISO is not an acronym (in which case it would be IOS), but a word, derived from the Greek isos,\nmeaning equal.\n\u0001 Border Gateway Protocol\n\u0001 File Transfer Protocol\nHTTP \u0001 Hypertext Transfer Protocol\nICMP \u0001 Internet Control Message Protocol\nIGMP \u0001 Internet Group Management Protocol\n\u0001 Internet Protocol\nMIME \u0001 Multipurpose Internet Mail Extension\nOSPF \u0001 Open Shortest Path First\nRSVP \u0001 Resource ReSerVation Protocol\nSMTP \u0001 Simple Mail Transfer Protocol\nSNMP \u0001 Simple Network Management Protocol\n\u0001 Transmission Control Protocol\n\u0001 User Datagram Protocol\nSome Protocols in the TCP/IP Protocol Suite\n2.3 / THE OSI MODEL\nThe designers of OSI assumed that this model and the protocols developed\nwithin this model would come to dominate computer communications, eventually\nreplacing proprietary protocol implementations and rival multivendor models such as\nTCP/IP.This has not happened.Although many useful protocols have been developed\nin the context of OSI, the overall seven-layer model has not flourished. Instead, the\nTCP/IP architecture has come to dominate.There are a number of reasons for this out-\ncome. Perhaps the most important is that the key TCP/IP protocols were mature and\nwell tested at a time when similar OSI protocols were in the development stage.When\nbusinesses began to recognize the need for interoperability across networks, only\nTCP/IP was available and ready to go.Another reason is that the OSI model is unnec-\nessarily complex, with seven layers to accomplish what TCP/IP does with fewer layers.\nFigure 2.7 illustrates the layers of the TCP/IP and OSI architectures, showing\nroughly the correspondence in functionality between the two.\nConcerned with transmission of unstructured bit stream over\nphysical medium; deals with the mechanical, electrical,\nfunctional, and procedural characteristics to access the\nphysical medium.\nProvides for the reliable transfer of information across the\nphysical link; sends blocks (frames) with the necessary\nsynchronization, error control, and flow control.\nProvides upper layers with independence from the data\ntransmission and switching technologies used to connect\nsystems; responsible for establishing, maintaining, and\nterminating connections.\nProvides reliable, transparent transfer of data between end\npoints; provides end-to-end error recovery and flow control.\nProvides the control structure for communication between\napplications; establishes, manages, and terminates\nconnections (sessions) between cooperating applications.\nPresentation\nProvides independence to the application processes from\ndifferences in data representation (syntax).\nApplication\nProvides access to the OSI environment for users and also\nprovides distributed information services.\nThe OSI Layers\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nStandardization within the OSI Framework3\nThe principal motivation for the development of the OSI model was to provide a\nframework for standardization. Within the model, one or more protocol stan-\ndards can be developed at each layer. The model defines in general terms the\nfunctions to be performed at that layer and facilitates the standards-making\nprocess in two ways:\n• Because the functions of each layer are well defined, standards can be devel-\noped independently and simultaneously for each layer. This speeds up the\nstandards-making process.\n• Because the boundaries between layers are well defined, changes in standards\nin one layer need not affect already existing software in another layer. This\nmakes it easier to introduce new standards.\nFigure 2.8 illustrates the use of the OSI model as such a framework. The\noverall communications function is decomposed into seven distinct layers. That\nis, the overall function is broken up into a number of modules, making the inter-\nfaces between modules as simple as possible. In addition, the design principle of\ninformation hiding is used: Lower layers are concerned with greater levels of\n3The concepts introduced in this subsection apply as well to the TCP/IP architecture.\nApplication\n(host-to-host)\nApplication\nPresentation\nA Comparison\nof the OSI and TCP/IP\nProtocol Architectures\n2.4 / STANDARDIZATION WITHIN A PROTOCOL ARCHITECURE\n(application)\ncommunication\n(modularity,\ninformation hiding)\nlayer N \u0002 1\nService from\nlayer N \u0003 1\nOSI-wide standards\n(e.g., network management, security)\nThe OSI Architecture as a Framework for Standardization\ndetail; upper layers are independent of these details. Each layer provides services\nto the next higher layer and implements a protocol to the peer layer in other \nFigure 2.9 shows more specifically the nature of the standardization required\nat each layer.Three elements are key:\n• Protocol specification: Two entities at the same layer in different systems\ncooperate and interact by means of a protocol. Because two different open\nsystems are involved, the protocol must be specified precisely. This includes\nthe format of the protocol data units exchanged, the semantics of all fields, and\nthe allowable sequence of PDUs.\n• Service definition: In addition to the protocol or protocols that operate at a\ngiven layer, standards are needed for the services that each layer provides to\nthe next higher layer. Typically, the definition of services is equivalent to a\nfunctional description that defines what services are provided, but not how the\nservices are to be provided.\n• Addressing: Each layer provides services to entities at the next higher layer.\nThese entities are referenced by means of a service access point (SAP).Thus, a\nnetwork service access point (NSAP) indicates a transport entity that is a user\nof the network service.\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nThe need to provide a precise protocol specification for open systems is\nself-evident. The other two items listed warrant further comment. With respect to\nservice definitions, the motivation for providing only a functional definition is as\nfollows. First, the interaction between two adjacent layers takes place within the\nconfines of a single open system and is not the concern of any other open system.\nThus, as long as peer layers in different systems provide the same services to their\nnext higher layers, the details of how the services are provided may differ \nfrom one system to another without loss of interoperability. Second, it will \nusually be the case that adjacent layers are implemented on the same processor.\nIn that case, we would like to leave the system programmer free to exploit \nthe hardware and operating system to provide an interface that is as efficient as\nWith respect to addressing, the use of an address mechanism at each layer,\nimplemented as a service access point, allows each layer to multiplex multiple users\nfrom the next higher layer. Multiplexing may not occur at each layer, but the model\nallows for that possibility.\nService Primitives and Parameters\nThe services between adjacent layers in the OSI architecture are expressed in\nterms of primitives and parameters. A primitive specifies the function to be per-\nformed, and the parameters are used to pass data and control information. The\nactual form of a primitive is implementation dependent. An example is a proce-\nFour types of primitives are used in standards to define the interaction\nbetween adjacent layers in the architecture. These are defined in Table 2.1. The lay-\nout of Figure 2.10a suggests the time ordering of these events. For example, consider\n(service access point)\nProtocol specification\n(precise syntax and\nsemantics for\ninteroperability)\nService definition\n(functional description\nfor internal use)\nLayer-Specific Standards\n2.4 / STANDARDIZATION WITHIN A PROTOCOL ARCHITECURE\nService Primitive Types\nA primitive issued by a service user to invoke some service and to pass the parameters needed\nto specify fully the requested service\nA primitive issued by a service provider either to\n1. indicate that a procedure has been invoked by the peer service user on the connection and\nto provide the associated parameters, or\n2. notify the service user of a provider-initiated action\nA primitive issued by a service user to acknowledge or complete some procedure previously\ninvoked by an indication to that user\nA primitive issued by a service provider to acknowledge or complete some procedure previously\ninvoked by a request by the service user\nthe transfer of data from an (N) entity to a peer (N) entity in another system. The\nfollowing steps occur:\n1. The source (N) entity invokes its (N – 1) entity with a request primitive.\nAssociated with the primitive are the parameters needed, such as the data to\nbe transmitted and the destination address.\n2. The source (N – 1) entity prepares an (N – 1) PDU to be sent to its peer (N – 1)\n3. The destination (N – 1) entity delivers the data to the appropriate destination (N)\nentity via an indication primitive, which includes the data and source address as\nparameters.\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nThis sequence of events is referred to as a confirmed service, as the initiator\nreceives confirmation that the requested service has had the desired effect at the\nother end. If only request and indication primitives are involved (corresponding to\nsteps 1 through 3), then the service dialogue is a nonconfirmed service; the initiator\nreceives no confirmation that the requested action has taken place (Figure 2.10b).\nTRADITIONAL INTERNET-BASED APPLICATIONS\nA number of applications have been standardized to operate on top of TCP. We\nmention three of the most common here.\nThe Simple Mail Transfer Protocol (SMTP) provides a basic electronic mail\ntransport facility. It provides a mechanism for transferring messages among sepa-\nrate hosts. Features of SMTP include mailing lists, return receipts, and forwarding.\nThe SMTP protocol does not specify the way in which messages are to be created;\nsome local editing or native electronic mail facility is required. Once a message is\ncreated, SMTP accepts the message and makes use of TCP to send it to an SMTP\nmodule on another host. The target SMTP module will make use of a local elec-\ntronic mail package to store the incoming message in a user’s mailbox.\nThe File Transfer Protocol (FTP) is used to send files from one system to\nanother under user command. Both text and binary files are accommodated, and the\nprotocol provides features for controlling user access.When a user wishes to engage\nin file transfer,FTP sets up a TCP connection to the target system for the exchange of\ncontrol messages.This connection allows user ID and password to be transmitted and\nallows the user to specify the file and file actions desired. Once a file transfer is\napproved, a second TCP connection is set up for the data transfer. The file is\ntransferred over the data connection, without the overhead of any headers or control\ninformation at the application level. When the transfer is complete, the control con-\nnection is used to signal the completion and to accept new file transfer commands.\nTELNET provides a remote logon capability, which enables a user at a termi-\nnal or personal computer to logon to a remote computer and function as if directly\nconnected to that computer. The protocol was designed to work with simple scroll-\nmode terminals. TELNET is actually implemented in two modules: User TELNET\ninteracts with the terminal I/O module to communicate with a local terminal. It con-\nverts the characteristics of real terminals to the network standard and vice versa.\nServer TELNET interacts with an application, acting as a surrogate terminal han-\ndler so that remote terminals appear as local to the application. Terminal traffic\nbetween User and Server TELNET is carried on a TCP connection.\nWith the increasing availability of broadband access to the Internet has come an\nincreased interest in Web-based and Internet-based multimedia applications. The\nterms multimedia and multimedia applications are used rather loosely in the litera-\nture and in commercial publications, and no single definition of the term multimedia\nhas been agreed (e.g., [JAIN94], [GRIM91], [PURC98], [PACK99]). For our pur-\nposes, the definitions in Table 2.2 provide a starting point.\n2.6 / MULTIMEDA\nMultimedia Terminology\nRefers to the form of information and includes text, still images, audio, and video.\nHuman-computer interaction involving text, graphics, voice and video. Multimedia also refers to storage\ndevices that are used to store multimedia content.\nStreaming media\nRefers to multimedia files, such as video clips and audio, that begin playing immediately or within seconds\nafter it is received by a computer from the Internet or Web.Thus, the media content is consumed as it is\ndelivered from the server rather than waiting until an entire file is downloaded.\nOne way to organize the concepts associated with multimedia is to look at a\ntaxonomy that captures a number of dimensions of this field. Figure 2.11 looks at\nmultimedia from the perspective of three different dimensions: type of media,\napplications, and the technology required to support the applications.\nMedia Types\nTypically, the term multimedia refers to four distinct types of media: text, audio,\ngraphics, and video.\nFrom a communications perspective,the term text is self-explanatory,referring to\ninformation that can be entered via a keyboard and is directly readable and printable.\nText messaging,instant messaging,and text (non-html) e-mail are common examples,as\nApplication\nTechnologies\nComputer architecture\nOperating system\nUser interface\nCompression\nSynchronization\nCommunications/networking\nQuality of service\nCollaborative work systems\nMM conferencing\nStreaming audio/video\nFigure 2.11\nA Multimedia Taxonomy\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\n4A pixel, or picture element, is the smallest element of a digital image that can be assigned a gray level.\nEquivalently, a pixel is an individual dot in a dot-matrix representation of a picture.\nare chat rooms and message boards. However, the term often is used in the broader\nsense of data that can be stored in files and databases and that does not fit into the other\nthree categories. For example, an organization’s database my contain files of numerical\ndata, in which the data are stored in a more compact form than printable characters.\nThe term audio generally encompasses two different ranges of sound. Voice, or\nspeech, refers to sounds that are produced by the human speech mechanism. Gener-\nally, a modest bandwidth (under 4 kHz) is required to transmit voice. Telephony and\nrelated applications (e.g., voice mail, audio teleconferencing, telemarketing) are the\nmost common traditional applications of voice communications technology. A\nbroader frequency spectrum is needed to support music applications, including the\ndownload of music files.\nThe image service supports the communication of individual pictures, charts, or\ndrawings. Image-based applications include facsimile, computer-aided design (CAD),\npublishing, and medical imaging. Images can be represented in a vector graphics for-\nmat, such as is used in drawing programs and PDF files. In a raster graphics format,\nan image is represented as a two-dimensional array of spots, called pixels.4 The\ncompressed JPG format is derived from a raster graphics format.\nThe video service carries sequences of pictures in time. In essence, video\nmakes use of a sequence of raster-scan images.\nMultimedia Applications\nThe Internet, until recently, has been dominated by information retrieval applications,\ne-mail, and file transfer, plus Web interfaces that emphasized text and images. Increas-\ningly, the Internet is being used for multimedia applications that involve massive\namounts of data for visualization and support of real-time interactivity.Streaming audio\nand video are perhaps the best known of such applications.An example of an interac-\ntive application is a virtual training environment involving distributed simulations and\nreal-time user interaction [VIN98]. Some other examples are shown in Table 2.3.\n[GONZ00] lists the following multimedia application domains:\n• Multimedia information systems: Databases, information kiosks, hypertexts,\nelectronic books, and multimedia expert systems\n• Multimedia communication systems: Computer-supported collaborative work,\nvideoconferencing, streaming media, and multimedia teleservices\n• Multimedia entertainment systems: 3D computer games, multiplayer network\ngames, infotainment, and interactive audiovisual productions\n• Multimedia business systems: Immersive electronic commerce, marketing,\nmultimedia presentations, video brochures, virtual shopping, and so on.\n• Multimedia educational systems: Electronic books, flexible teaching materials,\nsimulation systems, automatic testing, distance learning, and so on.\nOne point worth noting is highlighted in Figure 2.11. Although traditionally\nthe term multimedia has connoted the simultaneous use of multiple media types\n(e.g., video annotation of a text document), the term has also come to refer to\napplications that require real-time processing or communication of video or audio\n2.6 / MULTIMEDA\nDomains of Multimedia Systems and Example Applications\nExample Application\nInformation management\nHypermedia, multimedia-capable databases, content-based retrieval\nEntertainment\nComputer games, digital video, audio (MP3)\nTelecommunication\nVideoconferencing, shared workspaces, virtual communities\nInformation publishing/delivery\nOnline training, electronic books, streaming media\nalone. Thus, voice over IP (VoIP), streaming audio, and streaming video are con-\nsidered multimedia applications even though each involves a single media type.\nElastic and Inelastic Traffic\nBefore discussing multimedia technologies, it will be useful to look at a key consider-\nation,namely the type of network traffic generated by various media and applications.\nTraffic on a network or internet can be divided into two broad categories: elas-\ntic and inelastic. A consideration of their differing requirements clarifies the need\nfor an enhanced internet architecture.\nElastic traffic can adjust, over wide ranges, to changes in delay and through-\nput across an internet and still meet the needs of its applications. This is the\ntraditional type of traffic supported on TCP/IP-based internets and is the type of\ntraffic for which internets were designed. With TCP, traffic on individual\nconnections adjusts to congestion by reducing the rate at which data are\npresented to the network.\nElastic applications include common Internet-based applications, such as file\ntransfer, electronic mail, remote logon, network management, and Web access. But\nthere are differences among the requirements of these applications. For example,\n• E-mail is generally quite insensitive to changes in delay.\n• When file transfer is done online, as it frequently is, the user expects the delay\nto be proportional to the file size and so is sensitive to changes in throughput.\n• With network management, delay is generally not a serious concern. However,\nif failures in an internet are the cause of congestion, then the need for network\nmanagement messages to get through with minimum delay increases with\nincreased congestion.\n• Interactive applications, such as remote logon and Web access, are quite sensi-\ntive to delay.\nSo, even if we confine our attention to elastic traffic, an Internet service that\ncan allocate resources to traffic streams based on need, rather than just providing\nequal allocation, is useful.\nInelastic traffic does not easily adapt, if at all, to changes in delay and through-\nput across an internet. The prime example is real-time traffic, such as voice and\nvideo.The requirements for inelastic traffic may include the following:\n• Throughput: A minimum throughput value may be required. Unlike most\nelastic traffic, which can continue to deliver data with perhaps degraded ser-\nvice, many inelastic applications require a firm minimum throughput.\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\n• Delay: An example of a delay-sensitive application is stock trading; someone\nwho consistently receives later service will consistently act later, and with\ngreater disadvantage.\n• Delay variation: The larger the allowable delay, the longer the real delay in\ndelivering the data and the greater the size of the delay buffer required at\nreceivers. Real-time interactive applications, such as teleconferencing, may\nrequire a reasonable upper bound on delay variation.\n• Packet loss: Real-time applications vary in the amount of packet loss, if any,\nthat they can sustain.\nThese requirements are difficult to meet in an environment with variable\nqueuing delays and congestion losses. Accordingly, inelastic traffic introduces two\nnew requirements into the internet architecture. First, some means is needed to give\npreferential treatment to applications with more demanding requirements.Applica-\ntions need to be able to state their requirements, either ahead of time in some sort of\nservice request function, or on the fly, by means of fields in the IP packet header. A\nsecond requirement in supporting inelastic traffic in an internet architecture is that\nelastic traffic must still be supported.\nMultimedia Technologies\nFigure 2.11 lists some of the technologies that are relevant to the support of\nmultimedia applications. As can be seen, a wide range of technologies is involved.\nThe lowest four items on the list are beyond the scope of this book.The other items\nrepresent only a partial list of communications and networking technologies for\nmultimedia.These technologies and others are explored throughout the book. Here,\nwe give a brief comment on each area.\n• Compression: Digitized video, and to a much lesser extent audio, can generate\nan enormous amount of traffic on a network.A streaming application, which is\ndelivered to many users, magnifies the traffic. Accordingly, standards have\nbeen developed for producing significant savings through compression. The\nmost notable such standards are JPG for still images and MPG for video.\nCompression is examined in Part Six.\n• Communications/networking: This broad category refers to the transmission\nand networking technologies (e.g., SONET, ATM) that can support high-\nvolume multimedia traffic.\n• Protocols: A number of protocols are instrumental in supporting multimedia\ntraffic. One example is the Real-time Transport Protocol (RTP), which is\ndesigned to support inelastic traffic. RTP uses buffering and discarding strate-\ngies to assure that real-time traffic is received by the end user in a smooth con-\ntinuous stream. Another example is the Session Initiation Protocol (SIP), an\napplication-level control protocol for setting up, modifying, and terminating\nreal-time sessions between participants over an IP data network.\n• Quality of service (QoS): The Internet and its underlying local area and wide\narea networks must include a QoS capability to provide differing levels of service\n2.7 / RECOMMENDED READING AND WEB SITES\nto different types of application traffic. A QoS capability can deal with priority,\ndelay constraints, delay variability constraints, and other similar requirements.\nAll of these matters are explored subsequently in this text.\nFor the reader interested in greater detail on TCP/IP, there are two three-volume works that\nare more than adequate. The works by Comer and Stevens have become classics and are\nconsidered definitive [COME06, COME99, COME01]. The works by Stevens and Wright\nare equally worthwhile and more detailed with respect to protocol operation [STEV94,\nSTEV96, WRIG95]. A more compact and very useful reference work is [RODR02], which\ncovers the spectrum of TCP/IP-related protocols in a technically concise but thorough fash-\nion, including coverage of some protocols not found in the other two works.\n[GREE80] is a good tutorial overview of the concept of a layered protocol architec-\nture. Two early papers that provide good discussions of the design philosophy of the TCP/IP\nprotocol suite are [LEIN85] and [CLAR88].\nAlthough somewhat dated, [FURH94] remains a good overview of multimedia topics.\n[VOGE95] is a good introduction to QoS considerations for multimedia. [HELL01] is a\nlengthy and worthwhile theoretical treatment of multimedia.\nClark, D. “The Design Philosophy of the DARPA Internet Protocols.” ACM\nSIGCOMM Computer Communications Review, August 1988.\nComer, D., and Stevens, D. Internetworking with TCP/IP, Volume II: Design\nImplementation, and Internals. Upper Saddle River, NJ: Prentice Hall, 1994.\nComer, D., and Stevens, D. Internetworking with TCP/IP, Volume III: Client-\nServer Programming and Applications. Upper Saddle River, NJ: Prentice Hall, 2001.\nComer, D. Internetworking with TCP/IP, Volume I: Principles, Protocols, and\nArchitecture. Upper Saddle River, NJ: Prentice Hall, 2006.\nFurht, B.“Multimedia Systems:An Overview.” IEEE Multimedia, Spring 1994.\nGreen, P. “An Introduction to Network Architecture and Protocols.” IEEE\nTransactions on Communications, April 1980.\nHeller, R., et al. “Using a Theoretical Multimedia Taxonomy Framework.”\nACM Journal of Educational Resources in Computing, Spring 2001.\nLeiner, B.; Cole, R.; Postel, J.; and Mills, D. “The DARPA Internet Protocol\nSuite.” IEEE Communications Magazine, March 1985.\nRodriguez, A., et al. TCP/IP Tutorial and Technical Overview. Upper Saddle\nRiver: NJ: Prentice Hall, 2002.\nStevens, W. TCP/IP Illustrated, Volume 1: The Protocols. Reading, MA:\nAddison-Wesley, 1994.\nStevens, W. TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP, NNTP,\nand the UNIX(R) Domain Protocol. Reading, MA:Addison-Wesley, 1996.\nVogel,A.,et al.“Distributed Multimedia and QoS:A Survey.”IEEE Multimedia,\nSummer 1995.\nWright, G., and Stevens, W. TCP/IP Illustrated, Volume 2: The Implementation.\nReading, MA:Addison-Wesley, 1995.\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nRecommended Web sites:5\n• TCP/IP Resources List: A useful collection of FAQs, tutorials, guides, Web sites, and\nbooks about TCP/IP.\n• Networking Links: Excellent collection of links related to TCP/IP.\n• Bongo Project: Running IP over bongo drums.An excellent demonstration of the flex-\nibility of a layered protocol architecture and a source of ideas for projects.\nKEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nnetwork layer\nOpen Systems Interconnec-\nphysical layer\npresentation layer\nprotocol architecture\nprotocol data unit (PDU)\nquality of service (QoS)\nservice access point (SAP)\nsession layer\nTransmission Control Protocol\ntransport layer\nUser Datagram Protocol\napplication layer\ndata link layer\nelastic traffic\ninelastic traffic\nInternet Protocol (IP)\nInternetworking\nReview Questions\nWhat is the major function of the network access layer?\nWhat tasks are performed by the transport layer?\nWhat is a protocol?\nWhat is a protocol data unit (PDU)?\nWhat is a protocol architecture?\nWhat is TCP/IP?\nWhat are some advantages to layering as seen in the TCP/IP architecture?\nWhat is a router?\nWhich version of IP is the most prevalent today?\nDoes all traffic running on the Internet use TCP?\nCompare the address space between IPv4 and IPv6. How many bits are used in \n5Because URLs sometimes change, they are not included. For all of the Web sites listed in this and\nsubsequent chapters, the appropriate link is at this book’s Web site at williamstallings.com/DCC/\nDCC8e.html.\n2.8 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nUsing the layer models in Figure 2.12, describe the ordering and delivery of a pizza,\nindicating the interactions at each level.\na. The French and Chinese prime ministers need to come to an agreement by\ntelephone, but neither speaks the other’s language. Further, neither has on\nhand a translator that can translate to the language of the other. However,\nboth prime ministers have English translators on their staffs. Draw a diagram\nsimilar to Figure 2.12 to depict the situation, and describe the interaction and\neach level.\nb. Now suppose that the Chinese prime minister’s translator can translate only into\nJapanese and that the French prime minister has a German translator available.A\ntranslator between German and Japanese is available in Germany. Draw a new\ndiagram that reflects this arrangement and describe the hypothetical phone con-\nList the major disadvantages with the layered approach to protocols.\nTwo blue armies are each poised on opposite hills preparing to attack a single red\narmy in the valley. The red army can defeat either of the blue armies separately but\nwill fail to defeat both blue armies if they attack simultaneously.The blue armies com-\nmunicate via an unreliable communications system (a foot soldier). The commander\nwith one of the blue armies would like to attack at noon. His problem is this: If he\nsends a message to the other blue army, ordering the attack, he cannot be sure it will\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nA TCP segment consisting of 1500 bits of data and 160 bits of header is sent to the IP\nlayer, which appends another 160 bits of header.This is then transmitted through two\nnetworks, each of which uses a 24-bit packet header. The destination network has a\nmaximum packet size of 800 bits. How many bits, including headers, are delivered to\nthe network layer protocol at the destination?\nWhy is UDP needed? Why can’t a user program directly access IP?\nIP, TCP, and UDP all discard a packet that arrives with a checksum error and do not\nattempt to notify the source.Why?\nWhy does the TCP header have a header length field while the UDP header does\nThe previous version of the TFTP specification, RFC 783, included the following\nAll packets other than those used for termination are acknowledged\nindividually unless a timeout occurs.\nThe RFC 1350 specification revises this to say:\nAll packets other than duplicate ACK’s and those used for termination\nare acknowledged unless a timeout occurs.\nThe change was made to fix a problem referred to as the “Sorcerer’s Apprentice.”\nDeduce and explain the problem.\nWhat is the limiting factor in the time required to transfer a file using TFTP?\nA user on a UNIX host wants to transfer a 4000-byte text file to a Microsoft Windows\nhost. In order to do this, he transfers the file by means of TFTP, using the netascii trans-\nfer mode. Even though the transfer was reported as being performed successfully, the\nWindows host reports the resulting file size is 4050 bytes, rather than the original 4000\nbytes. Does this difference in the file sizes imply an error in the data transfer? Why or\nThe TFTP specification (RFC 1350) states that the transfer identifiers (TIDs) chosen\nfor a connection should be randomly chosen, so that the probability that the same\nnumber is chosen twice in immediate succession is very low.What would be the prob-\nlem of using the same TIDs twice in immediate succession?\nIn order to be able retransmit lost packets, TFTP must keep a copy of the data it\nsends. How many packets of data must TFTP keep at a time to implement this\nretransmission mechanism?\nTFTP, like most protocols, will never send an error packet in response to an error\npacket it receives.Why?\nWe have seen that in order to deal with lost packets,TFTP implements a timeout-and-\nretransmit scheme, by setting a retransmission timer when it transmits a packet to the\nremote host. Most TFTP implementations set this timer to a fixed value of about \n5 seconds. Discuss the advantages and the disadvantages of using a fixed value for the\nretransmission timer.\nTFTP’s timeout-and-retransmission scheme implies that all data packets will eventu-\nally be received by the destination host.Will these data also be received uncorrupted?\nWhy or why not?\nThis chapter mentions the use of Frame Relay as a specific protocol or system used to\nconnect to a wide area network. Each organization will have a certain collection of\nservices available (like Frame Relay) but this is dependent upon provider provision-\ning, cost and customer premises equipment. What are some of the services available\nto you in your area?\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\n• RRQ: The read request packet requests permission to transfer a file from the other sys-\ntem.The packet includes a file name, which is a sequence of ASCII6 bytes terminated by\na zero byte. The zero byte is the means by which the receiving TFTP entity knows\nwhen the file name is terminated. The packet also includes a mode field, which\nindicates whether the data file is to be interpreted as a string of ASCII bytes (netascii\nmode) or as raw 8-bit bytes (octet mode) of data. In netascii mode, the file is trans-\nferred as lines of characters, each terminated by a carriage return, line feed. Each sys-\ntem must translate between its own format for character files and the TFTP format.\n• WRQ: The write request packet requests permission to transfer a file to the other\n• Data: The block numbers on data packets begin with one and increase by one for each\nnew block of data. This convention enables the program to use a single number to dis-\ncriminate between new packets and duplicates.The data field is from zero to 512 bytes\nlong. If it is 512 bytes long, the block is not the last block of data; if it is from zero to \n511 bytes long, it signals the end of the transfer.\n• ACK: This packet is used to acknowledge receipt of a data packet or a WRQ packet.\nAn ACK of a data packet contains the block number of the data packet being acknowl-\nedged.An ACK of a WRQ contains a block number of zero.\nNational Standards Institute. It designates a unique 7-bit pattern for each letter, with an eighth bit\nused for parity. ASCII is equivalent to the International Reference Alphabet (IRA), defined in ITU-T\nCHAPTER 2 / PROTOCOL ARCHITECTURE,TCP/IP, AND INTERNET-BASED\nErrors and Delays\nIf TFTP operates over a network or internet (as opposed to a direct data link), it is possible\nfor packets to be lost. Because TFTP operates over UDP, which does not provide a reliable\ndelivery service, there needs to be some mechanism in TFTP to deal with lost packets. TFTP\nuses the common technique of a timeout mechanism. Suppose that A sends a packet to B that\nChapter 3 Data Transmission\nThe principles of data transmission underlie all of the concepts and\ntechniques presented in this book.To understand the need for encoding,mul-\ntiplexing, switching, error control, and so on, the reader should understand \nthe behavior of data signals propagated through a transmission medium.\nChapter 3 discusses the distinction between digital and analog data and\ndigital and analog transmission. Concepts of attenuation and noise are also\nChapter 4 Transmission Media\nTransmission media can be classified as either guided or wireless. The\nmost commonly used guided transmission media are twisted pair, coaxial\ncable, and optical fiber. Wireless techniques include terrestrial and\nsatellite microwave, broadcast radio, and infrared. Chapter 4 covers all of\nthese topics.\nChapter 5 Signal Encoding Techniques\nData come in both analog (continuous) and digital (discrete) form. For\ntransmission, input data must be encoded as an electrical signal that is\ntailored to the characteristics of the transmission medium. Both analog and\ndigital data can be represented by either analog or digital signals; each of\nthe four cases is discussed in Chapter 5.\nChapter 6 Digital Data Communication Techniques\nIn Chapter 6, the emphasis shifts from data transmission to data commu-\nnications. For two devices linked by a transmission medium to exchange\ndigital data, a high degree of cooperation is required. Typically, data are\ntransmitted one bit at a time over the medium.The timing (rate, duration,\nspacing) of these bits must be the same for transmitter and receiver. Two\ncommon communication techniques—asynchronous and synchronous—\nare explored. Following this, the chapter examines the topics of transmis-\nsion errors and error detection and correction techniques.\nChapter 7 Data Link Control Protocols\nTrue cooperative exchange of digital data between two devices requires\nsome form of data link control. Chapter 7 examines the fundamental tech-\nniques common to all data link control protocols, including flow control\nand error control, and then examines the most commonly used protocol,\nChapter 8 Multiplexing\nTransmission facilities are, by and large, expensive. It is often the case that\ntwo communication stations will not utilize the full capacity of a data link.\nFor efficiency, it should be possible to share that capacity. The generic\nterm for such sharing is multiplexing.\nChapter 8 concentrates on the three most common types of multiplexing\ntechniques. The first, frequency division multiplexing (FDM), is the most\nwidespread and is familiar to anyone who has ever used a radio or televi-\nsion set. The second is a particular case of time division multiplexing\n(TDM), often known as synchronous TDM. This is commonly used for\nmultiplexing digitized voice streams. The third type is another form of\nTDM that is more complex but potentially more efficient than synchro-\nnous TDM; it is referred to as statistical or asynchronous TDM.\nChapter 9 Spread Spectrum\nAn increasingly popular form of wireless communications is known as\nspread spectrum. Two general approaches are used: frequency hopping\nand direct sequence spread spectrum. Chapter 9 provides an overview of\nboth techniques. The chapter also looks at the concept of code division\nmultiple access (CDMA), which is an application of spread spectrum to\nprovide multiple access.\nConcepts and Terminology\nAnalog and Digital Data Transmission\nTransmission Impairments\nChannel Capacity\nRecommended Reading and Web Site\nKey Terms, Review Questions, and Problems\nCHAPTER 3 / DATA TRANSMISSION\nAll of the forms of information that are discussed in this book (voice,\ndata, image, video) can be represented by electromagnetic signals.\nDepending on the transmission medium and the communications\nenvironment, either analog or digital signals can be used to convey\ninformation.\nAny electromagnetic signal, analog or digital, is made up of a number\nof constituent frequencies.A key parameter that characterizes the sig-\nnal is bandwidth, which is the width of the range of frequencies that\ncomprises the signal. In general, the greater the bandwidth of the sig-\nnal, the greater its information-carrying capacity.\nA major problem in designing a communications facility is transmis-\nsion impairment. The most significant impairments are attenuation,\nattenuation distortion, delay distortion, and the various types of noise.\nThe various forms of noise include thermal noise, intermodulation\nnoise, crosstalk, and impulse noise. For analog signals, transmission\nimpairments introduce random effects that degrade the quality of the\nreceived information and may affect intelligibility. For digital signals,\ntransmission impairments may cause bit errors at the receiver.\nThe designer of a communications facility must deal with four factors:\nthe bandwidth of the signal, the data rate that is used for digital\ninformation, the amount of noise and other impairments, and the level\nof error rate that is acceptable. The bandwidth is limited by the\ntransmission medium and the desire to avoid interference with other\nnearby signals. Because bandwidth is a scarce resource, we would like\nto maximize the data rate that is achieved in a given bandwidth. The\ndata rate is limited by the bandwidth, the presence of impairments,\nand the error rate that is acceptable.\nToto, I’ve got a feeling we’re not in Kansas anymore.\nJudy Garland in The Wizard of Oz\nThe successful transmission of data depends principally on two factors: the qual-\nity of the signal being transmitted and the characteristics of the transmission\nmedium.The objective of this chapter and the next is to provide the reader with\nan intuitive feeling for the nature of these two factors.\nThe first section presents some concepts and terms from the field of electrical\nengineering.This should provide sufficient background to deal with the remainder\nof the chapter. Section 3.2 clarifies the use of the terms analog and digital. Either\nanalog or digital data may be transmitted using either analog or digital signals.Fur-\nthermore, it is common for intermediate processing to be performed between\nsource and destination,and this processing has either an analog or digital character.\n3.1 / CONCEPTS AND TERMINOLOGY\nSection 3.3 looks at the various impairments that may introduce errors into\nthe data during transmission.The chief impairments are attenuation, attenuation\ndistortion, delay distortion, and the various forms of noise. Finally, we look at the\nimportant concept of channel capacity.\nIn this section we introduce some concepts and terms that will be referred to\nthroughout the rest of the chapter and, indeed, throughout Part Two.\nTransmission Terminology\nData transmission occurs between transmitter and receiver over some transmission\nmedium.Transmission media may be classified as guided or unguided. In both cases,\ncommunication is in the form of electromagnetic waves. With guided media, the\nwaves are guided along a physical path; examples of guided media are twisted pair,\ncoaxial cable, and optical fiber. Unguided media, also called wireless, provide a\nmeans for transmitting electromagnetic waves but do not guide them; examples are\npropagation through air, vacuum, and seawater.\nThe term direct link is used to refer to the transmission path between two\ndevices in which signals propagate directly from transmitter to receiver with no\nintermediate devices, other than amplifiers or repeaters used to increase signal\nstrength. Note that this term can apply to both guided and unguided media.\nA guided transmission medium is point to point if it provides a direct link\nbetween two devices and those are the only two devices sharing the medium. In a\nmultipoint guided configuration, more than two devices share the same medium.\nA transmission may be simplex, half duplex, or full duplex. In simplex trans-\nmission, signals are transmitted in only one direction; one station is transmitter and\nthe other is receiver. In half-duplex operation, both stations may transmit, but only\none at a time. In full-duplex operation, both stations may transmit simultaneously. In\nthe latter case, the medium is carrying signals in both directions at the same time.\nHow this can be is explained in due course. We should note that the definitions just\ngiven are the ones in common use in the United States (ANSI definitions). Else-\nwhere (ITU-T definitions), the term simplex is used to correspond to half duplex as\ndefined previously, and duplex is used to correspond to full duplex as just defined.\nFrequency, Spectrum, and Bandwidth\nIn this book, we are concerned with electromagnetic signals used as a means to\ntransmit data. At point 3 in Figure 1.3, a signal is generated by the transmitter and\ntransmitted over a medium. The signal is a function of time, but it can also be\nexpressed as a function of frequency; that is, the signal consists of components of dif-\nferent frequencies. It turns out that the frequency domain view of a signal is more\nimportant to an understanding of data transmission than a time domain view. Both\nviews are introduced here.\nTime Domain Concepts Viewed as a function of time,an electromagnetic signal\ncan be either analog or digital. An analog signal is one in which the signal intensity\nCHAPTER 3 / DATA TRANSMISSION\n(b) Digital\nAnalog and Digital Waveforms\n1A mathematical definition: a signal s(t) is continuous if \n2This is an idealized definition. In fact, the transition from one voltage level to another will not be instan-\ntaneous, but there will be a small transition period. Nevertheless, an actual digital signal approximates\nclosely the ideal model of constant voltage levels with instantaneous transitions.\nt:a s1t2 = s1a2\nvaries in a smooth fashion over time. In other words, there are no breaks or disconti-\nnuities in the signal.1 A digital signal is one in which the signal intensity maintains a\nconstant level for some period of time and then abruptly changes to another constant\nlevel.2 Figure 3.1 shows an example of each kind of signal.The continuous signal might\nrepresent speech, and the discrete signal might represent binary 1s and 0s.\nThe simplest sort of signal is a periodic signal, in which the same signal pattern\nrepeats over time. Figure 3.2 shows an example of a periodic continuous signal (sine\nwave) and a periodic discrete signal (square wave). Mathematically, a signal s(t) is\ndefined to be periodic if and only if\nwhere the constant T is the period of the signal (T is the smallest value that satisfies\nthe equation). Otherwise, a signal is aperiodic.\nThe sine wave is the fundamental periodic signal. A general sine wave can be\nrepresented by three parameters: peak amplitude (A), frequency (f), and phase \nThe peak amplitude is the maximum value or strength of the signal over time;\ntypically, this value is measured in volts. The frequency is the rate [in cycles per\ns1t + T2 = s1t2\n- q 6 t 6 + q\n3.1 / CONCEPTS AND TERMINOLOGY\nAmplitude (volts)\nAmplitude (volts)\n(a) Sine wave\nPeriod \u0001 T \u0001 1/f\nPeriod \u0001 T \u0001 1/f\n(b) Square wave\nExamples of Periodic Signals\nsecond, or Hertz (Hz)] at which the signal repeats. An equivalent parameter is the\nperiod (T) of a signal, which is the amount of time it takes for one repetition; there-\nPhase is a measure of the relative position in time within a single\nperiod of a signal, as is illustrated subsequently. More formally, for a periodic signal\nf(t), phase is the fractional part t/T of the period T through which t has advanced rel-\native to an arbitrary origin. The origin is usually taken as the last previous passage\nthrough zero from the negative to the positive direction.\nThe general sine wave can be written\nA function with the form of the preceding equation is known as a sinusoid.\nFigure 3.3 shows the effect of varying each of the three parameters. In part (a) of the\nfigure, the frequency is 1 Hz; thus the period is \nsecond. Part (b) has the same\nfrequency and phase but a peak amplitude of 0.5. In part (c) we have \nis equivalent to \nFinally, part (d) shows the effect of a phase shift of \nradians, which is 45 degrees 12p radians = 360° = 1 period2.\ns1t2 = A sin12pft + f2\nCHAPTER 3 / DATA TRANSMISSION\n(c) A \u0001 1, f \u0001 2, F \u0001 0 \n(a) A \u0001 1, f \u0001 1, F \u0001 0\n(d) A \u0001 1, f \u0001 1, F \u0001 P/4\n(b) A \u0001 0.5, f \u0001 1, F \u0001 0\ns1t2 \u0001 A sin12pft + f2\nIn Figure 3.3, the horizontal axis is time; the graphs display the value of a sig-\nnal at a given point in space as a function of time.These same graphs, with a change\nof scale, can apply with horizontal axes in space. In this case, the graphs display the\nvalue of a signal at a given point in time as a function of distance. For example, for a\nsinusoidal transmission (e.g., an electromagnetic radio wave some distance from a\nradio antenna, or sound some distance from a loudspeaker), at a particular instant\nof time, the intensity of the signal varies in a sinusoidal way as a function of distance\nfrom the source.\nThere is a simple relationship between the two sine waves, one in time and one\nin space.The wavelength\nof a signal is the distance occupied by a single cycle, or,\nput another way, the distance between two points of corresponding phase of two\nconsecutive cycles. Assume that the signal is traveling with a velocity v. Then the\nwavelength is related to the period as follows:\nEquivalently,\nparticular relevance to this discussion is the case where \nthe speed of light in\nfree space, which is approximately \nFrequency Domain Concepts In practice, an electromagnetic signal will be\nmade up of many frequencies. For example, the signal\n3 * 108 m/s.\n3.1 / CONCEPTS AND TERMINOLOGY\n3The scaling factor of \nis used to produce a wave whose peak amplitude is close to 1.\nis shown in Figure 3.4c.The components of this signal are just sine waves of frequen-\ncies f and 3f; parts (a) and (b) of the figure show these individual components.3 There\nare two interesting points that can be made about this figure:\ns1t2 = [4/p2 * 1sin12pft2 + 11/32sin12p13f2t2]\n(a) sin(2pft)\n(b) (1/3) sin(2p(3f)t)\n(c) (4/p) [sin(2pft) + (1/3)sin(2p(3f)t)]\nAddition of Frequency Components 1T \u0001 1/f2\nCHAPTER 3 / DATA TRANSMISSION\n• The second frequency is an integer multiple of the first frequency.When all of\nthe frequency components of a signal are integer multiples of one frequency,\nthe latter frequency is referred to as the fundamental frequency.\n• The period of the total signal is equal to the period of the fundamental\nfrequency.The period of the component sin \nand the period\nof s(t) is also T, as can be seen from Figure 3.4c.\nIt can be shown, using a discipline known as Fourier analysis, that any signal is\nmade up of components at various frequencies, in which each component is a sinu-\nsoid. By adding together enough sinusoidal signals, each with the appropriate ampli-\ntude, frequency, and phase, any electromagnetic signal can be constructed. Put\nanother way, any electromagnetic signal can be shown to consist of a collection of\nperiodic analog signals (sine waves) at different amplitudes, frequencies, and phases.\nThe importance of being able to look at a signal from the frequency perspective\n(frequency domain) rather than a time perspective (time domain) should become\nclear as the discussion proceeds. For the interested reader, the subject of Fourier\nquencies. The presence of negative frequencies is a mathematical artifact whose explanation is beyond\nthe scope of this book.\n3.1 / CONCEPTS AND TERMINOLOGY\n(a) s(t) \u0001 (4/P)[sin(2Pft) \u0002 (1/3)sin(2P(3f)t)]\n(b) s(t) \u0001 1\n\u0003X/2 \u0002 t \u0002 X/2\nFrequency Domain Representations\nhere is that, although a given waveform may contain frequencies over a very broad\nrange, as a practical matter any transmission system (transmitter plus medium plus\nreceiver) will be able to accommodate only a limited band of frequencies.This, in turn,\nlimits the data rate that can be carried on the transmission medium.\nTo try to explain these relationships, consider the square wave of Figure 3.2b.\nSuppose that we let a positive pulse represent binary 0 and a negative pulse repre-\nsent binary 1. Then the waveform represents the binary stream 0101. . . . The dura-\ntion of each pulse is 1/(2f); thus the data rate is 2f bits per second (bps).What are the\nfrequency components of this signal? To answer this question, consider again\nCHAPTER 3 / DATA TRANSMISSION\n(a) s(t) \u0001 1 \u0002 (4/P) [sin(2Pft) \u0002 (1/3)sin(2P(3f)t)]\nSignal with dc Component\nFigure 3.4. By adding together sine waves at frequencies f and 3f, we get a waveform\nthat begins to resemble the original square wave. Let us continue this process by\nadding a sine wave of frequency 5f, as shown in Figure 3.7a, and then adding a sine\nwave of frequency 7f, as shown in Figure 3.7b.As we add additional odd multiples of\nf, suitably scaled, the resulting waveform approaches that of a square wave more\nand more closely.\nIndeed, it can be shown that the frequency components of the square wave\nwith amplitudes A and \ncan be expressed as follows:\nThus, this waveform has an infinite number of frequency components and hence an\ninfinite bandwidth. However, the peak amplitude of the kth frequency component,\nkf, is only 1/k, so most of the energy in this waveform is in the first few frequency\ncomponents. What happens if we limit the bandwidth to just the first three fre-\nquency components? We have already seen the answer, in Figure 3.7a. As we can\ns1t2 = A * 4\n3.1 / CONCEPTS AND TERMINOLOGY\n(a) (4/p) [sin(2pft) \u0004 (1/3)sin(2p(3f)t) \u0004 (1/5)sin(2p(5f)t)]\n(b) (4/p) [sin(2pft) \u0004 (1/3)sin(2p(3f)t) \u0004 (1/5)sin(2p(5f)t) \u0004 (1/7)sin(2p(7f)t)]\n(c) (4/p) \u0005 (1/k)sin(2p(kf)t), for k odd\nFrequency Components of Square Wave 1T = 1/f)\nsee, the shape of the resulting waveform is reasonably close to that of the original\nsquare wave.\nWe can use Figures 3.4 and 3.7 to illustrate the relationship between data rate\nand bandwidth. Suppose that we are using a digital transmission system that is capa-\nble of transmitting signals with a bandwidth of 4 MHz. Let us attempt to transmit a\nsequence of alternating 1s and 0s as the square wave of Figure 3.7c. What data rate\ncan be achieved? We look at three cases.\nCase I. Let us approximate our square wave with the waveform of Figure 3.7a.\nAlthough this waveform is a “distorted” square wave, it is sufficiently close to the\nsquare wave that a receiver should be able to discriminate between a binary 0\nCHAPTER 3 / DATA TRANSMISSION\nand a binary 1. If we let \nthen the bandwidth of\nNote that for \nthe period of the\nfundamental frequency is \nIf we treat this waveform as\na bit string of 1s and 0s, one bit occurs every \nfor a data rate of\nThus, for a bandwidth of 4 MHz, a data rate of 2 Mbps is\nCase II. Now suppose that we have a bandwidth of 8 MHz. Let us look again at\nFigure 3.7a, but now with \nUsing the same line of reasoning as\nbefore, the bandwidth of the signal is \nBut in this case \nAs a result, one bit occurs every \ndata rate of 4 Mbps. Thus, other things being equal, by doubling the bandwidth,\nwe double the potential data rate.\nCase III. Now suppose that the waveform of Figure 3.4c is considered adequate\nfor approximating a square wave. That is, the difference between a positive and\nnegative pulse in Figure 3.4c is sufficiently distinct that the waveform can be suc-\ncessfully used to represent a sequence of 1s and 0s. Assume as in Case II that\nso that one bit occurs every \ndata rate of 4 Mbps. Using the waveform of Figure 3.4c, the bandwidth of the sig-\nThus, a given bandwidth can sup-\nport various data rates depending on the ability of the receiver to discern the\ndifference between 0 and 1 in the presence of noise and other impairments.\nTo summarize,\n• Case III:\nWe can draw the following conclusions from the preceding discussion.In general,\nany digital waveform will have infinite bandwidth. If we attempt to transmit this wave-\nform as a signal over any medium,the transmission system will limit the bandwidth that\ncan be transmitted. Furthermore, for any given medium, the greater the bandwidth\ntransmitted,the greater the cost.Thus,on the one hand,economic and practical reasons\ndictate that digital information be approximated by a signal of limited bandwidth. On\nthe other hand, limiting the bandwidth creates distortions, which makes the task of\ninterpreting the received signal more difficult. The more limited the bandwidth, the\ngreater the distortion, and the greater the potential for error by the receiver.\nOne more illustration should serve to reinforce these concepts. Figure 3.8\nshows a digital bit stream with a data rate of 2000 bits per second.With a bandwidth\nof 2500 Hz, or even 1700 Hz, the representation is quite good. Furthermore, we can\ngeneralize these results. If the data rate of the digital signal is W bps, then a very\nBandwidth = 4 MHz; data rate = 4 Mbps\nBandwidth = 8 MHz; data rate = 4 Mbps\nBandwidth = 4 MHz; data rate = 2 Mbps\n13 * 2 * 1062 - 12 * 1062 = 4 MHz.\nT = 1/f = 0.5 ms,\nT = 1/f = 0.5 ms.\n15 * 2 * 1062 - 12 * 1062 = 8 MHz.\n2 * 106 = 2 Mbps.\nT = 1/106 = 10-6 = 1 ms.\n15 * 1062 - 106 = 4 MHz.\ncsin112p * 1062t2 + 1\n3 sin112p * 3 * 1062t2 + 1\n5 sin112p * 5 * 1062t2d\nf = 106 cycles/second = 1 MHz,\n3.1 / CONCEPTS AND TERMINOLOGY\nPulses before transmission:\nPulses after transmission:\nBit rate. 2000 bits per second\nBandwidth 500 Hz\nBandwidth 900 Hz\nBandwidth 1300 Hz\nBandwidth 1700 Hz\nBandwidth 2500 Hz\nBandwidth 4000 Hz\nEffect of Bandwidth on a Digital Signal\ngood representation can be achieved with a bandwidth of 2W Hz. However, unless\nnoise is very severe, the bit pattern can be recovered with less bandwidth than this\n(see the discussion of channel capacity in Section 3.4).\nThus, there is a direct relationship between data rate and bandwidth: The\nhigher the data rate of a signal, the greater is its required effective bandwidth.\nLooked at the other way, the greater the bandwidth of a transmission system, the\nhigher is the data rate that can be transmitted over that system.\nAnother observation worth making is this: If we think of the bandwidth of a\nsignal as being centered about some frequency, referred to as the center frequency,\nthen the higher the center frequency, the higher the potential bandwidth and there-\nfore the higher the potential data rate. For example, if a signal is centered at 2 MHz,\nits maximum potential bandwidth is 4 MHz.\nCHAPTER 3 / DATA TRANSMISSION\n5Note the use of a log scale for the x-axis. Because the y-axis is in units of decibels, it is effectively a log\nscale also. A basic review of log scales is in the math refresher document at the Computer Science\nStudent Resource Site at WilliamStallings.com/StudentSupport.html.\nCHAPTER 3 / DATA TRANSMISSION\n(c) Odd and even fields\n(a) Even field only\n(b) Odd field only\nFigure 3.10\nVideo Interlaced Scanning\n7IRA is defined in ITU-T Recommendation T.50 and was formerly known as International Alphabet\nNumber 5 (IA5). The U.S. national version of IRA is referred to as the American Standard Code for\nCHAPTER 3 / DATA TRANSMISSION\nIn the case of acoustic data (voice), the data can be represented directly by an\nelectromagnetic signal occupying the same spectrum. However, there is a need to\ncompromise between the fidelity of the sound as transmitted electrically and the\ncost of transmission, which increases with increasing bandwidth. As mentioned, the\nspectrum of speech is approximately 100 Hz to 7 kHz, although a much narrower\nbandwidth will produce acceptable voice reproduction.The standard spectrum for a\nvoice channel is 300 to 3400 Hz.This is adequate for speech transmission, minimizes\nrequired transmission capacity, and allows the use of rather inexpensive telephone\nsets. The telephone transmitter converts the incoming acoustic voice signal into an\nelectromagnetic signal over the range 300 to 3400 Hz.This signal is then transmitted\nthrough the telephone system to a receiver, which reproduces it as acoustic sound.\nNow let us look at the video signal. To produce a video signal, a TV camera,\nwhich performs similar functions to the TV receiver, is used. One component of the\ncamera is a photosensitive plate, upon which a scene is optically focused. An elec-\ntron beam sweeps across the plate from left to right and top to bottom, in the same\nfashion as depicted in Figure 3.10 for the receiver. As the beam sweeps, an analog\nelectric signal is developed proportional to the brightness of the scene at a particu-\nlar spot.We mentioned that a total of 483 lines are scanned at a rate of 30 complete\nscans per second. This is an approximate number taking into account the time lost\nduring the vertical retrace interval.The actual U.S. standard is 525 lines, but of these\nabout 42 are lost during vertical retrace. Thus the horizontal scanning frequency\nlines per second, or \nare allowed for horizontal retrace, leaving a total of \nper video line.\nNow we are in a position to estimate the bandwidth required for the video signal.\nTo do this we must estimate the upper (maximum) and lower (minimum) frequency of\nthe band. We use the following reasoning to arrive at the maximum frequency: The\nmaximum frequency would occur during the horizontal scan if the scene were alternat-\ning between black and white as rapidly as possible. We can estimate this maximum\nvalue by considering the resolution of the video image. In the vertical dimension, there\nare 483 lines, so the maximum vertical resolution would be 483. Experiments have\nshown that the actual subjective resolution is about 70% of that number, or about 338\nlines.In the interest of a balanced picture,the horizontal and vertical resolutions should\nbe about the same. Because the ratio of width to height of a TV screen is 4 : 3, the hori-\nzontal resolution should be about \nlines. As a worst case, a scanning\nline would be made up of 450 elements alternating black and white. The scan would\nresult in a wave, with each cycle of the wave consisting of one higher (black) and one\nlower (white) voltage level. Thus there would be \ncycles of the wave in\nfor a maximum frequency of about 4.2 MHz. This rough reasoning, in fact, is\nfairly accurate.The lower limit is a dc or zero frequency, where the dc component cor-\nresponds to the average illumination of the scene (the average value by which the\nbrightness exceeds the reference black level).Thus the bandwidth of the video signal is\napproximately\nThe foregoing discussion did not consider color or audio components of the\nsignal. It turns out that, with these included, the bandwidth remains about 4 MHz.\nFinally, the third example described is the general case of binary data. Binary\ndata is generated by terminals, computers, and other data processing equipment\n4 MHz - 0 = 4 MHz.\n450/2 = 225\n4/3 * 338 = 450\n63.5 ms/line.\n1525 lines2 * 130 scan/s2 = 15,750\n3.2 / ANALOG AND DIGITAL DATA TRANSMISSION\nUser input at a PC is converted into a stream of binary\ndigits (1s and 0s). In this graph of a typical digital signal,\nbinary one is represented by \u00035 volts and binary zero is\nrepresented by \u00045 volts. The signal for each bit has a duration\nof 0.02 ms, giving a data rate of 50,000 bits per second (50 kbps).\nFigure 3.13\nConversion of PC Input to Digital Signal\nand then converted into digital voltage pulses for transmission, as illustrated in \nFigure 3.13. A commonly used signal for such data uses two constant (dc) voltage\nlevels, one level for binary 1 and one level for binary 0. (In Chapter 5, we shall see\nthat this is but one alternative, referred to as NRZ.) Again, we are interested in\nthe bandwidth of such a signal. This will depend, in any specific case, on the exact\nshape of the waveform and the sequence of 1s and 0s. We can obtain some under-\nstanding by considering Figure 3.8 (compare Figure 3.7). As can be seen, the\ngreater the bandwidth of the signal, the more faithfully it approximates a digital\npulse stream.\nData and Signals In the foregoing discussion, we have looked at analog signals\nused to represent analog data and digital signals used to represent digital data. Gen-\nerally, analog data are a function of time and occupy a limited frequency spectrum;\nsuch data can be represented by an electromagnetic signal occupying the same spec-\ntrum. Digital data can be represented by digital signals, with a different voltage level\nfor each of the two binary digits.\nAs Figure 3.14 illustrates, these are not the only possibilities. Digital data can\nalso be represented by analog signals by use of a modem (modulator/demodulator).\nThe modem converts a series of binary (two-valued) voltage pulses into an analog\nsignal by encoding the digital data onto a carrier frequency. The resulting signal\noccupies a certain spectrum of frequency centered about the carrier and may be\npropagated across a medium suitable for that carrier. The most common modems\nrepresent digital data in the voice spectrum and hence allow those data to be prop-\nagated over ordinary voice-grade telephone lines. At the other end of the line,\nanother modem demodulates the signal to recover the original data.\nIn an operation very similar to that performed by a modem, analog data can\nbe represented by digital signals. The device that performs this function for voice\ndata is a codec (coder-decoder). In essence, the codec takes an analog signal that\ndirectly represents the voice data and approximates that signal by a bit stream. At\nthe receiving end, the bit stream is used to reconstruct the analog data.\nThus, Figure 3.14 suggests that data may be encoded into signals in a variety of\nways.We will return to this topic in Chapter 5.\nCHAPTER 3 / DATA TRANSMISSION\nAnalog signals: Represent data with continuously\nvarying electromagnetic wave\nAnalog data\n(voice sound waves)\nAnalog signal\nDigital data\n(binary voltage pulses)\nAnalog signal\n(modulated on\ncarrier frequency)\nDigital signals: Represent data with sequence\nof voltage pulses\nAnalog signal\nDigital signal\nDigital data\nDigital signal\ntransceiver\nFigure 3.14\nAnalog and Digital Signaling of Analog and Digital Data\nAnalog and Digital Transmission\nBoth analog and digital signals may be transmitted on suitable transmission\nmedia.The way these signals are treated is a function of the transmission system.\nTable 3.1 summarizes the methods of data transmission. Analog transmission is\na means of transmitting analog signals without regard to their content; the sig-\nnals may represent analog data (e.g., voice) or digital data (e.g., binary data that\npass through a modem). In either case, the analog signal will become weaker\n(attenuate) after a certain distance. To achieve longer distances, the analog\ntransmission system includes amplifiers that boost the energy in the signal.\nUnfortunately, the amplifier also boosts the noise components. With amplifiers\ncascaded to achieve long distances, the signal becomes more and more distorted.\n3.2 / ANALOG AND DIGITAL DATA TRANSMISSION\nAnalog and Digital Transmission\n(a) Data and Signals\nAnalog Signal\nDigital Signal\nAnalog Data\nTwo alternatives: (1) signal \nAnalog data are encoded using a codec to produce\noccupies the same spectrum as the \na digital bit stream.\nanalog data; (2) analog data are \nencoded to occupy a different \nportion of spectrum.\nDigital Data\nDigital data are encoded using a \nTwo alternatives: (1) signal consists of two voltage\nmodem to produce analog signal.\nlevels to represent the two binary values; (2) \ndigital data are encoded to produce a digital signal\nwith desir ed properties.\n(b) Treatment of Signals\nAnalog Transmission\nDigital Transmission\nAnalog Signal\nIs propagated through amplifiers;\nAssumes that the analog signal represents digital\nsame treatment whether signal is \ndata. Signal is propagated through repeaters;\nused to represent analog data or \nat each repeater, digital data are recovered from\ndigital data.\ninbound signal and used to generate a new analog \noutbound signal.\nDigital Signal\nDigital signal represents a stream of 1s and 0s,\nwhich may represent digital data or may be an \nencoding of analog data. Signal is propagated \nthrough repeaters; at each repeater, stream of 1s \nand 0s is recovered from inbound signal and used \nto generate a new digital outbound signal.\nFor analog data, such as voice, quite a bit of distortion can be tolerated and the\ndata remain intelligible. However, for digital data, cascaded amplifiers will intro-\nduce errors.\nDigital transmission, in contrast, assumes a binary content to the signal. A\ndigital signal can be transmitted only a limited distance before attenuation,\nnoise, and other impairments endanger the integrity of the data. To achieve\ngreater distances, repeaters are used. A repeater receives the digital signal,\nrecovers the pattern of 1s and 0s, and retransmits a new signal. Thus the attenua-\ntion is overcome.\nThe same technique may be used with an analog signal if it is assumed that the\nsignal carries digital data. At appropriately spaced points, the transmission system\nhas repeaters rather than amplifiers.The repeater recovers the digital data from the\nanalog signal and generates a new, clean analog signal.Thus noise is not cumulative.\nThe question naturally arises as to which is the preferred method of transmis-\nsion. The answer being supplied by the telecommunications industry and its cus-\ntomers is digital. Both long-haul telecommunications facilities and intrabuilding\nservices have moved to digital transmission and, where possible, digital signaling\ntechniques.The most important reasons are as follows:\nCHAPTER 3 / DATA TRANSMISSION\n• Digital technology: The advent of large-scale integration (LSI) and very-large-\nscale integration (VLSI) technology has caused a continuing drop in the cost\nand size of digital circuitry. Analog equipment has not shown a similar drop.\n• Data integrity: With the use of repeaters rather than amplifiers, the effects of\nnoise and other signal impairments are not cumulative. Thus it is possible to\ntransmit data longer distances and over lower quality lines by digital means\nwhile maintaining the integrity of the data.\n• Capacity utilization: It has become economical to build transmission links of\nvery high bandwidth, including satellite channels and optical fiber. A high\ndegree of multiplexing is needed to utilize such capacity effectively, and this is\nmore easily and cheaply achieved with digital (time division) rather than ana-\nlog (frequency division) techniques.This is explored in Chapter 8.\n• Security and privacy: Encryption techniques can be readily applied to digital\ndata and to analog data that have been digitized.\n• Integration: By treating both analog and digital data digitally, all signals have\nthe same form and can be treated similarly. Thus economies of scale and con-\nvenience can be achieved by integrating voice, video, and digital data.\nWith any communications system, the signal that is received may differ from the sig-\nnal that is transmitted due to various transmission impairments. For analog signals,\nthese impairments can degrade the signal quality. For digital signals, bit errors may\nbe introduced, such that a binary 1 is transformed into a binary 0 or vice versa.\nIn this section, we examine the various impairments and how they may affect the\ninformation-carrying capacity of a communication link; Chapter 5 looks at measures\nthat can be taken to compensate for these impairments.\nThe most significant impairments are\n• Attenuation and attenuation distortion\n• Delay distortion\nAttenuation\nThe strength of a signal falls off with distance over any transmission medium. For\nguided media, this reduction in strength, or attenuation, is generally exponential and\nthus is typically expressed as a constant number of decibels per unit distance. For\nunguided media, attenuation is a more complex function of distance and the\nmakeup of the atmosphere. Attenuation introduces three considerations for the\ntransmission engineer. First, a received signal must have sufficient strength so that\nthe electronic circuitry in the receiver can detect the signal. Second, the signal must\nmaintain a level sufficiently higher than noise to be received without error. Third,\nattenuation varies with frequency.\nThe first and second problems are dealt with by attention to signal strength and\nthe use of amplifiers or repeaters. For a point-to-point link, the signal strength of the\n3.3 / TRANSMISSION IMPAIRMENTS\n8In the remainder of this book, unless otherwise indicated, we use log(x) to mean log101x2.\ntransmitter must be strong enough to be received intelligibly, but not so strong as to\noverload the circuitry of the transmitter or receiver, which would cause distortion.\nBeyond a certain distance, the attenuation becomes unacceptably great, and repeaters\nor amplifiers are used to boost the signal at regular intervals.These problems are more\ncomplex for multipoint lines where the distance from transmitter to receiver is variable.\nThe third problem is particularly noticeable for analog signals. Because the\nattenuation varies as a function of frequency, the received signal is distorted, reduc-\ning intelligibility. To overcome this problem, techniques are available for equalizing\nattenuation across a band of frequencies. This is commonly done for voice-grade\ntelephone lines by using loading coils that change the electrical properties of the\nline; the result is to smooth out attenuation effects. Another approach is to use\namplifiers that amplify high frequencies more than lower frequencies.\nAn example is provided in Figure 3.15a, which shows attenuation as a function\nof frequency for a typical leased line. In the figure, attenuation is measured relative\nto the attenuation at 1000 Hz. Positive values on the y-axis represent attenuation\ngreater than that at 1000 Hz.A 1000-Hz tone of a given power level is applied to the\ninput, and the power,\nis measured at the output. For any other frequency f, the\nprocedure is repeated and the relative attenuation in decibels is8\nThe solid line in Figure 3.15a shows attenuation without equalization. As can\nbe seen, frequency components at the upper end of the voice band are attenuated\nmuch more than those at lower frequencies. It should be clear that this will result in\na distortion of the received speech signal.The dashed line shows the effect of equal-\nization. The flattened response curve improves the quality of voice signals. It also\nallows higher data rates to be used for digital data that are passed through a modem.\nAttenuation distortion can present less of a problem with digital signals. As\nwe have seen, the strength of a digital signal falls off rapidly with frequency \n(Figure 3.5b); most of the content is concentrated near the fundamental frequency\nor bit rate of the signal.\nDelay Distortion\nDelay distortion occurs because the velocity of propagation of a signal through a\nguided medium varies with frequency. For a bandlimited signal, the velocity tends to\nbe highest near the center frequency and fall off toward the two edges of the band.\nThus various frequency components of a signal will arrive at the receiver at different\ntimes, resulting in phase shifts between the different frequencies.\nThis effect is referred to as delay distortion because the received signal is\ndistorted due to varying delays experienced at its constituent frequencies. Delay dis-\ntortion is particularly critical for digital data. Consider that a sequence of bits is\nbeing transmitted, using either analog or digital signals. Because of delay distortion,\nsome of the signal components of one bit position will spill over into other bit posi-\ntions, causing intersymbol interference, which is a major limitation to maximum bit\nrate over a transmission channel.\nNf = -10 log10\nCHAPTER 3 / DATA TRANSMISSION\nFrequency (Hertz)\n(b) Delay distortion\nRelative envelope delay (microseconds)\nFrequency (Hertz)\n(a) Attenuation\nAttenuation (decibels) relative\nto attenuation at 1000 Hz\nequalization\nequalization\nequalization\nequalization\nFigure 3.15\nAttenuation and Delay Distortion Curves for \na Voice Channel\n3.3 / TRANSMISSION IMPAIRMENTS\n9A Joule (J) is the International System (SI) unit of electrical, mechanical, and thermal energy.A Watt is the\nSI unit of power,equal to one Joule per second.The kelvin (K) is the SI unit of thermodynamic temperature.\nFor a temperature in kelvins of T, the corresponding temperature in degrees Celsius is equal to T - 273.15.\nEqualizing techniques can also be used for delay distortion. Again using a\nleased telephone line as an example, Figure 3.15b shows the effect of equalization\non delay as a function of frequency.\nFor any data transmission event, the received signal will consist of the transmitted\nsignal, modified by the various distortions imposed by the transmission system, plus\nadditional unwanted signals that are inserted somewhere between transmission and\nreception. The latter, undesired signals are referred to as noise. Noise is the major\nlimiting factor in communications system performance.\nNoise may be divided into four categories:\n• Thermal noise\n• Intermodulation noise\n• Crosstalk\n• Impulse noise\nThermal noise is due to thermal agitation of electrons. It is present in all elec-\ntronic devices and transmission media and is a function of temperature. Thermal\nnoise is uniformly distributed across the bandwidths typically used in communica-\ntions systems and hence is often referred to as white noise. Thermal noise cannot be\neliminated and therefore places an upper bound on communications system perfor-\nmance. Because of the weakness of the signal received by satellite earth stations,\nthermal noise is particularly significant for satellite communication.\nThe amount of thermal noise to be found in a bandwidth of 1 Hz in any device\nor conductor is\n, where the \nsymbol K is used to represent 1 kelvin\nT = temperature, in kelvins 1absolute temperature2\n k = Boltzmann’s constant = 1.38 * 10-23 J/K\nN0 = noise power density in watts per 1 Hz of bandwidth\nN0 = kT1W/Hz2\nEXAMPLE 3.1 Room temperature is usually specified as \nAt this temperature, the thermal noise power density is\nCHAPTER 3 / DATA TRANSMISSION\nThe noise is assumed to be independent of frequency. Thus the thermal noise\nin watts present in a bandwidth of B Hertz can be expressed as\nor, in decibel-watts,\n= -228.6 dBW + 10 log T + 10 log B\nN = 10 log k + 10 log T + 10 log B\nEXAMPLE 3.2 Given a receiver with an effective noise temperature of 294 K\nand a 10-MHz bandwidth, the thermal noise level at the receiver’s output is\n= -133.9 dBW\n= -228.6 + 24.7 + 70\nN = -228.6 dBW + 10 log12942 + 10 log 107\nWhen signals at different frequencies share the same transmission medium,\nthe result may be intermodulation noise. The effect of intermodulation noise is to\nproduce signals at a frequency that is the sum or difference of the two original\nfrequencies or multiples of those frequencies. For example, the mixing of signals at\nfrequencies \nmight produce energy at the frequency \nThis derived\nsignal could interfere with an intended signal at the frequency \nIntermodulation noise is produced by nonlinearities in the transmitter, receiver,\nand/or intervening transmission medium. Ideally, these components behave as linear\nsystems; that is, the output is equal to the input times a constant. However, in any real\nsystem, the output is a more complex function of the input. Excessive nonlinearity can\nbe caused by component malfunction or overload from excessive signal strength. It is\nunder these circumstances that the sum and difference frequency terms occur.\nCrosstalk has been experienced by anyone who, while using the telephone, has\nbeen able to hear another conversation; it is an unwanted coupling between signal\npaths. It can occur by electrical coupling between nearby twisted pairs or, rarely,\ncoax cable lines carrying multiple signals. Crosstalk can also occur when microwave\nantennas pick up unwanted signals; although highly directional antennas are used,\nmicrowave energy does spread during propagation. Typically, crosstalk is of the\nsame order of magnitude as, or less than, thermal noise.\nAll of the types of noise discussed so far have reasonably predictable and rel-\natively constant magnitudes.Thus it is possible to engineer a transmission system to\ncope with them. Impulse noise, however, is noncontinuous, consisting of irregular\npulses or noise spikes of short duration and of relatively high amplitude. It is gener-\nated from a variety of causes, including external electromagnetic disturbances, such\nas lightning, and faults and flaws in the communications system.\nImpulse noise is generally only a minor annoyance for analog data. For exam-\nple, voice transmission may be corrupted by short clicks and crackles with no loss of\nintelligibility. However, impulse noise is the primary source of error in digital data\n3.4 / CHANNEL CAPACITY\ntransmitted:\nSignal plus\nData received:\nOriginal data:\nBits in error\nFigure 3.16\nEffect of Noise on a Digital Signal\ncommunication. For example, a sharp spike of energy of 0.01 s duration would not\ndestroy any voice data but would wash out about 560 bits of digital data being trans-\nmitted at 56 kbps. Figure 3.16 is an example of the effect of noise on a digital signal.\nHere the noise consists of a relatively modest level of thermal noise plus occasional\nspikes of impulse noise. The digital data can be recovered from the signal by sam-\npling the received waveform once per bit time.As can be seen, the noise is occasion-\nally sufficient to change a 1 to a 0 or a 0 to a 1.\nWe have seen that there are a variety of impairments that distort or corrupt a signal.\nFor digital data, the question that then arises is to what extent these impairments\nlimit the data rate that can be achieved. The maximum rate at which data can be\ntransmitted over a given communication path, or channel, under given conditions, is\nreferred to as the channel capacity.\nThere are four concepts here that we are trying to relate to one another.\n• Data rate: The rate, in bits per second (bps), at which data can be com-\nCHAPTER 3 / DATA TRANSMISSION\n• Bandwidth: The bandwidth of the transmitted signal as constrained by the\ntransmitter and the nature of the transmission medium, expressed in cycles per\nsecond, or Hertz\n• Noise: The average level of noise over the communications path\n• Error rate: The rate at which errors occur, where an error is the reception of a\n1 when a 0 was transmitted or the reception of a 0 when a 1 was transmitted\nThe problem we are addressing is this: Communications facilities are\nexpensive and, in general, the greater the bandwidth of a facility, the greater the\ncost. Furthermore, all transmission channels of any practical interest are of\nlimited bandwidth. The limitations arise from the physical properties of the\ntransmission medium or from deliberate limitations at the transmitter on the\nbandwidth to prevent interference from other sources. Accordingly, we would\nlike to make as efficient use as possible of a given bandwidth. For digital data,\nthis means that we would like to get as high a data rate as possible at a particu-\nlar limit of error rate for a given bandwidth. The main constraint on achieving\nthis efficiency is noise.\nNyquist Bandwidth\nTo begin, let us consider the case of a channel that is noise free. In this environ-\nment, the limitation on data rate is simply the bandwidth of the signal. A formu-\nlation of this limitation, due to Nyquist, states that if the rate of signal\ntransmission is 2B, then a signal with frequencies no greater than B is sufficient\nto carry the signal rate. The converse is also true: Given a bandwidth of B, the\nhighest signal rate that can be carried is 2B. This limitation is due to the effect of\nintersymbol interference, such as is produced by delay distortion. The result is\nuseful in the development of digital-to-analog encoding schemes and is, in\nessence, based on the same derivation as that of the sampling theorem, described\nported by B Hz is 2B bps. However, as we shall see in Chapter 5, signals with more\nthan two levels can be used; that is, each signal element can represent more than one\nbit. For example, if four possible voltage levels are used as signals, then each signal\nelement can represent two bits. With multilevel signaling, the Nyquist formulation\nwhere M is the number of discrete signal or voltage levels.\nSo, for a given bandwidth, the data rate can be increased by increasing the\nnumber of different signal elements. However, this places an increased burden\non the receiver: Instead of distinguishing one of two possible signal elements\nduring each signal time, it must distinguish one of M possible signal elements.\nNoise and other impairments on the transmission line will limit the practical\nvalue of M.\nC = 2B log2M\n3.4 / CHANNEL CAPACITY\n10Some of the literature uses SNR; others use S/N. Also, in some cases the dimensionless quantity is\nreferred to as SNR or S/N and the quantity in decibels is referred to as \nOthers use just\nSNR or S/N to mean the dB quantity.This text uses SNR and SNRdb.\nShannon Capacity Formula\nNyquist’s formula indicates that, all other things being equal, doubling the band-\nwidth doubles the data rate. Now consider the relationship among data rate, noise,\nand error rate.The presence of noise can corrupt one or more bits. If the data rate is\nincreased, then the bits become “shorter” so that more bits are affected by a given\npattern of noise.\nFigure 3.16 illustrates this relationship. If the data rate is increased, then more\nbits will occur during the interval of a noise spike, and hence more errors will occur.\nAll of these concepts can be tied together neatly in a formula developed by the\nmathematician Claude Shannon.As we have just illustrated, the higher the data rate,\nthe more damage that unwanted noise can do. For a given level of noise, we would\nexpect that a greater signal strength would improve the ability to receive data cor-\nrectly in the presence of noise. The key parameter involved in this reasoning is the\nsignal-to-noise ratio (SNR, or S/N),10 which is the ratio of the power in a signal to the\npower contained in the noise that is present at a particular point in the transmission.\nTypically, this ratio is measured at a receiver, because it is at this point that an attempt\nis made to process the signal and recover the data. For convenience, this ratio is often\nreported in decibels:\nThis expresses the amount, in decibels, that the intended signal exceeds the noise\nlevel. A high SNR will mean a high-quality signal and a low number of required\nintermediate repeaters.\nThe signal-to-noise ratio is important in the transmission of digital data\nbecause it sets the upper bound on the achievable data rate. Shannon’s result is that\nthe maximum channel capacity, in bits per second, obeys the equation\nwhere C is the capacity of the channel in bits per second and B is the bandwidth of\nthe channel in Hertz. The Shannon formula represents the theoretical maximum\nthat can be achieved. In practice, however, only much lower rates are achieved. One\nreason for this is that the formula assumes white noise (thermal noise). Impulse\nnoise is not accounted for, nor are attenuation distortion or delay distortion. Even in\nC = B log211 + SNR2\nSNRdB = 10 log10\nsignal power\nnoise power\nEXAMPLE 3.3 Consider a voice channel being used, via modem, to transmit\ndigital data.Assume a bandwidth of 3100 Hz.Then the Nyquist capacity, C, of the\na value used with some modems, C\nbecomes 18,600 bps for a bandwidth of 3100 Hz.\n2B = 6200 bps.\nCHAPTER 3 / DATA TRANSMISSION\nan ideal white noise environment, present technology still cannot achieve Shannon\ncapacity due to encoding issues, such as coding length and complexity.\nThe capacity indicated in the preceding equation is referred to as the error-free\ncapacity. Shannon proved that if the actual information rate on a channel is less than\nthe error-free capacity, then it is theoretically possible to use a suitable signal code to\nachieve error-free transmission through the channel. Shannon’s theorem unfortu-\nnately does not suggest a means for finding such codes,but it does provide a yardstick\nby which the performance of practical communication schemes may be measured.\nSeveral other observations concerning the preceding equation may be instructive.\nFor a given level of noise, it would appear that the data rate could be increased by\nincreasing either signal strength or bandwidth.However,as the signal strength increases,\nso do the effects of nonlinearities in the system, leading to an increase in intermodula-\ntion noise. Note also that, because noise is assumed to be white, the wider the band-\nwidth, the more noise is admitted to the system.Thus, as B increases, SNR decreases.\nEXAMPLE 3.4 Let us consider an example that relates the Nyquist and Shan-\nnon formulations. Suppose that the spectrum of a channel is between 3 MHz and \nUsing Shannon’s formula,\nThis is a theoretical limit and, as we have said, is unlikely to be reached. But\nassume we can achieve the limit. Based on Nyquist’s formula, how many signal-\ning levels are required? We have\n 8 * 106 = 2 * 11062 * log2 M\nC = 2B log2 M\nC = 106 * log211 + 2512 L 106 * 8 = 8 Mbps\n SNRdB = 24 dB = 10 log101SNR2\nB = 4 MHz - 3 MHz = 1 MHz\nSNRdB = 24 dB.\nThe Expression \nFinally,we mention a parameter related to SNR that is more convenient for determining\ndigital data rates and error rates and that is the standard quality measure for digital com-\nmunication system performance. The parameter is the ratio of signal energy per bit to\nnoise power density per Hertz,\nConsider a signal, digital or analog, that contains\nbinary digital data transmitted at a certain bit rate R. Recalling that \nenergy per bit in a signal is given by \nwhere S is the signal power and \ntime required to send one bit.The data rate R is just \n1 Watt = 1 J/s,\n3.4 / CHANNEL CAPACITY\nor, in decibel notation,\nis important because the bit error rate for digital data is a (decreas-\ning) function of this ratio. Given a value of \nneeded to achieve a desired error\nrate, the parameters in the preceding formula may be selected. Note that as the bit\nrate R increases, the transmitted signal power, relative to noise, must increase to\nmaintain the required \nLet us try to grasp this result intuitively by considering again Figure 3.16. The\nsignal here is digital, but the reasoning would be the same for an analog signal. In\nseveral instances, the noise is sufficient to alter the value of a bit. If the data rate\nwere doubled, the bits would be more tightly packed together, and the same passage\nof noise might destroy two bits.Thus, for constant signal to noise ratio, an increase in\ndata rate increases the error rate.\nThe advantage of \nover SNR is that the latter quantity depends on the\n= SdBW - 10 log R + 228.6 dBW - 10 log T\n= SdBW - 10 log R - 10 log k - 10 log T\nEXAMPLE 3.5 For binary phase-shift keying (defined in Chapter 5),\nis required for a bit error rate of \n(one bit error out of every 10,000). If\nthe effective noise temperature is 290°K (room temperature) and the data rate is\n2400 bps, what received signal level is required?\nS = -161.8 dBW\n= S1dBW2 - 110213.382 + 228.6 - 110212.462\n 8.4 = S1dBW2 - 10 log 2400 + 228.6 dBW - 10 log 290\nWe can relate \nto SNR as follows.We have\nThe parameter \nis the noise power density in Watts/Hertz. Hence, the noise in a\nsignal with bandwidth B is \nSubstituting, we have\nAnother formulation of interest relates \nto spectral efficiency. Shannon’s\nresult (Equation 3.1) can be rewritten as:\nCHAPTER 3 / DATA TRANSMISSION\nUsing Equation (3.2), and equating R with C, we have\nThis is a useful formula that relates the achievable spectral efficiency C/B to Eb/N0.\nC 12C>B - 12\nN = 2C>B - 1\nabsolute bandwidth\nanalog data\nanalog signal\nanalog transmission\nattenuation\nattenuation distortion\ncenter frequency\nchannel capacity\ndc component\ndecibel (dB)\ndelay distortion\ndigital data\ndigital signal\nEXAMPLE 3.6 Suppose we want to find the minimum \nrequired to\nachieve a spectral efficiency of 6 bps/Hz.Then \nEb/N0 = 11/62126 - 12 = 10.5 = 10.21 dB.\nThere are many books that cover the fundamentals of analog and digital transmission.\n[COUC01] is quite thorough. Other good reference works are [FREE05], which includes\nsome of the examples used in this chapter, and [HAYK01].\nCOUC01 Couch, L. Digital and Analog Communication Systems. Upper Saddle River, NJ:\nPrentice Hall, 2001.\nFREE05 Freeman, R. Fundamentals of Telecommunications. New York:Wiley, 2005.\nHAYK01 Haykin, S. Communication Systems. New York:Wiley, 2001.\nRecommended Web site:\n• Fourier series synthesis: An excellent visualization tool for Fourier series\nKEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\n3.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nReview Questions\nDifferentiate between guided media and unguided media.\nDifferentiate between an analog and a digital electromagnetic signal.\nWhat are three important characteristics of a periodic signal?\nHow many radians are there in a complete circle of 360 degrees?\nWhat is the relationship between the wavelength and frequency of a sine wave?\nDefine fundamental frequency.\nWhat is the relationship between a signal’s spectrum and its bandwidth?\nWhat is attenuation?\nDefine channel capacity.\nWhat key factors affect channel capacity?\nFor multipoint configuration, only one device at a time can transmit.Why?\nb. There are two methods of enforcing the rule that only one device can transmit. In\nthe centralized method, one station is in control and can either transmit or allow a\nspecified other station to transmit. In the decentralized method, the stations\njointly cooperate in taking turns. What do you see as the advantages and disad-\nvantages of the two methods?\nA signal has a fundamental frequency of 1000 Hz.What is its period?\nExpress the following in the simplest form you can:\nSound may be modeled as sinusoidal functions. Compare the relative frequency and\nwavelength of musical notes. Use 330 m/s as the speed of sound and the following\nfrequencies for the musical scale.\nIf the solid curve in Figure 3.17 represents \nwhat does the dotted curve\nrepresent? That is, the dotted curve can be written in the form A\nare A, f, and\nDecompose the signal \ninto a linear combination of sinu-\nsoidal functions, and find the amplitude, frequency, and phase of each component.\nHint: Use the identity for cos a cos b.\n11 + 0.1 cos 5t2cos 100t\nsin12pft + f2;\nsin 2pft + sin12pft - p2\nsin12pft - p2 + sin12pft + p2\ndigital transmission\ndirect link\neffective bandwidth\nfrequency domain\nfull duplex\nfundamental frequency\nguided media\nhalf duplex\nimpulse noise\ninterlacing\nintermodulation noise\nmultipoint link\nNyquist bandwidth\npeak amplitude\nperiodic signal\npoint-to-point link\nsignal-to-noise ratio (SNR)\nthermal noise\ntime domain\ntransmission\nunguided media\nCHAPTER 3 / DATA TRANSMISSION\nFind the period of the function \nConsider two periodic functions \nwith periods \nrespectively.\nIs it always the case that the function \nis periodic? If so, demon-\nstrate this fact. If not, under what conditions is f(t) periodic?\nFigure 3.4 shows the effect of eliminating higher-harmonic components of a square\nwave and retaining only a few lower harmonic components. What would the signal\nlook like in the opposite case; that is, retaining all higher harmonics and eliminating a\nfew lower harmonics?\nFigure 3.5b shows the frequency domain function for a single square pulse.The single\npulse could represent a digital 1 in a communication system. Note that an infinite\nnumber of higher frequencies of decreasing magnitudes is needed to represent the\nsingle pulse. What implication does that have for a real digital transmission system?\nIRA is a 7-bit code that allows 128 characters to be defined. In the 1970s, many news-\npapers received stories from the wire services in a 6-bit code called TTS. This code\ncarried upper- and lower case characters as well as many special characters and for-\nmatting commands. The typical TTS character set allowed over 100 characters to be\ndefined. How do you think this could be accomplished?\nFor a video signal, what increase in horizontal resolution is possible if a bandwidth of\n5 MHz is used? What increase in vertical resolution is possible? Treat the two ques-\ntions separately; that is, the increased bandwidth is to be used to increase either hori-\nzontal or vertical resolution, but not both.\nSuppose that a digitized TV picture is to be transmitted from a source that uses a\npicture elements (pixels), where each pixel can take on one\nof 32 intensity values. Assume that 30 pictures are sent per second. (This digital\nsource is roughly equivalent to broadcast TV standards that have been adopted.)\nFind the source rate R (bps).\nb. Assume that the TV picture is to be transmitted over a channel with 4.5-MHz band-\nwidth and a 35-dB signal-to-noise ratio. Find the capacity of the channel (bps).\nDiscuss how the parameters given in part (a) could be modified to allow transmis-\nsion of color TV signals without increasing the required value for R.\nGiven an amplifier with an effective noise temperature of 10,000 K and a 10-MHz\nbandwidth, what thermal noise level, in dBW, may we expect at its output?\nWhat is the channel capacity for a teleprinter channel with a 300-Hz bandwidth and a\nsignal-to-noise ratio of 3 dB, where the noise is white thermal noise?\nA digital signaling system is required to operate at 9600 bps.\nIf a signal element encodes a 4-bit word, what is the minimum required bandwidth\nof the channel?\nb. Repeat part (a) for the case of 8-bit words.\nf1t2 = f11t2 + f21t2\nf1t2 = 110 cos t22.\nFigure 3.17\nFigure for Problem 3.5\nCHAPTER 3 / DATA TRANSMISSION\nThe decibel is a measure of the ratio between two signal levels.The decibel gain is given by\nTable 3.2 shows the relationship between decibel values and powers of 10.\nThere is some inconsistency in the literature over the use of the terms gain and loss. If\nthe value of \nis positive, this represents an actual gain in power. For example, a gain of \n3 dB means that the power has doubled. If the value of \nis negative, this represents an\nactual loss in power. For example, a gain of \nmeans that the power has halved, and this\nis a loss of power. Normally, this is expressed by saying there is a loss of 3 dB. However, some\nof the literature would say that this is a loss of \nIt makes more sense to say that a\nnegative gain corresponds to a positive loss.Therefore, we define a decibel loss as\nLdB = -10 log10\n log10 = logarithm to the base 10\nPout = output power level\nPin = input power level\nGdB = gain, in decibels\nGdB = 10 log10\nDecibel Values\nPower Ratio\nPower Ratio\nEXAMPLE 3.7 If a signal with a power level of 10 mW is inserted onto a trans-\nmission line and the measured power some distance away is 5 mW, the loss can be\nexpressed as\nLdB = 10 log110/52 = 1010.32 = 3 dB.\nNote that the decibel is a measure of relative, not absolute, difference. A loss from 1000 mW\nto 500 mW is also a loss of 3 dB.\nThe decibel is also used to measure the difference in voltage, taking into account that\npower is proportional to the square of the voltage:\nbetween transmitter and receiver. Recall from Chapter 3 that for guided media,\nelectromagnetic waves are guided along a solid medium, such as copper twisted\npair, copper coaxial cable, and optical fiber. For unguided media, wireless trans-\nmission occurs through the atmosphere, outer space, or water.\nThe characteristics and quality of a data transmission are determined both\nby the characteristics of the medium and the characteristics of the signal. In the\ncase of guided media, the medium itself is more important in determining the\nlimitations of transmission.\nFor unguided media,the bandwidth of the signal produced by the transmit-\nting antenna is more important than the medium in determining transmission\ncharacteristics.One key property of signals transmitted by antenna is directional-\nity. In general, signals at lower frequencies are omnidirectional; that is, the signal\npropagates in all directions from the antenna.At higher frequencies, it is possible\nto focus the signal into a directional beam.\nCHAPTER 4 / TRANSMISSION MEDIA\nIn considering the design of data transmission systems, key concerns are\ndata rate and distance: the greater the data rate and distance the better. A\nnumber of design factors relating to the transmission medium and the signal\ndetermine the data rate and distance:\n• Bandwidth: All other factors remaining constant, the greater the band-\nwidth of a signal, the higher the data rate that can be achieved.\n• Transmission impairments: Impairments, such as attenuation, limit the dis-\ntance. For guided media, twisted pair generally suffers more impairment\nthan coaxial cable, which in turn suffers more than optical fiber.\n• Interference: Interference from competing signals in overlapping fre-\nquency bands can distort or wipe out a signal. Interference is of particular\nconcern for unguided media, but is also a problem with guided media. For\nguided media, interference can be caused by emanations from nearby\ncables. For example, twisted pairs are often bundled together and conduits\noften carry multiple cables. Interference can also be experienced from\nunguided transmissions. Proper shielding of a guided medium can mini-\nmize this problem.\n• Number of receivers: A guided medium can be used to construct a point-\nto-point link or a shared link with multiple attachments. In the latter case,\neach attachment introduces some attenuation and distortion on the line,\nlimiting distance and/or data rate.\nFigure 4.1 depicts the electromagnetic spectrum and indicates the fre-\nquencies at which various guided media and unguided transmission tech-\nniques operate. In this chapter we examine these guided and unguided\nalternatives. In all cases, we describe the systems physically, briefly discuss\napplications, and summarize key transmission characteristics.\nFor guided transmission media, the transmission capacity, in terms of either data\nrate or bandwidth, depends critically on the distance and on whether the medium is\npoint-to-point or multipoint. Table 4.1 indicates the characteristics typical for the\ncommon guided media for long-distance point-to-point applications; we defer a\ndiscussion of the use of these media for LANs to Part Four.\nThe three guided media commonly used for data transmission are twisted pair,\ncoaxial cable, and optical fiber (Figure 4.2).We examine each of these in turn.\nTwisted Pair\nThe least expensive and most widely used guided transmission medium is twisted\nPhysical Description A twisted pair consists of two insulated copper wires\narranged in a regular spiral pattern.A wire pair acts as a single communication link.\nPower and telephone\nRotating generators\nMusical instruments\nVoice microphones\nMicrowave antennas\nRadios and televisions\nElectronic tubes\nIntegrated circuits\nCellular telephony\nELF \u0001 Extremely low frequency\nVF \u0001 Voice frequency\nVLF \u0001 Very low frequency\nLF \u0001 Low frequency\nMF \u0001 Medium frequency\nHF \u0001 High frequency\nVHF \u0001 Very high frequency\nUHF \u0001 Ultra high frequency\nSHF \u0001 Super high frequency\nEHF \u0001 Extremely high frequency\nTwisted pair\nCoaxial cable\nTerrestrial\nand satellite\ntransmission\nGuided missiles\nRangefinders\nElectromagnetic Spectrum for Telecommunications\nCHAPTER 4 / TRANSMISSION MEDIA\nPoint-to-Point Transmission Characteristics of Guided Media [GLOV98]\nFrequency Range\nTypical Attenuation\nTypical Delay\nRepeater Spacing\nTwisted pair (with\n0 to 3.5 kHz\n0.2 dB/km @ 1 kHz\nTwisted pairs  \n0.7 dB/km @ 1 kHz\n(multipair cables)\nCoaxial cable\n0 to 500 MHz\n7 dB/km @ 10 MHz\nOptical fiber\n186 to 370 THz\n0.2 to 0.5 dB/km\nTHz = TeraHertz = 1012 Hz\nOuter sheath\n(b) Coaxial cable\n—Outer conductor is braided shield\n—Inner conductor is solid metal\n—Separated by insulating material\n—Covered by padding\n(c) Optical fiber\nLight at less than\ncritical angle is\nabsorbed in jacket\n—Glass or plastic core\n—Laser or light emitting diode\n—Specially designed jacket\n—Small size and weight\n(a) Twisted pair\n—Separately insulated\n—Twisted together\n—Often \"bundled\" into cables\n—Usually installed in building\n    during construction\nGuided Transmission Media\n4.1 / GUIDED TRANSMISSION MEDIA\nTypically, a number of these pairs are bundled together into a cable by wrapping\nthem in a tough protective sheath. Over longer distances, cables may contain\nhundreds of pairs.The twisting tends to decrease the crosstalk interference between\nadjacent pairs in a cable. Neighboring pairs in a bundle typically have somewhat dif-\nferent twist lengths to reduce the crosstalk interference. On long-distance links, the\ntwist length typically varies from 5 to 15 cm. The wires in a pair have thicknesses of\nfrom 0.4 to 0.9 mm.\nApplications By far the most common guided transmission medium for both ana-\nlog and digital signals is twisted pair. It is the most commonly used medium in the\ntelephone network and is the workhorse for communications within buildings.\nIn the telephone system, individual residential telephone sets are connected to\nthe local telephone exchange, or “end office,” by twisted-pair wire. These are\nreferred to as subscriber loops.Within an office building, each telephone is also con-\nnected to a twisted pair, which goes to the in-house private branch exchange (PBX)\nsystem or to a Centrex facility at the end office.These twisted-pair installations were\ndesigned to support voice traffic using analog signaling. However, by means of a\nmodem, these facilities can handle digital data traffic at modest data rates.\nTwisted pair is also the most common medium used for digital signaling. For\nconnections to a digital data switch or digital PBX within a building, a data rate of\n64 kbps is common. Twisted pair is also commonly used within a building for local\narea networks supporting personal computers. Data rates for such products are typ-\nically in the neighborhood of 100 Mbps. However, twisted-pair networks with data\nrates of to 10 Gbps have been developed, although these are quite limited in terms\nof the number of devices and geographic scope of the network. For long-distance\napplications, twisted pair can be used at data rates of 4 Mbps or more.\nTwisted pair is much less expensive than the other commonly used guided\ntransmission media (coaxial cable, optical fiber) and is easier to work with.\nTransmission Characteristics Twisted pair may be used to transmit both ana-\nlog and digital transmission. For analog signals, amplifiers are required about every\n5 to 6 km. For digital transmission (using either analog or digital signals), repeaters\nare required every 2 or 3 km.\nCompared to other commonly used guided transmission media (coaxial\ncable, optical fiber), twisted pair is limited in distance, bandwidth, and data rate.As\nFigure 4.3a shows, the attenuation for twisted pair is a very strong function of fre-\nquency. Other impairments are also severe for twisted pair. The medium is quite\nsusceptible to interference and noise because of its easy coupling with electromag-\nnetic fields. For example, a wire run parallel to an ac power line will pick up 60-Hz\nenergy. Impulse noise also easily intrudes into twisted pair. Several measures are\ntaken to reduce impairments. Shielding the wire with metallic braid or sheathing\nreduces interference. The twisting of the wire reduces low-frequency interference,\nand the use of different twist lengths in adjacent pairs reduces crosstalk.\nFor point-to-point analog signaling, a bandwidth of up to about 1 MHz is pos-\nsible. This accommodates a number of voice channels. For long-distance digital\npoint-to-point signaling, data rates of up to a few Mbps are possible; for very short\ndistances, data rates of up to 10 Gbps have been achieved in commercially available\n(a) Twisted pair (based on [REEV95])\n(b) Coaxial cable (based on [BELL90])\n(c) Optical fiber (based on [FREE02])\n(d) Composite graph\nFrequency (Hz)\nAttenuation (dB/km)\n26-AWG (0.4 mm)\n24-AWG (0.5 mm)\n22-AWG (0.6 mm)\n19-AWG (0.9 mm)\nAttenuation (dB/km)\n1000 1100 1200 1300 1400 1500 1600 1700\nWavelength in vacuum (nm)\ntwisted pair\nAttenuation (dB/km)\nFrequency (Hz)\ntypical optical\nFrequency (Hz)\nAttenuation (dB/km)\nAttenuation of Typical Guided Media\n4.1 / GUIDED TRANSMISSION MEDIA\nUnshielded and Shielded Twisted Pair Twisted pair comes in two varieties:\nunshielded and shielded. Unshielded twisted pair (UTP) is ordinary telephone wire.\nOffice buildings, by universal practice, are prewired with excess unshielded twisted\npair, more than is needed for simple telephone support.This is the least expensive of\nall the transmission media commonly used for local area networks and is easy to\nwork with and easy to install.\nUnshielded twisted pair is subject to external electromagnetic interference,\nincluding interference from nearby twisted pair and from noise generated in the envi-\nronment.A way to improve the characteristics of this medium is to shield the twisted\npair with a metallic braid or sheathing that reduces interference.This shielded twisted\npair (STP) provides better performance at higher data rates. However, it is more\nexpensive and more difficult to work with than unshielded twisted pair.\nCategory 3 and Category 5 UTP Most office buildings are prewired with a\ntype of 100-ohm twisted pair cable commonly referred to as voice grade. Because\nvoice-grade twisted pair is already installed, it is an attractive alternative for use as a\nLAN medium. Unfortunately, the data rates and distances achievable with voice-\ngrade twisted pair are limited.\nIn 1991, the Electronic Industries Association published standard EIA-568,\nCommercial Building Telecommunications Cabling Standard, which specifies the use\nof voice-grade unshielded twisted pair as well as shielded twisted pair for in-building\ndata applications.At that time, the specification was felt to be adequate for the range\nof frequencies and data rates found in office environments. Up to that time, the prin-\ncipal interest for LAN designs was in the range of data rates from 1 Mbps to 16 Mbps.\nSubsequently, as users migrated to higher-performance workstations and applica-\ntions, there was increasing interest in providing LANs that could operate up to\n100 Mbps over inexpensive cable. In response to this need, EIA-568-A was issued in\n1995.The new standard reflects advances in cable and connector design and test meth-\nods. It covers 150-ohm shielded twisted pair and 100-ohm unshielded twisted pair.\nEIA-568-A recognizes three categories of UTP cabling:\n• Category 3: UTP cables and associated connecting hardware whose transmis-\nsion characteristics are specified up to 16 MHz\n• Category 4: UTP cables and associated connecting hardware whose transmis-\nsion characteristics are specified up to 20 MHz\n• Category 5: UTP cables and associated connecting hardware whose transmis-\nsion characteristics are specified up to 100 MHz\nOf these, it is Category 3 and Category 5 cable that have received the most\nattention for LAN applications. Category 3 corresponds to the voice-grade cable\nfound in abundance in most office buildings. Over limited distances, and with proper\ndesign, data rates of up to 16 Mbps should be achievable with Category 3. Category\n5 is a data-grade cable that is becoming standard for preinstallation in new office\nbuildings. Over limited distances, and with proper design, data rates of up to\n100 Mbps are achievable with Category 5.\nA key difference between Category 3 and Category 5 cable is the number of\ntwists in the cable per unit distance. Category 5 is much more tightly twisted, with a\ntypical twist length of 0.6 to 0.85 cm, compared to 7.5 to 10 cm for Category 3. The\nCHAPTER 4 / TRANSMISSION MEDIA\ntighter twisting of Category 5 is more expensive but provides much better perform-\nance than Category 3.\nTable 4.2 summarizes the performance of Category 3 and 5 UTP, as well as the\nSTP specified in EIA-568-A. The first parameter used for comparison, attenuation,\nis fairly straightforward. The strength of a signal falls off with distance over any\ntransmission medium. For guided media attenuation is generally exponential and\ntherefore is typically expressed as a constant number of decibels per unit distance.\nNear-end crosstalk as it applies to twisted pair wiring systems is the coupling\nof the signal from one pair of conductors to another pair. These conductors may be\nthe metal pins in a connector or wire pairs in a cable.The near end refers to coupling\nthat takes place when the transmit signal entering the link couples back to the\nreceive conductor pair at that same end of the link (i.e., the near transmitted signal\nis picked up by the near receive pair).\nSince the publication of EIA-568-A, there has been ongoing work on the\ndevelopment of standards for premises cabling, driven by two issues. First, the\nGigabit Ethernet specification requires the definition of parameters that are not\nspecified completely in any published cabling standard. Second, there is a desire\nto specify cabling performance to higher levels, namely Enhanced Category 5\n(Cat 5E), Category 6, and Category 7. Tables 4.3 and 4.4 summarize these new\ncabling schemes and compare them to the existing standards.\nComparison of Shielded and Unshielded Twisted Pair\nAttenuation (dB per 100 m)\nNear-End Crosstalk (dB)\n150-ohm STP\nCategory 5E\nSSTP = Shielded screen twisted pair\nFTP = Foil twisted pair\nUTP = Unshielded twisted pair\n1Cat 5 \u0001 12\nTwisted Pair Categories and Classes\n4.1 / GUIDED TRANSMISSION MEDIA\nHigh-Performance LAN Copper Cabling Alternatives [JOHN98]\nConstruction\nExpected Performance\nCable consists of 4 pairs of 24 AWG \nMixed and matched cables and\n(0.50 mm) copper with thermoplastic \nconnecting hardware from various \npolyolefin or fluorinated ethylene\nmanufacturers that have a reasonable\npropylene (FEP) jacket. Outside sheath \nchance of meeting TIA Cat 5 Channel\nconsists of polyvinylchlorides (PVC), a \nand ISO Class D requirements. No \nfire retardant polyolefin or fluoropolymers.\nmanufacturer’s warranty is involved.\nCable consists of 4 pairs of 24 AWG\nCategory 5 components from one \n(0.50 mm) copper with thermoplastic \nsupplier or from multiple suppliers \npolyolefin or fluorinated ethylene  \nwhere components have been\nEnhanced Cat 5\npropylene (FEP) jacket. Outside sheath \ndeliberately matched for improved \nUTP (Cat 5E)\nconsists of polyvinylchlorides (PVC), a fire \nimpedance and balance. Offers ACR\nretardant polyolefin or fluoropolymers.\nperformance in excess of Cat 5 \nHigher care taken in design and \nChannel and Class D as well as a \nmanufacturing.\n10-year or greater warranty.\nCable consists of 4 pairs of 0.50 to 0.53 mm \nCategory 6 components from one\ncopper with thermoplastic polyolefin or\nsupplier that are extremely well \nfluorinated ethylene propylene (FEP) jacket.\nmatched. Channel zero ACR point\nOutside sheath consists of polyvinylchlorides \n(effective bandwidth) is guaranteed\n(PVC), a fire retardant polyolefin or \nto 200 MHz or beyond. Best available\nfluoropolymers. Extremely high care taken\nUTP. Performance specifications\nin design and manufacturing.Advanced\nfor Category 6 UTP to 250 MHz\nconnector designs.\nare under development.\nCable consists of 4 pairs of 24 AWG \nCategory 5 components from one \n(0.50 mm) copper with thermoplastic \nsupplier or from multiple suppliers\npolyolefin or fluorinated ethylene propylene\nwhere  components have been \nFoil Twisted \n(FEP) jacket. Pairs are surrounded by a \ndeliberately designed to minimize\ncommon metallic foil shield. Outside  sheath \nEMI susceptibility and maximize\nconsists of polyvinylchlorides (PVC), a fire-\nEMI immunity.Various grades may\nretardant  polyolefin or fluoropolymers.\noffer increased ACR performance.\nCable consists of 4 pairs of 24 AWG \nCategory 5 components from one\n(0.50 mm) copper with thermoplastic\nsupplier or from multiple suppliers \npolyolefin or  fluorinated ethylene\nwhere components have been\npropylene (FEP) jacket. Pairs are\ndeliberately designed to minimize\nShielded Foil\nsurrounded by a common metallic foil \nEMI susceptibility and maximize EMI\nTwisted Pair\nshield, followed by a braided metallic\nimmunity. Offers superior EMI\nshield. Outside sheath consists of\nprotection to FTP.\npolyvinylchlorides (PVC), a fire retardant\npolyolefin, or fluoropolymers\nAlso called PiMF (for Pairs in Metal Foil),\nCategory 7 cabling provides positive\nSSTP of 4 pairs of 22-23AWG copper with a\nACR to 600 to 1200 MHz. Shielding \nthermoplastic polyolefin or fluorinated\non the individual pairs gives it\nethylenepropylene  (FEP) jacket. Pairs  are\nphenomenal ACR.\nShielded-Screen\nindividually surrounded by a helical or \nTwisted Pair\nlongitudinal metallic foil shield, followed by\na braided metallic shield. Outside sheath of\npolyvinylchlorides (PVC), a fire-retardant\npolyolefin, or fluoropolymers.\nEMI = Electromagnetic interference\nACR = Attenuation to crosstalk ratio\nCHAPTER 4 / TRANSMISSION MEDIA\nCoaxial Cable\nPhysical Description Coaxial cable, like twisted pair, consists of two conduc-\ntors, but is constructed differently to permit it to operate over a wider range of fre-\nquencies. It consists of a hollow outer cylindrical conductor that surrounds a single\ninner wire conductor (Figure 4.2b). The inner conductor is held in place by either\nregularly spaced insulating rings or a solid dielectric material. The outer conductor\nis covered with a jacket or shield.A single coaxial cable has a diameter of from 1 to\n2.5 cm. Coaxial cable can be used over longer distances and support more stations\non a shared line than twisted pair.\nApplications Coaxial cable is a versatile transmission medium, used in a wide\nvariety of applications.The most important of these are\n• Television distribution\n• Long-distance telephone transmission\n• Short-run computer system links\n• Local area networks\nCoaxial cable is widely used as a means of distributing TV signals to individual\nhomes—cable TV. From its modest beginnings as Community Antenna Television\n(CATV), designed to provide service to remote areas, cable TV reaches almost as\nmany homes and offices as the telephone. A cable TV system can carry dozens or\neven hundreds of TV channels at ranges up to a few tens of kilometers.\nCoaxial cable has traditionally been an important part of the long-distance\ntelephone network. Today, it faces increasing competition from optical fiber, terres-\ntrial microwave, and satellite. Using frequency division multiplexing (FDM, see\nChapter 8), a coaxial cable can carry over 10,000 voice channels simultaneously.\nCoaxial cable is also commonly used for short-range connections between\ndevices. Using digital signaling, coaxial cable can be used to provide high-speed I/O\nchannels on computer systems.\nTransmission Characteristics Coaxial cable is used to transmit both analog\nand digital signals. As can be seen from Figure 4.3b, coaxial cable has frequency\ncharacteristics that are superior to those of twisted pair and can hence be used effec-\ntively at higher frequencies and data rates. Because of its shielded, concentric con-\nstruction, coaxial cable is much less susceptible to interference and crosstalk than\ntwisted pair. The principal constraints on performance are attenuation, thermal\nnoise, and intermodulation noise. The latter is present only when several channels\n(FDM) or frequency bands are in use on the cable.\nFor long-distance transmission of analog signals, amplifiers are needed every few\nkilometers,with closer spacing required if higher frequencies are used.The usable spec-\ntrum for analog signaling extends to about 500 MHz.For digital signaling,repeaters are\nneeded every kilometer or so, with closer spacing needed for higher data rates.\nOptical Fiber\nPhysical Description An optical fiber is a thin (2 to \n), flexible medium\ncapable of guiding an optical ray. Various glasses and plastics can be used to make\n4.1 / GUIDED TRANSMISSION MEDIA\noptical fibers. The lowest losses have been obtained using fibers of ultrapure fused\nsilica. Ultrapure fiber is difficult to manufacture; higher-loss multicomponent glass\nfibers are more economical and still provide good performance. Plastic fiber is even\nless costly and can be used for short-haul links, for which moderately high losses are\nacceptable.\nAn optical fiber cable has a cylindrical shape and consists of three concentric\nsections: the core, the cladding, and the jacket (Figure 4.2c). The core is the inner-\nmost section and consists of one or more very thin strands, or fibers, made of glass or\nplastic; the core has a diameter in the range of 8 to \nEach fiber is surrounded\nby its own cladding, a glass or plastic coating that has optical properties different\nfrom those of the core and a diameter of \nThe interface between the core\nand cladding acts as a reflector to confine light that would otherwise escape the\ncore. The outermost layer, surrounding one or a bundle of cladded fibers, is the\njacket. The jacket is composed of plastic and other material layered to protect\nagainst moisture, abrasion, crushing, and other environmental dangers.\nApplications\nOptical fiber already enjoys considerable use in long-distance\ntelecommunications, and its use in military applications is growing. The continuing\nimprovements in performance and decline in prices, together with the inherent\nadvantages of optical fiber, have made it increasingly attractive for local area net-\nworking. The following characteristics distinguish optical fiber from twisted pair or\ncoaxial cable:\n• Greater capacity: The potential bandwidth, and hence data rate, of optical\nfiber is immense; data rates of hundreds of Gbps over tens of kilometers have\nbeen demonstrated. Compare this to the practical maximum of hundreds of\nMbps over about 1 km for coaxial cable and just a few Mbps over 1 km or up\nto 100 Mbps to 10 Gbps over a few tens of meters for twisted pair.\n• Smaller size and lighter weight: Optical fibers are considerably thinner than\ncoaxial cable or bundled twisted-pair cable—at least an order of magnitude\nthinner for comparable information transmission capacity. For cramped con-\nduits in buildings and underground along public rights-of-way, the advantage\nof small size is considerable. The corresponding reduction in weight reduces\nstructural support requirements.\n• Lower attenuation: Attenuation is significantly lower for optical fiber than for\ncoaxial cable or twisted pair (Figure 4.3c) and is constant over a wide range.\n• Electromagnetic isolation: Optical fiber systems are not affected by external\nelectromagnetic fields. Thus the system is not vulnerable to interference,\nimpulse noise, or crosstalk. By the same token, fibers do not radiate energy, so\nthere is little interference with other equipment and there is a high degree of\nsecurity from eavesdropping. In addition, fiber is inherently difficult to tap.\n• Greater repeater spacing: Fewer repeaters mean lower cost and fewer sources\nof error. The performance of optical fiber systems from this point of view has\nbeen steadily improving. Repeater spacing in the tens of kilometers for optical\nfiber is common, and repeater spacings of hundreds of kilometers have been\ndemonstrated. Coaxial and twisted-pair systems generally have repeaters\nevery few kilometers.\nCHAPTER 4 / TRANSMISSION MEDIA\nFive basic categories of application have become important for optical fiber:\n• Long-haul trunks\n• Metropolitan trunks\n• Rural exchange trunks\n• Subscriber loops\n• Local area networks\nLong-haul fiber transmission is becoming increasingly common in the tele-\nphone network. Long-haul routes average about 1500 km in length and offer high\ncapacity (typically 20,000 to 60,000 voice channels).These systems compete econom-\nically with microwave and have so underpriced coaxial cable in many developed\ncountries that coaxial cable is rapidly being phased out of the telephone network in\nsuch countries. Undersea optical fiber cables have also enjoyed increasing use.\nMetropolitan trunking circuits have an average length of 12 km and may have as\nmany as 100,000 voice channels in a trunk group. Most facilities are installed in under-\nground conduits and are repeaterless,joining telephone exchanges in a metropolitan or\ncity area. Included in this category are routes that link long-haul microwave facilities\nthat terminate at a city perimeter to the main telephone exchange building downtown.\nRural exchange trunks have circuit lengths ranging from 40 to 160 km and link\ntowns and villages. In the United States, they often connect the exchanges of differ-\nent telephone companies. Most of these systems have fewer than 5000 voice chan-\nnels. The technology used in these applications competes with microwave facilities.\nSubscriber loop circuits are fibers that run directly from the central exchange\nto a subscriber. These facilities are beginning to displace twisted pair and coaxial\ncable links as the telephone networks evolve into full-service networks capable of\nhandling not only voice and data, but also image and video. The initial penetration\nof optical fiber in this application is for the business subscriber, but fiber transmis-\nsion into the home will soon begin to appear.\nA final important application of optical fiber is for local area networks. Stan-\ndards have been developed and products introduced for optical fiber networks that\nhave a total capacity of 100 Mbps to 10 Gbps and can support hundreds or even\nthousands of stations in a large office building or a complex of buildings.\nThe advantages of optical fiber over twisted pair and coaxial cable become\nmore compelling as the demand for all types of information (voice, data, image,\nvideo) increases.\nTransmission Characteristics Optical fiber transmits a signal-encoded beam\nof light by means of total internal reflection. Total internal reflection can occur in\nCHAPTER 4 / TRANSMISSION MEDIA\nFrequency Utilization for Fiber Applications\nFrequency Range \nApplication\n(in vacuum) Range (nm)\n1280 to 1350\nSingle mode\n1528 to 1561\nSingle mode\n1561 to 1620\nSingle mode\n(see Chapter 8)\nWDM = wavelength division multiplexing\nFor a definition of numerical prefixes in common use, see the supporting document at\nWilliamStallings.com.\n1 THz = 1012 Hz.\nNote the tremendous bandwidths available. For the four windows, the respec-\ntive bandwidths are 33 THz, 12 THz, 4 THz, and 7 THz.1 This is several orders of\nmagnitude greater than the bandwidth available in the radio-frequency spectrum.\nOne confusing aspect of reported attenuation figures for fiber optic transmis-\nsion is that, invariably, fiber optic performance is specified in terms of wavelength\nrather than frequency. The wavelengths that appear in graphs and tables are the\nwavelengths corresponding to transmission in a vacuum. However, on the fiber, the\nvelocity of propagation is less than the speed of light in a vacuum (c); the result is\nthat although the frequency of the signal is unchanged, the wavelength is changed.\nEXAMPLE 4.1 For a wavelength in vacuum of 1550 nm, the corresponding \nfrequency is \nFor a typical single mode fiber, the velocity of propagation is approximately\nIn this case, a frequency of 193.4 THz corresponds to a wave-\nTherefore, on this\nfiber, when a wavelength of 1550 nm is cited, the actual wavelength on the fiber\nis 1055 nm.\nl = v/f = 12.04 * 1082/1193.4 * 10122 = 1055 nm.\nv = 2.04 * 108.\nf = c/l = 13 * 1082/11550 * 10-92 = 193.4 * 1012 = 193.4 THz.\nThe four transmission windows are in the infrared portion of the frequency spec-\ntrum, below the visible-light portion, which is 400 to 700 nm.The loss is lower at higher\nwavelengths, allowing greater data rates over longer distances. Many local applications\ntoday use 850-nm LED light sources.Although this combination is relatively inexpen-\nsive, it is generally limited to data rates under 100 Mbps and distances of a few kilome-\nters.To achieve higher data rates and longer distances, a 1300-nm LED or laser source\nis needed.The highest data rates and longest distances require 1500-nm laser sources.\nFigure 4.3c shows attenuation versus wavelength for a typical optical fiber.The\nunusual shape of the curve is due to the combination of a variety of factors that con-\ntribute to attenuation. The two most important of these are absorption and scatter-\ning. In this context, the term scattering refers to the change in direction of light rays\nafter they strike small particles or impurities in the medium.\n4.2 / WIRELESS TRANSMISSION\nThree general ranges of frequencies are of interest in our discussion of wireless trans-\nmission. Frequencies in the range of about 1 GHz \nare referred to as microwave frequencies.At these frequencies,highly directional beams\nare possible,and microwave is quite suitable for point-to-point transmission.Microwave\nis also used for satellite communications. Frequencies in the range of 30 MHz to 1 GHz\nare suitable for omnidirectional applications.We refer to this range as the radio range.\nAnother important frequency range, for local applications, is the infrared por-\ntion of the spectrum. This covers, roughly, from \nInfrared is\nuseful to local point-to-point and multipoint applications within confined areas,\nsuch as a single room.\nFor unguided media, transmission and reception are achieved by means of an\nantenna. Before looking at specific categories of wireless transmission, we provide a\nbrief introduction to antennas.\nAn antenna can be defined as an electrical conductor or system of conductors used\neither for radiating electromagnetic energy or for collecting electromagnetic energy.\nFor transmission of a signal, radio-frequency electrical energy from the transmitter\nis converted into electromagnetic energy by the antenna and radiated into the sur-\nrounding environment (atmosphere, space, water). For reception of a signal, electro-\nmagnetic energy impinging on the antenna is converted into radio-frequency\nelectrical energy and fed into the receiver.\nIn two-way communication, the same antenna can be and often is used for\nboth transmission and reception. This is possible because any antenna transfers\nenergy from the surrounding environment to its input receiver terminals with the\nsame efficiency that it transfers energy from the output transmitter terminals into\nthe surrounding environment, assuming that the same frequency is used in both\ndirections. Put another way, antenna characteristics are essentially the same whether\nan antenna is sending or receiving electromagnetic energy.\nAn antenna will radiate power in all directions but, typically, does not perform\nequally well in all directions. A common way to characterize the performance of an\nantenna is the radiation pattern, which is a graphical representation of the radiation\nproperties of an antenna as a function of space coordinates. The simplest pattern is\nproduced by an idealized antenna known as the isotropic antenna. An isotropic\nantenna is a point in space that radiates power in all directions equally. The actual\nradiation pattern for the isotropic antenna is a sphere with the antenna at the center.\nParabolic Reflective Antenna An important type of antenna is the parabolic\nreflective antenna, which is used in terrestrial microwave and satellite applications.\nA parabola is the locus of all points equidistant from a fixed line and a fixed point\nnot on the line. The fixed point is called the focus and the fixed line is called the\ndirectrix (Figure 4.5a). If a parabola is revolved about its axis, the surface generated\nis called a paraboloid. A cross section through the paraboloid parallel to its axis\nforms a parabola and a cross section perpendicular to the axis forms a circle. Such\n2 * 1014 Hz.\n1gigahertz = 109 Hertz2\nCHAPTER 4 / TRANSMISSION MEDIA\nsurfaces are used in headlights, optical and radio telescopes, and microwave anten-\nnas because of the following property: If a source of electromagnetic energy (or\nsound) is placed at the focus of the paraboloid, and if the paraboloid is a reflecting\nsurface, then the wave will bounce back in lines parallel to the axis of the parabo-\nloid; Figure 4.5b shows this effect in cross section. In theory, this effect creates a par-\nallel beam without dispersion. In practice, there will be some dispersion, because the\nsource of energy must occupy more than one point. The larger the diameter of the\nantenna, the more tightly directional is the beam. On reception, if incoming waves\nare parallel to the axis of the reflecting paraboloid, the resulting signal will be con-\ncentrated at the focus.\nAntenna Gain Antenna gain is a measure of the directionality of an antenna.\nAntenna gain is defined as the power output, in a particular direction, compared to\nthat produced in any direction by a perfect omnidirectional antenna (isotropic\nantenna). For example, if an antenna has a gain of 3 dB, that antenna improves upon\nthe isotropic antenna in that direction by 3 dB, or a factor of 2.The increased power\nradiated in a given direction is at the expense of other directions. In effect, increased\npower is radiated in one direction by reducing the power radiated in other\ndirections. It is important to note that antenna gain does not refer to obtaining more\noutput power than input power but rather to directionality.\nA concept related to that of antenna gain is the effective area of an antenna.\nThe effective area of an antenna is related to the physical size of the antenna and to\nits shape.The relationship between antenna gain and effective area is\n(a) Parabola\n(b) Cross section of parabolic antenna\nshowing reflective property\nParabolic Reflective Antenna\n4.2 / WIRELESS TRANSMISSION\nFor example, the effective area of an ideal isotropic antenna is \npower gain of 1; the effective area of a parabolic antenna with a face area of A is\n0.56A, with a power gain of 7A/l2.\nl = carrier wavelength\n c = speed of light 1L 3 * 108 m/s2\nf = carrier frequency\nAe = effective area\nG = antenna gain\nEXAMPLE 4.2 For a parabolic reflective antenna with a diameter of 2 m,\noperating at 12 GHz, what is the effective area and the antenna gain? We have\nand an effective area of \nThe wavelength is\nGdB = 45.46 dB\nG = 17A2/l2 = 17 * p2/10.02522 = 35,186\nl = c/f = 13 * 1082/112 * 1092 = 0.025 m.\nAe = 0.56p.\nA = pr2 = p\nTerrestrial Microwave\nPhysical Description The most common type of microwave antenna is the par-\nabolic “dish.”A typical size is about 3 m in diameter.The antenna is fixed rigidly and\nfocuses a narrow beam to achieve line-of-sight transmission to the receiving\nantenna. Microwave antennas are usually located at substantial heights above\nground level to extend the range between antennas and to be able to transmit over\nintervening obstacles. To achieve long-distance transmission, a series of microwave\nrelay towers is used, and point-to-point microwave links are strung together over\nthe desired distance.\nApplications The primary use for terrestrial microwave systems is in long-haul\ntelecommunications service, as an alternative to coaxial cable or optical fiber. The\nmicrowave facility requires far fewer amplifiers or repeaters than coaxial cable over\nthe same distance but requires line-of-sight transmission. Microwave is commonly\nused for both voice and television transmission.\nAnother increasingly common use of microwave is for short point-to-point\nlinks between buildings. This can be used for closed-circuit TV or as a data link\nbetween local area networks. Short-haul microwave can also be used for the so-\ncalled bypass application. A business can establish a microwave link to a long-\ndistance telecommunications facility in the same city, bypassing the local telephone\nAnother important use of microwave is in cellular systems, examined in\nChapter 14.\nCHAPTER 4 / TRANSMISSION MEDIA\nTransmission Characteristics Microwave transmission covers a substantial\nportion of the electromagnetic spectrum. Common frequencies used for transmis-\nsion are in the range 1 to 40 GHz. The higher the frequency used, the higher the\npotential bandwidth and therefore the higher the potential data rate. Table 4.6\nindicates bandwidth and data rate for some typical systems.\nAs with any transmission system, a main source of loss is attenuation. For\nmicrowave (and radio frequencies), the loss can be expressed as\nwhere d is the distance and is the wavelength, in the same units.Thus, loss varies as\nthe square of the distance. In contrast, for twisted-pair and coaxial cable, loss varies\nexponentially with distance (linear in decibels).Thus repeaters or amplifiers may be\nplaced farther apart for microwave systems—10 to 100 km is typical.Attenuation is\nincreased with rainfall.The effects of rainfall become especially noticeable above 10\nGHz.Another source of impairment is interference.With the growing popularity of\nmicrowave, transmission areas overlap and interference is always a danger.Thus the\nassignment of frequency bands is strictly regulated.\nThe most common bands for long-haul telecommunications are the 4-GHz to\n6-GHz bands. With increasing congestion at these frequencies, the 11-GHz band is\nnow coming into use.The 12-GHz band is used as a component of cable TV systems.\nMicrowave links are used to provide TV signals to local CATV installations; the\nsignals are then distributed to individual subscribers via coaxial cable. Higher-\nfrequency microwave is being used for short point-to-point links between buildings;\ntypically, the 22-GHz band is used.The higher microwave frequencies are less useful\nfor longer distances because of increased attenuation but are quite adequate for\nshorter distances. In addition, at the higher frequencies, the antennas are smaller\nand cheaper.\nSatellite Microwave\nPhysical Description A communication satellite is, in effect, a microwave\nrelay station. It is used to link two or more ground-based microwave transmit-\nter/receivers, known as earth stations, or ground stations. The satellite receives\ntransmissions on one frequency band (uplink), amplifies or repeats the signal,\nand transmits it on another frequency (downlink). A single orbiting satellite will\nL = 10 loga 4pd\nTypical Digital Microwave Performance\nBandwidth (MHz)\nData Rate (Mbps)\n4.2 / WIRELESS TRANSMISSION\noperate on a number of frequency bands, called transponder channels, or simply\ntransponders.\nFigure 4.6 depicts in a general way two common configurations for satellite\ncommunication. In the first, the satellite is being used to provide a point-to-point\nlink between two distant ground-based antennas. In the second, the satellite pro-\nvides communications between one ground-based transmitter and a number of\nground-based receivers.\nFor a communication satellite to function effectively, it is generally required\nthat it remain stationary with respect to its position over the earth. Otherwise, it\nwould not be within the line of sight of its earth stations at all times. To remain sta-\ntionary, the satellite must have a period of rotation equal to the earth’s period of\nrotation.This match occurs at a height of 35,863 km at the equator.\n(a) Point-to-point link\n(b) Broadcast link\nTransmitter\nSatellite Communication Configurations\nCHAPTER 4 / TRANSMISSION MEDIA\nTwo satellites using the same frequency band, if close enough together, will\ninterfere with each other. To avoid this, current standards require a 4° spacing\n(angular displacement as measured from the earth) in the 4/6-GHz band and a 3°\nspacing at 12/14 GHz.Thus the number of possible satellites is quite limited.\nApplications The following are among the most important applications for satellites:\n• Television distribution\n• Long-distance telephone transmission\n• Private business networks\n• Global positioning\nBecause of their broadcast nature, satellites are well suited to television distri-\nbution and are being used extensively in the United States and throughout the\nworld for this purpose. In its traditional use, a network provides programming from\na central location. Programs are transmitted to the satellite and then broadcast\ndown to a number of stations, which then distribute the programs to individual\nviewers. One network, the Public Broadcasting Service (PBS), distributes its tele-\nvision programming almost exclusively by the use of satellite channels. Other com-\nmercial networks also make substantial use of satellite, and cable television systems\nare receiving an ever-increasing proportion of their programming from satellites.\nThe most recent application of satellite technology to television distribution is\ndirect broadcast satellite (DBS), in which satellite video signals are transmitted\ndirectly to the home user. The decreasing cost and size of receiving antennas have\nmade DBS economically feasible.\nSatellite transmission is also used for point-to-point trunks between telephone\nexchange offices in public telephone networks. It is the optimum medium for high-\nusage international trunks and is competitive with terrestrial systems for many long-\ndistance intranational links.\nThere are a number of business data applications for satellite. The satellite\nprovider can divide the total capacity into a number of channels and lease these\nchannels to individual business users. A user equipped with the antennas at a num-\nber of sites can use a satellite channel for a private network. Traditionally, such\napplications have been quite expensive and limited to larger organizations with\nhigh-volume requirements. A recent development is the very small aperture termi-\nnal (VSAT) system, which provides a low-cost alternative. Figure 4.7 depicts a typi-\ncal VSAT configuration. A number of subscriber stations are equipped with\nlow-cost VSAT antennas. Using some discipline, these stations share a satellite\ntransmission capacity for transmission to a hub station. The hub station can\nexchange messages with each of the subscribers and can relay messages between\nsubscribers.\nA final application of satellites, which has become pervasive, is worthy of note.\nThe Navstar Global Positioning System, or GPS for short, consists of three segments\nor components:\n• A constellation of satellites (currently 27) orbiting about 20,000 km above the\nearth’s surface, which transmit ranging signals on two frequencies in the\nmicrowave part of the radio spectrum\n4.2 / WIRELESS TRANSMISSION\nPoint-of-sale\nTypical VSAT Configuration\n• A control segment which maintains GPS through a system of ground monitor\nstations and satellite upload facilities\n• The user receivers—both civil and military\nEach satellite transmits a unique digital code sequence of 1s and 0s, precisely\ntimed by an atomic clock, which is picked up by a GPS receiver’s antenna and\nmatched with the same code sequence generated inside the receiver. By lining up or\nmatching the signals, the receiver determines how long it takes the signals to travel\nfrom the satellite to the receiver. These timing measurements are converted to dis-\ntances using the speed of light. Measuring distances to four or more satellites simul-\ntaneously and knowing the exact locations of the satellites (included in the signals\ntransmitted by the satellites), the receiver can determine its latitude, longitude, and\nheight while at the same time synchronizing its clock with the GPS time standard\nwhich also makes the receiver a precise time piece.\nTransmission Characteristics The optimum frequency range for satellite\ntransmission is in the range 1 to 10 GHz. Below 1 GHz, there is significant noise\nfrom natural sources, including galactic, solar, and atmospheric noise, and human-\nmade interference from various electronic devices. Above 10 GHz, the signal is\nseverely attenuated by atmospheric absorption and precipitation.\nMost satellites providing point-to-point service today use a frequency band-\nwidth in the range 5.925 to 6.425 GHz for transmission from earth to satellite\n(uplink) and a bandwidth in the range 3.7 to 4.2 GHz for transmission from satellite\nto earth (downlink). This combination is referred to as the 4/6-GHz band. Note\nthat the uplink and downlink frequencies differ. For continuous operation without\ninterference, a satellite cannot transmit and receive on the same frequency. Thus\nCHAPTER 4 / TRANSMISSION MEDIA\nsignals received from a ground station on one frequency must be transmitted back\non another.\nThe 4/6 GHz band is within the optimum zone of 1 to 10 GHz but has become\nsaturated. Other frequencies in that range are unavailable because of sources of\ninterference operating at those frequencies, usually terrestrial microwave. There-\nfore, the 12/14-GHz band has been developed (uplink: 14 to 14.5 GHz; downlink:\n11.7 to 12.2 GHz).At this frequency band, attenuation problems must be overcome.\nHowever, smaller and cheaper earth-station receivers can be used. It is anticipated\nthat this band will also saturate, and use is projected for the 20/30-GHz band\n(uplink: 27.5 to 30.0 GHz; downlink: 17.7 to 20.2 GHz). This band experiences even\ngreater attenuation problems but will allow greater bandwidth (2500 MHz versus\n500 MHz) and even smaller and cheaper receivers.\nSeveral properties of satellite communication should be noted. First, because\nof the long distances involved, there is a propagation delay of about a quarter sec-\nond from transmission from one earth station to reception by another earth station.\nThis delay is noticeable in ordinary telephone conversations. It also introduces\nproblems in the areas of error control and flow control, which we discuss in later\nchapters. Second, satellite microwave is inherently a broadcast facility. Many sta-\ntions can transmit to the satellite, and a transmission from a satellite can be received\nby many stations.\nBroadcast Radio\nPhysical Description The principal difference between broadcast radio and\nmicrowave is that the former is omnidirectional and the latter is directional. Thus\nbroadcast radio does not require dish-shaped antennas, and the antennas need not\nbe rigidly mounted to a precise alignment.\nApplications Radio is a general term used to encompass frequencies in the range\nof 3 kHz to 300 GHz. We are using the informal term broadcast radio to cover the\nVHF and part of the UHF band: 30 MHz to 1 GHz.This range covers FM radio and\nUHF and VHF television. This range is also used for a number of data networking\napplications.\nTransmission Characteristics The range 30 MHz to 1 GHz is an effective one\nfor broadcast communications. Unlike the case for lower-frequency electromagnetic\nwaves, the ionosphere is transparent to radio waves above 30 MHz. Thus transmis-\nsion is limited to the line of sight, and distant transmitters will not interfere with\neach other due to reflection from the atmosphere. Unlike the higher frequencies of\nthe microwave region, broadcast radio waves are less sensitive to attenuation from\nAs with microwave, the amount of attenuation due to distance obeys Equation\n(4.2), namely \nBecause of the longer wavelength, radio waves suffer\nrelatively less attenuation.\nA prime source of impairment for broadcast radio waves is multipath interfer-\nence. Reflection from land, water, and natural or human-made objects can create\nmultiple paths between antennas. This effect is frequently evident when TV recep-\ntion displays multiple images as an airplane passes by.\n4.3 / WIRELESS PROPAGATION\nInfrared communications is achieved using transmitters/receivers (transceivers)\nthat modulate noncoherent infrared light. Transceivers must be within the line of\nsight of each other either directly or via reflection from a light-colored surface such\nas the ceiling of a room.\nOne important difference between infrared and microwave transmission is\nthat the former does not penetrate walls. Thus the security and interference prob-\nlems encountered in microwave systems are not present. Furthermore, there is no\nfrequency allocation issue with infrared, because no licensing is required.\nA signal radiated from an antenna travels along one of three routes: ground wave,\nsky wave, or line of sight (LOS). Table 4.7 shows in which frequency range each pre-\ndominates. In this book, we are almost exclusively concerned with LOS communica-\ntion, but a short overview of each mode is given in this section.\nGround Wave Propagation\nGround wave propagation (Figure 4.8a) more or less follows the contour of the earth\nand can propagate considerable distances, well over the visual horizon. This effect is\nfound in frequencies up to about 2 MHz. Several factors account for the tendency of\nelectromagnetic wave in this frequency band to follow the earth’s curvature. One fac-\ntor is that the electromagnetic wave induces a current in the earth’s surface, the result\nof which is to slow the wavefront near the earth, causing the wavefront to tilt down-\nward and hence follow the earth’s curvature.Another factor is diffraction, which is a\nphenomenon having to do with the behavior of electromagnetic waves in the pres-\nence of obstacles. Electromagnetic waves in this frequency range are scattered by the\natmosphere in such a way that they do not penetrate the upper atmosphere.\nThe best-known example of ground wave communication is AM radio.\nSky Wave Propagation\nSky wave propagation is used for amateur radio, CB radio, and international broad-\ncasts such as BBC and Voice of America. With sky wave propagation, a signal from\nan earth-based antenna is reflected from the ionized layer of the upper atmosphere\n(ionosphere) back down to earth.Although it appears the wave is reflected from the\nionosphere as if the ionosphere were a hard reflecting surface, the effect is in fact\ncaused by refraction. Refraction is described subsequently.\nA sky wave signal can travel through a number of hops, bouncing back and\nforth between the ionosphere and the earth’s surface (Figure 4.8b).With this propa-\ngation mode, a signal can be picked up thousands of kilometers from the transmitter.\nLine-of-Sight Propagation\nAbove 30 MHz, neither ground wave nor sky wave propagation modes operate, and\ncommunication must be by line of sight (Figure 4.8c). For satellite communication, a\nsignal above 30 MHz is not reflected by the ionosphere and therefore a signal can be\nFrequency Bands\nFrequency Range\nPropagation Characteristics\nTypical Use\nWavelength Range\nELF (extremely low \n30 to 300 Hz\n10,000 to 1000 km\nPower line frequencies; used by some \nhome control systems.\nVF (voice frequency)\n300 to 3000 Hz\n1000 to 100 km\nUsed by the telephone system for \nanalog subscriber lines.\nVLF (very low frequency)\n3 to 30 kHz\n100 to 10 km\nGW; low attenuation day and night;\nLong-range navigation; submarine \nhigh atmospheric noise level\ncommunication\nLF (low frequency)\n30 to 300 kHz\nGW; slightly less reliable than VLF;\nLong-range navigation; marine\nabsorption in daytime\ncommunication radio beacons\nMF (medium frequency)\n300 to 3000 kHz\n1,000 to 100 m\nGW and night SW; attenuation low at \nMaritime radio; direction finding;\nnight, high in day; atmospheric noise\nAM broadcasting.\nHF (high frequency)\n3 to 30 MHz\n100 to 10 m\nSW; quality varies with time of day,\nAmateur radio; international \nseason, and frequency.\nbroadcasting, military communication;\nlong-distance aircraft and ship \ncommunication\nVHF (very high frequency)\n30 to 300 MHz\nLOS; scattering because of \nVHF television; FM broadcast and \ntemperature inversion; cosmic noise\ntwo-way radio,AM aircraft \ncommunication; aircraft navigational \nUHF (ultra high frequency)\n300 to 3000 MHz\n100 to 10 cm\nLOS; cosmic noise\nUHF television; cellular telephone;\nradar; microwave links; personal \ncommunications systems\nSHF (super high frequency)\n3 to 30 GHz\nLOS; rainfall attenuation above \nSatellite communication; radar;\n10 GHz; atmospheric attenuation due \nterrestrial microwave links; wireless\nto oxygen and water vapor\nEHF (extremely high \n30 to 300 GHz\nLOS; atmospheric attenuation due to\nExperimental; wireless local loop\noxygen and water vapor\n300 GHz to 400 THz\n1 mm to 770 nm\nInfrared LANs; consumer electronic \napplications\nVisible light\n400 THz to 900 THz\n770 nm to 330 nm\nOptical communication\n4.3 / WIRELESS PROPAGATION\n(b) Sky wave propagation (2 to 30 MHz)\n(a) Ground wave propagation (below 2 MHz)\npropagation\n(c) Line-of-sight (LOS) propagation (above 30 MHz)\npropagation\npropagation\nWireless Propagation Modes\ntransmitted between an earth station and a satellite overhead that is not beyond the\nhorizon. For ground-based communication, the transmitting and receiving antennas\nmust be within an effective line of sight of each other. The term effective is used\nbecause microwaves are bent or refracted by the atmosphere.The amount and even\nthe direction of the bend depends on conditions, but generally microwaves are bent\nwith the curvature of the earth and will therefore propagate farther than the optical\nline of sight.\nCHAPTER 4 / TRANSMISSION MEDIA\nBefore proceeding, a brief discussion of refraction is warranted.\nRefraction occurs because the velocity of an electromagnetic wave is a function of\nthe density of the medium through which it travels. In a vacuum, an electromagnetic\nwave (such as light or a radio wave) travels at approximately \nThis is the\nconstant, c, commonly referred to as the speed of light, but actually referring to the\nspeed of light in a vacuum.2 In air, water, glass, and other transparent or partially\ntransparent media, electromagnetic waves travel at speeds less than c.\nWhen an electromagnetic wave moves from a medium of one density to a\nmedium of another density, its speed changes.The effect is to cause a one-time bend-\ning of the direction of the wave at the boundary between the two media.Moving from\na less dense to a more dense medium, the wave will bend toward the more dense\nmedium.This phenomenon is easily observed by partially immersing a stick in water.\nCHAPTER 4 / TRANSMISSION MEDIA\nCHAPTER 4 / TRANSMISSION MEDIA\nAtmospheric Absorption\nAn additional loss between the transmitting and receiving antennas is atmo-\nspheric absorption. Water vapor and oxygen contribute most to attenuation. A\npeak attenuation occurs in the vicinity of 22 GHz due to water vapor. At frequen-\ncies below 15 GHz, the attenuation is less. The presence of oxygen results in an\nabsorption peak in the vicinity of 60 GHz but contributes less at frequencies\nbelow 30 GHz. Rain and fog (suspended water droplets) cause scattering of radio\nwaves that results in attenuation. In this context, the term scattering refers to the\nproduction of waves of changed direction or frequency when radio waves\nencounter matter. This can be a major cause of signal loss. Thus, in areas of signif-\nicant precipitation, either path lengths have to be kept short or lower-frequency\nbands should be used.\nFor wireless facilities where there is a relatively free choice of where antennas are to\nbe located, they can be placed so that if there are no nearby interfering obstacles,\nthere is a direct line-of-sight path from transmitter to receiver. This is generally the\ncase for many satellite facilities and for point-to-point microwave. In other cases,\nsuch as mobile telephony, there are obstacles in abundance. The signal can be\nreflected by such obstacles so that multiple copies of the signal with varying delays\ncan be received. In fact, in extreme cases, there may be no direct signal. Depending\non the differences in the path lengths of the direct and reflected waves, the compos-\nite signal can be either larger or smaller than the direct signal. Reinforcement and\ncancellation of the signal resulting from the signal following multiple paths can be\ncontrolled for communication between fixed, well-sited antennas, and between\nsatellites and fixed ground stations. One exception is when the path goes across\nwater, where the wind keeps the reflective surface of the water in motion. For\nmobile telephony and communication to antennas that are not well sited, multipath\nconsiderations can be paramount.\nFigure 4.11 illustrates in general terms the types of multipath interference typ-\nical in terrestrial, fixed microwave and in mobile communications. For fixed\nmicrowave, in addition to the direct line of sight, the signal may follow a curved path\nthrough the atmosphere due to refraction and the signal may also reflect from the\nground. For mobile communications, structures and topographic features provide\nreflection surfaces.\nNow consider the antenna gain of both the satellite- and ground-based antennas.\nTypical values are 44 dB and 48 dB, respectively.The free space loss is\nNow assume a transmit power of 250 W at the earth station. What is the power\nreceived at the satellite antenna? A power of 250 W translates into 24 dBW, so\nthe power at the receiving antenna is 24 - 103.6 = -79.6 dBW.\nLdB = 195.6 - 44 - 48 = 103.6 dB\n4.5 / RECOMMENDED READING AND WEB SITES\n(a) Microwave line of sight\n(b) Mobile radio\nFigure 4.11\nExamples of Multipath Interference\nRadio waves are refracted (or bent) when they propagate through the atmo-\nsphere. The refraction is caused by changes in the speed of the signal with altitude\nor by other spatial changes in the atmospheric conditions. Normally, the speed of\nthe signal increases with altitude, causing radio waves to bend downward. How-\never, on occasion, weather conditions may lead to variations in speed with height\nthat differ significantly from the typical variations.This may result in a situation in\nwhich only a fraction or no part of the line-of-sight wave reaches the receiving\nDetailed descriptions of the transmission characteristics of the transmission media discussed\nin this chapter can be found in [FREE98]. [REEV95] provides an excellent treatment of\ntwisted pair and optical fiber. [BORE97] is a thorough treatment of optical fiber transmission\ncomponents.Another good paper on the subject is [WILL97]. [FREE02] is a detailed techni-\ncal reference on optical fiber. [STAL00] discusses the characteristics of transmission media\nfor LANs in greater detail.\nFor a more thorough treatment on wireless transmission and propagation, see\n[STAL05] and [RAPP02]. [FREE97] is an excellent detailed technical reference on wireless\nCHAPTER 4 / TRANSMISSION MEDIA\nBorella, M., et al. “Optical Components for WDM Lightwave Networks.”\nProceedings of the IEEE, August 1997.\nFreeman, R. Radio System Design for Telecommunications. New York:Wiley, 1997.\nFreeman, R. Telecommunication Transmission Handbook. New York:Wiley, 1998.\nFreeman,R.Fiber-Optic Systems for Telecommunications. New York:Wiley,2002.\nRappaport, T. Wireless Communications. Upper Saddle River, NJ: Prentice Hall,\nReeve,W. Subscriber Loop Signaling and Transmission Handbook. Piscataway,\nNJ: IEEE Press, 1995.\nStallings,W. Local and Metropolitan Area Networks, Sixth Edition. Upper Saddle\nRiver, NJ: Prentice Hall, 2000.\nStallings, W. Wireless Communications and Networks, Second Edition. Upper\nSaddle River, NJ: Prentice Hall, 2005.\nWillner, A. “Mining the Optical Bandwidth for a Terabit per Second.” IEEE\nSpectrum, April 1997.\nRecommended Web sites:\n• Siemon Company: Good collection of technical articles on cabling, plus information\nabout cabling standards\n• Wireless developer network: News, tutorials, and discussions on wireless topics\n• About antennas: Good source of information and links\n• U.S. frequency allocation chart: Chart plus background paper\nKEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nantenna gain\natmospheric absorption\nattenuation\ncoaxial cable\ndirectional antenna\neffective area\nfree space loss\nglobal positioning system\nground wave propagation\nguided media\nb. An alternative is to use a modulation scheme,as described in Chapter 5,for transmit-\nting the voice signal by modulating a carrier frequency, so that the bandwidth of the\nCHAPTER 4 / TRANSMISSION MEDIA\nsignal is a narrow band centered on the carrier frequency. Suppose we would like a\nhalf-wave antenna to have a length of 1 meter.What carrier frequency would we use?\nStories abound of people who receive radio signals in fillings in their teeth. Suppose\nyou have one filling that is 2.5 mm (0.0025 m) long that acts as a radio antenna. That\nis, it is equal in length to one-half the wavelength.What frequency do you receive?\nYou are communicating between two satellites.The transmission obeys the free space\nlaw.The signal is too weak.Your vendor offers you two options.The vendor can use a\nhigher frequency that is twice the current frequency or can double the effective area\nof both of the antennas. Which will offer you more received power or will both offer\nthe same improvement, all other factors remaining equal? How much improvement\nin the received power do you obtain from the best option?\nIn satellite communications, different frequency bands are used for the uplink and the\ndownlink. Discuss why this pattern occurs.\nFor radio transmission in free space, signal power is reduced in proportion to the\nsquare of the distance from the source, whereas in wire transmission, the attenuation\nis a fixed number of dB per kilometer. The following table is used to show the dB\nreduction relative to some reference for free space radio and uniform wire. Fill in the\nmissing numbers to complete the table.\nDistance (km)\nSection 4.2 states that if a source of electromagnetic energy is placed at the focus of\nthe paraboloid, and if the paraboloid is a reflecting surface, then the wave will bounce\nback in lines parallel to the axis of the paraboloid. To demonstrate this, consider the\nshown in Figure 4.12. Let \nbe a point on the parabola,\nand PF be the line from P to the focus. Construct the line L through P parallel to the\nx-axis and the line M tangent to the parabola at P. The angle between L and M is\nand the angle between PF and M is \nis the angle at which a ray from F\nstrikes the parabola at P. Because the angle of incidence equals the angle of reflec-\ntion, the ray reflected from P must be at an angle \nto M. Thus, if we can show that\nwe have demonstrated that rays reflected from the parabola starting at F will\nbe parallel to the x-axis.\nFirst show that tan \nHint: Recall from trigonometry that the slope of a\nline is equal to the tangent of the angle the line makes with the positive \nx-direction.Also recall that the slope of the line tangent to a curve at a given point\nis equal to the derivative of the curve at that point.\nb. Now show that tan \nwhich demonstrates that \nHint: Recall from\ntrigonometry that the formula for the tangent of the difference between two\nIt is often more convenient to express distance in km rather than m and frequency in\nMHz rather than Hz. Rewrite Equation (4.3) using these dimensions.\nSuppose a transmitter produces 50 W of power.\nExpress the transmit power in units of dBm and dBW.\nb. If the transmitter’s power is applied to a unity gain antenna with a 900-MHz carrier\nfrequency, what is the received power in dBm at a free space distance of 100 m?\nRepeat (b) for a distance of 10 km.\nd. Repeat (c) but assume a receiver antenna gain of 2.\ntan1a2 - a12 = 1tan a2 - tan a12/11 + tan a2 *  tan a12.\na = 1p/y12,\nb = 1p/y12.\n4.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nA microwave transmitter has an output of 0.1 W at 2 GHz.Assume that this transmit-\nter is used in a microwave communication system where the transmitting and receiv-\ning antennas are parabolas, each 1.2 m in diameter.\nWhat is the gain of each antenna in decibels?\nb. Taking into account antenna gain, what is the effective radiated power of the\ntransmitted signal?\nIf the receiving antenna is located 24 km from the transmitting antenna over a\nfree space path, find the available signal power out of the receiving antenna in\nSection 4.3 states that with no intervening obstacles, the optical line of sight can be\nexpressed as \nwhere d is the distance between an antenna and the hori-\nzon in kilometers and h is the antenna height in meters. Using a value for the earth’s\nradius of 6370 km, derive this equation. Hint: Assume that the antenna is perpendicu-\nlar to the earth’s surface, and note that the line from the top of the antenna to the\nhorizon forms a tangent to the earth’s surface at the horizon. Draw a picture showing\nthe antenna, the line of sight, and the earth’s radius to help visualize the problem.\nDetermine the height of an antenna for a TV station that must be able to reach cus-\ntomers up to 80 km away.\nSuppose a ray of visible light passes from the atmosphere into water at an angle to the\nhorizontal of 30°. What is the angle of the ray in the water? Note: At standard atmo-\nIn Chapter 3 a distinction was made between analog and digital data and analog\nand digital signals. Figure 3.14 suggested that either form of data could be\nencoded into either form of signal.\nFigure 5.1 is another depiction that emphasizes the process involved. For\ndigital signaling, a data source g(t), which may be either digital or analog, is\nencoded into a digital signal x(t).The actual form of x(t) depends on the encoding\ntechnique and is chosen to optimize use of the transmission medium. For exam-\nple, the encoding may be chosen to conserve bandwidth or to minimize errors.\nThe basis for analog signaling is a continuous constant-frequency signal\nknown as the carrier signal. The frequency of the carrier signal is chosen to be\ncompatible with the transmission medium being used. Data may be transmitted\nusing a carrier signal by modulation. Modulation is the process of encoding\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nsource data onto a carrier signal with frequency \nAll modulation techniques\ninvolve operation on one or more of the three fundamental frequency domain\nparameters: amplitude, frequency, and phase.\nThe input signal m(t) may be analog or digital and is called the modulating\nsignal or baseband signal.The result of modulating the carrier signal is called the\nmodulated signal s(t). As Figure 5.1b indicates, s(t) is a bandlimited (bandpass)\nsignal.The location of the bandwidth on the spectrum is related to \nand is often\ncentered on \nAgain,the actual form of the encoding is chosen to optimize some\ncharacteristic of the transmission.\nEach of the four possible combinations depicted in Figure 5.1 is in wide-\nspread use.The reasons for choosing a particular combination for any given com-\nmunication task vary.We list here some representative reasons:\n• Digital data, digital signal: In general, the equipment for encoding digital\ndata into a digital signal is less complex and less expensive than digital-to-\nanalog modulation equipment.\n• Analog data, digital signal: Conversion of analog data to digital form per-\nmits the use of modern digital transmission and switching equipment.The\nadvantages of the digital approach were outlined in Section 3.2.\n• Digital data, analog signal: Some transmission media, such as optical fiber\nand unguided media, will only propagate analog signals.\n• Analog data, analog signal: Analog data in electrical form can be trans-\nmitted as baseband signals easily and cheaply. This is done with voice\ntransmission over voice-grade lines. One common use of modulation is to\nshift the bandwidth of a baseband signal to another portion of the\nspectrum. In this way multiple signals, each at a different position on the\n(b) Modulation onto an analog signal\nDemodulator\n(a) Encoding onto a digital signal\nEncoding and Modulation Techniques\n5.1 / DIGITAL DATA, DIGITAL SIGNALS\nspectrum, can share the same transmission medium. This is known as\nfrequency division multiplexing.\nWe now examine the techniques involved in each of these four combinations.\nDIGITAL DATA, DIGITAL SIGNALS\nA digital signal is a sequence of discrete, discontinuous voltage pulses. Each pulse is\na signal element. Binary data are transmitted by encoding each data bit into signal\nelements. In the simplest case, there is a one-to-one correspondence between bits\nand signal elements.An example is shown in Figure 3.16, in which binary 1 is repre-\nsented by a lower voltage level and binary 0 by a higher voltage level. We show in\nthis section that a variety of other encoding schemes are also used.\nFirst,we define some terms.If the signal elements all have the same algebraic sign,\nthat is, all positive or negative, then the signal is unipolar. In polar signaling, one logic\nstate is represented by a positive voltage level,and the other by a negative voltage level.\nThe data signaling rate, or just data rate, of a signal is the rate, in bits per second, that\ndata are transmitted.The duration or length of a bit is the amount of time it takes for the\ntransmitter to emit the bit;for a data rate R, the bit duration is 1/R.The modulation rate,\nin contrast,is the rate at which the signal level is changed.This will depend on the nature\nof the digital encoding, as explained later. The modulation rate is expressed in baud,\nwhich means signal elements per second. Finally, the terms mark and space, for histori-\ncal reasons, refer to the binary digits 1 and 0, respectively. Table 5.1 summarizes key\nterms; these should be clearer when we see an example later in this section.\nThe tasks involved in interpreting digital signals at the receiver can be summa-\nrized by again referring to Figure 3.16. First, the receiver must know the timing of\neach bit. That is, the receiver must know with some accuracy when a bit begins and\nends. Second, the receiver must determine whether the signal level for each bit\nposition is high (0) or low (1). In Figure 3.16, these tasks are performed by sampling\neach bit position in the middle of the interval and comparing the value to a thresh-\nold. Because of noise and other impairments, there will be errors, as shown.\nWhat factors determine how successful the receiver will be in interpreting\nthe incoming signal? We saw in Chapter 3 that three factors are important: the\nKey Data Transmission Terms\nData element\nA single binary one or zero\nBits per second (bps)\nThe rate at which data elements are transmitted\nSignal element\nDigital: a voltage pulse of \nThat part of a signal that occupies the shortest\nconstant amplitude\ninterval of a signaling code\nAnalog: a pulse of constant \nfrequency, phase, and amplitude\nSignaling rate or \nSignal elements per second\nThe rate at which signal \nmodulation rate\nelements are transmitted\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nsignal-to-noise ratio, the data rate, and the bandwidth. With other factors held\nconstant, the following statements are true:\n• An increase in data rate increases bit error rate (BER).1\n• An increase in SNR decreases bit error rate.\n• An increase in bandwidth allows an increase in data rate.\nThere is another factor that can be used to improve performance, and that is\nthe encoding scheme. The encoding scheme is simply the mapping from data bits\nto signal elements. A variety of approaches have been tried. In what follows, we\ndescribe some of the more common ones; they are defined in Table 5.2 and depicted\nin Figure 5.2.\nBefore describing these techniques, let us consider the following ways of eval-\nuating or comparing the various techniques.\nDefinition of Digital Signal Encoding Formats\nNonreturn to Zero-Level (NRZ-L)\nNonreturn to Zero Inverted (NRZI)\nBipolar-AMI\nPseudoternary\nDifferential Manchester\nAlways a transition in middle of interval\nSame as bipolar AMI, except that any string of eight zeros is replaced by a string with two code violations\nSame as bipolar AMI, except that any string of four zeros is replaced by a string with one code violation\n 1 = no transition at beginning of interval\n 0 = transition at beginning of interval\n 1 = transition from low to high in middle of interval\n 0 = transition from high to low in middle of interval\n 1 = no line signal\n 0 = positive or negative level, alternating for successive zeros\n 1 = positive or negative level, alternating for successive ones\n 0 = no line signal\n 1 = transition at beginning of interval\n 0 = no transition at beginning of interval 1one bit time2\n 1 = low level\n 0 = high level\n1The BER is the most common measure of error performance on a data circuit and is defined as the\nprobability that a bit is received in error. It is also called the bit error ratio. This latter term is clearer,\nbecause the term rate typically refers to some quantity that varies with time. Unfortunately, most books\nand standards documents refer to the R in BER as rate.\n5.1 / DIGITAL DATA, DIGITAL SIGNALS\n• Signal spectrum: Several aspects of the signal spectrum are important. A lack\nof high-frequency components means that less bandwidth is required for\ntransmission. In addition, lack of a direct-current (dc) component is also desir-\nable. With a dc component to the signal, there must be direct physical attach-\nment of transmission components. With no dc component, ac coupling via\ntransformer is possible; this provides excellent electrical isolation, reducing\ninterference. Finally, the magnitude of the effects of signal distortion and inter-\nference depend on the spectral properties of the transmitted signal. In prac-\ntice, it usually happens that the transmission characteristics of a channel are\nworse near the band edges.Therefore, a good signal design should concentrate\nthe transmitted power in the middle of the transmission bandwidth. In such a\ncase, a smaller distortion should be present in the received signal.To meet this\nobjective, codes can be designed with the aim of shaping the spectrum of the\ntransmitted signal.\n• Clocking: We mentioned the need to determine the beginning and end of each\nbit position. This is no easy task. One rather expensive approach is to provide\nBipolar-AMI\n(most recent\npreceding 1 bit has\nnegative voltage)\nPseudoternary\n(most recent\npreceding 0 bit has\nnegative voltage)\nDifferential\nDigital Signal Encoding Formats\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\na separate clock lead to synchronize the transmitter and receiver.The alterna-\ntive is to provide some synchronization mechanism that is based on the trans-\nmitted signal. This can be achieved with suitable encoding, as explained\nsubsequently.\n• Error detection: We will discuss various error-detection techniques in Chapter 6\nand show that these are the responsibility of a layer of logic above the signaling\nlevel that is known as data link control. However, it is useful to have some error\ndetection capability built into the physical signaling encoding scheme.This per-\nmits errors to be detected more quickly.\n• Signal interference and noise immunity: Certain codes exhibit superior perform-\nance in the presence of noise. Performance is usually expressed in terms of a\n• Cost and complexity: Although digital logic continues to drop in price, this fac-\ntor should not be ignored. In particular, the higher the signaling rate to achieve\na given data rate, the greater the cost. We shall see that some codes require a\nsignaling rate that is greater than the actual data rate.\nWe now turn to a discussion of various techniques.\nNonreturn to Zero (NRZ)\nThe most common, and easiest, way to transmit digital signals is to use two different\nvoltage levels for the two binary digits. Codes that follow this strategy share the\nproperty that the voltage level is constant during a bit interval; there is no transition\n(no return to a zero voltage level). For example, the absence of voltage can be used\nto represent binary 0, with a constant positive voltage used to represent binary 1.\nMore commonly, a negative voltage represents one binary value and a positive volt-\nage represents the other. This latter code, known as Nonreturn to Zero-Level\n(NRZ-L), is illustrated2 in Figure 5.2. NRZ-L is typically the code used to generate\nor interpret digital data by terminals and other devices. If a different code is to be\nused for transmission, it is generated from an NRZ-L signal by the transmission sys-\ntem [in terms of Figure 5.1, NRZ-L is g(t) and the encoded signal is x(t)].\nA variation of NRZ is known as NRZI (Nonreturn to Zero, invert on ones).\nAs with NRZ-L, NRZI maintains a constant voltage pulse for the duration of a bit\ntime.The data themselves are encoded as the presence or absence of a signal transi-\ntion at the beginning of the bit time. A transition (low to high or high to low) at the\nbeginning of a bit time denotes a binary 1 for that bit time; no transition indicates a\nNRZI is an example of differential encoding. In differential encoding, the\ninformation to be transmitted is represented in terms of the changes between suc-\ncessive signal elements rather than the signal elements themselves. The encoding\nof the current bit is determined as follows: If the current bit is a binary 0, then the\n2In this figure, a negative voltage is equated with binary 1 and a positive voltage with binary 0.This is the\nopposite of the definition used in virtually all other textbooks.The definition here conforms to the use of\nNRZ-L in data communications interfaces and the standards that govern those interfaces.\n5.1 / DIGITAL DATA, DIGITAL SIGNALS\ncurrent bit is encoded with the same signal as the preceding bit; if the current bit is\na binary 1, then the current bit is encoded with a different signal than the preced-\ning bit. One benefit of differential encoding is that it may be more reliable to\ndetect a transition in the presence of noise than to compare a value to a threshold.\nAnother benefit is that with a complex transmission layout, it is easy to lose the\nsense of the polarity of the signal. For example, on a multidrop twisted-pair line, if\nthe leads from an attached device to the twisted pair are accidentally inverted, all\n1s and 0s for NRZ-L will be inverted. This does not happen with differential\nThe NRZ codes are the easiest to engineer and, in addition, make efficient use\nof bandwidth. This latter property is illustrated in Figure 5.3, which compares the\nspectral density of various encoding schemes. In the figure, frequency is normalized\nto the data rate. Most of the energy in NRZ and NRZI signals is between dc and\nhalf the bit rate. For example, if an NRZ code is used to generate a signal with data\nrate of 9600 bps, most of the energy in the signal is concentrated between dc and\nThe main limitations of NRZ signals are the presence of a dc component and\nthe lack of synchronization capability. To picture the latter problem, consider that\nwith a long string of 1s or 0s for NRZ-L or a long string of 0s for NRZI, the output\nis a constant voltage over a long period of time. Under these circumstances, any drift\nbetween the clocks of transmitter and receiver will result in loss of synchronization\nbetween the two.\nBecause of their simplicity and relatively low frequency response characteris-\ntics, NRZ codes are commonly used for digital magnetic recording. However, their\nlimitations make these codes unattractive for signal transmission applications.\nAMI, pseudoternary\nManchester,\ndifferential Manchester\nNormalized frequency (f/R)\nMean square voltage per unit bandwidth\n\u0001 alternate mark inversion\n\u0001 bipolar with 8 zeros substitution\n\u0001 high-density bipolar—3 zeros\nNRZ-L \u0001 nonreturn to zero level\n\u0001 nonreturn to zero inverted\n\u0001 frequency\n\u0001 data rate\nSpectral Density of Various Signal Encoding Schemes\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nMultilevel Binary\nA category of encoding techniques known as multilevel binary addresses some of\nthe deficiencies of the NRZ codes.These codes use more than two signal levels.Two\nexamples of this scheme are illustrated in Figure 5.2, bipolar-AMI (alternate mark\ninversion) and pseudoternary.3\nIn the case of the bipolar-AMI scheme, a binary 0 is represented by no line\nsignal, and a binary 1 is represented by a positive or negative pulse. The binary 1\npulses must alternate in polarity. There are several advantages to this approach.\nFirst, there will be no loss of synchronization if a long string of 1s occurs. Each 1\nintroduces a transition, and the receiver can resynchronize on that transition. A\nlong string of 0s would still be a problem. Second, because the 1 signals alternate in\nvoltage from positive to negative, there is no net dc component. Also, the band-\nwidth of the resulting signal is considerably less than the bandwidth for NRZ\n(Figure 5.3). Finally, the pulse alternation property provides a simple means of\nerror detection.Any isolated error, whether it deletes a pulse or adds a pulse, causes\na violation of this property.\nThe comments of the previous paragraph also apply to pseudoternary. In this\ncase, it is the binary 1 that is represented by the absence of a line signal, and the\nbinary 0 by alternating positive and negative pulses. There is no particular advan-\ntage of one technique versus the other, and each is the basis of some applications.\nAlthough a degree of synchronization is provided with these codes, a long\nstring of 0s in the case of AMI or 1s in the case of pseudoternary still presents a\nproblem. Several techniques have been used to address this deficiency. One\napproach is to insert additional bits that force transitions. This technique is used in\nISDN (integrated services digital network) for relatively low data rate transmission.\nOf course, at a high data rate, this scheme is expensive, because it results in an\nincrease in an already high signal transmission rate. To deal with this problem at\nhigh data rates, a technique that involves scrambling the data is used. We examine\ntwo examples of this technique later in this section.\nThus, with suitable modification, multilevel binary schemes overcome the\nproblems of NRZ codes. Of course, as with any engineering design decision, there is\na tradeoff. With multilevel binary coding, the line signal may take on one of three\nlevels, but each signal element, which could represent \nbits of informa-\ntion, bears only one bit of information. Thus multilevel binary is not as efficient as\nNRZ coding. Another way to state this is that the receiver of multilevel binary sig-\nnals has to distinguish between three levels \ninstead of just two levels in\nthe signaling formats previously discussed. Because of this, the multilevel binary sig-\nnal requires approximately 3 dB more signal power than a two-valued signal for the\nsame probability of bit error.This is illustrated in Figure 5.4. Put another way, the bit\nerror rate for NRZ codes, at a given signal-to-noise ratio, is significantly less than\nthat for multilevel binary.\n1+A, -A, 02\nlog2 3 = 1.58\n3These terms are not used consistently in the literature. In some books, these two terms are used for dif-\nferent encoding schemes than those defined here, and a variety of terms have been used for the two\nschemes illustrated in Figure 5.2.The nomenclature used here corresponds to the usage in various ITU-T\nstandards documents.\n5.1 / DIGITAL DATA, DIGITAL SIGNALS\nThere is another set of coding techniques, grouped under the term biphase, that\novercomes the limitations of NRZ codes. Two of these techniques, Manchester and\ndifferential Manchester, are in common use.\nIn the Manchester code, there is a transition at the middle of each bit period.\nThe midbit transition serves as a clocking mechanism and also as data: a low-to-high\ntransition represents a 1, and a high-to-low transition represents a 0.4 In differential\nManchester, the midbit transition is used only to provide clocking. The encoding of\na 0 is represented by the presence of a transition at the beginning of a bit period, and\na 1 is represented by the absence of a transition at the beginning of a bit period. Dif-\nferential Manchester has the added advantage of employing differential encoding.\nAll of the biphase techniques require at least one transition per bit time and\nmay have as many as two transitions. Thus, the maximum modulation rate is twice\nthat for NRZ; this means that the bandwidth required is correspondingly greater.\nOn the other hand, the biphase schemes have several advantages:\n• Synchronization: Because there is a predictable transition during each bit\ntime, the receiver can synchronize on that transition. For this reason, the\nbiphase codes are known as self-clocking codes.\n• No dc component: Biphase codes have no dc component, yielding the benefits\ndescribed earlier.\nProbability of bit error (BER)\n(Eb/N0) (dB)\n10 11 12 13 14 15\nAMI, pseudoternary,\nNRZ, biphase\nTheoretical Bit Error Rate for Various Encoding\n4The definition of Manchester presented here is the opposite of that used in a number of respectable\ntextbooks, in which a low-to-high transition represents a binary 0 and a high-to-low transition represents\na binary 1. Here, we conform to industry practice and to the definition used in the various LAN stan-\ndards, such as IEEE 802.3.\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\n• Error detection: The absence of an expected transition can be used to detect\nerrors. Noise on the line would have to invert both the signal before and after\nthe expected transition to cause an undetected error.\nAs can be seen from Figure 5.3, the bandwidth for biphase codes is reasonably\nnarrow and contains no dc component. However, it is wider than the bandwidth for\nthe multilevel binary codes.\nBiphase codes are popular techniques for data transmission. The more com-\nmon Manchester code has been specified for the IEEE 802.3 (Ethernet) standard\nfor baseband coaxial cable and twisted-pair bus LANs. Differential Manchester\nhas been specified for the IEEE 802.5 token ring LAN, using shielded twisted\nModulation Rate\nWhen signal-encoding techniques are used, a distinction needs to be made between\ndata rate (expressed in bits per second) and modulation rate (expressed in baud).\nThe data rate, or bit rate, is \nduration.The modulation rate is the\nrate at which signal elements are generated. Consider, for example, Manchester\nencoding. The minimum size signal element is a pulse of one-half the duration of a\nbit interval. For a string of all binary zeroes or all binary ones, a continuous stream\nof such pulses is generated. Hence the maximum modulation rate for Manchester is\nThis situation is illustrated in Figure 5.5, which shows the transmission of a\nstream of binary 1s at a data rate of 1 Mbps using NRZI and Manchester. In general,\n5 bits \u0001 5 \u0004s\n1 signal element \u0001\n1 signal element \u0001\nA Stream of Binary Ones at 1 Mbps\n5.1 / DIGITAL DATA, DIGITAL SIGNALS\nOne way of characterizing the modulation rate is to determine the average\nnumber of transitions that occur per bit time. In general, this will depend on the\nexact sequence of bits being transmitted. Table 5.3 compares transition rates for var-\nious techniques. It indicates the signal transition rate in the case of a data stream of\nalternating 1s and 0s, and for the data stream that produces the minimum and maxi-\nmum modulation rate.\nScrambling Techniques\nAlthough the biphase techniques have achieved widespread use in local area net-\nwork applications at relatively high data rates (up to 10 Mbps), they have not been\nwidely used in long-distance applications. The principal reason for this is that they\nrequire a high signaling rate relative to the data rate.This sort of inefficiency is more\ncostly in a long-distance application.\nAnother approach is to make use of some sort of scrambling scheme.The idea\nbehind this approach is simple: Sequences that would result in a constant voltage\nlevel on the line are replaced by filling sequences that will provide sufficient transi-\ntions for the receiver’s clock to maintain synchronization.The filling sequence must\nbe recognized by the receiver and replaced with the original data sequence.The fill-\ning sequence is the same length as the original sequence, so there is no data rate\npenalty.The design goals for this approach can be summarized as follows:\n• No dc component\n• No long sequences of zero-level line signals\n• No reduction in data rate\n• Error-detection capability\nL = number of bits per signal element\nM = number of different signal elements = 2L\nR = data rate, bps\nD = modulation rate, baud\nNormalized Signal Transition Rate of Various Digital Signal Encoding\n101010 . . .\n0 (all 0s or 1s)\n1.0 (all 1s)\nBipolar-AMI\nPseudoternary\n1.0 (1010 . . .)\n2.0 (all 0s or 1s)\nDifferential Manchester\n1.0 (all 1s)\n2.0 (all 0s)\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nTwo techniques are commonly used in long-distance transmission services;\nthese are illustrated in Figure 5.6.\nA coding scheme that is commonly used in North America is known as bipolar\nwith 8-zeros substitution (B8ZS). The coding scheme is based on a bipolar-AMI.We\nhave seen that the drawback of the AMI code is that a long string of zeros may\nresult in loss of synchronization. To overcome this problem, the encoding is\namended with the following rules:\n• If an octet of all zeros occurs and the last voltage pulse preceding this octet\nwas positive, then the eight zeros of the octet are encoded as \n• If an octet of all zeros occurs and the last voltage pulse preceding this octet\nwas negative, then the eight zeros of the octet are encoded as \nThis technique forces two code violations (signal patterns not allowed in AMI)\nof the AMI code, an event unlikely to be caused by noise or other transmission\nimpairment. The receiver recognizes the pattern and interprets the octet as consist-\ning of all zeros.\nA coding scheme that is commonly used in Europe and Japan is known as the\nhigh-density bipolar-3 zeros (HDB3) code (Table 5.4). As before, it is based on the\nuse of AMI encoding. In this case, the scheme replaces strings of four zeros with\nsequences containing one or two pulses. In each case, the fourth zero is replaced\nwith a code violation. In addition, a rule is needed to ensure that successive viola-\ntions are of alternate polarity so that no dc component is introduced.Thus, if the last\nviolation was positive, this violation must be negative and vice versa. Table 5.4\nshows that this condition is tested for by determining (1) whether the number of\n000- +0+ -.\n000+ -0- +.\nBipolar-AMI\n0 V B 0 V B\nB \u0001 Valid bipolar signal\nV \u0001 Bipolar violation\n(odd number of 1s\nsince last substitution)\nEncoding Rules for B8ZS and HDB3\n5.2 / DIGITAL DATA, ANALOG SIGNALS\npulses since the last violation is even or odd and (2) the polarity of the last pulse\nbefore the occurrence of the four zeros.\nFigure 5.3 shows the spectral properties of these two codes. As can be seen,\nneither has a dc component. Most of the energy is concentrated in a relatively sharp\nspectrum around a frequency equal to one-half the data rate. Thus, these codes are\nwell suited to high data rate transmission.\nDIGITAL DATA,ANALOG SIGNALS\nWe turn now to the case of transmitting digital data using analog signals. The most\nfamiliar use of this transformation is for transmitting digital data through the public\ntelephone network. The telephone network was designed to receive, switch, and\ntransmit analog signals in the voice-frequency range of about 300 to 3400 Hz. It is\nnot at present suitable for handling digital signals from the subscriber locations\n(although this is beginning to change). Thus digital devices are attached to the net-\nwork via a modem (modulator-demodulator), which converts digital data to analog\nsignals, and vice versa.\nFor the telephone network, modems are used that produce signals in the\nvoice-frequency range. The same basic techniques are used for modems that pro-\nduce signals at higher frequencies (e.g., microwave). This section introduces these\ntechniques and provides a brief discussion of the performance characteristics of the\nalternative approaches.\nWe mentioned that modulation involves operation on one or more of the\nthree characteristics of a carrier signal: amplitude, frequency, and phase. Accord-\ningly, there are three basic encoding or modulation techniques for transforming dig-\nital data into analog signals, as illustrated in Figure 5.7: amplitude shift keying\n(ASK), frequency shift keying (FSK), and phase shift keying (PSK). In all these\ncases, the resulting signal occupies a bandwidth centered on the carrier frequency.\nAmplitude Shift Keying\nIn ASK, the two binary values are represented by two different amplitudes of the car-\nrier frequency.Commonly,one of the amplitudes is zero;that is,one binary digit is rep-\nresented by the presence, at constant amplitude, of the carrier, the other by the\nabsence of the carrier (Figure 5.7a).The resulting transmitted signal for one bit time is\ns1t2 = e A cos12pfct2\nHDB3 Substitution Rules\nNumber of Bipolar Pulses (ones) since Last Substitution\nPolarity of Preceding Pulse\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nwhere the carrier signal is \nASK is susceptible to sudden gain changes\nand is a rather inefficient modulation technique. On voice-grade lines, it is typically\nused only up to 1200 bps.\nThe ASK technique is used to transmit digital data over optical fiber. For LED\n(light-emitting diode) transmitters, Equation (5.2) is valid. That is, one signal ele-\nment is represented by a light pulse while the other signal element is represented by\nthe absence of light. Laser transmitters normally have a fixed “bias” current that\ncauses the device to emit a low light level. This low level represents one signal ele-\nment, while a higher-amplitude lightwave represents another signal element.\nFrequency Shift Keying\nThe most common form of FSK is binary FSK (BFSK), in which the two binary val-\nues are represented by two different frequencies near the carrier frequency (Figure\n5.7b).The resulting transmitted signal for one bit time is\nare typically offset from the carrier frequency \nby equal but oppo-\nsite amounts.\ns1t2 = e A cos12pf1t2\nA cos12pf2t2\nA cos12pfct2.\nModulation of Analog Signals for Digital Data\n5.2 / DIGITAL DATA, ANALOG SIGNALS\nFigure 5.8 shows an example of the use of BFSK for full-duplex operation over\na voice-grade line. The figure is a specification for the Bell System 108 series\nmodems. Recall that a voice-grade line will pass frequencies in the approximate\nrange 300 to 3400 Hz and that full duplex means that signals are transmitted in both\ndirections at the same time. To achieve full-duplex transmission, this bandwidth is\nsplit. In one direction (transmit or receive), the frequencies used to represent 1 and\n0 are centered on 1170 Hz, with a shift of 100 Hz on either side. The effect of alter-\nnating between those two frequencies is to produce a signal whose spectrum is indi-\ncated as the shaded area on the left in Figure 5.8. Similarly, for the other direction\n(receive or transmit) the modem uses frequencies shifted 100 Hz to each side of a\ncenter frequency of 2125 Hz.This signal is indicated by the shaded area on the right\nin Figure 5.8. Note that there is little overlap and thus little interference.\nBFSK is less susceptible to error than ASK. On voice-grade lines, it is typically\nused up to 1200 bps. It is also commonly used for high-frequency (3 to 30 MHz)\nradio transmission. It can also be used at even higher frequencies on local area\nnetworks that use coaxial cable.\nA signal that is more bandwidth efficient, but also more susceptible to error, is\nmultiple FSK (MFSK), in which more than two frequencies are used. In this case\neach signaling element represents more than one bit. The transmitted MFSK signal\nfor one signal element time can be defined as follows:\nTo match the data rate of the input bit stream, each output signal element is\nheld for a period of \nseconds, where T is the bit period (data \nThus, one signal element, which is a constant-frequency tone, encodes L bits. The\nL = number of bits per signal element\nM = number of different signal elements = 2L\nfd = the difference frequency\nfc = the carrier frequency\nfi = fc + 12i - 1 - M2fd\nsi1t2 = A cos 2pfit,\nFrequency (Hz)\nSignal strength\nSpectrum of signal\ntransmitted in one\nSpectrum of signal\ntransmitted in \nopposite direction\nFull-Duplex FSK Transmission on a Voice-Grade Line\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nMFSK Frequency Use 1M = 42\nEXAMPLE 5.2 Figure 5.9 shows an example of MFSK with \nAn input bit\nstream of 20 bits is encoded 2 bits at a time, with each of the four possible 2-bit\ncombinations transmitted as a different frequency. The display in the figure shows\nthe frequency transmitted (y-axis) as a function of time (x-axis). Each column rep-\nresents a time unit \nin which a single 2-bit signal element is transmitted. The\nshaded rectangle in the column indicates the frequency transmitted during that\nPhase Shift Keying\nIn PSK, the phase of the carrier signal is shifted to represent data.\nTwo-Level PSK The simplest scheme uses two phases to represent the two\nbinary digits (Figure 5.7c) and is known as binary phase shift keying. The resulting\ntransmitted signal for one bit time is\nBecause a phase shift of 180°\nis equivalent to flipping the sine wave or\nmultiplying it by \nthe rightmost expressions in Equation (5.5) can be used. This\nA cos12pfct2\nA cos12pfct + p2 \n= e A cos12pfct2\n-A cos12pfct2\nEXAMPLE 5.1 With \nhave the following frequency assignments for each of the eight possible 3-bit\ndata combinations:\nThis scheme can support a data rate of 1/T = 2Lfd = 150 kbps.\nf7 = 375 kHz 110\nf8 = 425 kHz 111\nf5 = 275 kHz 100\nf6 = 325 kHz 101\nf3 = 175 kHz 010\nf4 = 225 kHz 011\nf1 = 75 kHz 000\nf2 = 125 kHz 001\nM = 8 1L = 3 bits2,\nfc = 250 kHz, fd = 25 kHz,\ntotal bandwidth required is \nIt can be shown that the minimum frequency sep-\naration required is \nTherefore, the modulator requires a bandwidth of\nWd = 2Mfd = M/Ts.\n2fd = 1/Ts.\n5.2 / DIGITAL DATA, ANALOG SIGNALS\nleads to a convenient formulation. If we have a bit stream, and we define d(t) as the\ndiscrete function that takes on the value of \nfor one bit time if the corresponding\nbit in the bit stream is 1 and the value of \nfor one bit time if the corresponding bit\nin the bit stream is 0, then we can define the transmitted signal as\nAn alternative form of two-level PSK is differential PSK (DPSK). Figure 5.10\nshows an example. In this scheme, a binary 0 is represented by sending a signal burst\nof the same phase as the previous signal burst sent.A binary 1 is represented by send-\ning a signal burst of opposite phase to the preceding one.This term differential refers\nto the fact that the phase shift is with reference to the previous bit transmitted rather\nthan to some constant reference signal. In differential encoding, the information to\nbe transmitted is represented in terms of the changes between successive data sym-\nbols rather than the signal elements themselves. DPSK avoids the requirement for an\naccurate local oscillator phase at the receiver that is matched with the transmitter.As\nlong as the preceding phase is received correctly, the phase reference is accurate.\nFour-Level PSK More efficient use of bandwidth can be achieved if each signal-\ning element represents more than one bit. For example, instead of a phase shift of\n180°, as allowed in BPSK, a common encoding technique, known as quadrature\nphase shift keying (QPSK), uses phase shifts separated by multiples of \nThus each signal element represents two bits rather than one.\nA cosa2pfct + p\nA cosa2pfct + 3p\nA cosa2pfct - 3p\nA cosa2pfct - p\nsd1t2 = A d1t2cos12pfct2\nFigure 5.10\nDifferential Phase-Shift Keying (DPSK)\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nFigure 5.11 shows the QPSK modulation scheme in general terms. The input\nis a stream of binary digits with a data rate of \nis the width of\neach bit. This stream is converted into two separate bit streams of R/2 bps each, by\ntaking alternate bits for the two streams. The two data streams are referred to as\nthe I (in-phase) and Q (quadrature phase) streams. In the diagram, the upper\nstream is modulated on a carrier of frequency \nby multiplying the bit stream by\nthe carrier. For convenience of modulator structure we map binary 1 to \nbinary 0 to \nThus, a binary 1 is represented by a scaled version of the carrier\nwave and a binary 0 is represented by a scaled version of the negative of the carrier\nwave, both at a constant amplitude. This same carrier wave is shifted by 90° and\nused for modulation of the lower binary stream. The two modulated signals are\nthen added together and transmitted. The transmitted signal can be expressed as\nFigure 5.12 shows an example of QPSK coding. Each of the two modulated\nstreams is a BPSK signal at half the data rate of the original bit stream. Thus, the\ncombined signals have a symbol rate that is half the input bit rate. Note that from\none symbol time to the next, a phase change of as much as 180°\nis possible.\nFigure 5.11 also shows a variation of QPSK known as offset QPSK (OQPSK),\nor orthogonal QPSK. The difference is that a delay of one bit time is introduced in\nthe Q stream, resulting in the following signal:\nBecause OQPSK differs from QPSK only by the delay in the Q stream, its\nspectral characteristics and bit error performance are the same as that of QPSK.\nI1t2 cos 2pfct -\nQ1t - Tb2 sin 2pfct\nI1t2 cos 2pfct -\nQ1t2 sin 2pfct\nserial-to-parallel\nFigure 5.11\nQPSK and OQPSK Modulators\n5.2 / DIGITAL DATA, ANALOG SIGNALS\nFrom Figure 5.12, we can observe that only one of two bits in the pair can change\nsign at any time and thus the phase change in the combined signal never exceeds 90°\nThis can be an advantage because physical limitations on phase modulators\nmake large phase shifts at high transition rates difficult to perform. OQPSK also\nprovides superior performance when the transmission channel (including transmit-\nter and receiver) has significant nonlinear components. The effect of nonlinearities\nis a spreading of the signal bandwidth, which may result in adjacent channel inter-\nference. It is easier to control this spreading if the phase changes are smaller, hence\nthe advantage of OQPSK over QPSK.\nMultilevel PSK The use of multiple levels can be extended beyond taking bits\ntwo at a time. It is possible to transmit bits three at a time using eight different phase\nangles. Further, each angle can have more than one amplitude. For example, a stan-\ndard 9600 bps modem uses 12 phase angles, four of which have two amplitude val-\nues, for a total of 16 different signal elements.\nThis latter example points out very well the difference between the data rate R\n(in bps) and the modulation rate D (in baud) of a signal. Let us assume that this\nscheme is being employed with digital input in which each bit is represented by a\nconstant voltage pulse, one level for binary one and one level for binary zero. The\ndata rate is \nHowever, the encoded signal contains \nbits in each sig-\nnal element using \ndifferent combinations of amplitude and phase. The\nmodulation rate can be seen to be R/4, because each change of signal element com-\nmunicates four bits. Thus the line signaling speed is 2400 baud, but the data rate is\n3P/4 \u00033P/4\u00033P/4\nInput signal\noutput signal\noutput signal\nFigure 5.12\nExample of QPSK and OQPSK Waveforms\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\n9600 bps. This is the reason that higher bit rates can be achieved over voice-grade\nlines by employing more complex modulation schemes.\nPerformance\nIn looking at the performance of various digital-to-analog modulation schemes, the\nfirst parameter of interest is the bandwidth of the modulated signal.This depends on\na variety of factors, including the definition of bandwidth used and the filtering tech-\nnique used to create the bandpass signal. We will use some straightforward results\nfrom [COUC01].\nThe transmission bandwidth \nfor ASK is of the form\nwhere R is the bit rate and r is related to the technique by which the signal is filtered\nto establish a bandwidth for transmission; typically \nThus the bandwidth\nis directly related to the bit rate. The preceding formula is also valid for PSK and,\nunder certain assumptions, FSK.\nWith multilevel PSK (MPSK), significant improvements in bandwidth can be\nachieved. In general,\nwhere L is the number of bits encoded per signal element and M is the number of\ndifferent signal elements.\nFor multilevel FSK (MFSK), we have\nTable 5.5 shows the ratio of data rate, R, to transmission bandwidth for vari-\nous schemes.This ratio is also referred to as the bandwidth efficiency.As the name\nsuggests, this parameter measures the efficiency with which bandwidth can be\nused to transmit data. The advantage of multilevel signaling methods now\nbecomes clear.\nOf course, the preceding discussion refers to the spectrum of the input signal\nto a communications line. Nothing has yet been said of performance in the presence\nof noise. Figure 5.4 summarizes some results based on reasonable assumptions\nconcerning the transmission system [COUC01]. Here bit error rate is plotted as a\nfunction of the ratio \ndefined in Chapter 3. Of course, as that ratio increases,\nthe bit error rate drops. Further, DPSK and BPSK are about 3 dB superior to ASK\nFigure 5.13 shows the same information for various levels of M for MFSK and\nMPSK. There is an important difference. For MFSK, the error probability for a\ngiven value \nof decreases as M increases, while the opposite is true for MPSK.\nOn the other hand, comparing Equations (5.10) and (5.11), the bandwidth efficiency\nof MFSK decreases as M increases, while the opposite is true of MPSK.Thus, in both\nBT = a 1 + r\nbR = a 1 + r\nBT = 11 + r2R\n5.2 / DIGITAL DATA, ANALOG SIGNALS\n(b) Multilevel PSK (MPSK)\n(Eb/N0) (dB)\n9 10 11 12 13 14 15\nProbability of bit error (BER)\nProbability of bit error (BER)\n(Eb/N0) (dB)\n(a) Multilevel FSK (MFSK)\n9 10 11 12 13 14 15\nFigure 5.13\nTheoretical Bit Error Rate for Multilevel FSK and PSK\nBandwidth Efficiency \nfor Various Digital-to-Analog Encoding Schemes\nMultilevel FSK\nMultilevel PSK\nM = 32, L = 5\nM = 16, L = 4\nM = 8, L = 3\nM = 4, L = 2\nM = 32, L = 5\nM = 16, L = 4\nM = 8, L = 3\nM = 4, L = 2\ncases, there is a tradeoff between bandwidth efficiency and error performance: An\nincrease in bandwidth efficiency results in an increase in error probability. The fact\nthat these tradeoffs move in opposite directions with respect to the number of levels\nM for MFSK and MPSK can be derived from the underlying equations.A discussion\nof the reasons for this difference is beyond the scope of this book. See [SKLA01] for\na full treatment.\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nEXAMPLE 5.3 What is the bandwidth efficiency for FSK, ASK, PSK, and\nQPSK for a bit error rate of \non a channel with an SNR of 12 dB?\nUsing Equation (3.2), we have\nFor FSK and ASK, from Figure 5.4,\nFor PSK, from Figure 5.4,\nThe result for QPSK must take into account that the baud rate \n= 12 dB - a R\nAs the preceding example shows, ASK and FSK exhibit the same bandwidth\nefficiency, PSK is better, and even greater improvement can be achieved with multi-\nlevel signaling.\nIt is worthwhile to compare these bandwidth requirements with those for dig-\nital signaling.A good approximation is\nwhere D is the modulation rate. For NRZ,\nand we have\nThus digital signaling is in the same ballpark, in terms of bandwidth efficiency, as\nASK, FSK, and PSK.A significant advantage for analog signaling is seen with multi-\nlevel techniques.\nBT = 0.511 + r2D\n5.2 / DIGITAL DATA, ANALOG SIGNALS\nQuadrature Amplitude Modulation\nQuadrature amplitude modulation (QAM) is a popular analog signaling technique\nthat is used in the asymmetric digital subscriber line (ADSL), described in Chapter 8,\nand in some wireless standards.This modulation technique is a combination of ASK\nand PSK. QAM can also be considered a logical extension of QPSK. QAM takes\nadvantage of the fact that it is possible to send two different signals simultaneously\non the same carrier frequency, by using two copies of the carrier frequency, one\nshifted by 90° with respect to the other. For QAM, each carrier is ASK modu-\nlated. The two independent signals are simultaneously transmitted over the same\nmedium.At the receiver, the two signals are demodulated and the results combined\nto produce the original binary input.\nFigure 5.14 shows the QAM modulation scheme in general terms.The input is a\nstream of binary digits arriving at a rate of R bps. This stream is converted into two\nseparate bit streams of R/2 bps each, by taking alternate bits for the two streams. In\nthe diagram, the upper stream is ASK modulated on a carrier of frequency \ntiplying the bit stream by the carrier.Thus, a binary zero is represented by the absence\nof the carrier wave and a binary one is represented by the presence of the carrier wave\nat a constant amplitude. This same carrier wave is shifted by 90° and used for ASK\nmodulation of the lower binary stream. The two modulated signals are then added\ntogether and transmitted.The transmitted signal can be expressed as follows:\nIf two-level ASK is used,then each of the two streams can be in one of two states\nand the combined stream can be in one of \nstates.This is essentially QPSK.\nIf four-level ASK is used (i.e., four different amplitude levels), then the combined\nstream can be in one of \nstates. Systems using 64 and even 256 states have\nbeen implemented. The greater the number of states, the higher the data rate that is\npossible within a given bandwidth. Of course, as discussed previously, the greater the\nnumber of states, the higher the potential error rate due to noise and attenuation.\ns1t2 = d11t2cos 2pfct + d21t2sin 2pfct\nserial-to-parallel\nFigure 5.14\nQAM Modulator\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nANALOG DATA, DIGITAL SIGNALS\nIn this section we examine the process of transforming analog data into digital sig-\nnals. Strictly speaking, it might be more correct to refer to this as a process of con-\nverting analog data into digital data; this process is known as digitization. Once\nanalog data have been converted into digital data, a number of things can happen.\nThe three most common are as follows:\n1. The digital data can be transmitted using NRZ-L. In this case, we have in fact\ngone directly from analog data to a digital signal.\n2. The digital data can be encoded as a digital signal using a code other than \nNRZ-L.Thus an extra step is required.\n3. The digital data can be converted into an analog signal, using one of the mod-\nulation techniques discussed in Section 5.2.\nThis last, seemingly curious, procedure is illustrated in Figure 5.15, which\nshows voice data that are digitized and then converted to an analog ASK signal.\nThis allows digital transmission in the sense defined in Chapter 3. The voice data,\nbecause they have been digitized, can be treated as digital data, even though trans-\nmission requirements (e.g., use of microwave) dictate that an analog signal be\nThe device used for converting analog data into digital form for transmission,\nand subsequently recovering the original analog data from the digital, is known as a\ncodec (coder-decoder). In this section we examine the two principal techniques used\nin codecs, pulse code modulation and delta modulation. The section closes with a\ndiscussion of comparative performance.\nPulse Code Modulation\nPulse code modulation (PCM) is based on the sampling theorem:\nAnalog data\nDigital data\nAnalog signal\nFigure 5.15\nDigitizing Analog Data\nSAMPLING THEOREM: If a signal f(t) is sampled at regular intervals of time\nand at a rate higher than twice the highest signal frequency, then the samples con-\ntain all the information of the original signal. The function f(t) may be recon-\nstructed from these samples by the use of a lowpass filter.\n5.3 / ANALOG DATA, DIGITAL SIGNALS\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nnoise. The signal-to-noise ratio for quantizing noise can be expressed as\nThus each additional bit used for quantizing increases SNR by about 6 dB, which is\na factor of 4.\nTypically, the PCM scheme is refined using a technique known as nonlinear\nencoding, which means, in effect, that the quantization levels are not equally\nspaced. The problem with equal spacing is that the mean absolute error for each\nsample is the same, regardless of signal level. Consequently, lower amplitude values\nare relatively more distorted. By using a greater number of quantizing steps for sig-\nnals of low amplitude, and a smaller number of quantizing steps for signals of large\namplitude, a marked reduction in overall signal distortion is achieved (e.g., see\nFigure 5.18).\nThe same effect can be achieved by using uniform quantizing but compand-\ning (compressing-expanding) the input analog signal. Companding is a process\nthat compresses the intensity range of a signal by imparting more gain to weak sig-\nnals than to strong signals on input.At output, the reverse operation is performed.\nFigure 5.19 shows typical companding functions. Note that the effect on the input\nside is to compress the sample so that the higher values are reduced with respect\nSNRdB = 20 log 2n + 1.76 dB = 6.02n + 1.76 dB\n(a) Without nonlinear encoding\nStrong signal\nWeak signal\n(b) With nonlinear encoding\nFigure 5.18\nEffect of Nonlinear Coding\nContinuous-time,\ncontinuous-amplitude\n(analog) input signal\nDiscrete-time\ncontinuous-amplitude\nsignal (PAM pulses)\nDiscrete-time\ndiscrete-amplitude\nsignal (PCM pulses)\nDigital bit\nstream output\nFigure 5.17\nPCM Block Diagram\n5.3 / ANALOG DATA, DIGITAL SIGNALS\nOutput signal magnitude\nInput signal magnitude\nNo companding\nFigure 5.19\nTypical Companding Functions\nto the lower values.Thus, with a fixed number of quantizing levels, more levels are\navailable for lower-level signals. On the output side, the compander expands the\nsamples so the compressed values are restored to their original values.\nNonlinear encoding can significantly improve the PCM SNR ratio. For voice\nsignals, improvements of 24 to 30 dB have been achieved.\nDelta Modulation (DM)\nA variety of techniques have been used to improve the performance of PCM or to\nreduce its complexity. One of the most popular alternatives to PCM is delta modu-\nlation (DM).\nWith delta modulation, an analog input is approximated by a staircase function\nthat moves up or down by one quantization level \nat each sampling interval \nAn example is shown in Figure 5.20, where the staircase function is overlaid on the\noriginal analog waveform. The important characteristic of this staircase function is\nthat its behavior is binary: At each sampling time, the function moves up or down a\nconstant amount \nThus, the output of the delta modulation process can be repre-\nsented as a single binary digit for each sample. In essence, a bit stream is produced by\napproximating the derivative of an analog signal rather than its amplitude:A 1 is gen-\nerated if the staircase function is to go up during the next interval; a 0 is generated\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nThe transition (up or down) that occurs at each sampling interval is chosen so\nthat the staircase function tracks the original analog waveform as closely as possible.\nFigure 5.21 illustrates the logic of the process, which is essentially a feedback mech-\nanism. For transmission, the following occurs: At each sampling time, the analog\ninput is compared to the most recent value of the approximating staircase function.\nIf the value of the sampled waveform exceeds that of the staircase function, a 1 is\ngenerated; otherwise, a 0 is generated. Thus, the staircase is always changed in the\ndirection of the input signal. The output of the DM process is therefore a binary\nsequence that can be used at the receiver to reconstruct the staircase function. The\nstaircase function can then be smoothed by some type of integration process or by\npassing it through a lowpass filter to produce an analog approximation of the analog\ninput signal.\nThere are two important parameters in a DM scheme: the size of the step\nassigned to each binary digit,\nand the sampling rate. As Figure 5.20 illustrates,\nmust be chosen to produce a balance between two types of errors or noise.When the\nanalog waveform is changing very slowly, there will be quantizing noise. This noise\nincreases as is increased. On the other hand, when the analog waveform is chang-\ning more rapidly than the staircase can follow, there is slope overload noise. This\nnoise increases as is decreased.\nIt should be clear that the accuracy of the scheme can be improved by\nincreasing the sampling rate. However, this increases the data rate of the output\nFigure 5.20\nExample of Delta Modulation\n5.3 / ANALOG DATA, DIGITAL SIGNALS\nThe principal advantage of DM over PCM is the simplicity of its implementa-\ntion. In general, PCM exhibits better SNR characteristics at the same data rate.\nPerformance\nGood voice reproduction via PCM can be achieved with 128 quantization levels, or\n7-bit coding \nA voice signal, conservatively, occupies a bandwidth of \n4 kHz.Thus, according to the sampling theorem, samples should be taken at a rate of\n8000 samples per second. This implies a data rate of \nPCM-encoded digital data.\nConsider what this means from the point of view of bandwidth requirement.\nAn analog voice signal occupies 4 kHz. Using PCM this 4-kHz analog signal can\nbe converted into a 56-kbps digital signal. But using the Nyquist criterion from\nChapter 3, this digital signal could require on the order of 28 kHz of bandwidth.\nEven more severe differences are seen with higher bandwidth signals. For example,\na common PCM scheme for color television uses 10-bit codes, which works out to\n92 Mbps for a 4.6-MHz bandwidth signal. In spite of these numbers, digital tech-\nniques continue to grow in popularity for transmitting analog data. The principal\nreasons for this are as follows:\n8000 * 7 = 56 kbps\n127 = 1282.\nReconstructed\n(a) Transmission\nReconstructed\n(b) Reception\nFigure 5.21\nDelta Modulation\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\n• Because repeaters are used instead of amplifiers, there is no cumulative noise.\n• As we shall see, time division multiplexing (TDM) is used for digital signals\ninstead of the frequency division multiplexing (FDM) used for analog signals.\nWith TDM, there is no intermodulation noise, whereas we have seen that this\nis a concern for FDM.\n• The conversion to digital signaling allows the use of the more efficient digital\nswitching techniques.\nFurthermore, techniques have been developed to provide more efficient\ncodes. In the case of voice, a reasonable goal appears to be in the neighborhood of \n4 kbps. With video, advantage can be taken of the fact that from frame to frame,\nmost picture elements will not change. Interframe coding techniques should allow\nthe video requirement to be reduced to about 15 Mbps, and for slowly changing\nscenes, such as found in a video teleconference, down to 64 kbps or less.\nAs a final point, we mention that in many instances, the use of a telecommuni-\ncations system will result in both digital-to-analog and analog-to-digital processing.\nThe overwhelming majority of local terminations into the telecommunications \nnetwork is analog, and the network itself uses a mixture of analog and digital\ntechniques. Thus digital data at a user’s terminal may be converted to analog by a\nmodem, subsequently digitized by a codec, and perhaps suffer repeated conversions\nbefore reaching its destination.\nThus, telecommunication facilities handle analog signals that represent both\nvoice and digital data. The characteristics of the waveforms are quite different.\nWhereas voice signals tend to be skewed to the lower portion of the bandwidth \n(Figure 3.9), analog encoding of digital signals has a more uniform spectral content\nover the bandwidth and therefore contains more high-frequency components.\nStudies have shown that, because of the presence of these higher frequencies,\nPCM-related techniques are preferable to DM-related techniques for digitizing\nanalog signals that represent digital data.\nANALOG DATA,ANALOG SIGNALS\nModulation has been defined as the process of combining an input signal m(t) and a\ncarrier at frequency \nto produce a signal s(t) whose bandwidth is (usually) cen-\nFor digital data, the motivation for modulation should be clear: When\nonly analog transmission facilities are available, modulation is required to convert\nthe digital data to analog form. The motivation when the data are already analog is\nless clear. After all, voice signals are transmitted over telephone lines at their origi-\nnal spectrum (referred to as baseband transmission). There are two principal rea-\nsons for analog modulation of analog signals:\n• A higher frequency may be needed for effective transmission. For unguided\ntransmission, it is virtually impossible to transmit baseband signals; the\nrequired antennas would be many kilometers in diameter.\n• Modulation permits frequency division multiplexing, an important technique\nexplored in Chapter 8.\n5.4 / ANALOG DATA, ANALOG SIGNALS\nIn this section we look at the principal techniques for modulation using analog\ndata:amplitude modulation (AM),frequency modulation (FM),and phase modulation\n(PM).As before, the three basic characteristics of a signal are used for modulation.\nAmplitude Modulation\nAmplitude modulation (AM) is the simplest form of modulation and is depicted in\nFigure 5.22. Mathematically, the process can be expressed as\nis the carrier and x(t) is the input signal (carrying data), both nor-\nmalized to unity amplitude. The parameter \nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\n(a) Spectrum of modulating signal\n(b) Spectrum of AM signal with carrier at fc\nDiscrete carrier\nFigure 5.23\nSpectrum of an AM Signal\n(a) Sinusoidal modulating wave\n(b) Resulting AM signal\nFigure 5.22\nAmplitude Modulation\n5.4 / ANALOG DATA, ANALOG SIGNALS\nis the total transmitted power in s(t) and \nis the transmitted power in the\ncarrier.We would like \nas large as possible so that most of the signal power is used\nto carry information. However,\nmust remain below 1.\nIt should be clear that s(t) contains unnecessary components, because each of\nthe sidebands contains the complete spectrum of m(t). A popular variant of AM,\nknown as single sideband (SSB), takes advantage of this fact by sending only one of\nthe sidebands, eliminating the other sideband and the carrier. The principal advan-\ntages of this approach are as follows:\n• Only half the bandwidth is required, that is,\nwhere B is the bandwidth\nof the original signal. For DSBTC,\n• Less power is required because no power is used to transmit the carrier or the\nother sideband. Another variant is double sideband suppressed carrier\n(DSBSC), which filters out the carrier frequency and sends both sidebands.\nThis saves some power but uses as much bandwidth as DSBTC.\nThe disadvantage of suppressing the carrier is that the carrier can be used for\nsynchronization purposes. For example, suppose that the original analog signal is an\nASK waveform encoding digital data.The receiver needs to know the starting point\nof each bit time to interpret the data correctly. A constant carrier provides a clock-\ning mechanism by which to time the arrival of bits.A compromise approach is vesti-\ngial sideband (VSB), which uses one sideband and a reduced-power carrier.\nAngle Modulation\nFrequency modulation (FM) and phase modulation (PM) are special cases of angle\nmodulation.The modulated signal is expressed as\nFor phase modulation, the phase is proportional to the modulating signal:\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nand the instantaneous frequency deviation from the carrier frequency is \nwhich in FM is proportional to m(t).\nFigure 5.24 illustrates amplitude, phase, and frequency modulation by a sine\nwave.The shapes of the FM and PM signals are very similar. Indeed, it is impossible\nto tell them apart without knowledge of the modulation function.\nSeveral observations about the FM process are in order. The peak deviation\ncan be seen to be\nis the maximum value of m(t).Thus an increase in the magnitude of m(t)\nwill increase \nwhich, intuitively, should increase the transmitted bandwidth \nHowever, as should be apparent from Figure 5.24, this will not increase the average\npower level of the FM signal, which is \nThis is distinctly different from AM,\nwhere the level of modulation affects the power in the AM signal but does not affect\nits bandwidth.\nEXAMPLE 5.5 Derive an expression for s(t) if \nis the phase-modulating sig-\nAssume that \nThis can be seen directly to be\nThe instantaneous phase deviation from the carrier signal is \nphase angle of the signal varies from its unmodulated value in a simple sinusoidal\nfashion, with the peak phase deviation equal to \nThe preceding expression can be expanded using Bessel’s trigonometric \nidentities:\nis the nth-order Bessel function of the first kind. Using the property\nthis can be rewritten as\nThe resulting signal has a component at the original carrier frequency plus a set\nof sidebands displaced from \nby all possible multiples of \nhigher-order terms fall off rapidly.\n+ cosa2p1fc - nfm2t +\ns1t2 = J01np2 cos 2pfct+ a\nJn1np2ccosa2p1fc + nfm2t + np\nJ-n1x2 = 1-12nJn1x2\nn= - qJn1np2 cosa2pfct + 2pnfmt + np\nnp cos 2pfmt.\ns1t2 = cos[2pfct + np cos 2pfmt]\nnp cos 2pfmt.\n5.4 / ANALOG DATA, ANALOG SIGNALS\nModulating sine-wave signal\nAmplitude-modulated (DSBTC) wave\nPhase-modulated wave\nFrequency-modulated wave\nFigure 5.24\nAmplitude, Phase, and Frequency Modulation of a Sine-Wave Carrier\nby a Sine-Wave Signal\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nEXAMPLE 5.6 Derive an expression for s(t) if \nis the frequency modulating\nThe form of \nwas chosen for convenience.We have\nThe instantaneous frequency deviation from the carrier signal is\nThe frequency of the signal varies from its unmodulated value in a\nsimple sinusoidal fashion, with the peak frequency deviation equal to \nThe equation for the FM signal has the identical form as for the PM signal,\nsubstituted for \nThus the Bessel expansion is the same.\n-nf sin 2pfmt.\n= cosc2pfct + ¢F\ncos 2pfmt d\ns1t2 = cosc2pfct +\ncos 2pfmt d\nnf sin 2pfmt dt =\n-nf sin 2pfmt.\nAs with AM, both FM and PM result in a signal whose bandwidth is cen-\nHowever, we can now see that the magnitude of that bandwidth is\nvery different. Amplitude modulation is a linear process and produces fre-\nquencies that are the sum and difference of the carrier signal and the compo-\nnents of the modulating signal. Hence, for AM,\nHowever,angle modulation includes a term of the form \nwhich is non-\nlinear and will produce a wide range of frequencies. In essence, for a modulating\nsinusoid of frequency \ns(t) will contain components at \nand so on. In the most general case, infinite bandwidth is required to transmit an\nFM or PM signal. As a practical matter, a very good rule of thumb, known as\nCarson’s rule [COUC01], is\nWe can rewrite the formula for FM as\nThus both FM and PM require greater bandwidth than AM.\nBT = 2¢F + 2B\nBT = 21b + 12B\nfc + fm, fc + 2fm,\n5.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nIt is difficult, for some reason, to find solid treatments of digital-to-digital encoding schemes.\nUseful accounts include [SKLA01] and [BERG96].\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nReview Questions\nList and briefly define important factors that can be used in evaluating or comparing\nthe various digital-to-digital encoding techniques.\nWhat is differential encoding?\nExplain the difference between NRZ-L and NRZI.\nDescribe two multilevel binary digital-to-digital encoding techniques.\nDefine biphase encoding and describe two biphase encoding techniques.\nExplain the function of scrambling in the context of digital-to-digital encoding\ntechniques.\nWhat function does a modem perform?\nHow are binary values represented in amplitude shift keying, and what is the limita-\ntion of this approach?\nWhat is the difference between QPSK and offset QPSK?\nWhat is QAM?\nWhat does the sampling theorem tell us concerning the rate of sampling required for\nan analog signal?\nWhat are the differences among angle modulation, PM, and FM?\nWhich of the signals of Table 5.2 use differential encoding?\nDevelop algorithms for generating each of the codes of Table 5.2 from NRZ-L.\nA modified NRZ code known as enhanced-NRZ (E-NRZ) is sometimes used for\nhigh-density magnetic tape recording. E-NRZ encoding entails separating the\nNRZ-L data stream into 7-bit words; inverting bits 2, 3, 6, and 7; and adding one\nparity bit to each word. The parity bit is chosen to make the total number of 1s in\nthe 8-bit word an odd count.What are the advantages of E-NRZ over NRZ-L? Any\ndisadvantages?\nDevelop a state diagram (finite state machine) representation of pseudoternary coding.\nConsider the following signal encoding technique. Binary data are presented as input,\nTwo levels of processing occur. First, a new set of binary num-\nbers are produced:\nThese are then encoded as\nOn reception, the original data are recovered by\nVerify that the received values of \nequal the transmitted values of \nb. What sort of encoding is this?\nFor the bit stream 01001110, sketch the waveforms for each of the codes of Table 5.2.\nAssume that the signal level for the preceding bit for NRZI was high; the most recent\npreceding 1 bit (AMI) has a negative voltage; and the most recent preceding 0 bit\n(pseudoternary) has a negative voltage.\nThe waveform of Figure 5.25 belongs to a Manchester encoded binary data stream.\nDetermine the beginning and end of bit periods (i.e., extract clock information) and\ngive the data sequence.\nam = cm mod 2\ncm = bm - bm-1\nbm = 1am + bm-12mod 2\nm = 1, 2, 3, Á\n5.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nConsider a stream of binary data consisting of a long sequence of 1s followed by a\nzero followed by a long string of 1s, with the same assumptions as Problem 5.6. Draw\nthe waveform for this sequence using\nb. Bipolar-AMI\nPseudoternary\nThe bipolar-AMI waveform representing the binary sequence 0100101011 is trans-\nmitted over a noisy channel. The received waveform is shown in Figure 5.26; it\ncontains a single error. Locate the position of this error and explain your answer.\nOne positive side effect of bipolar encoding is that a bipolar violation (two consecutive\npulses or two consecutive \npulses separated by any number of zeros) indicates to\nthe receiver that an error has occurred in transmission. Unfortunately, upon the\nreceipt of such a violation, the receiver does not know which bit is in error (only that\nan error has occurred). For the received bipolar sequence\nwhich has one bipolar violation, construct two scenarios (each of which involves a dif-\nferent transmitted bit stream with one transmitted bit being converted via an error)\nthat will produce this same received bit pattern.\nGiven the bit pattern 01100, encode this data using ASK, BFSK, and BPSK.\nA sine wave is to be used for two different signaling schemes: (a) PSK; (b) QPSK.The\nduration of a signal element is \nIf the received signal is of the following form:\nand if the measured noise power at the receiver is \ndetermine the\n(in dB) for each case.\nDerive an expression for baud rate D as a function of bit rate R for QPSK using the\ndigital encoding techniques of Table 5.2.\n2.5 * 10-8 watts,\ns1t2 = 0.005 sin12p 106t + u2 volts\n+ - 0 + - 0 - +\nFigure 5.25\nA Manchester Stream\nFigure 5.26\nA Received Bipolar-AMI Waveform\nCHAPTER 5 / SIGNAL ENCODING TECHNIQUES\nWhat SNR ratio is required to achieve a bandwidth efficiency of 1.0 for ASK, FSK,\nPSK, and QPSK? Assume that the required bit error rate is \nAn NRZ-L signal is passed through a filter with \nand then modulated onto a\ncarrier.The data rate is 2400 bps. Evaluate the bandwidth for ASK and FSK. For FSK\nassume that the two frequencies used are 50 kHz and 55 kHz.\nAssume that a telephone line channel is equalized to allow bandpass data transmis-\nsion over a frequency range of 600 to 3000 Hz. The available bandwidth is 2400 Hz.\nevaluate the required bandwidth for 2400 bps QPSK and 4800-bps, eight-\nlevel multilevel signaling. Is the bandwidth adequate?\nFigure 5.27 shows the QAM demodulator corresponding to the QAM modulator of\nFigure 5.14. Show that this arrangement does recover the two signals \nwhich can be combined to recover the original input.\nWhy should PCM be preferable to DM for encoding analog signals that represent\ndigital data?\nAre the modem and the codec functional inverses (i.e., could an inverted modem\nfunction as a codec, or vice versa)?\nA signal is quantized using 10-bit PCM. Find the signal-to-quantization noise ratio.\nConsider an audio signal with spectral components in the range 300 to 3000 Hz.\nAssume that a sampling rate of 7000 samples per second will be used to generate a\nPCM signal.\nwhat is the number of uniform quantization levels needed?\nb. What data rate is required?\nFind the step size \nrequired to prevent slope overload noise as a function of the fre-\nquency of the highest-frequency component of the signal. Assume that all compo-\nnents have amplitude A.\nA PCM encoder accepts a signal with a full-scale voltage of 10 V and generates 8-bit\ncodes using uniform quantization. The maximum normalized quantized voltage is\nDetermine (a) normalized step size, (b) actual step size in volts, (c) actual\nmaximum quantized level in volts, (d) normalized resolution, (e) actual resolution,\nand (f) percentage resolution.\nThe analog waveform shown in Figure 5.28 is to be delta modulated. The sampling\nperiod and the step size are indicated by the grid on the figure.The first DM output and\nthe staircase function for this period are also shown. Show the rest of the staircase func-\ntion and give the DM output. Indicate regions where slope overload distortion exists.\nSNR = 30 dB,\ny2(t) \u0001 d2(t)/2\ny1(t) \u0001 d1(t)/2\nFigure 5.27\nQAM Demodulator\n5.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nConsider the angle-modulated signal\nFind the maximum phase deviation and the maximum frequency deviation.\nConsider the angle-modulated signal\nExpress s(t) as a PM signal with \nb. Express s(t) as an FM signal with \nbe message signals and let \nbe the corresponding\nmodulated signals using a carrier frequency of \nShow that if simple AM modulation is used, then \nproduces a mod-\nulated signal equal that is a linear combination of \nThis is why AM\nis sometimes referred to as linear modulation.\nb. Show that if simple PM modulation is used, then \nproduces a mod-\nulated signal that is not a linear combination of \nThis is why angle\nmodulation is sometimes referred to as nonlinear modulation.\nm11t2 + m21t2\nm11t2 + m21t2\ns1t2 = 10 cos[2p11062t + 0.1 sin11032pt]\ns1t2 = 10 cos[11082pt + 5 sin 2p11032t]\nFigure 5.28\nDelta Modulation Example\nAsynchronous and Synchronous Transmission\nTypes of Errors\nError Detection\nError Correction\nLine Configurations\nRecommended Reading\nKey Terms, Review Questions, and Problems\nA conversation forms a two-way communication link; there is a measure of symme-\ntry between the two parties, and messages pass to and fro. There is a continual stim-\nulus-response, cyclic action; remarks call up other remarks, and the behavior of the\ntwo individuals becomes concerted, co-operative, and directed toward some goal.\nThis is true communication.\n—On Human Communication, Colin Cherry\nThe transmission of a stream of bits from one device to another\nacross a transmission link involves a great deal of cooperation and\nagreement between the two sides. One of the most fundamental\nrequirements is synchronization. The receiver must know the rate at\nwhich bits are being received so that it can sample the line at appro-\npriate intervals to determine the value of each received bit.Two tech-\nniques are in common use for this purpose. In asynchronous\ntransmission, each character of data is treated independently. Each\ncharacter begins with a start bit that alerts the receiver that a charac-\nter is arriving.The receiver samples each bit in the character and then\nlooks for the beginning of the next character. This technique would\nnot work well for long blocks of data because the receiver’s clock\nmight eventually drift out of synchronization with the transmitter’s\nclock. However, sending data in large blocks is more efficient than\nsending data one character at a time. For large blocks, synchronous\ntransmission is used. Each block of data is formatted as a frame that\nincludes a starting and an ending flag. Some form of synchronization,\nsuch as the use of Manchester encoding, is employed.\nError detection is performed by calculating an error-detecting code\nthat is a function of the bits being transmitted.The code is appended\nto the transmitted bits.The receiver calculates the code based on the\nincoming bits and compares it to the incoming code to check for\nError correction operates in a fashion similar to error detection but is\ncapable of correcting certain errors in a transmitted bit stream.\nThe preceding three chapters have been concerned primarily with the\nattributes of data transmission, such as the characteristics of data signals\nand transmission media, the encoding of signals, and transmission perform-\nance. In this chapter, we shift our emphasis from data transmission to data\ncommunications.\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nFor two devices linked by a transmission medium to exchange data, a\nhigh degree of cooperation is required. Typically, data are transmitted one bit\nat a time over the medium. The timing (rate, duration, spacing) of these bits\nmust be the same for transmitter and receiver. Two common techniques for\ncontrolling this timing—asynchronous and synchronous—are explored in\nSection 6.1. Next, we look at the problem of bit errors. As we have seen, data\ntransmission is not an error-free process, and some means of accounting for\nthese errors is needed. After a brief discussion of the distinction between sin-\ngle-bit errors and burst errors, the chapter turns to two approaches to dealing\nwith errors: error detection and error correction.\nNext, the chapter provides an overview of the types of line configurations\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\n(b) 8-bit asynchronous character stream\n(a) Character format\n5 to 8 data bits\n1 to 2 bit times\nOdd or even\nparity or unused\nRemain idle or\nnext start bit\nUnpredictable time interval\nbetween characters\n(c) Effect of timing error\nTransmitter timing (\u0006s)\nReceiver timing (\u0006s)\nAsynchronous Transmission\nEXAMPLE 6.1\nFigure 6.1c shows the effects of a timing error of sufficient\nmagnitude to cause an error in reception. In this example we assume a data rate\nof 10,000 bits per second (10 kbps); therefore, each bit is of 0.1 millisecond (ms),\nduration. Assume that the receiver is fast by 6%, or \nper bit time.\nThus, the receiver samples the incoming character every \n(based on the\ntransmitter’s clock).As can be seen, the last sample is erroneous.\nAn error such as just described actually results in two errors. First, the last\nsampled bit is incorrectly received. Second, the bit count may now be out of align-\nment. If bit 7 is a 1 and bit 8 is a 0, bit 8 could be mistaken for a start bit.This condi-\ntion is termed a framing error, as the character plus start bit and stop element are\nsometimes referred to as a frame.A framing error can also occur if some noise con-\ndition causes the false appearance of a start bit during the idle state.\nAsynchronous transmission is simple and cheap but requires an overhead of\ntwo to three bits per character. For example, for an 8-bit character with no parity\nbit, using a 1-bit-long stop element, two out of every ten bits convey no informa-\ntion but are there merely for synchronization; thus the overhead is 20%. Of\n6.1 / ASYNCHRONOUS AND SYNCHRONOUS TRANSMISSION\ncourse, the percentage overhead could be reduced by sending larger blocks of bits\nbetween the start bit and stop element. However, as Figure 6.1c indicates, the\nlarger the block of bits, the greater the cumulative timing error.To achieve greater\nefficiency, a different form of synchronization, known as synchronous transmis-\nsion, is used.\nSynchronous Transmission\nWith synchronous transmission, a block of bits is transmitted in a steady stream\nwithout start and stop codes. The block may be many bits in length. To prevent tim-\ning drift between transmitter and receiver, their clocks must somehow be synchro-\nnized. One possibility is to provide a separate clock line between transmitter and\nreceiver. One side (transmitter or receiver) pulses the line regularly with one short\npulse per bit time.The other side uses these regular pulses as a clock.This technique\nworks well over short distances, but over longer distances the clock pulses are sub-\nject to the same impairments as the data signal, and timing errors can occur. The\nother alternative is to embed the clocking information in the data signal. For digital\nsignals, this can be accomplished with Manchester or differential Manchester encod-\ning. For analog signals, a number of techniques can be used; for example, the carrier\nfrequency itself can be used to synchronize the receiver based on the phase of the\nWith synchronous transmission, there is another level of synchronization\nrequired, to allow the receiver to determine the beginning and end of a block of data.\nTo achieve this, each block begins with a preamble bit pattern and generally ends\nwith a postamble bit pattern. In addition, other bits are added to the block that con-\nvey control information used in the data link control procedures discussed in\nChapter 7. The data plus preamble, postamble, and control information are called a\nframe. The exact format of the frame depends on which data link control procedure\nis being used.\nFigure 6.2 shows, in general terms, a typical frame format for synchronous\ntransmission.Typically, the frame starts with a preamble called a flag, which is 8 bits\nlong. The same flag is used as a postamble. The receiver looks for the occurrence of\nthe flag pattern to signal the start of a frame. This is followed by some number of\ncontrol fields (containing data link control protocol information), then a data field\n(variable length for most protocols), more control fields, and finally the flag is\nFor sizable blocks of data, synchronous transmission is far more efficient than\nasynchronous. Asynchronous transmission requires 20% or more overhead. The\ncontrol information, preamble, and postamble in synchronous transmission are typ-\nically less than 100 bits.\nSynchronous Frame Format\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nEXAMPLE 6.2\nOne of the more common schemes,HDLC (described in Chapter\n7), contains 48 bits of control, preamble, and postamble. Thus, for a 1000-character\nblock of data, each frame consists of 48 bits of overhead and \nof data, for a percentage overhead of only 48/8048 * 100% = 0.6%.\n1000 * 8 = 8,000\nIn digital transmission systems, an error occurs when a bit is altered between trans-\nmission and reception; that is, a binary 1 is transmitted and a binary 0 is received, or\na binary 0 is transmitted and a binary 1 is received. Two general types of errors can\noccur: single-bit errors and burst errors.A single-bit error is an isolated error condi-\ntion that alters one bit but does not affect nearby bits.A burst error of length B is a\ncontiguous sequence of B bits in which the first and last bits and any number of\nintermediate bits are received in error. More precisely, IEEE Std 100 and ITU-T\nRecommendation Q.9 both define an error burst as follows:\nEXAMPLE 6.3\nAn impulse noise event or a fading event of \noccurs. At a\ndata rate of 10 Mbps, there is a resulting error burst of 10 bits. At a data rate of\n100 Mbps, there is an error burst of 100 bits.\nError burst: A group of bits in which two successive erroneous bits are\nalways separated by less than a given number x of correct bits.The last erro-\nneous bit in the burst and the first erroneous bit in the following burst are\naccordingly separated by x correct bits or more.\nThus, in an error burst, there is a cluster of bits in which a number of errors\noccur, although not necessarily all of the bits in the cluster suffer an error.\nA single-bit error can occur in the presence of white noise, when a slight ran-\ndom deterioration of the signal-to-noise ratio is sufficient to confuse the receiver’s\ndecision of a single bit. Burst errors are more common and more difficult to deal\nwith. Burst errors can be caused by impulse noise, which was described in Chapter 3.\nAnother cause is fading in a mobile wireless environment; fading is described in\nChapter 14.\nNote that the effects of burst errors are greater at higher data rates.\nRegardless of the design of the transmission system, there will be errors, resulting\nin the change of one or more bits in a transmitted frame. In what follows, we\n6.3 / ERROR DETECTION\nassume that data are transmitted as one or more contiguous sequences of bits,\ncalled frames. We define these probabilities with respect to errors in transmitted\nProbability that a bit is received in error; also known as the bit error rate\nProbability that a frame arrives with no bit errors\nProbability that, with an error-detecting algorithm in use, a frame arrives with\none or more undetected errors\nProbability that, with an error-detecting algorithm in use, a frame arrives\nwith one or more detected bit errors but no undetected bit errors\nFirst consider the case in which no means are taken to detect errors. Then the\nprobability of detected errors \nis zero. To express the remaining probabilities,\nassume the probability that any bit is in error \nis constant and independent for\neach bit.Then we have\nwhere F is the number of bits per frame. In words, the probability that a frame\narrives with no bit errors decreases when the probability of a single bit error\nincreases, as you would expect.Also, the probability that a frame arrives with no bit\nerrors decreases with increasing frame length; the longer the frame, the more bits it\nhas and the higher the probability that one of these is in error.\nP2 = 1 - P1\nP1 = 11 - Pb2F\nEXAMPLE 6.4\nA defined objective for ISDN (integrated services digital net-\nwork) connections is that the BER on a 64-kbps channel should be less than \non at least 90% of observed 1-minute intervals. Suppose now that we have the\nrather modest user requirement that on average one frame with an undetected\nbit error should occur per day on a continuously used 64-kbps channel, and let\nus assume a frame length of 1000 bits. The number of frames that can be trans-\nmitted in a day comes out to \nwhich yields a desired frame error\nBut if we assume a value of \nand therefore \nwhich is about\nthree orders of magnitude too large to meet our requirement.\nP1 = 10.99999921000\n0.18 * 10-6.\nP2 = 1/15.529 * 1062 =\n5.529 * 106,\nThis is the kind of result that motivates the use of error-detecting techniques.All\nof these techniques operate on the following principle (Figure 6.3). For a given frame\nof bits, additional bits that constitute an error-detecting code are added by the trans-\nmitter.This code is calculated as a function of the other transmitted bits.Typically, for\na data block of k bits, the error-detecting algorithm yields an error-detecting code of\nbits, where \nThe error-detecting code, also referred to as the\ncheck bits, is appended to the data block to produce a frame of n bits, which is then\n1n - k2 6 k.\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nError Detection Process\nE \u0001 f(data)\nTransmitter\nE' \u0001 f(data')\nE, E' \u0001 error-detecting codes\n\u0001 error-detecting code function\ntransmitted. The receiver separates the incoming frame into the k bits of data and\nbits of the error-detecting code.The receiver performs the same error-detect-\ning calculation on the data bits and compares this value with the value of the incoming\nerror-detecting code. A detected error occurs if and only if there is a mismatch. Thus\nis the probability that a frame contains errors and that the error-detecting scheme\nwill detect that fact.\nis known as the residual error rate and is the probability that an\nerror will be undetected despite the use of an error-detecting scheme.\nParity Check\nThe simplest error-detecting scheme is to append a parity bit to the end of a block of\ndata.A typical example is character transmission, in which a parity bit is attached to\neach 7-bit IRA character.The value of this bit is selected so that the character has an\neven number of 1s (even parity) or an odd number of 1s (odd parity).\n2Recall from our discussion in Section 5.1 that the least significant bit of a character is transmitted first\nand that the parity bit is the most significant bit.\nEXAMPLE 6.5\nIf the transmitter is transmitting an IRA character G (1110001)\nand using odd parity, it will append a 1 and transmit 11110001.2 The receiver exam-\nines the received character and, if the total number of 1s is odd, assumes that no\nerror has occurred. If one bit (or any odd number of bits) is erroneously inverted\nduring transmission (for example, 11100001), then the receiver will detect an error.\n6.3 / ERROR DETECTION\nNote, however, that if two (or any even number) of bits are inverted due to\nerror, an undetected error occurs. Typically, even parity is used for synchronous\ntransmission and odd parity for asynchronous transmission.\nThe use of the parity bit is not foolproof, as noise impulses are often long\nenough to destroy more than one bit, particularly at high data rates.\nCyclic Redundancy Check (CRC)\nOne of the most common, and one of the most powerful, error-detecting codes is the\ncyclic redundancy check (CRC), which can be described as follows. Given a k-bit\nblock of bits, or message, the transmitter generates an \nsequence, known\nas a frame check sequence (FCS), such that the resulting frame, consisting of n bits,\nis exactly divisible by some predetermined number. The receiver then divides the\nincoming frame by that number and, if there is no remainder, assumes there was no\nTo clarify this, we present the procedure in three equivalent ways: modulo 2\narithmetic, polynomials, and digital logic.\nModulo 2 Arithmetic Modulo 2 arithmetic uses binary addition with no carries,\nwhich is just the exclusive-OR (XOR) operation. Binary subtraction with no carries\nis also interpreted as the XOR operation: For example,\nWe would like T/P to have no remainder. It should be clear that\nThat is, by multiplying D by \nwe have in effect shifted it to the left by \nand padded out the result with zeroes. Adding F yields the concatenation of D and\nT = 2n-kD + F\nP = pattern of n - k + 1 bits; this is the predetermined divisor\nF = 1n - k2-bit FCS, the last 1n - k2 bits of T\nD = k-bit block of data, or message, the first k bits of T\nT = n-bit frame to be transmitted\n1n - k2-bit\n3This procedure is slightly different from that of Figure 6.3. As shall be seen, the CRC process could be\nimplemented as follows.The receiver could perform a division operation on the incoming k data bits and\ncompare the result to the incoming \ncheck bits.\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nF, which is T. We want T to be exactly divisible by P. Suppose that we divide \nThere is a quotient and a remainder. Because division is modulo 2, the remainder is\nalways at least one bit shorter than the divisor. We will use this remainder as our\nDoes this R satisfy our condition that T/P have no remainder? To see that it does,\nSubstituting Equation (6.1), we have\nHowever, any binary number added to itself modulo 2 yields zero.Thus\nThere is no remainder, and therefore T is exactly divisible by P. Thus, the FCS is eas-\nily generated: Simply divide \nby P and use the \nremainder as the\nFCS. On reception, the receiver will divide T by P and will get no remainder if there\nhave been no errors.\n1n - k2-bit\nP = Q + R + R\nP = 2n-kD + R\nT = 2n-kD + R\nEXAMPLE 6.6\nThe message is multiplied by \nyielding 101000110100000.\nThis product is divided by P:\n1n - k2 = 5.\nn = 15, k = 10,\n FCS R = to be calculated 15 bits2\n Pattern P = 110101 16 bits2\n Message D = 1010001101 110 bits2\n6.3 / ERROR DETECTION\nThe remainder is added to \nwhich is trans-\nIf there are no errors, the receiver receives T intact. The received frame is\ndivided by P:\nBecause there is no remainder, it is assumed that there have been no errors.\nT = 101000110101110,\nThe pattern P is chosen to be one bit longer than the desired FCS, and the\nexact bit pattern chosen depends on the type of errors expected.At minimum, both\nthe high- and low-order bits of P must be 1.\nThere is a concise method for specifying the occurrence of one or more errors.\nAn error results in the reversal of a bit. This is equivalent to taking the XOR of the\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nbit and 1 (modulo 2 addition of 1 to the bit):\nThus, the errors\nin an n-bit frame can be represented by an n-bit field with 1s in each error position.\nThe resulting frame \ncan be expressed as\nIf there is an error \nthe receiver will fail to detect the error if and only if \nis divisible by P, which is equivalent to E divisible by P. Intuitively, this seems an\nunlikely occurrence.\nPolynomials A second way of viewing the CRC process is to express all values as\npolynomials in a dummy variable X, with binary coefficients. The coefficients\ncorrespond to the bits in the binary number. Thus, for \nArithmetic operations are again modulo 2. The CRC process can now be described\nCompare these equations with Equations (6.1) and (6.2).\nT1X2 = Xn-kD1X2 + R1X2\nP1X2 = X4 + X3 + 1.\nD1X2 = X5 + X4 + X + 1,\nD = 110011,\n\u0001 = bitwise exclusive-OR1XOR2\nTr = received frame\nE = error pattern with 1s in positions where errors occur\nT = transmitted frame\n0 + 1 = 1; 1 + 1 = 0.\nEXAMPLE 6.7\nUsing the preceding example, for \nWe should end up with \nwhich corre-\nFigure 6.4 shows the polynomial division that\ncorresponds to the binary division in the preceding example.\nR1X2 = X3 + X2 + X.\nP1X2 = X5 + X4 + X2 + 1.\nP = 110101,\nD1X2 = X9 + X7 + X3 + X2 + 1,\nD = 1010001101,\nAn error E(X) will only be undetectable if it is divisible by P(X). It can be\nshown [PETE 61, RAMA88] that all of the following errors are not divisible by a\nsuitably chosen P(X) and hence are detectable:\n• All single-bit errors, if P(X) has more than one nonzero term\n• All double-bit errors, as long as P(X) is a special type of polynomial, called a\nprimitive polynomial, with maximum exponent L, and the frame length is less\nthan 2L - 1.\n6.3 / ERROR DETECTION\n• Any odd number of errors, as long as P(X) contains a factor \n• Any burst error for which the length of the burst is less than or equal to \nthat is, less than or equal to the length of the FCS\n• A fraction of error bursts of length \nthe fraction equals\n• A fraction of error bursts of length greater than \nthe fraction\nIn addition, it can be shown that if all error patterns are considered equally\nlikely, then for a burst error of length \nthe probability of an undetected error\n(E(X) is divisible by P(X)) is \nand for a longer burst, the probability is \nwhere r is the length of the FCS.\nFour versions of P(X) are widely used:\nThe CRC-12 system is used for transmission of streams of 6-bit characters and gen-\nerates a 12-bit FCS. Both CRC-16 and CRC-CCITT are popular for 8-bit characters,\nin the United States and Europe, respectively, and both result in a 16-bit FCS. This\nwould seem adequate for most applications, although CRC-32 is specified as an\noption in some point-to-point synchronous transmission standards and is used in\nIEEE 802 LAN standards.\n+ X10 + X8 + X7 + X5 + X4 + X2 + X + 1\n CRC-32 = X32 + X26 + X23 + X22 + X16 + X12 + X11\n CRC-CCITT = X16 + X12 + X5 + 1\n CRC-16 = X16 + X15 + X2 + 1\n CRC-12 = X12 + X11 + X3 + X2 + X + 1\n1 - 2-1n-k2\n1 - 2-1n-k-12\nX5 \u0002 X4 \u0002 X2 \u0002 1\nX13 \u0002 X12 \u0002 X11 \u0002\nX11 \u0002 X10 \u0002 X9 \u0002\nX9 \u0002 X8 \u0002 X6 \u0002 X4 \u0002 X2 \u0002 X\nX9 \u0002 X8 \u0002 X7 \u0002 X6 \u0002 X5\nX14 \u0002 X13 \u0002\nX13 \u0002 X12 \u0002\nX11 \u0002 X10 \u0002\nX3 \u0002 X2 \u0002 X\nExample of Polynomial Division\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nDigital Logic The CRC process can be represented by, and indeed imple-\nmented as, a dividing circuit consisting of XOR gates and a shift register.The shift\nregister is a string of 1-bit storage devices. Each device has an output line, which\nindicates the value currently stored, and an input line. At discrete time instants,\nknown as clock times, the value in the storage device is replaced by the value indi-\ncated by its input line.The entire register is clocked simultaneously, causing a 1-bit\nshift along the entire register. The circuit is implemented as follows:\n1. The register contains \nbits, equal to the length of the FCS.\n2. There are up to \n3. The presence or absence of a gate corresponds to the presence or absence of\na term in the divisor polynomial, P(X), excluding the terms 1 and Xn-k.\nEXAMPLE 6.8\nThe architecture of a CRC circuit is best explained by first con-\nsidering an example, which is illustrated in Figure 6.5. In this example, we use\nwhich were used earlier in the discussion.\nFigure 6.5a shows the shift register implementation. The process begins with\nthe shift register cleared (all zeros). The message, or dividend, is then entered,\none bit at a time, starting with the most significant bit. Figure 6.5b is a table that\nshows the step-by-step operation as the input is applied one bit at a time. Each\nrow of the table shows the values currently stored in the five shift-register ele-\nments. In addition, the row shows the values that appear at the outputs of the\nthree XOR circuits. Finally, the row shows the value of the next input bit, which is\navailable for the operation of the next step.\nNote that the XOR operation affects \non the next shift.This is\nidentical to the binary long division process illustrated earlier. The process con-\ntinues through all the bits of the message. To produce the proper output, two\nswitches are used.The input data bits are fed in with both switches in the A posi-\ntion. As a result, for the first 10 steps, the input bits are fed into the shift register\nand also used as output bits. After the last data bit is processed, the shift register\ncontains the remainder (FCS) (shown shaded).As soon as the last data bit is pro-\nvided to the shift register, both switches are set to the B position. This has two\neffects: (1) All of the XOR gates become simple pass-throughs; no bits are\nchanged, and (2) as the shifting process continues, the 5 CRC bits are output.\nAt the receiver, the same logic is used. As each bit of M arrives, it is inserted\ninto the shift register. If there have been no errors, the shift register should con-\ntain the bit pattern for R at the conclusion of M. The transmitted bits of R now\nbegin to arrive, and the effect is to zero out the register so that, at the conclusion\nof reception, the register contains all 0s.\n Divisor P = 110101;\nP1X2 = X5 + X4 + X2 + 1\n Data D = 1010001101;\nD1X2 = X9 + X7 + X3 + X2 + 1\n6.3 / ERROR DETECTION\n(a) Shift-register implementation\n(b) Example with input of 1010001101\n\u0001 1-bit shift register\n\u0001 Exclusive-OR circuit\nC4\u0001C3\u0001I C4\u0001C1\u0001I C4\u0001I\nCircuit with Shift Registers for Dividing by the Polynomial\nX5 + X4 + X2 + 1\nGeneral CRC Architecture to Implement Divisor \nAn-1Xn-k-1 + Xn-k2\n11 + A1X + A2X2 + Á +\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nFigure 6.6 indicates the general architecture of the shift register implementa-\ntion of a CRC for the polynomial \nequal either 0 or 1.4\nError detection is a useful technique, found in data link control protocols, such as\nHDLC, and in transport protocols, such as TCP. However, correction of errors\nusing an error-detecting code, requires that block of data be retransmitted, as\nexplained in Chapter 7. For wireless applications this approach is inadequate for\ntwo reasons.\n1. The bit error rate on a wireless link can be quite high, which would result in a\nlarge number of retransmissions.\n2. In some cases, especially satellite links, the propagation delay is very long com-\npared to the transmission time of a single frame.The result is a very inefficient\nsystem. As is discussed in Chapter 7, the common approach to retransmission\nis to retransmit the frame in error plus all subsequent frames.With a long data\nlink, an error in a single frame necessitates retransmitting many frames.\nInstead, it would be desirable to enable the receiver to correct errors in an\nincoming transmission on the basis of the bits in that transmission. Figure 6.7 shows\nin general how this is done. On the transmission end, each k-bit block of data is\nA0 = An-k = 1\nP1X2 = gn-k\nTransmitter\nNo error or\ncorrectable\nDetectable but not\ncorrectable error\nError Correction Process\n4It is common for the CRC register to be shown shifting to the right, which is the reverse of the analogy\nto binary division. Because binary numbers are usually shown with the most significant bit on the left, a\nleft-shifting register, as is used here, is more appropriate.\n6.4 / ERROR CORRECTION\nmapped into an n-bit block \ncalled a codeword, using an FEC (forward\nerror correction) encoder. The codeword is then transmitted. During transmission,\nthe signal is subject to impairments, which may produce bit errors in the signal. At\nthe receiver, the incoming signal is demodulated to produce a bit string that is simi-\nlar to the original codeword but may contain errors.This block is passed through an\nFEC decoder, with one of four possible outcomes:\n1. If there are no bit errors, the input to the FEC decoder is identical to the\noriginal codeword, and the decoder produces the original data block as\n2. For certain error patterns, it is possible for the decoder to detect and correct\nthose errors.Thus, even though the incoming data block differs from the trans-\nmitted codeword, the FEC decoder is able to map this block into the original\ndata block.\n3. For certain error patterns, the decoder can detect but not correct the errors. In\nthis case, the decode simply reports an uncorrectable error.\n4. For certain, typically rare, error patterns, the decoder does not detect that any\nerrors have occurred and maps the incoming n-bit data block into a k-bit block\nthat differs from the original k-bit block.\nHow is it possible for the decoder to correct bit errors? In essence, error cor-\nrection works by adding redundancy to the transmitted message. The redundancy\nmakes it possible for the receiver to deduce what the original message was, even in\nthe face of a certain level of error rate. In this section we look at a widely used form\nof error-correcting code known as a block error-correcting code. Our discussion\nonly deals with basic principles; a discussion of specific error-correcting codes is\nbeyond our scope.\nBefore proceeding,we note that in many cases,the error-correcting code follows\nthe same general layout as shown for error-detecting codes in Figure 6.3. That is, the\nFEC algorithm takes as input a k-bit block and adds \ncheck bits to that block\nto produce an n-bit block; all of the bits in the original k-bit block show up in the n-bit\nblock. For some FEC algorithms, the FEC algorithm maps the k-bit input into an n-bit\ncodeword in such a way that the original k bits do not appear in the codeword.\nBlock Code Principles\nTo begin, we define a term that shall be of use to us.The Hamming distance\nbetween two n-bit binary sequences \nis the number of bits in which \ndisagree. For example, if\nNow let us consider the block code technique for error correction. Suppose we\nwish to transmit blocks of data of length k bits. Instead of transmitting each block as\nk bits, we map each k-bit sequence into a unique n-bit codeword.\nd1v1, v22 = 3\nv1 = 011011,\nv2 = 110001\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nEXAMPLE 6.9\nwe can make the following assignment:\nNow, suppose that a codeword block is received with the bit pattern 00100.This is\nnot a valid codeword, and so the receiver has detected an error. Can the error be\ncorrected? We cannot be sure which data block was sent because 1, 2, 3, 4, or even\nall 5 of the bits that were transmitted may have been corrupted by noise. How-\never, notice that it would require only a single bit change to transform the valid\ncodeword 00000 into 00100. It would take two bit changes to transform 00111 to\n00100, three bit changes to transform 11110 to 00100, and it would take four bit\nchanges to transform 11001 into 00100. Thus, we can deduce that the most likely\ncodeword that was sent was 00000 and that therefore the desired data block is 00.\nThis is error correction. In terms of Hamming distances, we have\nSo the rule we would like to impose is that if an invalid codeword is received,\nthen the valid codeword that is closest to it (minimum distance) is selected. This\nwill only work if there is a unique valid codeword at a minimum distance from\neach invalid codeword.\nFor our example, it is not true that for every invalid codeword there is one\nand only one valid codeword at a minimum distance. There are \ncodewords of which 4 are valid, leaving 28 invalid codewords. For the invalid\ncodewords, we have the following:\n00000 or 11110\n00111 or 11001\n00000 or 11110\n00111 or 11001\n00000 or 11110\n00111 or 11001\nd111001, 001002 = 4; d111110, 001002 = 3\nd100000, 001002 = 1; d100111, 001002 = 2;\n6.4 / ERROR CORRECTION\nThe preceding example illustrates the essential properties of a block error-\ncorrecting code. An (n, k) block code encodes k data bits into n-bit codewords.\nTypically, each valid codeword reproduces the original k data bits and adds to them\ncheck bits to form the n-bit codeword. Thus the design of a block code is\nequivalent to the design of a function of the form \nis a vector of\nk data bits and \nis a vector of n codeword bits.\nWith an (n, k) block code, there are \nvalid codewords out of a total of \npossible codewords. The ratio of redundant bits to data bits,\nthe redundancy of the code, and the ratio of data bits to total bits, k/n, is called the\ncode rate. The code rate is a measure of how much additional bandwidth is\nrequired to carry data at the same data rate as without the code. For example, a\ncode rate of 1/2 requires double the transmission capacity of an uncoded system to\nmaintain the same data rate. Our example has a code rate of 2/5 and so requires\n2.5 times the capacity of an uncoded system. For example, if the data rate input to\nthe encoder is 1 Mbps, then the output from the encoder must be at a rate of 2.5\nMbps to keep up.\nFor a code consisting of the codewords \nmum distance \nof the code is defined as\ni Z j [d1wi, wj2]\nw1, w2, Á , ws,\nvc = f1vd2,\n00000 or 11110\n00111 or 11001\nThere are eight cases in which an invalid codeword is at a distance 2 from two\ndifferent valid codewords.Thus, if one such invalid codeword is received, an error\nin 2 bits could have caused it and the receiver has no way to choose between the\ntwo alternatives.An error is detected but cannot be corrected. However, in every\ncase in which a single bit error occurs, the resulting codeword is of distance 1\nfrom only one valid codeword and the decision can be made. This code is there-\nfore capable of correcting all single-bit errors but cannot correct double bit\nerrors. Another way to see this is to look at the pairwise distances between valid\nThe minimum distance between valid codewords is 3. Therefore, a single bit\nerror will result in an invalid codeword that is a distance 1 from the original valid\ncodeword but a distance at least 2 from all other valid codewords.As a result, the\ncode can always correct a single-bit error. Note that the code also will always\ndetect a double-bit error.\nd100111, 110012 = 4;\nd100111, 111102 = 3; d111001, 111102 = 3;\nd100000, 001112 = 3;\nd100000, 110012 = 3; d100000, 111102 = 4;\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nIt can be shown that the following conditions hold. For a given positive integer t,\nif a code satisfies \nthen the code can correct all bit errors up to\nand including errors of t bits. If \nbits can be\ncorrected and errors of t bits can be detected but not, in general, corrected. Con-\nversely, any code for which all errors of \nare corrected must satisfy\nand any code for which all errors of \ncorrected and all errors of magnitude t are detected must satisfy \nAnother way of putting the relationship between \nand t is to say that the\nmaximum number of guaranteed correctable errors per codeword satisfies\nmeans the largest integer not to exceed x (e.g.,\n). Furthermore,\nif we are concerned only with error detection and not error correction, then the\nnumber of errors, t, that can be detected satisfies\nTo see this, consider that if \nerrors occur, this could change one valid code-\nword into another. Any number of errors less than \ncan not result in another\nvalid codeword.\nThe design of a block code involves a number of considerations.\n1. For given values of n and k, we would like the largest possible value of \n2. The code should be relatively easy to encode and decode, requiring minimal\nmemory and processing time.\n3. We would like the number of extra bits,\nto be small, to reduce band-\n4. We would like the number of extra bits,\nto be large, to reduce error\nClearly, the last two objectives are in conflict, and tradeoffs must be made.\nIt is instructive to examine Figure 6.8, based on [LEBO98]. The literature on\nerror-correcting codes frequently includes graphs of this sort to demonstrate the\neffectiveness of various encoding schemes. Recall from Chapter 5 that coding can be\nused to reduce the required \nvalue to achieve a given bit error rate.5 The coding\ndiscussed in Chapter 5 has to do with the definition of signal elements to represent\nbits. The coding discussed in this chapter also has an effect on \nIn Figure 6.8,\nthe curve on the right is for an uncoded modulation system; the shaded region rep-\nresents the area in which improvement can be achieved. In this region, a smaller\nBER (bit error rate) is achieved for a given \nand conversely, for a given BER,\nis required. The other curve is a typical result of a code rate of one-\nhalf (equal number of data and check bits). Note that at an error rate of \nuse of coding allows a reduction in \nof 2.77 dB.This reduction is referred to as\nt = dmin - 1\nt = j dmin - 1\nmagnitude … 1t - 12\ndmin Ú 12t + 12,\nmagnitude … t\nerrors … 1t - 12\ndmin Ú 12t + 12,\nis the ratio of signal energy per bit to noise power density per Hertz; it is defined and discussed in\n6.5 / LINE CONFIGURATIONS\nthe coding gain, which is defined as the reduction, in decibels, in the required \nto achieve a specified BER of an error-correcting coded system compared to an\nuncoded system using the same modulation.\nIt is important to realize that the BER for the second rate 1/2 curve refers to\nthe rate of uncorrected errors and that the \nvalue refers to the energy per data bit.\nBecause the rate is 1/2, there are two bits on the channel for each data bit, and the\nenergy per coded bit is half that of the energy per data bit, or a reduction of 3 dB to\na value of 8 dB. If we look at the energy per coded bit for this system, then we see\nthat the channel bit error rate is about \nFinally, note that below a certain threshold of \n, the coding scheme actu-\nally degrades performance. In our example of Figure 6.8, the threshold occurs at\nabout 5.4 dB. Below the threshold, the extra check bits add overhead to the system\nthat reduces the energy per data bit causing increased errors. Above the threshold,\nthe error-correcting power of the code more than compensates for the reduced \nresulting in a coding gain.\nTwo characteristics that distinguish various data link configurations are topology\nand whether the link is half duplex or full duplex.\nThe topology of a data link refers to the physical arrangement of stations on a trans-\nmission medium. If there are only two stations (e.g., a terminal and a computer or\n2.4 * 10-2,\nHow Coding Improves System Performance \nProbability of bit error (BER)\n(Eb/N0) (dB)\nChannel bit\nerror probability\ncoding gain\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nTraditional Computer/Terminal Configurations\n(b) Multipoint\n(a) Point-to-point\n(secondaries)\ntwo computers), the link is point to point. If there are more than two stations, then it\nis a multipoint topology.Traditionally, a multipoint link has been used in the case of\na computer (primary station) and a set of terminals (secondary stations). In today’s\nenvironments, the multipoint topology is found in local area networks.\nTraditional multipoint topologies are made possible when the terminals are\nonly transmitting a fraction of the time. Figure 6.9 illustrates the advantages of the\nmultipoint configuration. If each terminal has a point-to-point link to its computer,\nthen the computer must have one I/O port for each terminal. Also there is a sepa-\nrate transmission line from the computer to each terminal. In a multipoint configu-\nration, the computer needs only a single I/O port and a single transmission line,\nwhich saves costs.\nFull Duplex and Half Duplex\nData exchanges over a transmission line can be classified as full duplex or half\nduplex. With half-duplex transmission, only one of two stations on a point-to-point\nlink may transmit at a time. This mode is also referred to as two-way alternate, sug-\ngestive of the fact that two stations must alternate in transmitting. This can be\n6.6 / RECOMMENDED READING\ncompared to a one-lane, two-way bridge.This form of transmission is often used for\nterminal-to-computer interaction.While a user is entering and transmitting data, the\ncomputer is prevented from sending data to the terminal, which would appear on\nthe terminal screen and cause confusion.\nFor full-duplex transmission, two stations can simultaneously send and receive\ndata from each other.Thus, this mode is known as two-way simultaneous and may be\ncompared to a two-lane, two-way bridge. For computer-to-computer data exchange,\nthis form of transmission is more efficient than half-duplex transmission.\nWith digital signaling, which requires guided transmission, full-duplex opera-\ntion usually requires two separate transmission paths (e.g., two twisted pairs), while\nhalf duplex requires only one. For analog signaling, it depends on frequency: If a sta-\ntion transmits and receives on the same frequency, it must operate in half-duplex\nmode for wireless transmission, although it may operate in full-duplex mode for\nguided transmission using two separate transmission lines. If a station transmits on\none frequency and receives on another, it may operate in full-duplex mode for wire-\nless transmission and in full-duplex mode with a single line for guided transmission.\nIt is possible to transmit digital signals simultaneously in both directions on a\nsingle transmission line using a technique called echo cancellation. This is a signal\nprocessing technique whose explanation is beyond the scope of this book.\nThe classic treatment of error detecting codes and CRC is [PETE61]. [RAMA88] is an excel-\nlent tutorial on CRC.\n[STAL05] discusses most of the widely used error-correcting codes. [ADAM91] pro-\nvides comprehensive treatment of error-correcting codes. [SKLA01] contains a clear, well-\nwritten section on the subject. Two useful survey articles are [BERL87] and [BHAR83]. A\nquite readable theoretical and mathematical treatment of error-correcting codes is [ASH90].\n[FREE98] provides good coverage of many physical layer interface standards.\nAdamek, J. Foundations of Coding. New York:Wiley, 1991.\nAsh, R. Information Theory. New York: Dover, 1990.\nBerlekamp, E.; Peile, R.; and Pope, S. “The Application of Error Control to\nCommunications.” IEEE Communications Magazine, April 1987.\nBhargava, V. “Forward Error Correction Schemes for Digital Communica-\ntions.” IEEE Communications Magazine, January 1983.\nFreeman, R. Telecommunication Transmission Handbook. New York: Wiley,\nPeterson, W., and Brown, D. “Cyclic Codes for Error Detection.” Proceedings\nof the IEEE, January 1961.\nRamabadran, T., and Gaitonde, S. “A Tutorial on CRC Computations.” IEEE\nMicro, August 1988.\nSklar, B. Digital Communications: Fundamentals and Applications. Upper Sad-\ndle River, NJ: Prentice Hall, 2001.\nStallings, W. Wireless Communications and Networks, Second Edition. Upper\nSaddle River, NJ: Prentice Hall, 2005.\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nKEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nasynchronous transmission\ncyclic code\ncyclic redundancy check (CRC)\nerror correction\nerror-correcting code (ECC)\nerror detection\nerror-detecting code\nforward error correction (FEC)\nframe check sequence (FCS)\nfull duplex\nhalf duplex\nHamming code\nHamming distance\ninterchange circuits\nIntegrated Services Digital\nNetwork (ISDN)\nparity check\npoint-to-point\nsynchronous transmission\nReview Questions\nHow is the transmission of a single character differentiated from the transmission of\nthe next character in asynchronous transmission?\nWhat is a major disadvantage of asynchronous transmission?\nHow is synchronization provided for synchronous transmission?\nWhat is a parity bit?\nWhat is the CRC?\nWhy would you expect a CRC to detect more errors than a parity bit?\nList three different ways in which the CRC algorithm can be described.\nIs it possible to design an ECC that will correct some double bit errors but not all\ndouble bit errors? Why or why not?\nIn an (n, k) block ECC, what do n and k represent?\nSuppose a file of 10,000 bytes is to be sent over a line at 2400 bps.\nCalculate the overhead in bits and time in using asynchronous communication.\nAssume one start bit and a stop element of length one bit, and 8 bits to send the\nbyte itself for each character. The 8-bit character consists of all data bits, with no\nparity bit.\nb. Calculate the overhead in bits and time using synchronous communication.\nAssume that the data are sent in frames. Each frame consists of 1000 characters \u0001\n8000 bits and an overhead of 48 control bits per frame.\nWhat would the answers to parts (a) and (b) be for a file of 100,000 characters?\nd. What would the answers to parts (a) and (b) be for the original file of 10,000 char-\nacters except at a data rate of 9600 bps?\nA data source produces 7-bit IRA characters. Derive an expression of the maximum\neffective data rate (rate of IRA data bits) over an x-bps line for the following:\nAsynchronous transmission, with a 1.5-unit stop element and a parity bit.\nb. Synchronous transmission, with a frame consisting of 48 control bits and 128 infor-\nmation bits. The information field contains 8-bit (parity included) IRA characters.\nSame as part (b), except that the information field is 1024 bits.\nDemonstrate by example (write down a few dozen arbitrary bit patterns; assume one\nstart bit and a stop element of length one bit) that a receiver that suffers a framing\nerror on asynchronous transmission will eventually become realigned.\n6.7 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nSuppose that a sender and receiver use asynchronous transmission and agree \nnot to use any stop elements. Could this work? If so, explain any necessary conditions.\nAn asynchronous transmission scheme uses 8 data bits, an even parity bit, and a stop\nelement of length 2 bits. What percentage of clock inaccuracy can be tolerated at the\nreceiver with respect to the framing error? Assume that the bit samples are taken at\nthe middle of the clock period. Also assume that at the beginning of the start bit the\nclock and incoming bits are in phase.\nSuppose that a synchronous serial data transmission is clocked by two clocks (one at\nthe sender and one at the receiver) that each have a drift of 1 minute in one year.\nHow long a sequence of bits can be sent before possible clock drift could cause a\nproblem? Assume that a bit waveform will be good if it is sampled within 40% of its\ncenter and that the sender and receiver are resynchronized at the beginning of each\nframe. Note that the transmission rate is not a factor, as both the bit period and the\nabsolute timing error decrease proportionately at higher transmission rates.\nWould you expect that the inclusion of a parity bit with each character would change\nthe probability of receiving a correct message?\nTwo communicating devices are using a single-bit even parity check for error detec-\ntion. The transmitter sends the byte 10101010 and, because of channel noise, the\nreceiver gets the byte 10011010. Will the receiver detect the error? Why or why not?\nWhat is the purpose of using modulo 2 arithmetic rather than binary arithmetic in\ncomputing an FCS?\nConsider a frame consisting of two characters of four bits each.Assume that the prob-\nability of bit error is \nand that it is independent for each bit.\nWhat is the probability that the received frame contains at least one error?\nb. Now add a parity bit to each character.What is the probability?\nUsing the CRC-CCITT polynomial, generate the 16-bit CRC code for a message con-\nsisting of a 1 followed by 15 0s.\nUse long division.\nb. Use the shift register mechanism shown in Figure 6.6.\nExplain in words why the shift register implementation of CRC will result in all 0s at\nthe receiver if there are no errors. Demonstrate by example.\nfind the CRC.\nA CRC is constructed to generate a 4-bit FCS for an 11-bit message. The generator\npolynomial is \nDraw the shift register circuit that would perform this task (see Figure 6.6).\nb. Encode the data bit sequence 10011011100 (leftmost bit is the least significant)\nusing the generator polynomial and give the codeword.\nNow assume that bit 7 (counting from the LSB) in the codeword is in error and\nshow that the detection algorithm detects the error.\nIn a CRC error-detecting scheme, choose \nEncode the bits\n10010011011.\nb. Suppose the channel introduces an error pattern 100010000000000 (i.e., a flip\nfrom 1 to 0 or from 0 to 1 in position 1 and 5).What is received? Can the error be\nRepeat part (b) with error pattern 100110000000000.\nA modified CRC procedure is commonly used in communications standards. It is\ndefined as follows:\nFCS = L1X2 + R1X2\nX16D1X2 + XkL1X2\nP1x2 = x4 + x + 1.\nX4 + X3 + 1.\nM = 11100011,\nCHAPTER 6 / DIGITAL DATA COMMUNICATION TECHNIQUES\nand k is the number of bits being checked (address, control, and information fields).\nDescribe in words the effect of this procedure.\nb. Explain the potential benefits.\nShow a shift register implementation for \nCalculate the Hamming pairwise distances among the following codewords:\n00000, 10101, 01010\nb. 000000, 010101, 101010, 110110\nSection 6.4 discusses block error-correcting codes that make a decision on the\nbasis of minimum distance. That is, given a code consisting of s equally likely\ncodewords of length n, for each received sequence v, the receiver selects the\ncodeword w for which the distance d(w, v) is a minimum. We would like to prove\nthat this scheme is “ideal” in the sense that the receiver always selects the code-\nword for which the probability of w given v,\nis a maximum. Because all\ncodewords are assumed equally likely, the codeword that maximizes \nsame as the codeword that maximizes \nIn order that w be received as v, there must be exactly d(w, v) errors in transmis-\nsion, and these errors must occur in those bits where w and v disagree. Let \nthe probability that a given bit is transmitted incorrectly and n be the length of a\ncodeword.Write an expression for \nas a function of \nd(w, v), and n. Hint:\nThe number of bits in error is d(w, v) and the number of bits not in error is\nb. Now compare \nfor two different codewords \nAssume that \nand show that \nif and only if\nThis proves that the codeword w that gives the largest value\nis that word whose distance from v is a minimum.\nSection 6.4 states that for a given positive integer t, if a code satisfies \nthen the code can correct all bit errors up to and including errors of t bits. Prove this\nassertion. Hint: Start by observing that for a codeword w to be decoded as another\nthe received sequence must be at least as close to \nis added above the physical layer discussed in Chapter 6;this logic is referred to as\ndata link control or a data link control protocol.When a data link control protocol\nis used, the transmission medium between systems is referred to as a data link.\nTo see the need for data link control, we list some of the requirements and\nobjectives for effective data communication between two directly connected\ntransmitting-receiving stations:\n• Frame synchronization: Data are sent in blocks called frames.The beginning\nand end of each frame must be recognizable.We briefly introduced this topic\nwith the discussion of synchronous frames (Figure 6.2).\n• Flow control: The sending station must not send frames at a rate faster\nthan the receiving station can absorb them.\n• Error control: Bit errors introduced by the transmission system should be\n• Addressing: On a shared link, such as a local area network (LAN), the\nidentity of the two stations involved in a transmission must be specified.\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\n7.1 / FLOW CONTROL\n• Control and data on same link: It is usually not desirable to have a physi-\ncally separate communications path for control information. Accordingly,\nthe receiver must be able to distinguish control information from the data\nbeing transmitted.\n• Link management: The initiation, maintenance, and termination of a sus-\ntained data exchange require a fair amount of coordination and coopera-\ntion among stations. Procedures for the management of this exchange are\nNone of these requirements is satisfied by the techniques described in\nChapter 6.We shall see in this chapter that a data link protocol that satisfies these\nrequirements is a rather complex affair.We begin by looking at two key mecha-\nnisms that are part of data link control: flow control and error control. Following\nthis background we look at the most important example of a data link control\nprotocol: HDLC (high-level data link control).This protocol is important for two\nreasons: First, it is a widely used standardized data link control protocol. Second,\nHDLC serves as a baseline from which virtually all other important data link\n1On a direct point-to-point link, the amount of delay is fixed rather than variable. However, a data link\ncontrol protocol can be used over a network connection, such as a circuit-switched or ATM network, in\nwhich case the delay may be variable.\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nDestination\n(a) Error-free transmission\nDestination\n(b) Transmission with\nlosses and errors\nModel of Frame Transmission\nStop-and-Wait Flow Control\nThe simplest form of flow control, known as stop-and-wait flow control, works as\nfollows. A source entity transmits a frame. After the destination entity receives the\nframe, it indicates its willingness to accept another frame by sending back an\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nWhen a is greater than 1, the propagation time is greater than the transmission time.\nIn this case, the sender completes transmission of the entire frame before the lead-\ning bits of that frame arrive at the receiver. Put another way, larger values of a are\nconsistent with higher data rates and/or longer distances between stations. Appen-\ndix 7A discusses a and data link performance.\nBoth parts of Figure 7.2 (a and b) consist of a sequence of snapshots of\nthe transmission process over time. In both cases, the first four snapshots show the\nprocess of transmitting a frame containing data, and the last snapshot shows the\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\n0 through 7, and then the same numbers are reused for subsequent frames. The\nshaded rectangle indicates the frames that may be sent; in this figure, the sender may\ntransmit five frames, beginning with frame 0. Each time a frame is sent, the shaded\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nError control refers to mechanisms to detect and correct errors that occur in the\ntransmission of frames. The model that we will use, which covers the typical case, is\nillustrated in Figure 7.1b. As before, data are sent as a sequence of frames; frames\narrive in the same order in which they are sent; and each transmitted frame suffers\nan arbitrary and potentially variable amount of delay before reception. In addition,\nwe admit the possibility of two types of errors:\n• Lost frame: A frame fails to arrive at the other side. For example, a noise burst\nmay damage a frame to the extent that the receiver is not aware that a frame\nhas been transmitted.\n• Damaged frame: A recognizable frame does arrive, but some of the bits are in\nerror (have been altered during transmission).\nThe most common techniques for error control are based on some or all of the\nfollowing ingredients:\n• Error detection: As discussed in the Chapter 6.\n2This figure indicates the time required to transmit a frame. For simplicity, other figures in this chapter do\nnot show this time.\nAll of these forms are based on the use of the flow control techniques dis-\ncussed in Section 7.1.We examine each in turn.\nStop-and-Wait ARQ\nStop-and-wait ARQ is based on the stop-and-wait flow control technique outlined\npreviously. The source station transmits a single frame and then must await an\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nStop-and-Wait ARQ\nmission time\nmission time\nPropagation time\nTimeout interval\nPDU 0 lost;\nA retransmits\nTimeout interval\nA retransmits\nduplicate PDU\nsliding-window flow control technique. While no errors occur, the destination will\nacknowledge incoming frames as usual (\nor piggybacked\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nSliding-Window ARQ Protocols\nRR (P bit \u0001 1)\n(a) Go-back-N ARQ\n4, 5, and 6\nretransmitted\nDiscarded by\nRR (P bit \u0001 1)\n(b) Selective-reject ARQ\n4 retransmitted\nBuffered by\nIn Section 7.1, we mentioned that for a k-bit sequence number field, which pro-\nvides a sequence number range of \nthe maximum window size is limited to \nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nHIGH-LEVEL DATA LINK CONTROL (HDLC)\nThe most important data link control protocol is HDLC (ISO 3009, ISO 4335). Not\nonly is HDLC widely used, but it is the basis for many other important data link\ncontrol protocols, which use the same or similar formats and the same mechanisms\nas employed in HDLC.\nBasic Characteristics\nTo satisfy a variety of applications, HDLC defines three types of stations, two \nlink configurations, and three data transfer modes of operation. The three station\n• Primary station: Responsible for controlling the operation of the link. Frames\nissued by the primary are called commands.\n• Secondary station: Operates under the control of the primary station. Frames\nissued by a secondary are called responses. The primary maintains a separate\nlogical link with each secondary station on the line.\n• Combined station: Combines the features of primary and secondary. A com-\nbined station may issue both commands and responses.\nThe two link configurations are\n• Unbalanced configuration: Consists of one primary and one or more sec-\nondary stations and supports both full-duplex and half-duplex transmis-\n• Balanced configuration: Consists of two combined stations and supports both\nfull-duplex and half-duplex transmission.\nThe three data transfer modes are\n• Normal response mode (NRM): Used with an unbalanced configuration. The\nprimary may initiate data transfer to a secondary, but a secondary may only\ntransmit data in response to a command from the primary.\n• Asynchronous balanced mode (ABM): Used with a balanced configuration.\nEither combined station may initiate transmission without receiving permis-\nsion from the other combined station.\n• Asynchronous response mode (ARM): Used with an unbalanced configura-\ntion. The secondary may initiate transmission without explicit permission of\nthe primary. The primary still retains responsibility for the line, including ini-\ntialization, error recovery, and logical disconnection.\nNRM is used on multidrop lines, in which a number of terminals are connected\nto a host computer. The computer polls each terminal for input. NRM is also some-\ntimes used on point-to-point links, particularly if the link connects a terminal or\nother peripheral to a computer.ABM is the most widely used of the three modes; it\nmakes more efficient use of a full-duplex point-to-point link because there is no\npolling overhead. ARM is rarely used; it is applicable to some special situations in\nwhich a secondary may need to initiate transmission.\n7.3 / HIGH-LEVEL DATA LINK CONTROL (HDLC)\nInformation\n(a) Frame format\nI: Information\nS: Supervisory\nU: Unnumbered\nN(S) \u0001 Send sequence number\nN(R) \u0001 Receive sequence number\nS \u0001 Supervisory function bits\nM \u0001 Unnumbered function bits\nP/F \u0001 Poll/final bit\n(c) 8-bit control field format\nInformation\nSupervisory\n(d) 16-bit control field format\n9 10 11 12 13 14 15 16\n(b) Extended address field\nFrame Structure\nHDLC uses synchronous transmission. All transmissions are in the form of frames,\nand a single frame format suffices for all types of data and control exchanges.\nFigure 7.7 depicts the structure of the HDLC frame.The flag, address, and con-\ntrol fields that precede the information field are known as a header. The FCS and\nflag fields following the data field are referred to as a trailer.\nFlag Fields Flag fields delimit the frame at both ends with the unique pattern\n01111110.A single flag may be used as the closing flag for one frame and the opening\nflag for the next. On both sides of the user-network interface, receivers are continu-\nously hunting for the flag sequence to synchronize on the start of a frame. While\nreceiving a frame, a station continues to hunt for that sequence to determine the end\nof the frame. Because the protocol allows the presence of arbitrary bit patterns (i.e.,\nthere are no restrictions on the content of the various fields imposed by the link pro-\ntocol), there is no assurance that the pattern 01111110 will not appear somewhere\ninside the frame, thus destroying synchronization.To avoid this problem, a procedure\nknown as bit stuffing is used. For all bits between the starting and ending flags, the\ntransmitter inserts an extra 0 bit after each occurrence of five 1s in the frame. After\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nBit Stuffing\nOriginal pattern:\nAfter bit-stuffing:\ndetecting a starting flag, the receiver monitors the bit stream.When a pattern of five\n1s appears, the sixth bit is examined. If this bit is 0, it is deleted. If the sixth bit is a 1\nand the seventh bit is a 0, the combination is accepted as a flag. If the sixth and sev-\nenth bits are both 1, the sender is indicating an abort condition.\nWith the use of bit stuffing, arbitrary bit patterns can be inserted into the data\nfield of the frame.This property is known as data transparency.\nFigure 7.8 shows an example of bit stuffing. Note that in the first two cases, the\nextra 0 is not strictly necessary for avoiding a flag pattern but is necessary for the\noperation of the algorithm.\nAddress Field The address field identifies the secondary station that transmitted\nor is to receive the frame. This field is not needed for point-to-point links but is\nalways included for the sake of uniformity. The address field is usually 8 bits long\nbut, by prior agreement, an extended format may be used in which the actual\naddress length is a multiple of 7 bits. The leftmost bit of each octet is 1 or 0 accord-\ning as it is or is not the last octet of the address field. The remaining 7 bits of each\noctet form part of the address.The single-octet address of 11111111 is interpreted as\nthe all-stations address in both basic and extended formats. It is used to allow the\nprimary to broadcast a frame for reception by all secondaries.\nControl Field HDLC defines three types of frames, each with a different control\nfield format. Information frames (I-frames) carry the data to be transmitted for the\nuser (the logic above HDLC that is using HDLC). Additionally, flow and error con-\ntrol data, using the ARQ mechanism, are piggybacked on an information frame.\nSupervisory frames (S-frames) provide the ARQ mechanism when piggybacking is\nnot used. Unnumbered frames (U-frames) provide supplemental link control func-\ntions.The first one or two bits of the control field serves to identify the frame type.The\nremaining bit positions are organized into subfields as indicated in Figures 7.7c and d.\nTheir use is explained in the discussion of HDLC operation later in this chapter.\nAll of the control field formats contain the poll/final (P/F) bit. Its use depends\non context. Typically, in command frames, it is referred to as the P bit and is set to\n1 to solicit (poll) a response frame from the peer HDLC entity. In response frames,\nit is referred to as the F bit and is set to 1 to indicate the response frame transmitted\nas a result of a soliciting command.\nNote that the basic control field for S- and I-frames uses 3-bit sequence num-\nbers. With the appropriate set-mode command, an extended control field can be\nused for S- and I-frames that employs 7-bit sequence numbers. U-frames always\ncontain an 8-bit control field.\nInformation Field The information field is present only in I-frames and some\nU-frames. The field can contain any sequence of bits but must consist of an integral\n7.3 / HIGH-LEVEL DATA LINK CONTROL (HDLC)\nnumber of octets.The length of the information field is variable up to some system-\ndefined maximum.\nFrame Check Sequence Field The frame check sequence (FCS) is an error-\ndetecting code calculated from the remaining bits of the frame, exclusive of flags.\nThe normal code is the 16-bit CRC-CCITT defined in Section 6.3. An optional \n32-bit FCS, using CRC-32, may be employed if the frame length or the line reliabil-\nity dictates this choice.\nHDLC operation consists of the exchange of I-frames, S-frames, and U-frames\nbetween two stations.The various commands and responses defined for these frame\ntypes are listed in Table 7.1. In describing HDLC operation, we will discuss these\nthree types of frames.\nHDLC Commands and Responses\nDescription\nInformation (I)\nExchange user data\nSupervisory (S)\nReceive ready (RR)\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nThe operation of HDLC involves three phases. First, one side or another ini-\ntializes the data link so that frames may be exchanged in an orderly fashion. Dur-\ning this phase, the options that are to be used are agreed upon. After\ninitialization, the two sides exchange user data and the control information to\nexercise flow and error control. Finally, one of the two sides signals the termina-\ntion of the operation.\nInitialization Either side may request initialization by issuing one of the six set-\nmode commands.This command serves three purposes:\n1. It signals the other side that initialization is requested.\n2. It specifies which of the three modes (NRM,ABM,ARM) is requested.\n3. It specifies whether 3- or 7-bit sequence numbers are to be used.\nIf the other side accepts this request, then the HDLC module on that end\ntransmits an unnumbered acknowledged (UA) frame back to the initiating side. If\nthe request is rejected, then a disconnected mode (DM) frame is sent.\nData Transfer When the initialization has been requested and accepted, then a\nlogical connection is established. Both sides may begin to send user data in I-\nframes, starting with sequence number 0. The N(S) and N(R) fields of the I-frame\nare sequence numbers that support flow control and error control. An HDLC\nmodule sending a sequence of I-frames will number them sequentially, modulo 8\nor 128, depending on whether 3- or 7-bit sequence numbers are used, and place\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nfigure (Figure 7.9a) shows the disconnect procedure. One side issues a DISC com-\nmand, and the other responds with a UA response.\nFigure 7.9b illustrates the full-duplex exchange of I-frames. When an entity\nsends a number of I-frames in a row with no incoming data, then the receive\nsequence number is simply repeated (e.g., I,1,1; I,2.1 in the A-to-B direction).When\nan entity receives a number of I-frames in a row with no outgoing frames, then the\nreceive sequence number in the next outgoing frame must reflect the cumulative\nactivity (e.g., I,1,3 in the B-to-A direction). Note that, in addition to I-frames, data\nexchange may involve supervisory frames.\nFigure 7.9c shows an operation involving a busy condition.Such a condition may\narise because an HDLC entity is not able to process I-frames as fast as they are arriv-\ning, or the intended user is not able to accept data as fast as they arrive in I-frames. In\neither case, the entity’s receive buffer fills up and it must halt the incoming flow of \nI-frames, using an RNR command. In this example,A issues an RNR, which requires\nB to halt transmission of I-frames.The station receiving the RNR will usually poll the\nbusy station at some periodic interval by sending an RR with the P bit set. This\nrequires the other side to respond with either an RR or an RNR.When the busy con-\ndition has cleared,A returns an RR, and I-frame transmission from B can resume.\nAn example of error recovery using the REJ command is shown in Figure\n7.9d. In this example, A transmits I-frames numbered 3, 4, and 5. Number 4 suffers\nan error and is lost. When B receives I-frame number 5, it discards this frame\nbecause it is out of order and sends an REJ with an N(R) of 4. This causes A to ini-\ntiate retransmission of I-frames previously sent, beginning with frame 4.A may con-\ntinue to send additional frames after the retransmitted frames.\nAn example of error recovery using a timeout is shown in Figure 7.9e. In this\nexample,A transmits I-frame number 3 as the last in a sequence of I-frames.The frame\nsuffers an error. B detects the error and discards it. However, B cannot send an REJ,\nbecause there is no way to know if this was an I-frame.If an error is detected in a frame,\nall of the bits of that frame are suspect, and the receiver has no way to act upon it. A,\nhowever,would have started a timer as the frame was transmitted.This timer has a dura-\ntion long enough to span the expected response time.When the timer expires,A initiates\nrecovery action.This is usually done by polling the other side with an RR command with\nthe P bit set, to determine the status of the other side. Because the poll demands a\nresponse, the entity will receive a frame containing an N(R) field and be able to pro-\nceed. In this case, the response indicates that frame 3 was lost, which A retransmits.\nThese examples are not exhaustive. However, they should give the reader a\ngood feel for the behavior of HDLC.\nAn excellent and very detailed treatment of flow control and error control is to be found in\n[BERT92]. [FIOR95] points out some of the real-world reliability problems with HDLC.\nThere is a large body of literature on the performance of ARQ link control protocols.\nThree classic papers, well worth reading, are [BENE64], [KONH80], and [BUX80].A readable\nsurvey with simplified performance results is [LIN84]. A more recent analysis is [ZORZ96].\nTwo books with good coverage of link-level performance are [SPRA91] and [WALR98].\n[KLEI92] and [KLEI93] are two key papers that look at the implications of gigabit\ndata rates on performance.\n7.5 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nBenice, R. “An Analysis of Retransmission Systems.” IEEE Transactions on\nCommunication Technology, December 1964.\nBertsekas, D., and Gallager, R. Data Networks. Englewood Cliffs, NJ: Prentice\nHall, 1992.\nBux,W.; Kummerle, K.; and Truong, H.“Balanced HDLC Procedures:A Perfor-\nmance Analysis.” IEEE Transactions on Communications, November 1980.\nFiorini, D.; Chiani, M.; Tralli, V.; and Salati, C. “Can We Trust HDLC?” ACM\nComputer Communications Review, October 1995.\nKleinrock, L. “The Latency/Bandwidth Tradeoff in Gigabit Networks.” IEEE\nCommunications Magazine, April 1992.\nKleinrock, L. “On the Modeling and Analysis of Computer Networks.”\nProceedings of the IEEE, August 1993.\nKonheim,A.“A Queuing Analysis of Two ARQ Protocols.” IEEE Transactions\non Communications, July 1980.\nLin, S.; Costello, D.; and Miller, M. “Automatic-Repeat-Request Error-Control\nSchemes.” IEEE Communications Magazine, December 1984.\nSpragins, J.; Hammond, J.; and Pawlikowski, K. Telecommunications: Protocols\nand Design. Reading, MA:Addison-Wesley, 1991.\nWalrand, J. Communication Networks: A First Course. New York: McGraw-\nHill, 1998.\nZorzi, M., and Rao, R. “On the Use of Renewal Theory in the Analysis of\nARQ Protocols.” IEEE Transactions on Communications, September 1996.\nKEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nautomatic repeat request\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nDefine error control.\nList common ingredients for error control for a link control protocol.\nDescribe automatic repeat request (ARQ).\nList and briefly define three versions of ARQ.\nWhat are the station types supported by HDLC? Describe each.\nWhat are the transfer modes supported by HDLC? Describe each.\nWhat is the purpose of the flag field?\nDefine data transparency.\nWhat are the three frame types supported by HDLC? Describe each.\nConsider a half-duplex point-to-point link using a stop-and-wait scheme, in which a\nseries of messages is sent, with each message segmented into a number of frames.\nIgnore errors and frame overhead.\nWhat is the effect on line utilization of increasing the message size so that fewer\nmessages will be required? Other factors remain constant.\nb. What is the effect on line utilization of increasing the number of frames for a con-\nstant message size?\nWhat is the effect on line utilization of increasing frame size?\nThe number of bits on a transmission line that are in the process of actively being trans-\nmitted (i.e.,the number of bits that have been transmitted but have not yet been received)\nis referred to as the bit length of the line. Plot the line distance versus the transmission\nspeed for a bit length of 1000 bits.Assume a propagation velocity of \nA channel has a data rate of 4 kbps and a propagation delay of 20 ms. For what range\nof frame sizes does stop-and-wait give an efficiency of at least 50%?\nConsider the use of 1000-bit frames on a 1-Mbps satellite channel with a 270-ms\ndelay.What is the maximum link utilization for\nStop-and-wait flow control?\nb. Continuous flow control with a window size of 7?\nContinuous flow control with a window size of 127?\nd. Continuous flow control with a window size of 255?\nIn Figure 7.10 frames are generated at node A and sent to node C through node B.\nDetermine the minimum data rate required between nodes B and C so that the\nbuffers of node B are not flooded, based on the following:\nThe data rate between A and B is 100 kbps.\nThe propagation delay is \nfor both lines.\nThere are full duplex lines between the nodes.\nAll data frames are 1000 bits long; ACK frames are separate frames of negligible\nBetween A and B, a sliding-window protocol with a window size of 3 is used.\nBetween B and C, stop-and-wait is used.\nThere are no errors.\nHint: In order not to flood the buffers of B, the average number of frames enter-\ning and leaving B must be the same over a long interval.\n2 * 108 m/s.\nFigure 7.10\nConfiguration for Problem 7.4\n7.5 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nA channel has a data rate of R bps and a propagation delay of t s/km. The distance\nbetween the sending and receiving nodes is L kilometers. Nodes exchange fixed-size\nframes of B bits. Find a formula that gives the minimum sequence field size of the\nframe as a function of R, t, B, and L.(considering maximum utilization).Assume that\nACK frames are negligible in size and the processing at the nodes is instantaneous.\nNo mention was made of reject (REJ) frames in the stop-and-wait ARQ discussion.\nWhy is it not necessary to have REJ0 and REJ1 for stop-and-wait ARQ?\nSuppose that a selective-reject ARQ is used where \nShow, by example, that a\n3-bit sequence number is needed.\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nSuggest improvements to the bit stuffing-algorithm to overcome the problems of sin-\ngle-bit errors described in the preceding problem.\nUsing the example bit string of Figure 7.8, show the signal pattern on the line using\nNRZ-L coding. Does this suggest a side benefit of bit stuffing?\nAssume that the primary HDLC station in NRM has sent six I-frames to a secondary.\nThe primary’s N(S) count was three (011 binary) prior to sending the six frames. If the\npoll bit is on in the sixth frame, what will be the N(R) count back from the secondary\nafter the last frame? Assume error-free operation.\nConsider that several physical links connect two stations. We would like to use a \n“multilink HDLC” that makes efficient use of these links by sending frames on a\nFIFO basis on the next available link.What enhancements to HDLC are needed?\nA World Wide Web server is usually set up to receive relatively small messages from\nits clients but to transmit potentially very large messages to them. Explain, then,\nwhich type of ARQ protocol (selective reject, go-back-N) would provide less of a bur-\nden to a particularly popular WWW server.\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nError-Free Sliding-Window Flow Control\nFor sliding-window flow control, the throughput on the line depends on both the window\nsize W and the value of a. For convenience, let us again normalize frame transmission time\nto a value of 1; thus, the propagation time is a. Figure 7.11 illustrates the efficiency of a full\nduplex point-to-point line.3 Station A begins to emit a sequence of frames at time \nThe leading edge of the first frame reaches station B at \nThe first frame is entirely\nabsorbed by \nAssuming negligible processing time, B can immediately acknowl-\n3For simplicity, we assume that a is an integer, so that an integer number of frames exactly fills the line.\nThe argument does not change for noninteger values of a.\nlink, then the propagation time is (106 meters)/(2 * 108 m/s) = 0.5 * 10-2 seconds.\nAt the other extreme, in terms of distance, is the local area network (LAN).\nDistances range from 0.1 to 10 km, with data rates of 10 Mbps to 1 Gbps; higher\ndata rates tend to be associated with shorter distances. Using a value of\na frame size of 1000 bits, and a data rate of 10 Mbps, the value\nof a is in the range of 0.005 to 0.5. This yields a utilization in the range of 0.5 to\n0.99. For a 100-Mbps LAN, given the shorter distances, comparable utilizations\nare possible.\nWe can see that LANs are typically quite efficient, whereas high-speed\nWANs are not. As a final example, let us consider digital data transmission via\nmodem over a voice-grade line. A typical data rate is 56 kbps. Again, let us con-\nsider a 1000-bit frame. The link distance can be anywhere from a few tens of\nmeters to thousands of kilometers. If we pick, say, as a short distance \nthen a = (56,000 bps * 1000 m) /(2 * 108 m/s * 1000 bits) = 2.8 * 10-4, and utiliza-\ntion is effectively 1.0. Even in a long-distance case, such as \nand efficiency equals 0.26.\na = 156,000 * 5 * 1062/12 * 108 * 1000 bits2 = 1.4\nd = 5000 km,\nd = 1000 m,\nV = 2 * 108 m/s,\n1/3701 = 0.00027.\na = 10.5 * 10-22/12.7 * 10-62 L 1850,\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nWe have seen that sliding-window flow control is more efficient than stop-and-wait\nflow control. We would expect that when error control functions are added that this\nwould still be true: that is, that go-back-N and selective-reject ARQ are more efficient\nthan stop-and-wait ARQ. Let us develop some approximations to determine the\ndegree of improvement to be expected.\nFirst, consider stop-and-wait ARQ. With no errors, the maximum utilization is\nas shown in Equation (7.4).We want to account for the possibility that some\nframes are repeated because of bit errors. To start, note that the utilization U can be\nFor error-free operation using stop-and-wait ARQ,\nis the propagation time. Dividing by \nand remembering that \nagain have Equation (7.4). If errors occur, we must modify Equation (7.7) to\nTt = total time that line is engaged in the transmission of a single frame\nTf = time for transmitter to emit a single frame\nUtilization\nFigure 7.12\nSliding-Window Utilization as a Function of a\nCHAPTER 7 / DATA LINK CONTROL PROTOCOLS\nNote that for \nboth selective-reject and go-back-N ARQ reduce to stop and wait.\nFigure 7.136 compares these three error control techniques for a value of \nfigure and the equations are only approximations. For example, we have ignored errors in\nCHAPTER 8 / MULTIPLEXING\nIt was impossible to get a conversation going, everybody was talking too much.\nTo make efficient use of high-speed telecommunications lines, some\nform of multiplexing is used. Multiplexing allows several transmission\nsources to share a larger transmission capacity. The two common\nforms of multiplexing are frequency division multiplexing (FDM) and\ntime division multiplexing (TDM).\nFrequency division multiplexing can be used with analog signals. A\nnumber of signals are carried simultaneously on the same medium by\nallocating to each signal a different frequency band. Modulation equip-\nment is needed to move each signal to the required frequency band,and\nmultiplexing equipment is needed to combine the modulated signals.\nSynchronous time division multiplexing can be used with digital signals\nor analog signals carrying digital data. In this form of multiplexing, data\nfrom various sources are carried in repetitive frames.Each frame consists\nof a set of time slots, and each source is assigned one or more time slots\nper frame.The effect is to interleave bits of data from the various sources.\nStatistical time division multiplexing provides a generally more effi-\ncient service than synchronous TDM for the support of terminals.\nWith statistical TDM, time slots are not preassigned to particular data\nsources. Rather, user data are buffered and transmitted as rapidly as\npossible using available time slots.\nIn Chapter 7, we described efficient techniques for utilizing a data link under\nheavy load. Specifically, with two devices connected by a point-to-point link, it is\ngenerally desirable to have multiple frames outstanding so that the data link does\nnot become a bottleneck between the stations. Now consider the opposite prob-\nlem. Typically, two communicating stations will not utilize the full capacity of a\ndata link. For efficiency, it should be possible to share that capacity. A generic\nterm for such sharing is multiplexing.\nA common application of multiplexing is in long-haul communications.\nTrunks on long-haul networks are high-capacity fiber, coaxial, or microwave\nlinks.These links can carry large numbers of voice and data transmissions simul-\ntaneously using multiplexing.\nFigure 8.1 depicts the multiplexing function in its simplest form.There are n\ninputs to a multiplexer. The multiplexer is connected by a single data link to a\n1 link, n channels\nMultiplexing\ndemultiplexer. The link is able to carry n separate channels of data. The multi-\nplexer combines (multiplexes) data from the n input lines and transmits over a\nhigher-capacity data link.The demultiplexer accepts the multiplexed data stream,\nseparates (demultiplexes) the data according to channel, and delivers data to the\nappropriate output lines.\nThe widespread use of multiplexing in data communications can be\nexplained by the following:\n• The higher the data rate, the more cost-effective the transmission facility.\nThat is, for a given application and over a given distance, the cost per kbps\ndeclines with an increase in the data rate of the transmission facility. Simi-\nlarly, the cost of transmission and receiving equipment, per kbps, declines\nwith increasing data rate.\n• Most individual data communicating devices require relatively modest\ndata rate support. For example, for many terminal and personal computer\napplications that do not involve Web access or intensive graphics, a data\nrate of between 9600 bps and 64 kbps is generally adequate.\nThe preceding statements were phrased in terms of data communicating\ndevices. Similar statements apply to voice communications. That is, the greater\nthe capacity of a transmission facility, in terms of voice channels, the less the cost\nper individual voice channel, and the capacity required for a single voice channel\nThis chapter concentrates on three types of multiplexing techniques. The\nfirst, frequency division multiplexing (FDM), is the most heavily used and is\nfamiliar to anyone who has ever used a radio or television set. The second is a\nparticular case of time division multiplexing (TDM) known as synchronous\nTDM. This is commonly used for multiplexing digitized voice streams and data\nstreams.The third type seeks to improve on the efficiency of synchronous TDM\nby adding complexity to the multiplexer. It is known by a variety of names,\nincluding statistical TDM, asynchronous TDM, and intelligent TDM. This book\nuses the term statistical TDM, which highlights one of its chief properties. Finally,\nwe look at the digital subscriber line, which combines FDM and synchronous\nTDM technologies.\nCHAPTER 8 / MULTIPLEXING\nCharacteristics\nFDM is possible when the useful bandwidth of the transmission medium exceeds the\nrequired bandwidth of signals to be transmitted. A number of signals can be carried\nsimultaneously if each signal is modulated onto a different carrier frequency and the\ncarrier frequencies are sufficiently separated that the bandwidths of the signals do\nnot significantly overlap. A general case of FDM is shown in Figure 8.2a. Six signal\n(b) Time division multiplexing\n(a) Frequency division multiplexing\nFDM and TDM\n8.1 / FREQUENCY DIVISION MULTIPLEXING\n1The term baseband is used to designate the band of frequencies of the signal delivered by the source and\npotentially used as a modulating signal. Typically, the spectrum of a baseband signal is significant in a\nband that includes or is in the vicinity of f = 0.\nsources are fed into a multiplexer, which modulates each signal onto a different fre-\nEach modulated signal requires a certain bandwidth centered on\nits carrier frequency, referred to as a channel. To prevent interference, the channels\nare separated by guard bands, which are unused portions of the spectrum.\nThe composite signal transmitted across the medium is analog. Note, however,\nthat the input signals may be either digital or analog. In the case of digital input, the\ninput signals must be passed through modems to be converted to analog. In either\ncase, each input analog signal must then be modulated to move it to the appropriate\nfrequency band.\n1f1, Á , f62.\nA generic depiction of an FDM system is shown in Figure 8.4.A number of analog\nor digital signals \nare to be multiplexed onto the same transmission\nmedium. Each signal \nis modulated onto a carrier ; because multiple carriers are\nto be used,each is referred to as a subcarrier.Any type of modulation may be used.The\nresulting analog,modulated signals are then summed to produce a composite baseband1\nFigure 8.4b shows the result. The spectrum of signal \nis shifted to\nbe centered on \nFor this scheme to work,\nmust be chosen so that the bandwidths\nof the various signals do not significantly overlap. Otherwise, it will be impossible to\nrecover the original signals.\nThe composite signal may then be shifted as a whole to another carrier \nfrequency by an additional modulation step. We will see examples of this later.\nThis second modulation step need not use the same modulation technique as the first.\n[mi1t2, i = 1, n]\nEXAMPLE 8.1\nA familiar example of FDM is broadcast and cable television.\nThe television signal discussed in Chapter 3 fits comfortably into a 6-MHz\nbandwidth. Figure 8.3 depicts the transmitted TV signal and its bandwidth. The\nblack-and-white video signal is AM modulated on a carrier signal \nthe baseband video signal has a bandwidth of 4 MHz, we would expect the mod-\nulated signal to have a bandwidth of 8 MHz centered on \nTo conserve band-\nwidth, the signal is passed through a sideband filter so that most of the lower\nsideband is suppressed. The resulting signal extends from about \nA separate color carrier,\nis used to transmit color informa-\ntion. This is spaced far enough from \nthat there is essentially no interference.\nFinally, the audio portion of the signal is modulated on \noutside the effective\nbandwidth of the other two signals. A bandwidth of 50 kHz is allocated for the\naudio signal. The composite signal fits into a 6-MHz bandwidth with the video,\ncolor, and audio signal carriers at 1.25 MHz, 4.799545 MHz, and 5.75 MHz above\nthe lower edge of the band, respectively. Thus, multiple TV signals can be fre-\nquency division multiplexed on a CATV cable, each with a bandwidth of 6 MHz.\nGiven the enormous bandwidth of coaxial cable (as much as 500 MHz), dozens\nof TV signals can be simultaneously carried using FDM. Of course, using radio-\nfrequency propagation through the atmosphere is also a form of FDM.\nfcv + 4.2 MHz.\nfcv - 0.75 MHz\nCHAPTER 8 / MULTIPLEXING\nWhite level\n(a) Amplitude modulation with video signal\n(b) Magnitude spectrum of RF video signal\n4.799545 MHz\nVideo signal\nTransmitted TV Signal\nThe FDM signal s(t) has a total bandwidth B, where \n. This analog\nsignal may be transmitted over a suitable medium. At the receiving end, the FDM\nsignal is demodulated to retrieve \nwhich is then passed through n bandpass\nfilters, each filter centered on \nand having a bandwidth \nway, the signal is again split into its component parts. Each component is then\ndemodulated to recover the original signal.\nEXAMPLE 8.2\nLet us consider a simple example of transmitting three voice\nsignals simultaneously over a medium. As was mentioned, the bandwidth of a\nvoice signal is generally taken to be 4 kHz, with an effective spectrum of 300 to\n3400 Hz (Figure 8.5a). If such a signal is used to amplitude-modulate a 64-kHz\ncarrier, the spectrum of Figure 8.5b results.The modulated signal has a bandwidth\n8.1 / FREQUENCY DIVISION MULTIPLEXING\n(a) Transmitter\n(c) Receiver\n(b) Spectrum of composite baseband modulating signal\nSubcarrier modulator\nSubcarrier modulator\nSubcarrier modulator\nTransmitter\nComposite baseband\nmodulating signal\nBandpass filter, f1\nDemodulator, f1\nBandpass filter, f2\nDemodulator, f2\nBandpass filter, fn\nDemodulator, fn\nComposite baseband\nFDM System [COUC01]\nFigure 8.5 points out two problems that an FDM system must cope with. The\nfirst is crosstalk, which may occur if the spectra of adjacent component signals over-\nlap significantly. In the case of voice signals, with an effective bandwidth of only\n3100 Hz (300 to 3400), a 4-kHz bandwidth is adequate. The spectra of signals pro-\nduced by modems for voiceband transmission also fit well in this bandwidth.\nof 8 kHz, extending from 60 to 68 kHz. To make efficient use of bandwidth, we\nelect to transmit only the lower sideband. If three voice signals are used to mod-\nulate carriers at 64, 68, and 72 kHz, and only the lower sideband of each is taken,\nthe spectrum of Figure 8.5c results.\nCHAPTER 8 / MULTIPLEXING\n(a) Spectrum of voice signal\n(b) Spectrum of voice signal modulated on 64 kHz frequency\n(c) Spectrum of composite signal using subcarriers at 64 kHz, 68 kHz, and 72 kHz\nsideband, s1(t)\nsideband, s2(t)\nsideband, s3(t)\nFDM of Three Voiceband Signals\nAnother potential problem is intermodulation noise, which was discussed in\nChapter 3. On a long link, the nonlinear effects of amplifiers on a signal in one chan-\nnel could produce frequency components in other channels.\nAnalog Carrier Systems\nThe long-distance carrier system provided in the United States and throughout the\nworld is designed to transmit voiceband signals over high-capacity transmission\nlinks, such as coaxial cable and microwave systems.The earliest, and still a very com-\nmon, technique for utilizing high-capacity links is FDM. In the United States,AT&T\nhas designated a hierarchy of FDM schemes to accommodate transmission systems\nof various capacities. A similar, but unfortunately not identical, system has been\nadopted internationally under the auspices of ITU-T (Table 8.1).\nAt the first level of the AT&T hierarchy, 12 voice channels are combined to\nproduce a group signal with a bandwidth of \nin the range 60\nto 108 kHz. The signals are produced in a fashion similar to that described previ-\nously, using subcarrier frequencies of from 64 to 108 kHz in increments of 4 kHz.\n12 * 4 kHz = 48 kHz,\n8.1 / FREQUENCY DIVISION MULTIPLEXING\nThe next basic building block is the 60-channel supergroup, which is formed by fre-\nquency division multiplexing five group signals.At this step, each group is treated as\na single signal with a 48-kHz bandwidth and is modulated by a subcarrier. The\nsubcarriers have frequencies from 420 to 612 kHz in increments of 48 kHz. The\nresulting signal occupies 312 to 552 kHz.\nThere are several variations to supergroup formation. Each of the five inputs\nto the supergroup multiplexer may be a group channel containing 12 multiplexed\nvoice signals. In addition, any signal up to 48 kHz wide whose bandwidth is con-\ntained within 60 to 108 kHz may be used as input to the supergroup multiplexer.As\nanother variation, it is possible to combine 60 voiceband channels into a super-\ngroup. This may reduce multiplexing costs where an interface with existing group\nmultiplexer is not required.\nThe next level of the hierarchy is the mastergroup, which combines 10 super-\ngroup inputs. Again, any signal with a bandwidth of 240 kHz in the range 312 to \n552 kHz can serve as input to the mastergroup multiplexer. The mastergroup has a\nbandwidth of 2.52 MHz and can support 600 voice frequency (VF) channels. Higher-\nlevel multiplexing is defined above the mastergroup, as shown in Table 8.1.\nNote that the original voice or data signal may be modulated many times. For\nexample,a data signal may be encoded using QPSK to form an analog voice signal.This\nsignal could then be used to modulate a 76-kHz carrier to form a component of a group\nsignal. This group signal could then be used to modulate a 516-kHz carrier to form a\ncomponent of a supergroup signal.Each stage can distort the original data;this is so,for\nexample, if the modulator/multiplexer contains nonlinearities or introduces noise.\nWavelength Division Multiplexing\nThe true potential of optical fiber is fully exploited when multiple beams of light at\ndifferent frequencies are transmitted on the same fiber. This is a form of frequency\ndivision multiplexing (FDM) but is commonly called wavelength division multiplex-\ning (WDM).With WDM, the light streaming through the fiber consists of many col-\nors, or wavelengths, each carrying a separate channel of data. In 1997, a landmark\nwas reached when Bell Laboratories was able to demonstrate a WDM system with\n100 beams each operating at 10 Gbps, for a total data rate of 1 trillion bits per second\nNorth American and International FDM Carrier Standards\nNumber of Voice\n312–552 kHz\n812–2044 kHz\nMastergroup\n564–3084 kHz\nMastergroup\n8.516–12.388 MHz\nSupermaster group\nMastergroup multiplex\n0.564–17.548 MHz\n3.124–60.566 MHz\nJumbogroup multiplex\nCHAPTER 8 / MULTIPLEXING\n(also referred to as 1 terabit per second or 1 Tbps). Commercial systems with 160\nchannels of 10 Gbps are now available. In a lab environment,Alcatel has carried 256\nchannels at 39.8 Gbps each, a total of 10.1 Tbps, over a 100-km span.\nA typical WDM system has the same general architecture as other FDM systems.\nA number of sources generate a laser beam at different wavelengths.These are sent to\na multiplexer, which consolidates the sources for transmission over a single fiber line.\nOptical amplifiers, typically spaced tens of kilometers apart, amplify all of the wave-\nlengths simultaneously. Finally, the composite signal arrives at a demultiplexer, where\nthe component channels are separated and sent to receivers at the destination point.\nMost WDM systems operate in the 1550-nm range. In early systems, 200 GHz\nwas allocated to each channel, but today most WDM systems use 50-GHz spacing.\nThe channel spacing defined in ITU-T G.692, which accommodates 80 50-GHz\nchannels, is summarized in Table 8.2.\nThe term dense wavelength division multiplexing (DWDM) is often seen in\nthe literature. There is no official or standard definition of this term. The term con-\nnotes the use of more channels, more closely spaced, than ordinary WDM. In gen-\neral, a channel spacing of 200 GHz or less could be considered dense.\nCharacteristics\nSynchronous time division multiplexing is possible when the achievable data rate\n(sometimes, unfortunately, called bandwidth) of the medium exceeds the data rate\nof digital signals to be transmitted. Multiple digital signals (or analog signals carry-\ning digital data) can be carried on a single transmission path by interleaving por-\ntions of each signal in time. The interleaving can be at the bit level or in blocks of\nITU WDM Channel Spacing (G.692)\nFrequency (THz)\nWavelength in\nVacuum (nm)\n8.2 / SYNCHRONOUS TIME DIVISION MULTIPLEXING\n(a) Transmitter\n(c) Receiver\n(b) TDM frames\nTime slot: may be\nempty or occupied\nSynchronous TDM System\nbytes or larger quantities. For example, the multiplexer in Figure 8.2b has six inputs\nthat might each be, say, 9.6 kbps. A single line with a capacity of at least 57.6 kbps\n(plus overhead capacity) could accommodate all six sources.\nA generic depiction of a synchronous TDM system is provided in Figure 8.6.A\nnumber of signals \nare to be multiplexed onto the same transmission\nmedium.The signals carry digital data and are generally digital signals.The incoming\ndata from each source are briefly buffered. Each buffer is typically one bit or one\ncharacter in length.The buffers are scanned sequentially to form a composite digital\ndata stream \nThe scan operation is sufficiently rapid so that each buffer is emp-\ntied before more data can arrive.Thus, the data rate of \nmust at least equal the\nsum of the data rates of the \nThe digital signal \nmay be transmitted\n[mi1t2, i = 1, n]\nCHAPTER 8 / MULTIPLEXING\ndirectly, or passed through a modem so that an analog signal is transmitted. In either\ncase, transmission is typically synchronous.\nThe transmitted data may have a format something like Figure 8.6b. The data\nare organized into frames. Each frame contains a cycle of time slots. In each frame,\none or more slots are dedicated to each data source.The sequence of slots dedicated\nto one source, from frame to frame, is called a channel. The slot length equals the\ntransmitter buffer length, typically a bit or a byte (character).\nThe byte-interleaving technique is used with asynchronous and synchronous\nsources. Each time slot contains one character of data. Typically, the start and stop\nbits of each character are eliminated before transmission and reinserted by the\nreceiver, thus improving efficiency. The bit-interleaving technique is used with syn-\nchronous sources and may also be used with asynchronous sources. Each time slot\ncontains just one bit.\nAt the receiver, the interleaved data are demultiplexed and routed to the\nappropriate destination buffer. For each input source \nthere is an identical out-\nput destination that will receive the output data at the same rate at which it was\nSynchronous TDM is called synchronous not because synchronous transmis-\nsion is used, but because the time slots are preassigned to sources and fixed. The\ntime slots for each source are transmitted whether or not the source has data to\nsend. This is, of course, also the case with FDM. In both cases, capacity is wasted to\nachieve simplicity of implementation. Even when fixed assignment is used, however,\nit is possible for a synchronous TDM device to handle sources of different data\nrates. For example, the slowest input device could be assigned one slot per cycle,\nwhile faster devices are assigned multiple slots per cycle.\nTDM Link Control\nThe reader will note that the transmitted data stream depicted in Figure 8.6b does\nnot contain the headers and trailers that we have come to associate with synchronous\ntransmission.The reason is that the control mechanisms provided by a data link pro-\ntocol are not needed. It is instructive to ponder this point, and we do so by consider-\ning two key data link control mechanisms:flow control and error control.It should be\nclear that, as far as the multiplexer and demultiplexer (Figure 8.1) are concerned,\nflow control is not needed.The data rate on the multiplexed line is fixed,and the mul-\ntiplexer and demultiplexer are designed to operate at that rate. But suppose that one\nof the individual output lines attaches to a device that is temporarily unable to accept\ndata. Should the transmission of TDM frames cease? Clearly not, because the\nremaining output lines are expecting to receive data at predetermined times. The\nsolution is for the saturated output device to cause the flow of data from the corre-\nsponding input device to cease. Thus, for a while, the channel in question will carry\nempty slots, but the frames as a whole will maintain the same transmission rate.\nThe reasoning for error control is the same. It would not do to request retrans-\nmission of an entire TDM frame because an error occurs on one channel. The\ndevices using the other channels do not want a retransmission nor would they know\nthat a retransmission has been requested by some other device on another channel.\nAgain, the solution is to apply error control on a per-channel basis.\n8.2 / SYNCHRONOUS TIME DIVISION MULTIPLEXING\n(a) Configuration\n(c) Multiplexed data stream\n(b) Input data streams\nInput2\u0007\u0007\u0007 F2\nInput1\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\nf2 F1 d2 f1 d2 f1 d2 d1 d2 d1 C2 d1 A2 C1 F2 A1 f2 F1 f2 f1 d2 f1 d2 d1 d2 d1 d2 d1 C2 C1 A2 A1 F2 F1\nF \u0001 flag field\nA \u0001 address field\nC \u0001 control field\nd \u0001 one octet of data field\nf \u0001 one octet of FCS field\nUse of Data Link Control on TDM Channels\nFlow control and error control can be provided on a per-channel basis\nby using a data link control protocol such as HDLC on a per-channel basis. A\nsimplified example is shown in Figure 8.7. We assume two data sources, each using\nHDLC. One is transmitting a stream of HDLC frames containing three octets of\ndata each, and the other is transmitting HDLC frames containing four octets of\ndata. For clarity, we assume that character-interleaved multiplexing is used,\nalthough bit interleaving is more typical. Notice what is happening.The octets of the\nHDLC frames from the two sources are shuffled together for transmission over the\nmultiplexed line. The reader may initially be uncomfortable with this diagram,\nbecause the HDLC frames have lost their integrity in some sense. For example, each\nframe check sequence (FCS) on the line applies to a disjointed set of bits. Even the\nFCS is not in one piece. However, the pieces are reassembled correctly before they\nare seen by the device on the other end of the HDLC protocol. In this sense, the mul-\ntiplexing/demultiplexing operation is transparent to the attached stations; to each\ncommunicating pair of stations, it appears that they have a dedicated link.\nOne refinement is needed in Figure 8.7. Both ends of the line need to be a com-\nbination multiplexer/demultiplexer with a full-duplex line in between. Then each\nchannel consists of two sets of slots, one traveling in each direction. The individual\ndevices attached at each end can, in pairs, use HDLC to control their own channel.\nThe multiplexer/demultiplexers need not be concerned with these matters.\nFraming We have seen that a link control protocol is not needed to manage the\noverall TDM link. There is, however, a basic requirement for framing. Because we\nare not providing flag or SYNC characters to bracket TDM frames, some means is\nneeded to assure frame synchronization. It is clearly important to maintain framing\nsynchronization because, if the source and destination are out of step, data on all\nchannels are lost.\nPerhaps the most common mechanism for framing is known as added-digit\nframing. In this scheme, typically, one control bit is added to each TDM frame.\nAn identifiable pattern of bits, from frame to frame, is used as a “control channel.”\nCHAPTER 8 / MULTIPLEXING\nA typical example is the alternating bit pattern, 101010. . . .This is a pattern unlikely\nto be sustained on a data channel. Thus, to synchronize, a receiver compares the\nincoming bits of one frame position to the expected pattern. If the pattern does not\nmatch, successive bit positions are searched until the pattern persists over multiple\nframes. Once framing synchronization is established, the receiver continues to\nmonitor the framing bit channel. If the pattern breaks down, the receiver must\nagain enter a framing search mode.\nPulse Stuffing Perhaps the most difficult problem in the design of a synchro-\nnous time division multiplexer is that of synchronizing the various data sources. If\neach source has a separate clock, any variation among clocks could cause loss of\nsynchronization. Also, in some cases, the data rates of the input data streams are\nnot related by a simple rational number. For both these problems, a technique\nknown as pulse stuffing is an effective remedy. With pulse stuffing, the outgoing\ndata rate of the multiplexer, excluding framing bits, is higher than the sum of the\nmaximum instantaneous incoming rates.The extra capacity is used by stuffing extra\ndummy bits or pulses into each incoming signal until its rate is raised to that of a\nlocally generated clock signal. The stuffed pulses are inserted at fixed locations in\nthe multiplexer frame format so that they may be identified and removed at the\ndemultiplexer.\nEXAMPLE 8.3\nAn example, from [COUC01], illustrates the use of synchronous\nTDM to multiplex digital and analog sources (Figure 8.8). Consider that there are\n11 sources to be multiplexed on a single link:\nAnalog, 2-kHz bandwidth\nAnalog, 4-kHz bandwidth\nAnalog, 2-kHz bandwidth\nSources 4–11: Digital, 7200 bps synchronous\nAs a first step, the analog sources are converted to digital using PCM.\nRecall from Chapter 5 that PCM is based on the sampling theorem, which dic-\ntates that a signal be sampled at a rate equal to twice its bandwidth. Thus, the\nrequired sampling rate is 4000 samples per second for sources 1 and 3, and 8000\nsamples per second for source 2. These samples, which are analog (PAM), must\nthen be quantized or digitized. Let us assume that 4 bits are used for each analog\nsample. For convenience, these three sources will be multiplexed first, as a unit.\nAt a scan rate of 4 kHz, one PAM sample each is taken from sources 1 and 3, and\ntwo PAM samples are taken from source 2 per scan.These four samples are inter-\nleaved and converted to 4-bit PCM samples.Thus, a total of 16 bits is generated at\na rate of 4000 times per second, for a composite bit rate of 64 kbps.\nFor the digital sources, pulse stuffing is used to raise each source to a rate of\n8 kbps, for an aggregate data rate of 64 kbps. A frame can consist of multiple\ncycles of 32 bits, each containing 16 PCM bits and two bits from each of the eight\ndigital sources.\n8.2 / SYNCHRONOUS TIME DIVISION MULTIPLEXING\nFrom source 1\n2 kHz, analog\n4 kHz, analog\nTDM PCM signal\nTDM PAM signal\n16 k samples/s\noutput signal\nFrom source 2\nFrom source 3\n2 kHz, analog\nFrom source 4\n7.2 kbps, digital\nFrom source 5\n7.2 kbps, digital\n8 kbps, digital\n8 kbps, digital\n8 kbps, digital\nFrom source 11\n7.2 kbps, digital\nTDM of Analog and Digital Sources [COUC01]\nNorth American and International TDM Carrier Standards\nNorth American\nInternational (ITU-T)\nDesignation\nDigital Carrier Systems\nThe long-distance carrier system provided in the United States and throughout the\nworld was designed to transmit voice signals over high-capacity transmission links,\nsuch as optical fiber, coaxial cable, and microwave. Part of the evolution of these\ntelecommunications networks to digital technology has been the adoption of\nsynchronous TDM transmission structures. In the United States,AT&T developed a\nhierarchy of TDM structures of various capacities; this structure is used in Canada\nand Japan as well as the United States. A similar, but unfortunately not identical,\nhierarchy has been adopted internationally under the auspices of ITU-T (Table 8.3).\nCHAPTER 8 / MULTIPLEXING\n1. The first bit is a framing bit, used for synchronization.\n2. Voice channels:\n\u00078-bit PCM used on five of six frames.\n\u00077-bit PCM used on every sixth frame; bit 8 of each channel is a signaling bit.\n3. Data channels:\n\u0007Channel 24 is used for signaling only in some schemes.\n\u0007Bits 1–7 used for 56-kbps service\n\u0007Bits 2–7 used for 9.6-, 4.8-, and 2.4-kbps service.\nDS-1 Transmission Format\nThe basis of the TDM hierarchy (in North America and Japan) is the DS-1\ntransmission format (Figure 8.9), which multiplexes 24 channels. Each frame con-\ntains 8 bits per channel plus a framing bit for \ntransmission, the following rules apply. Each channel contains one word of digitized\nvoice data.The original analog voice signal is digitized using pulse code modulation\n(PCM) at a rate of 8000 samples per second.Therefore, each channel slot and hence\neach frame must repeat 8000 times per second. With a frame length of 193 bits, we\nhave a data rate of \nFor five of every six frames, 8-bit\nPCM samples are used. For every sixth frame, each channel contains a 7-bit PCM\nword plus a signaling bit. The signaling bits form a stream for each voice channel\nthat contains network control and routing information. For example, control signals\nare used to establish a connection or terminate a call.\nThe same DS-1 format is used to provide digital data service. For compatibility\nwith voice, the same 1.544-Mbps data rate is used. In this case, 23 channels of data\nare provided.The twenty-fourth channel position is reserved for a special sync byte,\nwhich allows faster and more reliable reframing following a framing error. Within\neach channel, 7 bits per frame are used for data, with the eighth bit used to indicate\nwhether the channel, for that frame, contains user data or system control data.With\n7 bits per channel, and because each frame is repeated 8000 times per second, a data\nrate of 56 kbps can be provided per channel. Lower data rates are provided using a\ntechnique known as subrate multiplexing. For this technique, an additional bit is\nrobbed from each channel to indicate which subrate multiplexing rate is being pro-\nvided.This leaves a total capacity per channel of \nThis capacity\nis used to multiplex five 9.6-kbps channels, ten 4.8-kbps channels, or twenty 2.4-kbps\nchannels. For example, if channel 2 is used to provide 9.6-kbps service, then up to\nfive data subchannels share this channel.The data for each subchannel appear as six\nbits in channel 2 every fifth frame.\n6 * 8000 = 48 kbps.\n8000 * 193 = 1.544 Mbps.\n24 * 8 + 1 = 193 bits.\n8.2 / SYNCHRONOUS TIME DIVISION MULTIPLEXING\nSONET/SDH Signal Hierarchy\nSONET Designation\nITU-T Designation\nPayload Rate (Mbps)\n50.112 Mbps\n155.52 Mbps\n150.336 Mbps\n466.56 Mbps\n451.008 Mbps\nSTS-12/OC-12\n622.08 Mbps\n601.344 Mbps\nSTS-18/OC-18\n933.12 Mbps\n902.016 Mbps\nSTS-24/OC-24\n1.24416 Gbps\n1.202688 Gbps\nSTS-36/OC-36\n1.86624 Gbps\n1.804032 Gbps\nSTS-48/OC-48\n2.48832 Gbps\n2.405376 Gbps\nSTS-96/OC-96\n4.87664 Gbps\n4.810752 Gbps\nSTS-192/OC-192\n9.95328 Gbps\n9.621504 Gbps\n39.81312 Gbps\n38.486016 Gbps\n159.25248 Gbps\n153.944064 Gbps\n2In what follows, we will use the term SONET to refer to both specifications. Where differences exist,\nthese will be addressed.\n3An OC-N rate is the optical equivalent of an STS-N electrical signal. End-user devices transmit and\nreceive electrical signals; these must be converted to and from optical signals for transmission over opti-\nFinally, the DS-1 format can be used to carry a mixture of voice and data chan-\nnels. In this case, all 24 channels are utilized; no sync byte is provided.\nAbove the DS-1 data rate of 1.544 Mbps, higher-level multiplexing is achieved\nby interleaving bits from DS-1 inputs. For example, the DS-2 transmission system\ncombines four DS-1 inputs into a 6.312-Mbps stream. Data from the four sources\nare interleaved 12 bits at a time. Note that \nThe remaining\ncapacity is used for framing and control bits.\nSONET (Synchronous Optical Network) is an optical transmission interface orig-\ninally proposed by BellCore and standardized by ANSI. A compatible version,\nreferred to as Synchronous Digital Hierarchy (SDH), has been published by ITU-\nT in Recommendation G.707.2 SONET is intended to provide a specification for\ntaking advantage of the high-speed digital transmission capability of optical fiber.\nSignal Hierarchy The SONET specification defines a hierarchy of standardized\ndigital data rates (Table 8.4). The lowest level, referred to as STS-1 (Synchronous\nTransport Signal level 1) or OC-1 (Optical Carrier level 1),3 is 51.84 Mbps.This rate\ncan be used to carry a single DS-3 signal or a group of lower-rate signals, such as\nDS1, DS1C, DS2, plus ITU-T rates (e.g., 2.048 Mbps).\nMultiple STS-1 signals can be combined to form an STS-N signal.The signal is\ncreated by interleaving bytes from N STS-1 signals that are mutually synchronized.\nFor the ITU-T Synchronous Digital Hierarchy, the lowest rate is 155.52 Mbps,\nwhich is designated STM-1.This corresponds to SONET STS-3.\n1.544 * 4 = 6.176 Mbps.\nCHAPTER 8 / MULTIPLEXING\nTransport overhead\nSynchronous payload envelope (SPE)\nSection overhead\nLine overhead\nPath overhead\n(a) STS-1 frame format\n(b) STM-N frame format\nSection overhead\n9 \u0006 N octets\nSTM-N payload\n261 \u0006 N octets\n270 \u0006 N octets\nFigure 8.10\nSONET/SDH Frame Formats\nFrame Format The basic SONET building block is the STS-1 frame, which con-\nsists of 810 octets and is transmitted once every \nfor an overall data rate of\n51.84 Mbps (Figure 8.10a). The frame can logically be viewed as a matrix of 9 rows\nof 90 octets each, with transmission being one row at a time, from left to right and\ntop to bottom.\nThe first three columns \nof the frame are\ndevoted to overhead octets. Nine octets are devoted to section-related overhead\n13 octets * 9 rows = 27 octets2\n(b) Path overhead\n(a) Transport overhead\nFigure 8.11\nSONET STS-1 Overhead Octets\n8.2 / SYNCHRONOUS TIME DIVISION MULTIPLEXING\nSTS-1 Overhead Bits\nSection Overhead\nhex; used to synchronize the beginning of the frame.\nSTS-1 ID identifies the STS-1 number (1 to N) for each STS-1 within an STS-N multiplex.\nBit-interleaved parity byte providing even parity over previous STS-N frame after scrambling;\nthe ith bit of this octet contains the even parity value calculated from the ith bit position of all\noctets in the previous frame.\nSection level 64-kbps PCM orderwire; optional 64-kbps voice channel to be used between \nsection terminating equipment, hubs, and remote terminals.\n64-kbps channel set aside for user purposes.\n192-kbps data communications channel for alarms, maintenance, control, and administration\nbetween sections.\nLine Overhead\nPointer bytes used in frame alignment and frequency adjustment of payload data.\nBit-interleaved parity for line level error monitoring.\nTwo bytes allocated for signaling between line level automatic protection switching equipment;\nuses a bit-oriented protocol that provides for error protection and management of the SONET\noptical link.\n576-kbps data communications channel for alarms, maintenance, control, monitoring, and \nadministration at the line level.\nReserved for future use.\n64-kbps PCM voice channel for line level orderwire.\nPath Overhead\n64-kbps channel used to send repetitively a 64-octet fixed-length string so a receiving terminal\nCHAPTER 8 / MULTIPLEXING\nFigure 8.11b shows the arrangement of path overhead octets, and Table 8.5 defines\nFigure 8.10b shows the general format for higher-rate frames, using the ITU-T\ndesignation.\nCharacteristics\nIn a synchronous time division multiplexer, it is often the case that many of the time\nslots in a frame are wasted. A typical application of a synchronous TDM involves\nlinking a number of terminals to a shared computer port. Even if all terminals are\nactively in use, most of the time there is no data transfer at any particular terminal.\nAn alternative to synchronous TDM is statistical TDM. The statistical multi-\nplexer exploits this common property of data transmission by dynamically allocating\ntime slots on demand. As with a synchronous TDM, the statistical multiplexer has a\nnumber of I/O lines on one side and a higher-speed multiplexed line on the other.\nEach I/O line has a buffer associated with it. In the case of the statistical multiplexer,\nthere are n I/O lines, but only k, where \ntime slots available on the TDM\nframe.For input,the function of the multiplexer is to scan the input buffers,collecting\ndata until a frame is filled, and then send the frame. On output, the multiplexer\nreceives a frame and distributes the slots of data to the appropriate output buffers.\nBecause statistical TDM takes advantage of the fact that the attached devices\nare not all transmitting all of the time, the data rate on the multiplexed line is less\nthan the sum of the data rates of the attached devices.Thus, a statistical multiplexer\ncan use a lower data rate to support as many devices as a synchronous multiplexer.\nAlternatively, if a statistical multiplexer and a synchronous multiplexer both use a\nlink of the same data rate, the statistical multiplexer can support more devices.\nFigure 8.12 contrasts statistical and synchronous TDM.The figure depicts four\ndata sources and shows the data produced in four time epochs \ncase of the synchronous multiplexer, the multiplexer has an effective output rate of\nfour times the data rate of any of the input devices. During each epoch, data are col-\nlected from all four sources and sent out. For example, in the first epoch, sources C\nand D produce no data. Thus, two of the four time slots transmitted by the multi-\nplexer are empty.\nIn contrast, the statistical multiplexer does not send empty slots if there are\ndata to send. Thus, during the first epoch, only slots for A and B are sent. However,\nthe positional significance of the slots is lost in this scheme. It is not known ahead of\ntime which source’s data will be in any particular slot. Because data arrive from and\nare distributed to I/O lines unpredictably, address information is required to assure\nproper delivery. Thus, there is more overhead per slot for statistical TDM because\neach slot carries an address as well as data.\nThe frame structure used by a statistical multiplexer has an impact on perfor-\nmance. Clearly, it is desirable to minimize overhead bits to improve throughput.Typi-\ncally, a statistical TDM system will use a synchronous protocol such as HDLC.Within\nthe HDLC frame, the data frame must contain control bits for the multiplexing\n1t0, t1, t2, t32.\n8.3 / STATISTICAL TIME DIVISION MULTIPLEXING\nt0 t1 t2 t3 t4\nTo remote computer\nSynchronous time\ndivision multiplexing\nStatistical time\ndivision multiplexing\nFirst cycle\nSecond cycle\nFirst cycle\nSecond cycle\nUnused capacity\nFigure 8.12\nSynchronous TDM Compared with Statistical TDM\nStatistical TDM subframe\n(a) Overall frame\n(b) Subframe with one source per frame\n(c) Subframe with multiple sources per frame\nFigure 8.13\nStatistical TDM Frame Formats\noperation. Figure 8.13 shows two possible formats. In the first case, only one source of\ndata is included per frame. That source is identified by an address. The length of the\ndata field is variable, and its end is marked by the end of the overall frame. This\nscheme can work well under light load but is quite inefficient under heavy load.\nA way to improve efficiency is to allow multiple data sources to be packaged\nin a single frame. Now, however, some means is needed to specify the length of\ndata for each source. Thus, the statistical TDM subframe consists of a sequence of\ndata fields, each labeled with an address and a length. Several techniques can be\nused to make this approach even more efficient. The address field can be reduced\nby using relative addressing. That is, each address specifies the number of the cur-\nrent source relative to the previous source, modulo the total number of sources.\nSo, for example, instead of an 8-bit address field, a 4-bit field might suffice.\nCHAPTER 8 / MULTIPLEXING\nAnother refinement is to use a 2-bit label with the length field. A value of 00,\n01, or 10 corresponds to a data field of 1, 2, or 3 bytes; no length field is necessary.A\nvalue of 11 indicates that a length field is included.\nYet another approach is to multiplex one character from each data source that\nhas a character to send in a single data frame. In this case the frame begins with a bit\nmap that has a bit length equal to the number of sources. For each source that trans-\nmits a character during a given frame, the corresponding bit is set to one.\nPerformance\nWe have said that the data rate of the output of a statistical multiplexer is less than the\nsum of the data rates of the inputs.This is allowable because it is anticipated that the\naverage amount of input is less than the capacity of the multiplexed line.The difficulty\nwith this approach is that, while the average aggregate input may be less than the mul-\ntiplexed line capacity, there may be peak periods when the input exceeds capacity.\nThe solution to this problem is to include a buffer in the multiplexer to hold\ntemporary excess input. Table 8.6 gives an example of the behavior of such systems.\nWe assume 10 sources, each capable of 1000 bps, and we assume that the average\nExample of Statistical Multiplexer Performance\nCapacity \u0001 5000 bps\nCapacity \u0001 7000 bps\naInput = 10 sources, 1000 bps/source; average input \nof maximum.\n8.3 / STATISTICAL TIME DIVISION MULTIPLEXING\ninput per source is 50% of its maximum.Thus, on average, the input load is 5000 bps.\nTwo cases are shown: multiplexers of output capacity 5000 bps and 7000 bps. The\nentries in the table show the number of bits input from the 10 devices each millisec-\nond and the output from the multiplexer.When the input exceeds the output, back-\nlog develops that must be buffered.\nThere is a tradeoff between the size of the buffer used and the data rate of the\nline.We would like to use the smallest possible buffer and the smallest possible data\nrate, but a reduction in one requires an increase in the other. Note that we are not so\nmuch concerned with the cost of the buffer—memory is cheap—as we are with the\nfact that the more buffering there is, the longer the delay.Thus, the tradeoff is really\none between system response time and the speed of the multiplexed line. In this sec-\ntion, we present some approximate measures that examine this tradeoff. These are\nsufficient for most purposes.\nLet us define the following parameters for a statistical time division multiplexer:\nWe have defined M taking into account the overhead bits introduced by the\nmultiplexer. That is, M represents the maximum rate at which data bits can be\ntransmitted.\nThe parameter K is a measure of the compression achieved by the multiplexer.\nFor example, for a given data rate M, if \nthere are four times as many\ndevices being handled as by a synchronous time division multiplexer using the same\nlink capacity.The value of K can be bounded:\ncorresponds to a synchronous time division multiplexer, because\nthe system has the capacity to service all input devices at the same time. If \nthe input will exceed the multiplexer’s capacity.\nSome results can be obtained by viewing the multiplexer as a single-server\nqueue.A queuing situation arises when a “customer” arrives at a service facility and,\nfinding it busy, is forced to wait. The delay incurred by a customer is the time spent\nwaiting in the queue plus the time for the service.The delay depends on the pattern\nof arriving traffic and the characteristics of the server. Table 8.7 summarizes results\nfor the case of random (Poisson) arrivals and constant service time. For details, see\nCHAPTER 8 / MULTIPLEXING\nThe average arrival rate \nin bps, is the total potential input (IR) times the fraction\nthat each source is transmitting.The service time \nin seconds, is the time\nit takes to transmit one bit, which is 1/M. Note that\nThe parameter \nis the utilization or fraction of total link capacity being used. For\nexample, if the capacity M is 50 kbps and \nthe load on the system is 25 kbps.\nThe parameter N in Table 8.7 is a measure of the amount of buffer space being used\nin the multiplexer. Finally,\nis a measure of the average delay encountered by an\ninput source.\nFigure 8.14 gives some insight into the nature of the tradeoff between system\nresponse time and the speed of the multiplexed line. It assumes that data are being\ntransmitted in 1000-bit frames. Figure 8.14a shows the average number of frames\nthat must be buffered as a function of the average utilization of the multiplexed line.\nThe utilization is expressed as a percentage of the total line capacity. Thus, if the\naverage input load is 5000 bps, the utilization is 100% for a line capacity of 5000 bps\nand about 71% for a line capacity of 7000 bps. Figure 8.14b shows the average delay\nexperienced by a frame as a function of utilization and data rate. Note that as the\nutilization rises, so do the buffer requirements and the delay. A utilization above\n80% is clearly undesirable.\nNote that the average buffer size being used depends only on \ndirectly on M. For example, consider the following two cases:\nr = lTs = aIR\nSingle-Server Queues with Constant Service Times and Poisson (Random) Arrivals\nnumber of arrivals per second\ntime for each arrival\nfraction of time server is busy\nnumber of items in system (waiting and being served)\ntime; mean time an item spends in system (waiting and being served)\ndeviation of \n211 - r2 + r\nsr = standard\nTr = residence\nr = utilization;\nTs = service\n8.3 / STATISTICAL TIME DIVISION MULTIPLEXING\n(a) Mean buffer size versus utilization\n(b) Mean delay versus utilization\nLine utilization\nBuffer size (frames)\nLine utilization\nM \u0001 100 kbps\nM \u0001 50 kbps\nM \u0001 25 kbps\nFigure 8.14\nBuffer Size and Delay for a Statistical Multiplexer\nIn both cases, the value of \nis 0.8 and the mean buffer size is \nportionately, a smaller amount of buffer space per source is needed for multiplexers\nthat handle a larger number of sources. Figure 8.14b also shows that the average\ndelay will be smaller as the link capacity increases, for constant utilization.\nSo far, we have been considering average queue length, and hence the average\namount of buffer capacity needed. Of course, there will be some fixed upper bound\non the buffer size available. The variance of the queue size grows with utilization.\nThus, at a higher level of utilization, a larger buffer is needed to hold the backlog.\nEven so, there is always a finite probability that the buffer will overflow. Figure 8.15\nM = 5000 bps\nM = 500 bps\nR = 100 bps\nR = 100 bps\nCHAPTER 8 / MULTIPLEXING\nshows the strong dependence of overflow probability on utilization. This figure and\nFigure 8.14, suggest that utilization above about 0.8 is undesirable.\nCable Modem\nTo support data transfer to and from a cable modem, a cable TV provider dedicates\ntwo channels, one for transmission in each direction. Each channel is shared by a\nnumber of subscribers, and so some scheme is needed for allocating capacity on\neach channel for transmission. Typically, a form of statistical TDM is used, as illus-\ntrated in Figure 8.16. In the downstream direction, cable headend to subscriber, a\ncable scheduler delivers data in the form of small packets. Because the channel is\nshared by a number of subscribers, if more than one subscriber is active, each sub-\nscriber gets only a fraction of the downstream capacity.An individual cable modem\nsubscriber may experience access speeds from 500 kbps to 1.5 Mbps or more,\ndepending on the network architecture and traffic load. The downstream direction\nis also used to grant time slots to subscribers. When a subscriber has data to trans-\nmit, it must first request time slots on the shared upstream channel. Each subscriber\nis given dedicated time slots for this request purpose. The headend scheduler\nresponds to a request packet by sending back an assignment of future time slots to\nbe used by this subscriber. Thus, a number of subscribers can share the same\nupstream channel without conflict.\nBuffer size (characters)\nProbability of overflow\nFigure 8.15\nProbabilty of Overflow as a Function\nof Buffer Size\n8.4 / ASYMMETRIC DIGITAL SUBSCRIBER LINE\n2 minislots\nfor station X \nfrom station B \nRequest from\nfor station Y \nfrom station X \nFigure 8.16\nCable Modem Scheme [DUTT99]\nIn the implementation and deployment of a high-speed wide area public digital net-\nwork, the most challenging part is the link between subscriber and network: the dig-\nital subscriber line. With billions of potential endpoints worldwide, the prospect of\ninstalling new cable for each new customer is daunting. Instead, network designers\nhave sought ways of exploiting the installed base of twisted-pair wire that links\nvirtually all residential and business customers to telephone networks. These links\nwere installed to carry voice-grade signals in a bandwidth from zero to 4 kHz. How-\never, the wires are capable of transmitting signals over a far broader spectrum—\n1 MHz or more.\nADSL is the most widely publicized of a family of new modem technologies\ndesigned to provide high-speed digital data transmission over ordinary telephone\nwire.ADSL is now being offered by a number of carriers and is defined in an ANSI\nstandard. In this section, we first look at the overall design of ADSL and then exam-\nine the key underlying technology, known as DMT.\nADSL Design\nThe term asymmetric refers to the fact that ADSL provides more capacity down-\nstream (from the carrier’s central office to the customer’s site) than upstream (from\ncustomer to carrier). ADSL was originally targeted at the expected need for video\non demand and related services. This application has not materialized. However,\nsince the introduction of ADSL technology, the demand for high-speed access to the\nInternet has grown. Typically, the user requires far higher capacity for downstream\nthan for upstream transmission. Most user transmissions are in the form of key-\nboard strokes or transmission of short e-mail messages, whereas incoming traffic,\nespecially Web traffic, can involve large amounts of data and include images or even\nvideo.Thus,ADSL provides a perfect fit for the Internet requirement.\nCHAPTER 8 / MULTIPLEXING\nADSL uses frequency division multiplexing (FDM) in a novel way to exploit\nthe 1-MHz capacity of twisted pair. There are three elements of the ADSL strategy\n(Figure 8.17):\n• Reserve lowest 25 kHz for voice, known as POTS (plain old telephone ser-\nvice). The voice is carried only in the 0 to 4 kHz band; the additional band-\nwidth is to prevent crosstalk between the voice and data channels.\n• Use either echo cancellation4 or FDM to allocate two bands, a smaller\nupstream band and a larger downstream band.\n• Use FDM within the upstream and downstream bands. In this case, a single bit\nstream is split into multiple parallel bit streams and each portion is carried in a\nseparate frequency band.\n4Echo cancellation is a signal processing technique that allows transmission of digital signals in both\ndirections on a single transmission line simultaneously. In essence, a transmitter must subtract the echo of\nits own transmission from the incoming signal to recover the signal sent by the other side.\n(a) Frequency division multiplexing\n(b) Echo cancellation\ncancellation\nFigure 8.17\nADSL Channel Configuration\n8.4 / ASYMMETRIC DIGITAL SUBSCRIBER LINE\nBits per hertz\nBits per hertz\nFigure 8.18\nDMT Bits per Channel Allocation\nWhen echo cancellation is used, the entire frequency band for the upstream\nchannel overlaps the lower portion of the downstream channel. This has two advan-\ntages compared to the use of distinct frequency bands for upstream and downstream.\n• The higher the frequency,the greater the attenuation.With the use of echo cancel-\nlation, more of the downstream bandwidth is in the “good” part of the spectrum.\n• The echo cancellation design is more flexible for changing upstream capacity.\nThe upstream channel can be extended upward without running into the\ndownstream; instead, the area of overlap is extended.\nThe disadvantage of the use of echo cancellation is the need for echo cancella-\ntion logic on both ends of the line.\nThe ADSL scheme provides a range of up to 5.5 km, depending on the diame-\nter of the cable and its quality. This is sufficient to cover about 95% of all U.S. sub-\nscriber lines and should provide comparable coverage in other nations.\nDiscrete Multitone\nDiscrete multitone (DMT) uses multiple carrier signals at different frequencies,\nsending some of the bits on each channel. The available transmission band\n(upstream or downstream) is divided into a number of 4-kHz subchannels. On ini-\ntialization, the DMT modem sends out test signals on each subchannel to determine\nthe signal-to-noise ratio. The modem then assigns more bits to channels with better\nsignal transmission qualities and less bits to channels with poorer signal transmis-\nsion qualities. Figure 8.18 illustrates this process. Each subchannel can carry a data\nrate of from 0 to 60 kbps. The figure shows a typical situation in which there is\nincreasing attenuation and hence decreasing signal-to-noise ratio at higher frequen-\ncies.As a result, the higher-frequency subchannels carry less of the load.\nFigure 8.19 provides a general block diagram for DMT transmission.After ini-\ntialization, the bit stream to be transmitted is divided into a number of substreams,\none for each subchannel that will carry data. The sum of the data rates of the sub-\nstreams is equal to the total data rate. Each substream is then converted to an ana-\nlog signal using quadrature amplitude modulation (QAM), described in Chapter 5.\nThis scheme works easily because of QAM’s ability to assign different numbers of\nbits per transmitted signal. Each QAM signal occupies a distinct frequency band, so\nthese signals can be combined by simple addition to produce the composite signal\nfor transmission.\nCHAPTER 8 / MULTIPLEXING\nPresent ADSL/DMT designs employ 256 downstream subchannels. In theory,\nwith each 4-kHz subchannel carrying 60 kbps, it would be possible to transmit at a\nrate of 15.36 Mbps. In practice, transmission impairments prevent attainment of this\ndata rate. Current implementations operate at from 1.5 to 9 Mbps, depending on\nline distance and quality.\nADSL is one of a number of recent schemes for providing high-speed digital trans-\nmission of the subscriber line. Table 8.8 summarizes and compares some of the most\nimportant of these new schemes, which collectively are referred to as xDSL.\nHigh Data Rate Digital Subscriber Line\nHDSL was developed in the late 1980s by BellCore to provide a more cost-effective\nmeans of delivering a T1 data rate (1.544 Mbps). The standard T1 line uses alternate\nmark inversion (AMI) coding,which occupies a bandwidth of about 1.5 MHz.Because\nsuch high frequencies are involved, the attenuation characteristics limit the use of T1\nto a distance of about 1 km between repeaters.Thus, for many subscriber lines one or\nmore repeaters are required, which adds to the installation and maintenance expense.\nHDSL uses the 2B1Q coding scheme to provide a data rate of up to 2 Mbps\nover two twisted-pair lines within a bandwidth that extends only up to about 196\nkHz.This enables a range of about 3.7 km to be achieved.\nFigure 8.19\nDMT Transmitter\nserial-to-parallel\nfi\u00041 \u0001 fi \u0004 4 kHz\n8.6 / RECOMMENDED READING AND WEB SITES\nComparison of xDSL Alternatives\n1.5 to 9 Mbps\n1.544 or 2.048 Mbps\n1.544 or 2.048 Mbps\n13 to 52 Mbps \n16 to 640 kbps\n1.5 to 2.3 Mbps \nCopper pairs\nRange (24-gauge UTP)\n3.7 to 5.5 km\ntwisted pair\nUTP = unshielded\nSingle Line Digital Subscriber Line\nAlthough HDSL is attractive for replacing existing T1 lines, it is not suitable for resi-\ndential subscribers because it requires two twisted pair, whereas the typical residen-\ntial subscriber has a single twisted pair.SDSL was developed to provide the same type\nof service as HDSL but over a single twisted-pair line.As with HDSL, 2B1Q coding is\nused. Echo cancellation is used to achieve full-duplex transmission over a single pair.\nVery High Data Rate Digital Subscriber Line\nOne of the newest xDSL schemes is VDSL.As of this writing, many of the details of\nthis signaling specification remain to be worked out. The objective is to provide a\nscheme similar to ADSL at a much higher data rate by sacrificing distance. The\nlikely signaling technique is DMT/QAM.\nVDSL does not use echo cancellation but provides separate bands for differ-\nent services, with the following tentative allocation:\n• POTS: 0–4 kHz\n• ISDN: 4–80 kHz\n• Upstream: 300–700 kHz\n• Downstream:\nA discussion of FDM and TDM carrier systems can be found in [FREE98] and [CARN99].\nSONET is treated in greater depth in [STAL99] and in [TEKT01]. Useful articles on SONET\nare [BALL89] and [BOEH90[ A good overview of WDM is [MUKH00].\nTwo good articles on cable modems are [FELL01] and [CICI01].\n[MAXW96] provides a useful a discussion of ADSL. Recommended treatments of\nxDSL are [HAWL97] and [HUMP97].\nCHAPTER 8 / MULTIPLEXING\nBallart, R., and Ching, Y. “SONET: Now It’s the Standard Optical Network.”\nIEEE Communications Magazine, March 1989.\nBoehm, R. “Progress in Standardization of SONET.” IEEE LCS, May 1990.\nCarne, E. Telecommunications Primer: Data,Voice, and Video Communications.\nUpper Saddle River, NJ: Prentice Hall, 1999.\nCiciora,W.“The Cable Modem.” IEEE Spectrum, June 2001.\nFellows, D., and Jones, D. “DOCSIS Cable Modem Technology.” IEEE\nCommunications Magazine, March 2001.\nFreeman, R. Telecommunications Transmission Handbook. New York: Wiley,\nHawley, G. “Systems Considerations for the Use of xDSL Technology for\nData Access.” IEEE Communications Magazine, March 1997.\nHumphrey, M., and Freeman, J. “How xDSL Supports Broadband Services to\nthe Home.” IEEE Network, January/March 1997.\nMaxwell, K. “Asymmetric Digital Subscriber Line: Interim Technology for\nthe Next Forty Years.” IEEE Communications Magazine, October 1996.\nMUKH00 Mukherjee, B. “WDM Optical Communication Networks: Progress and\nChallenges.” IEEE Journal on Selected Areas in Communications, October 2000.\nStallings, W. ISDN and Broadband ISDN, with Frame Relay and ATM. Upper\nSaddle River, NJ: Prentice Hall, 1999.\nTektronix. SONET Telecommunications Standard Primer. Tektronix White\nPaper, 2001, www.tektronix.com/optical.\nRecommended Web sites:\n• DSL Forum: Includes a FAQ and technical information about ADSL and other xDSL\ntechnologies\n• Network and Services Integration Forum: Discusses current products, technology,\nand standards\n• SONET Home Page: Useful links, tutorials, white papers, FAQs\nKEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\ncable modem\ndemultiplexer\ndigital carrier system\ndiscrete multitone\necho cancellation\nfrequency division\nmultiplexing (FDM)\nmultiplexer\nmultiplexing\n8.7 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nReview Questions\nWhy is multiplexing so cost-effective?\nHow is interference avoided by using frequency division multiplexing?\nWhat is echo cancellation?\nDefine upstream and downstream with respect to subscriber lines.\nExplain how synchronous time division multiplexing (TDM) works.\nWhy is a statistical time division multiplexer more efficient than a synchronous time\ndivision multiplexer?\nUsing Table 8.3 as a guide, indicate the major difference between North American\nand international TDM carrier standards.\nUsing Figure 8.14 as a guide, indicate the relationship between buffer size and line\nutilization.\nThe information in four analog signals is to be multiplexed and transmitted over a\ntelephone channel that has a 400- to 3100-Hz bandpass. Each of the analog baseband\nsignals is bandlimited to 500 Hz. Design a communication system (block diagram)\nthat will allow the transmission of these four sources over the telephone channel using\nFrequency division multiplexing with SSB (single sideband) subcarriers\nb. Time division multiplexing using PCM; assume 4-bit samples\nShow the block diagrams of the complete system, including the transmission, channel,\nand reception portions. Include the bandwidths of the signals at the various points in\nthe systems.\nTo paraphrase Lincoln: . . . all of the channel some of the time, some of the channel all\nof the time. . . . Refer to Figure 8.2 and relate the preceding to the figure.\nConsider a transmission system using frequency division multiplexing.What cost fac-\ntors are involved in adding one more pair of stations to the system?\nIn synchronous TDM, it is possible to interleave bits, one bit from each channel par-\nticipating in a cycle. If the channel is using a self-clocking code to assist synchroniza-\ntion, might this bit interleaving introduce problems because there is not a continuous\nstream of bits from one source?\nWhy is it that the start and stop bits can be eliminated when character interleaving is\nused in synchronous TDM?\nExplain in terms of data link control and physical layer concepts how error and flow\ncontrol are accomplished in synchronous time division multiplexing.\nOne of the 193 bits in the DS-1 transmission format is used for frame synchroniza-\ntion. Explain its use.\nIn the DS-1 format, what is the control signal data rate for each voice channel?\nTwenty-four voice signals are to be multiplexed and transmitted over twisted pair.\nWhat is the bandwidth required for FDM? Assuming a bandwidth efficiency (ratio of\npulse stuffing\nstatistical TDM\nsynchronous TDM\ntime division multiplexing\nwavelength division \nmultiplexing (WDM)\nCHAPTER 8 / MULTIPLEXING\ndata rate to transmission bandwidth, as explained in Chapter 5) of 1 bps/Hz, what is\nthe bandwidth required for TDM using PCM?\nDraw a block diagram similar to Figure 8.8 for a TDM PCM system that will accommo-\ndate four 300-bps, synchronous, digital inputs and one analog input with a bandwidth of\n500 Hz.Assume that the analog samples will be encoded into 4-bit PCM words.\nA character-interleaved time division multiplexer is used to combine the data streams\nof a number of 110-bps asynchronous terminals for data transmission over a 2400-bps\ndigital line.Each terminal sends asynchronous characters consisting of 7 data bits,1 par-\nity bit, 1 start bit, and 2 stop bits. Assume that one synchronization character is sent\nevery 19 data characters and, in addition, at least 3% of the line capacity is reserved for\npulse stuffing to accommodate speed variations from the various terminals.\nDetermine the number of bits per character.\nb. Determine the number of terminals that can be accommodated by the multiplexer.\nSketch a possible framing pattern for the multiplexer.\nFind the number of the following devices that could be accommodated by a T1-type\nTDM line if 1% of the T1 line capacity is reserved for synchronization purposes.\n110-bps teleprinter terminals\nb. 300-bps computer terminals\n1200-bps computer terminals\nd. 9600-bps computer output ports\n64-kbps PCM voice-frequency lines\nHow would these numbers change if each of the sources were transmitting an average\nof 10% of the time and a statistical multiplexer was used?\nTen 9600-bps lines are to be multiplexed using TDM. Ignoring overhead bits in the\nTDM frame, what is the total capacity required for synchronous TDM? Assuming\nthat we wish to limit average TDM link utilization to 0.8, and assuming that each\nTDM link is busy 50% of the time, what is the capacity required for statistical\nA synchronous nonstatistical TDM is to be used to combine four 4.8-kbps and one\n9.6-kbps signals for transmission over a single leased line. For framing, a block of\n7 bits (pattern 1011101) is inserted for each 48 data bits. The reframing algorithm (at\nthe receiving demultiplex) is as follows:\nArbitrarily select a bit position.\nConsider the block of 7 contiguous bits starting with that position.\nObserve that block of 7 bits each frame for 12 consecutive frames.\nIf 10 of the 12 blocks match the framing pattern the system is “in-frame”; if not\nadvance one bit position and return to step 2.\na. Draw the multiplexed bit stream (note that the 9.6kbps input may be treated as\ntwo 4.8-kbps inputs).\nb. What is the % overhead in the multiplexed bit stream?\nc. What is the multiplexed output bit rate?\nd. What is the minimum reframe time? What is the maximum reframe time?\nWhat is the Average reframe time?\nA company has two locations: a headquarters and a factory about 25 km away. The\nfactory has four 300-bps terminals that communicate with the central computer facil-\nities over leased voice-grade lines.The company is considering installing TDM equip-\nment so that only one line will be needed. What cost factors should be considered in\nthe decision?\nIn synchronous TDM, the I/O lines serviced by the two multiplexers may be either\nsynchronous or asynchronous although the channel between the two multiplexers\nmust be synchronous. Is there any inconsistency in this? Why or why not?\nAssume that you are to design a TDM carrier, say DS-489, to support 30 voice channels\nusing 6-bit samples and a structure similar to DS-1. Determine the required bit rate.\n8.7 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nFor a statistical time division multiplexer, define the following parameters:\nExpress F as a function of the other parameters. Explain why F can be viewed as a\nvariable rather than a constant.\nb. Plot F versus L for \nand values of \n80, 120. Comment on\nthe results and compare to Figure 8.14.\nPlot F versus L for \nand values of \nand 8.2 kbps. Comment\non the results and compare to Figure 8.14.\nIn statistical TDM, there may be a length field.What alternative could there be to the\ninclusion of a length field? What problem might this solution cause and how could it\nC = 9.6 kbps\nC = 9.6 kbps\nC = capacity of link, bps\nL = load of data in the frame, bps\nOH = overhead in a frame, bits\nF = frame length, bits\nThe Concept of Spread Spectrum\nFrequency-Hopping Spread Spectrum\nDirect Sequence Spread Spectrum\nCode Division Multiple Access\nRecommended Reading and Web Site\nKey Terms, Review Questions, and Problems\n1Spread spectrum (using frequency hopping) was invented, believe it or not, by Hollywood screen siren\nHedy Lamarr in 1940 at the age of 26. She and a partner who later joined her effort were granted a patent\nin 1942 (U.S. Patent 2,292,387; 11 August 1942). Lamarr considered this her contribution to the war effort\nand never profited from her invention.\nSpread spectrum is an important form of encoding for wireless communications.\nThis technique does not fit neatly into the categories defined in Chapter 5, as it\ncan be used to transmit either analog or digital data, using an analog signal.\nThe spread spectrum technique was developed initially for military and\nintelligence requirements. The essential idea is to spread the information signal\nover a wider bandwidth to make jamming and interception more difficult. The\nfirst type of spread spectrum developed is known as frequency hopping.1 A more\nrecent type of spread spectrum is direct sequence. Both of these techniques are\nused in various wireless communications standards and products.\nAfter a brief overview, we look at these two spread spectrum techniques.\nWe then examine a multiple access technique based on spread spectrum.\nSpread spectrum is an important form of encoding for wireless com-\nmunications. The use of spread spectrum makes jamming and inter-\nception more difficult and provides improved reception.\nThe basic idea of spread spectrum is to modulate the signal so as to\nincrease significantly the bandwidth (spread the spectrum) of the\nsignal to be transmitted.\nFrequency-hopping spread spectrum is a form of spread spectrum in\nwhich the signal is broadcast over a seemingly random series of radio\nfrequencies, hopping from frequency to frequency at fixed intervals.\nDirect sequence spread spectrum is a form of spread spectrum in\nwhich each bit in the original signal is represented by multiple bits in\nthe transmitted signal, using a spreading code.\nCode division multiple access exploits the nature of spread spectrum\ntransmission to enable multiple users to independently use the same\nbandwidth with very little interference.\nAll creative people want to do the unexpected.\n—Ecstasy and Me: My Life as a Woman, Hedy Lamarr\nCHAPTER 9 / SPREAD SPECTRUM\nOutput data\nPseudonoise\nPseudonoise\nGeneral Model of Spread Spectrum Digital Communication System\n2See [STAL05] for a more detailed discussion of pseudorandom numbers.\nFigure 9.1 highlights the key characteristics of any spread spectrum system. Input is\nfed into a channel encoder that produces an analog signal with a relatively narrow\nbandwidth around some center frequency. This signal is further modulated using a\nsequence of digits known as a spreading code or spreading sequence. Typically, but\nnot always, the spreading code is generated by a pseudonoise, or pseudorandom\nnumber, generator. The effect of this modulation is to increase significantly the\nbandwidth (spread the spectrum) of the signal to be transmitted. On the receiving\nend, the same digit sequence is used to demodulate the spread spectrum signal.\nFinally, the signal is fed into a channel decoder to recover the data.\nSeveral things can be gained from this apparent waste of spectrum:\n• The signals gains immunity from various kinds of noise and multipath distor-\ntion. The earliest applications of spread spectrum were military, where it was\nused for its immunity to jamming.\n• It can also be used for hiding and encrypting signals. Only a recipient who\nknows the spreading code can recover the encoded information.\n• Several users can independently use the same higher bandwidth with very lit-\ntle interference.This property is used in cellular telephony applications, with a\ntechnique know as code division multiplexing (CDM) or code division multi-\nple access (CDMA).\nA comment about pseudorandom numbers is in order. These numbers are\ngenerated by an algorithm using some initial value called the seed.The algorithm is\ndeterministic and therefore produces sequences of numbers that are not statistically\nrandom. However, if the algorithm is good, the resulting sequences will pass many\nreasonable tests of randomness. Such numbers are often referred to as pseudoran-\ndom numbers.2 The important point is that unless you know the algorithm and the\nseed, it is impractical to predict the sequence. Hence, only a receiver that shares this\ninformation with a transmitter will be able to decode the signal successfully.\n9.2 / FREQUENCY-HOPPING SPREAD SPECTRUM\nFREQUENCY-HOPPING SPREAD SPECTRUM\nWith frequency-hopping spread spectrum (FHSS), the signal is broadcast over a\nseemingly random series of radio frequencies, hopping from frequency to frequency\nat fixed intervals. A receiver, hopping between frequencies in synchronization with\nthe transmitter, picks up the message. Would-be eavesdroppers hear only unintelli-\ngible blips. Attempts to jam the signal on one frequency succeed only at knocking\nout a few bits of it.\nBasic Approach\nFigure 9.2 shows an example of a frequency-hopping signal.A number of channels are\nallocated for the FH signal.Typically,there are \ncarrier frequencies forming \nnels. The spacing between carrier frequencies and hence the width of each channel\nusually corresponds to the bandwidth of the input signal.The transmitter operates in\none channel at a time for a fixed interval; for example, the IEEE 802.11 standard uses\na 300-ms interval. During that interval, some number of bits (possibly a fraction of a\nbit, as discussed subsequently) is transmitted using some encoding scheme.A spread-\ning code dictates the sequence of channels used. Both transmitter and receiver use the\nsame code to tune into a sequence of channels in synchronization.\nA typical block diagram for a frequency-hopping system is shown in Figure\n9.3. For transmission, binary data are fed into a modulator using some digital-to-\nanalog encoding scheme, such as frequency shift keying (FSK) or binary phase shift\nkeying (BPSK). The resulting signal is centered on some base frequency. A\nCHAPTER 9 / SPREAD SPECTRUM\n(a) Transmitter\nPseudonoise\nsynthesizer\nfilter (about\nBinary data\nSpread spectrum\nFH spreader\n(b) Receiver\nSpread spectrum\nDemodulator\nBinary data\nPseudonoise\nsynthesizer\nfilter (about\nFH despreader\nFrequency-Hopping Spread Spectrum System\nthe spread spectrum signal is demodulated using the same sequence of PN-derived\nfrequencies and then demodulated to produce the output data.\nFigure 9.3 indicates that the two signals are multiplied. Let us give an example\nof how this works, using BFSK as the data modulation scheme. We can define the\nFSK input to the FHSS system as [compare to Equation (5.3)]:\nT = bit duration; data rate = 1/T\n¢f = frequency separation\nbi = value of the ith bit of data 1+1 for binary 1, -1 for binary 02\nf0 = base frequency\nA = amplitude of signal\nsd1t2 = A cos12p1f0 + 0.51bi + 12 ¢f2t2\nfor iT 6 t 6 1i + 12T\n9.2 / FREQUENCY-HOPPING SPREAD SPECTRUM\nThus, during the ith bit interval, the frequency of the data signal is \nif the data bit is\nif the data bit is \nThe frequency synthesizer generates a constant-frequency tone whose fre-\nquency hops among a set of \nfrequencies, with the hopping pattern determined by\nk bits from the PN sequence. For simplicity, assume the duration of one hop is the\nsame as the duration of one bit and we ignore phase differences between the data\nand the spreading signal, also called a chipping signal, c(t). Then the\nproduct signal during the ith hop (during the ith bit) is\nis the frequency of the signal generated by the frequency synthesizer during\nthe ith hop. Using the trigonometric identity3\nA bandpass filter (Figure 9.3) is used to block the difference frequency and pass the\nsum frequency, yielding an FHSS signal of\nThus, during the ith bit interval, the frequency of the data signal is \nif the data\nif the data bit is \nAt the receiver, a signal of the form s(t) just defined will be received. This is\nmultiplied by a replica of the spreading signal to yield a product signal of the form\nAgain using the trigonometric identity, we have\nA bandpass filter (Figure 9.3) is used to block the sum frequency and pass the dif-\nference frequency, yielding a signal of the form of \ndefined in Equation (9.1):\nFHSS Using MFSK\nA common modulation technique used in conjunction with FHSS is multiple FSK\n(MFSK). Recall from Chapter 5 that MFSK uses \ndifferent frequencies\nto encode the digital input L bits at a time. The transmitted signal is of the form\n(Equation 5.4):\nsi1t2 = A cos 2pfit,\n0.25A cos12p1f0 + 0.51bi + 12 ¢f2t2\n+ cos12p1f0 + 0.51bi + 12 ¢f2t2]\np1t2 = s1t2c1t2 = 0.25A[cos12p1f0 + 0.51bi + 12 ¢f + fi + fi2t2\np1t2 = s1t2c1t2 = 0.5A cos12p1f0 + 0.51bi + 12 ¢f + fi2t2cos12pfit2\nf0 + fi + ¢f\ns1t2 = 0.5A cos12p1f0 + 0.51bi + 12 ¢f + fi2t2\n+ cos12p1f0 + 0.51bi + 12 ¢f - fi2t2]\np1t2 = 0.5A [cos12p1f0 + 0.51bi + 12 ¢f + fi2t2\ncos1x - y22,\ncos1x2cos1y2 = 11/221cos1x + y2 +\np1t2 = sd1t2c1t2 = A cos12p1f0 + 0.51bi + 12 ¢f2t2cos12pfit2\n3See the math refresher document at WilliamStallings.com/StudentSupport.html for a summary of\ntrigonometric identities.\nCHAPTER 9 / SPREAD SPECTRUM\nInput binary data\nPN sequence\n1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1\nSlow Frequency Hop Spread Spectrum Using MFSK 1M = 4, k = 22\n4Some authors use a somewhat different definition (e.g., [PICK82]) of multiple hops per bit for fast fre-\nquency hop, multiple bits per hop for slow frequency hop, and one hop per bit if neither fast nor slow.The\nmore common definition, which we use, relates hops to signal elements rather than bits.\nFor FHSS, the MFSK signal is translated to a new frequency every \nby modulating the MFSK signal with the FHSS carrier signal. The effect is to\ntranslate the MFSK signal into the appropriate FHSS channel. For a data rate of R,\nthe duration of a bit is \nseconds and the duration of a signal element is\nseconds. If \nis greater than or equal to \n, the spreading modulation is\nreferred to as slow-frequency-hop spread spectrum; otherwise it is known as \nfast-frequency-hop spread spectrum.4 To summarize,\nSlow-frequency-hop spread spectrum\nFast-frequency-hop spread spectrum\nFigure 9.4 shows an example of slow FHSS, using the MFSK example from\nFigure 5.9. Here we have \nwhich means that four different frequencies are\nused to encode the data input 2 bits at a time. Each signal element is a discrete fre-\nquency tone, and the total MFSK bandwidth is \nWe use an FHSS scheme\nThat is, there are \ndifferent channels, each of width \nFHSS bandwidth is \nEach 2 bits of the PN sequence is used to select one\nof the four channels. That channel is held for a duration of two signal elements, or\nfour bits 1Tc = 2Ts = 4T2.\nL = number of bits per signal element\nM = number of different signal elements = 2L\nfd = denotes the difference frequency\nfc = denotes the carrier frequency\nfi = fc + 12i - 1 - M2fd\n9.2 / FREQUENCY-HOPPING SPREAD SPECTRUM\nInput binary data\nPN sequence\nFast Frequency Hop Spread Spectrum Using MFSK 1M = 4, k = 22\nCHAPTER 9 / SPREAD SPECTRUM\nFigure 9.5 shows an example of fast FHSS, using the same MFSK example.\nIn this case, however, each signal element is represented\nby two frequency tones. Again,\nIn this example\nIn general, fast FHSS provides improved performance compared\nto slow FHSS in the face of noise or jamming. For example, if three or more fre-\nquencies (chips) are used for each signal element, the receiver can decide which\nsignal element was sent on the basis of a majority of the chips being correct.\nFHSS Performance Considerations\nTypically, a large number of frequencies is used in FHSS so that \nis much larger than\nOne benefit of this is that a large value of k results in a system that is quite resistant\nto jamming. For example, suppose we have an MFSK transmitter with bandwidth \nand a noise jammer of the same bandwidth and fixed power \non the signal carrier fre-\nquency.Then we have a ratio of signal energy per bit to noise power density per Hertz of\nIf frequency hopping is used, the jammer must jam all \nfrequencies. With a fixed\npower, this reduces the jamming power in any one frequency band to \nin signal-to-noise ratio, or processing gain, is\nWith direct sequence spread spectrum (DSSS), each bit in the original signal is rep-\nresented by multiple bits in the transmitted signal, using a spreading code. The\nspreading code spreads the signal across a wider frequency band in direct proportion\nto the number of bits used. Therefore, a 10-bit spreading code spreads the signal\nacross a frequency band that is 10 times greater than a 1-bit spreading code.\nOne technique with direct sequence spread spectrum is to combine the digital\ninformation stream with the spreading code bit stream using an exclusive-OR\n(XOR).The XOR obeys the following rules:\nFigure 9.6 shows an example. Note that an information bit of one inverts the spread-\ning code bits in the combination, while an information bit of zero causes the spread-\ning code bits to be transmitted without inversion. The combination bit stream has\nthe data rate of the original spreading code sequence, so it has a wider bandwidth\nthan the information stream. In this example, the spreading code bit stream is\nclocked at four times the information rate.\nDSSS Using BPSK\nTo see how this technique works out in practice, assume that a BPSK modulation\nscheme is to be used. Rather than represent binary data with 1 and 0, it is more\nGP = 2k = Ws\nTs = 2Tc = 2T.\n9.3 / DIRECT SEQUENCE SPREAD SPECTRUM\nconvenient for our purposes to use \nto represent the two binary digits. In\nthat case, a BPSK signal can be represented as was shown in Equation (5.6):\nbit time if the corresponding bit in the bit stream is 1 and\nthe value of \nfor one bit time if the corresponding bit in\nthe bit stream is 0\nTo produce the DSSS signal, we multiply the preceding by c(t), which is the PN\nsequence taking on values of \nAt the receiver, the incoming signal is multiplied again by c(t). But\nand therefore the original signal is recovered:\nEquation (9.5) can be interpreted in two ways, leading to two different\nimplementations. The first interpretation is to first multiply d(t) and c(t) together\nand then perform the BPSK modulation. That is the interpretation we have been\ndiscussing. Alternatively, we can first perform the BPSK modulation on the \ndata stream d(t) to generate the data signal \nThis signal can then be multi-\nplied by c(t).\ns1t2c1t2 = A d1t2c1t2c1t2 cos12pfct2 = sd1t2\nc1t2 * c1t2 = 1\ns1t2 = A d1t2c1t2 cos12pfct2\nd1t2 = the discrete function that takes on the value of +1\nfc = carrier frequency\nA = amplitude of signal\nsd1t2 = Ad1t2 cos12pfct2\nData input A\nLocally generated\nPN bit stream\nTransmitted signal\nTransmitter\nReceived signal C \nLocally generated\nPN bit stream\nidentical to B\nData output\nExample of Direct Sequence Spread Spectrum\nCHAPTER 9 / SPREAD SPECTRUM\n(a) Transmitter\nPseudonoise\nDS spreader\nBinary data\nSpread spectrum\n(b) Receiver\nDS despreader\nSpread spectrum\nDemodulator\nBinary data\nPseudonoise\nDirect Sequence Spread Spectrum System\nAn implementation using the second interpretation is shown in Figure 9.7.\nFigure 9.8 is an example of this approach.\nDSSS Performance Considerations\nThe spectrum spreading achieved by the direct sequence technique is easily deter-\nmined (Figure 9.9). In our example, the information signal has a bit width of T,\nwhich is equivalent to a data rate of 1/T. In that case, the spectrum of the signal,\ndepending on the encoding technique, is roughly 2/T. Similarly, the spectrum of the\nPN signal is \nFigure 9.9c shows the resulting spectrum spreading.The amount of\nspreading that is achieved is a direct result of the data rate of the PN stream.\nAs with FHSS, we can get some insight into the performance of DSSS by look-\ning at its effectiveness against jamming. Let us assume a simple jamming signal at\nthe center frequency of the DSSS system.The jamming signal has the form\nand the received signal is\nsr1t2 = s1t2 + sj1t2 + n1t2\nsj1t2 = 22Sj cos12pfct2\n9.3 / DIRECT SEQUENCE SPREAD SPECTRUM\nSpreading code\nExample of Direct-Sequence Spread Spectrum Using BPSK\nThe despreader at the receiver multiplies \nby c(t), so the signal component\ndue to the jamming signal is\nThis is simply a BPSK modulation of the carrier tone. Thus, the carrier power \nspread over a bandwidth of approximately \nHowever, the BPSK demodulator\n(Figure 9.7) following the DSSS despreader includes a bandpass filter matched to\nthe BPSK data, with bandwidth of 2/T. Thus, most of the jamming power is filtered.\nyj1t2 = 22Sjc1t2cos12pfct2\nSj = jammer signal power\nn1t2 = additive white noise\nsj1t2 = jamming signal\ns1t2 = transmitted signal\nCHAPTER 9 / SPREAD SPECTRUM\n(a) Spectrum of data signal\nSignal energy\n(b) Spectrum of pseudonoise signal\n(c) Spectrum of combined signal\nApproximate Spectrum of Direct Sequence Spread Spectrum Signal\nAlthough a number of factors come into play, as an approximation, we can say that\nthe jamming power passed by the filter is\nThe jamming power has been reduced by a factor of \nthrough the use of\nspread spectrum.The inverse of this factor is the gain in signal-to-noise ratio:\nis the spreading bit rate, R is the data rate,\nis the signal bandwidth, and\nis the spread spectrum signal bandwidth. The result is similar to the result for\nFHSS (Equation 9.3).\nSjF = Sj12/T2/12/Tc2 = Sj1Tc/T2\n9.4 / CODE DIVISION MULTIPLE ACCESS\n1 \u00031\u00031 1 \u00031 1\n1 1 \u00031 \u00031 1 1\n1 1 \u00031 1 1 \u00031\nMessage \"1101\" encoded\nFigure 9.10\nCDMA Example\nBasic Principles\nCDMA is a multiplexing technique used with spread spectrum. The scheme works in\nthe following manner.We start with a data signal with rate D, which we call the bit data\nrate.We break each bit into k chips according to a fixed pattern that is specific to each\nuser,called the user’s code.The new channel has a chip data rate of kD chips per second.\nAs an illustration we consider a simple example5 with \nIt is simplest to charac-\nterize a code as a sequence of 1s and \nFigure 9.10 shows the codes for three users,\nA, B, and C, each of which is communicating with the same base station receiver,\nR. Thus, the code for user A is \nSimilarly, user B has\nand user C has \nWe now consider the case of user A communicating with the base station.The\nbase station is assumed to know A’s code. For simplicity, we assume that communi-\ncation is already synchronized so that the base station knows when to look for\ncodes. If A wants to send a 1 bit, A transmits its code as a chip pattern\nIf a 0 bit is to be sent, A transmits the complement (1s and\nreversed) of its code,\nAt the base station the receiver\ndecodes the chip patterns. In our simple version, if the receiver R receives a chip\nand the receiver is seeking to communicate\nd = 6d1, d2, d3, d4, d5, d67,\n6 -1, 1, 1, -1, 1, -17.\n61, -1, -1, 1, -1, 17.\ncC = 61, 1, -1, 1, 1, -17.\ncB = 61, 1, -1, -1, 1, 17,\ncA = 61, -1, -1, 1, -1, 17.\n5This example was provided by Professor Richard Van Slyke of the Polytechnic University of Brooklyn.\nCHAPTER 9 / SPREAD SPECTRUM\nCHAPTER 9 / SPREAD SPECTRUM\nFigure 9.11\nCDMA in a DSSS Environment\nother users are not despread by the spreading code from user 1 and hence retain their\nbandwidth of \nThus the unwanted signal energy remains spread over a large band-\nwidth and the wanted signal is concentrated in a narrow bandwidth.The bandpass fil-\nter at the demodulator can therefore recover the desired signal.\n[DIXO94] provides comprehensive treatment of spread spectrum. [TANT98] contains\nreprints of many important papers in the field, including [PICK82], which provides an excel-\nlent introduction to spread spectrum.\nDixon, R. Spread Spectrum Systems with Commercial Applications. New York:\nWiley, 1994.\nPickholtz, R.; Schilling, D.; and Milstein, L. “Theory of Spread Spectrum Com-\nmunications—A Tutorial.” IEEE Transactions on Communications, May 1982.\nReprinted in [TANT98].\nTantaratana, S, and Ahmed, K., eds. Wireless Applications of Spread Spectrum\nSystems: Selected Readings. Piscataway, NJ: IEEE Press, 1998.\nRecommended Web site:\n• Spread Spectrum Scene: Excellent source of information and links\n9.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nKEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nchipping signal\ncode division multiple access\ndirect sequence spread\nspectrum (DSSS)\nfrequency-hopping spread\nspectrum (FHSS)\npseudonoise (PN)\nspread spectrum\nspreading code\nspreading sequence\nReview Questions\nWhat is the relationship between the bandwidth of a signal before and after it has\nbeen encoded using spread spectrum?\nList three benefits of spread spectrum.\nWhat is frequency-hopping spread spectrum?\nExplain the difference between slow FHSS and fast FHSS.\nWhat is direct sequence spread spectrum?\nWhat is the relationship between the bit rate of a signal before and after it has been\nencoded using DSSS?\nWhat is CDMA?\nAssume we wish to transmit a 56-kbps data stream using spread spectrum.\nFind the channel bandwidth required to achieve a 56-kbps channel capacity when\n0.01, and 0.001.\nb. In an ordinary (not spread spectrum) system, a reasonable goal for bandwidth\nefficiency might be 1 bps/Hz.That is, to transmit a data stream of 56 kbps, a band-\nwidth of 56 kHz is used. In this case, what is the minimum SNR that can be\nendured for transmission without appreciable errors? Compare to the spread\nspectrum case.\nHint: Review the discussion of channel capacity in Section 3.4.\nAn FHSS system employs a total bandwidth of \nand an individual\nchannel bandwidth of 100 Hz. What is the minimum number of PN bits required for\neach frequency hop?\nAn FHSS system using MFSK with \nemploys 1000 different frequencies.What\nis the processing gain?\nThe following table illustrates the operation of an FHSS system for one complete\nperiod of the PN sequence.\nWs = 400 MHz\nPN sequence\nCHAPTER 9 / SPREAD SPECTRUM\nWhat is the period of the PN sequence, in terms of bits in the sequence?\nb. The system makes use of a form of FSK.What form of FSK is it?\nWhat is the number of bits per signal element?\nd. What is the number of FSK frequencies?\nWhat is the length of a PN sequence per hop?\nIs this a slow or fast FH system?\nWhat is the total number of possible carrier frequencies?\nh. Show the variation of the base, or demodulated, frequency with time.\nThe following table illustrates the operation of a FHSS system using the same PN\nsequence as Problem 9.4.\nPN sequence\nPN sequence\nPN sequence\nWhat is the period of the PN sequence?\nb. The system makes use of a form of FSK.What form of FSK is it?\nWhat is the number of bits per signal element?\nd. What is the number of FSK frequencies?\nWhat is the length of a PN sequence per hop?\nIs this a slow or fast FH system?\nWhat is the total number of possible carrier frequencies?\nh. Show the variation of the base, or demodulated, frequency with time.\nConsider an MFSK scheme with \nMake a frequency assignment for each of the eight possible 3-bit data combinations.\nb. We wish to apply FHSS to this MFSK scheme with \nthat is, the system will\nhop among four different carrier frequencies. Expand the results of part (a) to\nfrequency assignments.\nFigure 9.12, based on one in [BELL00], depicts a simplified scheme for CDMA\nencoding and decoding. There are seven logical channels, all using DSSS with a\nspreading code of 7 bits.Assume that all sources are synchronized. If all seven sources\ntransmit a data bit, in the form of a 7-bit sequence, the signals from all sources com-\nbine at the receiver so that two positive or two negative values reinforce and a posi-\ntive and negative value cancel. To decode a given channel, the receiver multiplies the\nincoming composite signal by the spreading code for that channel, sums the result,\nand assigns binary 1 for a positive value and binary 0 for a negative value.\nM = 8 1L = 3 bits2.\nfc = 250 kHz, fd = 25 kHz,\n9.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nWhat are the spreading codes for the seven channels?\nb. Determine the receiver output measurement for channel 1 and the bit value\nRepeat part (b) for channel 2.\nBy far, the most widely used technique for pseudorandom number generation is the\nlinear congruential method.The algorithm is parameterized with four numbers, as fol-\nThe sequence of pseudorandom numbers \nis obtained via the following iterative\nIf m, a, c, and \nare integers, then this technique will produce a sequence of integers\nwith each integer in the range \nAn essential characteristic of a pseudo-\nrandom number generator is that the generated sequence should appear random.\nAlthough the sequence is not random, because it is generated deterministically, there\nis a variety of statistical tests that can be used to assess the degree to which a sequence\nexhibits randomness.Another desirable characteristic is that the function should be a\nfull-period generating function. That is, the function should generate all the numbers\nbetween 0 and m before repeating.\nWith the linear congruential algorithm, a choice of parameters that provides a full\nperiod does not necessarily provide a good randomization. For example, consider the\ntwo generators:\nXn+1 = 17Xn2mod 13\nXn+1 = 16Xn2mod 13\n0 … Xn 6 m.\nXn+1 = 1aXn + c2mod m\nthe starting value, or seed 0 … X0 6 m\n the increment\n the multiplier  0 … a 6 m\n the modulus m 7 0\nChannel 0 code\nComposite signal\nIndividual channel\nFigure 9.12\nExample Seven-Channel CDMA Encoding and Decoding\nCHAPTER 9 / SPREAD SPECTRUM\nWrite out the two sequences to show that both are full period. Which one appears\nmore random to you?\nWe would like m to be very large so that there is the potential for producing a long\nseries of distinct random numbers. A common criterion is that m be nearly equal to\nthe maximum representable nonnegative integer for a given computer. Thus, a value\nof m near to or equal to \nis typically chosen. Many experts recommend a value of\nYou may wonder why one should not simply use \nbecause this latter num-\nber can be represented with no additional bits, and the mod operation should be eas-\nier to perform. In general, the modulus \nis preferable to \nWhy is this so?\nIn any use of pseudorandom numbers, whether for encryption, simulation, or statisti-\ncal design, it is dangerous to trust blindly the random number generator that happens\nto be available in your computer’s system library. [PARK88] found that many con-\ntemporary textbooks and programming packages make use of flawed algorithms for\npseudorandom number generation. This exercise will enable you to test your system.\nThe test is based on a theorem attributed to Ernesto Cesaro (see [KNUT98] for a\nproof), which states that the probability is equal to \nthat the greatest common\ndivisor of two randomly chosen integers is 1. Use this theorem in a program to deter-\nmine statistically the value of \nThe main program should call three subprograms:\nthe random number generator from the system library to generate the random inte-\ngers; a subprogram to calculate the greatest common divisor of two integers using\nEuclid’s algorithm; and a subprogram that calculates square roots. If these latter two\nprograms are not available, you will have to write them as well. The main program\nshould loop through a large number of random numbers to give an estimate of the\naforementioned probability. From this, it is a simple matter to solve for your estimate\nIf the result is close to 3.14, congratulations! If not, then the result is probably low,\nusually a value of around 2.7.Why would such an inferior result be obtained?\nWide Area Networks\nart Two dealt with the transfer of data between devices that are directly\nconnected, generally by a point-to-point link. Often, however, this\narrangement is impractical, and a data communication network is\nrequired to transmit data between devices, either because the devices are very\nfar apart or because there are many devices to be interconnected. In general\nterms, communications networks can be categorized as wide area networks\n(WANs) and local area networks (LANs). Part Three focuses on WANs, while\nPart Four covers LANs.\nTwo perspectives on the material in this part are of particular significance as\nthey relate to the treatment of internetworking in Part Five. First, the\nconstituent networks of the Internet and other internetworks are LANs and\nWANs. Thus, a full understanding of the technology and architecture of inter-\nnetworks includes an understanding of the underlying networks. Second, and\nperhaps more important, many of the technologies developed for switched data\nWANs (including packet-switching, frame relay, and ATM networks) carry over\ninto the design of internetworks. This is especially true in the cases of routing\nand congestion control.\nChapter 10 Circuit Switching and Packet Switching\nOur treatment of the technology and architecture of circuit-switching net-\nworks begins with the internal operation of a single switch. This is in con-\ntrast to packet-switching networks, which are best explained by the\ncollective behavior of the set of switches that make up a network. Thus,\nChapter 10 begins by examining digital switching concepts, including\nspace and time division switching.\nThe remainder of Chapter 10 introduces packet-switching technol-\nogy. The chapter covers the basic principles of packet switching and ana-\nlyzes datagram and virtual circuit approaches. The chapter also covers\nframe relay networks.\nChapter 11 Asynchronous Transfer Mode\nChapter 11 focuses on the transmission technology that is the foundation\nof broadband ISDN: asynchronous transfer mode (ATM). ATM is also\nfinding widespread application beyond its use as part of broadband ISDN.\nATM is in essence a packet-switching technology, but it is far more\nstreamlined and efficient than traditional packet switching and is designed\nto support very high data rates. This chapter begins with a description of\nthe ATM protocol and format. Then the physical layer issues relating to\nthe transmission of ATM cells are discussed.\nChapter 12 Routing in Switched Networks\nOne significant technical issue associated with switched networks is rout-\ning. Because the source and destination nodes are not directly connected,\nthe network must route each packet, from node to node, through the net-\nwork. Chapter 12 discusses the common routing algorithms used both in\nswitched data networks, such as frame relay and ATM, and in the Internet.\nChapter 13 Congestion Control in Data Networks\nA critical design issue for switched data networks and the Internet is con-\ngestion control. Chapter 13 begins with an explanation of the nature of\ncongestion in switched networks and both the importance and difficulty of\ncontrolling congestion. The chapter provides a general discussion of con-\ngestion control in traditional packet-switching networks and also exam-\nines frame relay congestion control. The focus of the remainder of the\nchapter is on congestion and traffic control for ATM networks.This is one\nof the most complex aspects of ATM and is the subject of intensive ongo-\ning research. This chapter surveys those techniques that have been\naccepted as having broad utility in ATM environments. All of the tech-\nniques surveyed in this chapter also have relevance in the context of the\nChapter 14: Cellular Wireless Networks\nChapter 14 begins with a discussion of the important design issues related\nto cellular wireless networks. Next, the chapter covers the traditional\nmobile telephony service, now known as first-generation analog. Chapter\n14 then examines second-generation digital cellular networks. Finally, an\noverview of third-generation networks is provided.\nSwitched Communications Networks\nCircuit-Switching Networks\nCircuit-Switching Concepts\nSoftswitch Architecture\nPacket-Switching Principles\nFrame Relay\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nHe got into a District Line train at Wimbledon Park, changed on to the Victoria Line at\nVictoria and on to the Jubilee Line at Green Park for West Hampstead. It was a long\nand awkward journey but he enjoyed it.\n—King Solomon’s Carpet, Barbara Vine (Ruth Rendell)\nCircuit switching is used in public telephone networks and is the basis\nfor private networks built on leased lines and using on-site circuit\nswitches. Circuit switching was developed to handle voice traffic but\ncan also handle digital data, although this latter use is often inefficient.\nWith circuit switching, a dedicated path is established between two\nstations for communication. Switching and transmission resources\nwithin the network are reserved for the exclusive use of the circuit for\nthe duration of the connection.The connection is transparent:Once it is\nestablished, it appears to attached devices as if there were a direct con-\nPacket switching was designed to provide a more efficient facility\nthan circuit switching for bursty data traffic. With packet switching, a\nstation transmits data in small blocks, called packets. Each packet con-\ntains some portion of the user data plus control information needed\nfor proper functioning of the network.\nA key distinguishing element of packet-switching networks is whether\nthe internal operation is datagram or virtual circuit.With internal vir-\ntual circuits, a route is defined between two endpoints and all packets\nfor that virtual circuit follow the same route.With internal datagrams,\neach packet is treated independently, and packets intended for the\nsame destination may follow different routes.\nX.25 is the standard protocol for the interface between an end system\nand a packet-switching network.\nFrame relay is a form of packet switching that provides a streamlined\ninterface compared to X.25, with improved performance.\nPart Two describes how information can be encoded and transmitted over a com-\nmunications link.We now turn to the broader discussion of networks, which can\nbe used to interconnect many devices.The chapter begins with a general discus-\nsion of switched communications networks.The remainder of the chapter focuses\non wide area networks and, in particular, on traditional approaches to wide area\nnetwork design: circuit switching and packet switching.\n10.1 / SWITCHED COMMUNICATIONS NETWORKS\n1We use this term here in a very general sense, to include voice, image, and video, as well as ordinary data\n(e.g., numerical, text).\nSince the invention of the telephone, circuit switching has been the dominant\ntechnology for voice communications, and it has remained so well into the digital\nera. This chapter looks at the key characteristics of a circuit-switching network.\nAround 1970, research began on a new form of architecture for long-dis-\ntance digital data communications: packet switching.Although the technology of\npacket switching has evolved substantially since that time, it is remarkable that\n(1) the basic technology of packet switching is fundamentally the same today as it\nwas in the early 1970s networks, and (2) packet switching remains one of the few\neffective technologies for long-distance data communications.\nThis chapter provides an overview of packet-switching technology.We will\nsee, in this chapter and later in this part, that many of the advantages of packet\nswitching (flexibility, resource sharing, robustness, responsiveness) come with a\ncost.The packet-switching network is a distributed collection of packet-switching\nnodes. Ideally, all packet-switching nodes would always know the state of the\nentire network. Unfortunately, because the nodes are distributed, there is a time\ndelay between a change in status in one portion of the network and knowledge of\nthat change elsewhere. Furthermore, there is overhead involved in communicat-\ning status information.As a result,a packet-switching network can never perform\n“perfectly,” and elaborate algorithms are used to cope with the time delay and\noverhead penalties of network operation. These same issues will appear again\nwhen we discuss internetworking in Part Five.\nFinally, this chapter provides an overview of a popular form of packet\nswitching known as frame relay.\n10.1 SWITCHED COMMUNICATIONS NETWORKS\nFor transmission of data1 beyond a local area, communication is typically achieved\nby transmitting data from source to destination through a network of intermediate\nswitching nodes; this switched network design is typically used to implement LANs\nas well. The switching nodes are not concerned with the content of the data; rather,\ntheir purpose is to provide a switching facility that will move the data from node to\nnode until they reach their destination. Figure 10.1 illustrates a simple network.The\ndevices attached to the network may be referred to as stations. The stations may\nbe computers, terminals, telephones, or other communicating devices. We refer to\nthe switching devices whose purpose is to provide communication as nodes. Nodes\nare connected to one another in some topology by transmission links. Each station\nattaches to a node, and the collection of nodes is referred to as a communications\nIn a switched communication network, data entering the network from a\nstation are routed to the destination by being switched from node to node. For\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nFigure 10.1\nSimple Switching Network\nexample, in Figure 10.1, data from station A intended for station F are sent to\nnode 4. They may then be routed via nodes 5 and 6 or nodes 7 and 6 to the desti-\nnation. Several observations are in order:\n1. Some nodes connect only to other nodes (e.g., 5 and 7). Their sole task is the\ninternal (to the network) switching of data. Other nodes have one or more sta-\ntions attached as well; in addition to their switching functions, such nodes\naccept data from and deliver data to the attached stations.\n2. Node-station links are generally dedicated point-to-point links. Node-node links\nare usually multiplexed, using either frequency division multiplexing (FDM) or\ntime division multiplexing (TDM).\n3. Usually, the network is not fully connected; that is, there is not a direct link\nbetween every possible pair of nodes. However, it is always desirable to have\nmore than one possible path through the network for each pair of stations.\nThis enhances the reliability of the network.\nTwo different technologies are used in wide area switched networks: circuit\nswitching and packet switching. These two technologies differ in the way the\nnodes switch information from one link to another on the way from source to\ndestination.\n10.2 / CIRCUIT-SWITCHING NETWORKS\n10.2 CIRCUIT-SWITCHING NETWORKS\nCommunication via circuit switching implies that there is a dedicated communica-\ntion path between two stations. That path is a connected sequence of links between\nnetwork nodes. On each physical link, a logical channel is dedicated to the connec-\ntion. Communication via circuit switching involves three phases, which can be\nexplained with reference to Figure 10.1.\n1. Circuit establishment. Before any signals can be transmitted, an end-to-end\n(station-to-station) circuit must be established. For example, station A sends a\nrequest to node 4 requesting a connection to station E.Typically, the link from A\nto 4 is a dedicated line, so that part of the connection already exists. Node 4 must\nfind the next leg in a route leading to E. Based on routing information and mea-\nsures of availability and perhaps cost, node 4 selects the link to node 5, allocates a\nfree channel (using FDM or TDM) on that link, and sends a message requesting\nconnection to E. So far, a dedicated path has been established from A through\n4 to 5. Because a number of stations may attach to 4, it must be able to establish\ninternal paths from multiple stations to multiple nodes. How this is done is dis-\ncussed later in this section.The remainder of the process proceeds similarly.Node\n5 allocates a channel to node 6 and internally ties that channel to the channel from\nnode 4. Node 6 completes the connection to E. In completing the connection, a\ntest is made to determine if E is busy or is prepared to accept the connection.\n2. Data transfer. Data can now be transmitted from A through the network to E.\nThe transmission may be analog or digital, depending on the nature of the net-\nwork.As the carriers evolve to fully integrated digital networks, the use of digital\n(binary) transmission for both voice and data is becoming the dominant method.\nThe path is A-4 link, internal switching through 4, 4-5 channel, internal switching\nthrough 5, 5-6 channel, internal switching through 6, 6-E link. Generally, the con-\nnection is full duplex.\n3. Circuit disconnect. After some period of data transfer, the connection is ter-\nminated, usually by the action of one of the two stations. Signals must be prop-\nagated to nodes 4, 5, and 6 to deallocate the dedicated resources.\nNote that the connection path is established before data transmission begins.\nThus, channel capacity must be reserved between each pair of nodes in the path, and\neach node must have available internal switching capacity to handle the requested\nconnection. The switches must have the intelligence to make these allocations and\nto devise a route through the network.\nCircuit switching can be rather inefficient. Channel capacity is dedicated for\nthe duration of a connection, even if no data are being transferred. For a voice con-\nnection, utilization may be rather high, but it still does not approach 100%. For a\nclient/server or terminal-to-computer connection, the capacity may be idle during\nmost of the time of the connection. In terms of performance, there is a delay prior to\nsignal transfer for call establishment. However, once the circuit is established, the\nnetwork is effectively transparent to the users. Information is transmitted at a fixed\ndata rate with no delay other than the propagation delay through the transmission\nlinks.The delay at each node is negligible.\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nFigure 10.2\nExample Connection Over a Public Circuit-Switching Network\nLong-distance\nLong-distance\nDigital PBX\nIntercity trunk\nConnecting trunk\nConnecting trunk\nSubscriber loop\nCircuit switching was developed to handle voice traffic but is now also used for\ndata traffic. The best-known example of a circuit-switching network is the public\ntelephone network (Figure 10.2). This is actually a collection of national networks\ninterconnected to form the international service. Although originally designed and\nimplemented to service analog telephone subscribers, it handles substantial data\ntraffic via modem and is gradually being converted to a digital network. Another\nwell-known application of circuit switching is the private branch exchange (PBX),\nused to interconnect telephones within a building or office. Circuit switching is also\nused in private networks. Typically, such a network is set up by a corporation or\nother large organization to interconnect its various sites. Such a network usually\nconsists of PBX systems at each site interconnected by dedicated, leased lines\nobtained from one of the carriers, such as AT&T. A final common example of the\napplication of circuit switching is the data switch. The data switch is similar to the\nPBX but is designed to interconnect digital data processing devices, such as termi-\nnals and computers.\nA public telecommunications network can be described using four generic\narchitectural components:\n• Subscribers: The devices that attach to the network. It is still the case that\nmost subscriber devices to public telecommunications networks are tele-\nphones, but the percentage of data traffic increases year by year.\n• Subscriber line: The link between the subscriber and the network, also\nreferred to as the subscriber loop or local loop. Almost all local loop connec-\ntions use twisted-pair wire. The length of a local loop is typically in a range\nfrom a few kilometers to a few tens of kilometers.\n• Exchanges: The switching centers in the network. A switching center that\ndirectly supports subscribers is known as an end office.Typically, an end office\nwill support many thousands of subscribers in a localized area.There are over\n19,000 end offices in the United States, so it is clearly impractical for each end\n10.2 / CIRCUIT-SWITCHING NETWORKS\noffice to have a direct link to each of the other end offices; this would require\non the order of \nlinks. Rather, intermediate switching nodes are used.\n• Trunks: The branches between exchanges. Trunks carry multiple voice-\nfrequency circuits using either FDM or synchronous TDM. We referred to\nthese as carrier systems in Chapter 8.\nSubscribers connect directly to an end office, which switches traffic between\nsubscribers and between a subscriber and other exchanges. The other exchanges\nare responsible for routing and switching traffic between end offices. This distinc-\ntion is shown in Figure 10.3. To connect two subscribers attached to the same end\noffice, a circuit is set up between them in the same fashion as described before. If\ntwo subscribers connect to different end offices, a circuit between them consists of\na chain of circuits through one or more intermediate offices. In the figure, a con-\nnection is established between lines a and b by simply setting up the connection\nthrough the end office.The connection between c and d is more complex. In c’s end\noffice, a connection is established between line c and one channel on a TDM trunk\nto the intermediate switch. In the intermediate switch, that channel is connected to\na channel on a TDM trunk to d’s end office. In that end office, the channel is con-\nnected to line d.\nCircuit-switching technology has been driven by those applications that han-\ndle voice traffic. One of the key requirements for voice traffic is that there must be\nvirtually no transmission delay and certainly no variation in delay.A constant signal\ntransmission rate must be maintained, because transmission and reception occur at\nthe same signal rate.These requirements are necessary to allow normal human con-\nversation. Further, the quality of the received signal must be sufficiently high to pro-\nvide, at a minimum, intelligibility.\nCircuit switching achieved its widespread, dominant position because it is well\nsuited to the analog transmission of voice signals. In today’s digital world, its ineffi-\nciencies are more apparent. However, despite the inefficiency, circuit switching will\nFigure 10.3\nCircuit Establishment\nIntermediate\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nremain an attractive choice for both local area and wide area networking. One of its\nkey strengths is that it is transparent. Once a circuit is established, it appears as a\ndirect connection to the two attached stations; no special networking logic is needed\nat the station.\n10.3 CIRCUIT-SWITCHING CONCEPTS\nThe technology of circuit switching is best approached by examining the operation\nof a single circuit-switching node. A network built around a single circuit-switching\nnode consists of a collection of stations attached to a central switching unit.The cen-\ntral switch establishes a dedicated path between any two devices that wish to com-\nmunicate. Figure 10.4 depicts the major elements of such a one-node network. The\ndotted lines inside the switch symbolize the connections that are currently active.\nThe heart of a modern system is a digital switch. The function of the digital\nswitch is to provide a transparent signal path between any pair of attached devices.\nThe path is transparent in that it appears to the attached pair of devices that there is\nFigure 10.4\nElements of a Circuit-Switch Node\nControl unit\nFull-duplex lines\nto attached devices\nDigital switch\n10.3 / CIRCUIT-SWITCHING CONCEPTS\na direct connection between them. Typically, the connection must allow full-duplex\ntransmission.\nThe network interface element represents the functions and hardware needed\nto connect digital devices, such as data processing devices and digital telephones, to\nthe network. Analog telephones can also be attached if the network interface con-\ntains the logic for converting to digital signals.Trunks to other digital switches carry\nTDM signals and provide the links for constructing multiple-node networks.\nThe control unit performs three general tasks. First, it establishes connections.\nThis is generally done on demand,that is,at the request of an attached device.To estab-\nlish the connection, the control unit must handle and acknowledge the request, deter-\nmine if the intended destination is free, and construct a path through the switch.\nSecond, the control unit must maintain the connection. Because the digital switch uses\ntime division principles, this may require ongoing manipulation of the switching ele-\nments.However,the bits of the communication are transferred transparently (from the\npoint of view of the attached devices).Third, the control unit must tear down the con-\nnection, either in response to a request from one of the parties or for its own reasons.\nAn important characteristic of a circuit-switching device is whether it is block-\ning or nonblocking. Blocking occurs when the network is unable to connect two sta-\ntions because all possible paths between them are already in use. A blocking\nnetwork is one in which such blocking is possible. Hence a nonblocking network\npermits all stations to be connected (in pairs) at once and grants all possible con-\nnection requests as long as the called party is free. When a network is supporting\nonly voice traffic, a blocking configuration is generally acceptable, because it is\nexpected that most phone calls are of short duration and that therefore only a frac-\ntion of the telephones will be engaged at any time. However, when data processing\ndevices are involved, these assumptions may be invalid. For example, for a data\nentry application, a terminal may be continuously connected to a computer for\nhours at a time. Hence, for data applications, there is a requirement for a nonblock-\ning or “nearly nonblocking” (very low probability of blocking) configuration.\nWe turn now to an examination of the switching techniques internal to a single\ncircuit-switching node.\nSpace Division Switching\nSpace division switching was originally developed for the analog environment and\nhas been carried over into the digital realm.The fundamental principles are the same,\nwhether the switch is used to carry analog or digital signals. As its name implies, a\nspace division switch is one in which the signal paths are physically separate from one\nanother (divided in space). Each connection requires the establishment of a physical\npath through the switch that is dedicated solely to the transfer of signals between the\ntwo endpoints.The basic building block of the switch is a metallic crosspoint or semi-\nconductor gate that can be enabled and disabled by a control unit.\nFigure 10.5 shows a simple crossbar matrix with 10 full-duplex I/O lines. The\nmatrix has 10 inputs and 10 outputs; each station attaches to the matrix via one input\nand one output line. Interconnection is possible between any two lines by enabling\nthe appropriate crosspoint. Note that a total of 100 crosspoints is required. The\ncrossbar switch has a number of limitations:\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nFigure 10.5\nSpace Division Switch\nInput lines\nOutput lines\n• The number of crosspoints grows with the square of the number of attached\nstations.This is costly for a large switch.\n• The loss of a crosspoint prevents connection between the two devices whose\nlines intersect at that crosspoint.\n• The crosspoints are inefficiently utilized; even when all of the attached devices\nare active, only a small fraction of the crosspoints are engaged.\nTo overcome these limitations, multiple-stage switches are employed. Figure\n10.6 is an example of a three-stage switch.This type of arrangement has two advan-\ntages over a single-stage crossbar matrix:\n• The number of crosspoints is reduced, increasing crossbar utilization. In this\nexample,the total number of crosspoints for 10 stations is reduced from 100 to 48.\n• There is more than one path through the network to connect two endpoints,\nincreasing reliability.\nOf course, a multistage network requires a more complex control scheme. To\nestablish a path in a single-stage network, it is only necessary to enable a single gate.\nIn a multistage network, a free path through the stages must be determined and the\nappropriate gates enabled.\nA consideration with a multistage space division switch is that it may be blocking.\nIt should be clear from Figure 10.5 that a single-stage crossbar matrix is nonblocking;\nthat is, a path is always available to connect an input to an output.That this may not be\nthe case with a multiple-stage switch can be seen in Figure 10.6.The heavier lines indi-\ncate the lines that are already in use. In this state, input line 10, for example, cannot be\nconnected to output line 3, 4, or 5, even though all of these output lines are available.A\n10.4 / SOFTSWITCH ARCHITECTURE\nFigure 10.6\nThree-Stage Space Division Switch\n2 \u0007 2 switch\n2 \u0007 2 switch\nmultiple-stage switch can be made nonblocking by increasing the number or size of the\nintermediate switches, but of course this increases the cost.\nTime Division Switching\nThe technology of switching has a long history, most of it covering an era when ana-\nlog signal switching predominated. With the advent of digitized voice and synchro-\nnous time division multiplexing techniques, both voice and data can be transmitted\nvia digital signals.This has led to a fundamental change in the design and technology\nof switching systems. Instead of relatively dumb space division systems, modern dig-\nital systems rely on intelligent control of space and time division elements.\nVirtually all modern circuit switches use digital time division techniques for\nestablishing and maintaining “circuits.” Time division switching involves the parti-\ntioning of a lower-speed bit stream into pieces that share a higher-speed stream with\nother bit streams.The individual pieces, or slots, are manipulated by control logic to\nroute data from input to output.There are a number of variations on this basic con-\ncept, which are beyond the scope of this book\n10.4 SOFTSWITCH ARCHITECTURE\nThe latest trend in the development of circuit-switching technology is generally referred\nto as the softswitch. In essence, a softswitch is a general-purpose computer running spe-\ncialized software that turns it into a smart phone switch. Softswitches cost significantly\nless than traditional circuit switches and can provide more functionality.In particular,in\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\naddition to handling the traditional circuit-switching functions, a softswitch can convert\na stream of digitized voice bits into packets.This opens up a number of options for trans-\nmission, including the increasingly popular voice over IP (Internet Protocol) approach.\nIn any telephone network switch, the most complex element is the software\nthat controls call processing. This software performs call routing and implements\ncall-processing logic for hundreds of custom calling features.Typically, this software\nruns on a proprietary processor that is integrated with the physical circuit-switching\nhardware. A more flexible approach is to physically separate the call processing\nfunction from the hardware switching function. In softswitch terminology, the phys-\nical switching function is performed by a media gateway (MG) and the call process-\ning logic resides in a media gateway controller (MGC).\nFigure 10.7 contrasts the architecture of a traditional telephone network circuit\nswitch with the softswitch architecture. In the latter case, the MG and MGC are dis-\ntinct entities and may be provided by different vendors. To facilitate interoperability,\ntwo Internet standards have been issued for a media gateway control protocol\nbetween the MG and MGC:RFC 2805 (Media Gateway Control Protocol Architecture\nFigure 10.7\nComparison between Traditional Circuit Switching and Softswitch\n(a) Traditional circuit switching\nSupervisory events\n(e.g., off-hook, on-hook)\nSupervisory events\n(e.g., off-hook, on-hook)\nRequest to generate progress\ntones (e.g., ringback, engaged).\nInstructions to establish switch\nfabric connections.\nCircuit-switched\n(b) Softswitch architecture\nRequest to generate progress\ntones (e.g., ringback, engaged).\nInstructions to establish switch\nfabric connections.\nCircuit-or packet-\nCircuit-or packet-\n10.5 / PACKET-SWITCHING PRINCIPLES\nand Requirements) and RFC 3525 (Gateway Control Protocol Version 1). Softswitch\nfunctionality is also defined in the H series or ITU-T Recommendations,which covers\naudiovisual and multimedia systems.\n10.5 PACKET-SWITCHING PRINCIPLES\nThe long-haul circuit-switching telecommunications network was originally designed\nto handle voice traffic, and the majority of traffic on these networks continues to be\nvoice. A key characteristic of circuit-switching networks is that resources within the\nnetwork are dedicated to a particular call. For voice connections, the resulting circuit\nwill enjoy a high percentage of utilization because, most of the time, one party or the\nother is talking. However, as the circuit-switching network began to be used increas-\ningly for data connections, two shortcomings became apparent:\n• In a typical user/host data connection (e.g., personal computer user logged on\nto a database server), much of the time the line is idle.Thus, with data connec-\ntions, a circuit-switching approach is inefficient.\n• In a circuit-switching network, the connection provides for transmission at a\nconstant data rate. Thus, each of the two devices that are connected must\ntransmit and receive at the same data rate as the other.This limits the utility of\nthe network in interconnecting a variety of host computers and workstations.\nTo understand how packet switching addresses these problems, let us briefly\nsummarize packet-switching operation. Data are transmitted in short packets.A typ-\nical upper bound on packet length is 1000 octets (bytes). If a source has a longer mes-\nsage to send, the message is broken up into a series of packets (Figure 10.8). Each\npacket contains a portion (or all for a short message) of the user’s data plus some\ncontrol information. The control information, at a minimum, includes the informa-\ntion that the network requires to be able to route the packet through the network\nand deliver it to the intended destination. At each node en route, the packet is\nreceived, stored briefly, and passed on to the next node.\nLet us return to Figure 10.1, but now assume that it depicts a simple packet-\nswitching network.Consider a packet to be sent from station A to station E.The packet\nincludes control information that indicates that the intended destination is E. The\nFigure 10.8\nThe Use of Packets\nApplication data\nControl information\n(packet header)\nPacket-switching\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\npacket is sent from A to node 4. Node 4 stores the packet, determines the next leg of\nthe route (say 5), and queues the packet to go out on that link (the 4–5 link).When the\nlink is available,the packet is transmitted to node 5,which forwards the packet to node\n6, and finally to E.This approach has a number of advantages over circuit switching:\n• Line efficiency is greater, because a single node-to-node link can be dynami-\ncally shared by many packets over time.The packets are queued up and trans-\nmitted as rapidly as possible over the link. By contrast, with circuit switching,\ntime on a node-to-node link is preallocated using synchronous time division\nmultiplexing. Much of the time, such a link may be idle because a portion of its\ntime is dedicated to a connection that is idle.\n• A packet-switching network can perform data-rate conversion.Two stations of\ndifferent data rates can exchange packets because each connects to its node at\nits proper data rate.\n• When traffic becomes heavy on a circuit-switching network, some calls are\nblocked; that is, the network refuses to accept additional connection requests\nuntil the load on the network decreases. On a packet-switching network, pack-\nets are still accepted, but delivery delay increases.\n• Priorities can be used. If a node has a number of packets queued for transmis-\nsion, it can transmit the higher-priority packets first. These packets will there-\nfore experience less delay than lower-priority packets.\nSwitching Technique\nIf a station has a message to send through a packet-switching network that is of\nlength greater than the maximum packet size, it breaks the message up into packets\nand sends these packets, one at a time, to the network. A question arises as to how\nthe network will handle this stream of packets as it attempts to route them through\nthe network and deliver them to the intended destination.Two approaches are used\nin contemporary networks: datagram and virtual circuit.\nIn the datagram approach,each packet is treated independently,with no reference\nto packets that have gone before.This approach is illustrated in Figure 10.9,which shows\na time sequence of snapshots of the progress of three packets through the network.\nEach node chooses the next node on a packet’s path, taking into account information\nreceived from neighboring nodes on traffic, line failures, and so on. So the packets, each\nwith the same destination address,do not all follow the same route,and they may arrive\nout of sequence at the exit point. In this example, the exit node restores the packets to\ntheir original order before delivering them to the destination. In some datagram net-\nworks,it is up to the destination rather than the exit node to do the reordering.Also,it is\npossible for a packet to be destroyed in the network. For example, if a packet-switching\nnode crashes momentarily, all of its queued packets may be lost.Again, it is up to either\nthe exit node or the destination to detect the loss of a packet and decide how to recover\nit. In this technique, each packet, treated independently, is referred to as a datagram.\nIn the virtual circuit approach, a preplanned route is established before any\npackets are sent. Once the route is established, all the packets between a pair of com-\nmunicating parties follow this same route through the network. This is illustrated in\nFigure 10.10. Because the route is fixed for the duration of the logical connection, it is\nsomewhat similar to a circuit in a circuit-switching network and is referred to as a\n10.5 / PACKET-SWITCHING PRINCIPLES\nFigure 10.9\nPacket Switching: Datagram Approach\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nFigure 10.10\nPacket Switching:Virtual-Circuit Approach\n10.5 / PACKET-SWITCHING PRINCIPLES\nvirtual circuit. Each packet contains a virtual circuit identifier as well as data. Each\nnode on the preestablished route knows where to direct such packets; no routing\ndecisions are required. At any time, each station can have more than one virtual cir-\ncuit to any other station and can have virtual circuits to more than one station.\nSo the main characteristic of the virtual circuit technique is that a route\nbetween stations is set up prior to data transfer. Note that this does not mean that\nthis is a dedicated path, as in circuit switching. A transmitted packet is buffered at\neach node, and queued for output over a line, while other packets on other virtual\ncircuits may share the use of the line.The difference from the datagram approach is\nthat, with virtual circuits, the node need not make a routing decision for each\npacket. It is made only once for all packets using that virtual circuit.\nIf two stations wish to exchange data over an extended period of time, there\nare certain advantages to virtual circuits. First, the network may provide services\nrelated to the virtual circuit, including sequencing and error control. Sequencing\nrefers to the fact that, because all packets follow the same route, they arrive in the\noriginal order. Error control is a service that assures not only that packets arrive in\nproper sequence, but also that all packets arrive correctly. For example, if a packet in\na sequence from node 4 to node 6 fails to arrive at node 6, or arrives with an error,\nnode 6 can request a retransmission of that packet from node 4.Another advantage\nis that packets should transit the network more rapidly with a virtual circuit; it is not\nnecessary to make a routing decision for each packet at each node.\nOne advantage of the datagram approach is that the call setup phase is\navoided. Thus, if a station wishes to send only one or a few packets, datagram deliv-\nery will be quicker. Another advantage of the datagram service is that, because it is\nmore primitive, it is more flexible. For example, if congestion develops in one part of\nthe network, incoming datagrams can be routed away from the congestion.With the\nuse of virtual circuits, packets follow a predefined route, and thus it is more difficult\nfor the network to adapt to congestion.A third advantage is that datagram delivery\nis inherently more reliable. With the use of virtual circuits, if a node fails, all virtual\ncircuits that pass through that node are lost. With datagram delivery, if a node fails,\nsubsequent packets may find an alternate route that bypasses that node. A data-\ngram-style of operation is common in internetworks, discussed in Part Five.\nPacket Size\nThere is a significant relationship between packet size and transmission time, as\nshown in Figure 10.11. In this example, it is assumed that there is a virtual circuit from\nstation X through nodes a and b to station Y. The message to be sent comprises\n40 octets, and each packet contains 3 octets of control information, which is placed at\nthe beginning of each packet and is referred to as a header. If the entire message is\nsent as a single packet of 43 octets (3 octets of header plus 40 octets of data), then the\npacket is first transmitted from station X to node a (Figure 10.11a). When the entire\npacket is received, it can then be transmitted from a to b. When the entire packet is\nreceived at node b, it is then transferred to station Y. Ignoring switching time, total\ntransmission time is 129 octet-times (\ntransmissions).\nSuppose now that we break the message up into two packets, each containing\n20 octets of the message and, of course, 3 octets each of header, or control information.\n43 octets * 3 packet\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nFigure 10.11\nEffect of Packet Size on Transmission Time\n(a) 1-packet message\n(b) 2-packet message\n(c) 5-packet message\n(d) 10-packet message\nIn this case, node a can begin transmitting the first packet as soon as it has\narrived from X,without waiting for the second packet.Because of this overlap\nin transmission, the total transmission time drops to 92 octet-times. By break-\ning the message up into five packets, each intermediate node can begin trans-\nmission even sooner and the savings in time is greater, with a total of\n77 octet-times for transmission. However, this process of using more and\nsmaller packets eventually results in increased, rather than reduced, delay as\nillustrated in Figure 10.11d. This is because each packet contains a fixed\namount of header, and more packets mean more of these headers. Further-\nmore, the example does not show the processing and queuing delays at each\n10.5 / PACKET-SWITCHING PRINCIPLES\nnode. These delays are also greater when more packets are handled for a single mes-\nsage. However, we shall see in the next chapter that an extremely small packet size (53\noctets) can result in an efficient network design.\nComparison of Circuit Switching and Packet Switching\nHaving looked at the internal operation of packet switching, we can now return to a\ncomparison of this technique with circuit switching. We first look at the important\nissue of performance and then examine other characteristics.\nPerformance A simple comparison of circuit switching and the two forms of\npacket switching is provided in Figure 10.12.The figure depicts the transmission of a\nmessage across four nodes, from a source station attached to node 1 to a destination\nstation attached to node 4. In this figure, we are concerned with three types of delay:\n• Propagation delay: The time it takes a signal to propagate from one node to\nthe next.This time is generally negligible.The speed of electromagnetic signals\nthrough a wire medium, for example, is typically \n• Transmission time: The time it takes for a transmitter to send out a block of data.\nFor example,it takes 1 s to transmit a 10,000-bit block of data onto a 10-kbps line.\n• Node delay: The time it takes for a node to perform the necessary processing\nas it switches data.\n2 * 108 m/s.\nFigure 10.12\nEvent Timing for Circuit Switching and Packet Switching\n(a) Circuit switching\nPropagation\nAcknowledge-\nment signal\n(b) Virtual circuit packet switching\nAcknowledge-\nment packet\n(c) Datagram packet switching\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nFor circuit switching, there is a certain amount of delay before the message can\nbe sent. First, a Call Request signal is sent through the network, to set up a connec-\ntion to the destination. If the destination station is not busy, a Call Accepted signal\nreturns. Note that a processing delay is incurred at each node during the call\nrequest; this time is spent at each node setting up the route of the connection. On\nthe return, this processing is not needed because the connection is already set up.\nAfter the connection is set up, the message is sent as a single block, with no notice-\nable delay at the switching nodes.\nVirtual circuit packet switching appears quite similar to circuit switching. A\nvirtual circuit is requested using a Call Request packet, which incurs a delay at each\nnode. The virtual circuit is accepted with a Call Accept packet. In contrast to the\ncircuit-switching case, the call acceptance also experiences node delays, even\nthough the virtual circuit route is now established. The reason is that this packet is\nqueued at each node and must wait its turn for transmission. Once the virtual cir-\ncuit is established, the message is transmitted in packets. It should be clear that this\nphase of the operation can be no faster than circuit switching, for comparable net-\nworks. This is because circuit switching is an essentially transparent process, pro-\nviding a constant data rate across the network. Packet switching involves some\ndelay at each node in the path. Worse, this delay is variable and will increase with\nincreased load.\nDatagram packet switching does not require a call setup. Thus, for short mes-\nsages, it will be faster than virtual circuit packet switching and perhaps circuit\nswitching. However, because each individual datagram is routed independently, the\nprocessing for each datagram at each node may be longer than for virtual circuit\npackets.Thus, for long messages, the virtual circuit technique may be superior.\nFigure 10.12 is intended only to suggest what the relative performance of the\ntechniques might be; actual performance depends on a host of factors, including the\nsize of the network, its topology, the pattern of load, and the characteristics of typi-\ncal exchanges.\nOther Characteristics Besides performance, there are a number of other\ncharacteristics that may be considered in comparing the techniques we have\nbeen discussing. Table 10.1 summarizes the most important of these. Most of\nthese characteristics have already been discussed. A few additional comments\nAs was mentioned, circuit switching is essentially a transparent service.\nOnce a connection is established, a constant data rate is provided to the con-\nnected stations. This is not the case with packet switching, which typically intro-\nduces variable delay, so that data arrive in a choppy manner. Indeed, with\ndatagram packet switching, data may arrive in a different order than they were\ntransmitted.\nAn additional consequence of transparency is that there is no over-\nhead required to accommodate circuit switching. Once a connection is established,\nthe analog or digital data are passed through, as is, from source to destination.\nFor packet switching, analog data must be converted to digital before transmis-\nsion; in addition, each packet includes overhead bits, such as the destination\n10.6 / X.25\nComparison of Communication Switching Techniques\nCircuit Switching\nDatagram Packet Switching\nVirtual Circuit Packet\nDedicated transmission path\nNo dedicated path\nNo dedicated path\nContinuous transmission of \nTransmission of packets\nTransmission of packets\nFast enough for interactive\nFast enough for interactive\nFast enough for interactive\nMessages are not stored\nPackets may be stored until \nPackets stored until delivered\nThe path is established for \nRoute established for each \nRoute established for entire \nentire conversation\nconversation\nCall setup delay; negligible \nPacket transmission delay\nCall setup delay; packet\ntransmission delay\ntransmission delay\nBusy signal if called party \nSender may be notified if \nSender notified of connection\npacket not delivered\nOverload may block call \nOverload increases packet \nOverload may block call\nsetup; no delay for established \nsetup; increases packet delay\nElectromechanical or \nSmall switching nodes\nSmall switching nodes\ncomputerized switching nodes\nUser responsible for message \nNetwork may be responsible \nNetwork may be responsible\nloss protection\nfor individual packets\nfor packet sequences\nUsually no speed or code \nSpeed and code conversion\nSpeed and code conversion\nFixed bandwidth\nDynamic use of bandwidth\nDynamic use of bandwidth\nNo overhead bits after call \nOverhead bits in each packet\nOverhead bits in each packet\nOne technical aspect of packet-switching networks remains to be examined: the inter-\nface between attached devices and the network.We have seen that a circuit-switching\nnetwork provides a transparent communications path for attached devices that makes\nit appear that the two communicating stations have a direct link. However, in the case\nof packet-switching networks, the attached stations must organize their data into\npackets for transmission.This requires a certain level of cooperation between the net-\nwork and the attached stations.This cooperation is embodied in an interface standard.\nThe standard used for traditional packet-switching networks is X.25.\nX.25 is an ITU-T standard that specifies an interface between a host system\nand a packet-switching network.The functionality of X.25 is specified on three levels:\n• Physical level\n• Link level\n• Packet level\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nThe physical level deals with the physical interface between an attached station\n(computer, terminal) and the link that attaches that station to the packet-switching\nnode.It makes use of the physical-level specification in a standard known as X.21,but\nin many cases other standards, such as EIA-232, are substituted. The link level pro-\nvides for the reliable transfer of data across the physical link, by transmitting the data\nas a sequence of frames.The link level standard is referred to as LAPB (Link Access\nProtocol–Balanced). LAPB is a subset of HDLC, which was described in Chapter 7.\nThe packet level provides a virtual circuit service.This service enables any sub-\nscriber to the network to set up logical connections, called virtual circuits, to other\nsubscribers. An example is shown in Figure 10.13 (compare Figure 10.1). In this\nexample, station A has a virtual circuit connection to C; station B has two virtual cir-\ncuits established, one to C and one to D; and stations E and F each have a virtual cir-\ncuit connection to D.\nIn this context, the term virtual circuit refers to the logical connection between\ntwo stations through the network; this is perhaps best termed an external virtual cir-\ncuit. Earlier, we used the term virtual circuit to refer to a specific preplanned route\nthrough the network between two stations; this could be called an internal virtual\ncircuit. Typically, there is a one-to-one relationship between external and internal\nvirtual circuits. However, it is also possible to employ X.25 with a datagram-style\nnetwork. What is important for an external virtual circuit is that there is a logical\nrelationship, or logical channel, established between two stations, and all of the data\nFigure 10.13\nThe Use of Virtual Circuits\nPacket-switching\nSolid line \u0001 physical link\nDashed line \u0001 virtual circuit\n10.7 / FRAME RELAY\nFigure 10.14\nUser Data and X.25 Protocol Control Information\nX.25 packet\nassociated with that logical channel are considered as part of a single stream of data\nbetween the two stations. For example, in Figure 10.13, station D keeps track of data\npackets arriving from three different workstations (B, E, F) on the basis of the vir-\ntual circuit number associated with each incoming packet.\nFigure 10.14 illustrates the relationship among the levels of X.25. User data\nare passed down to X.25 level 3, which appends control information as a header, cre-\nating a packet.This control information serves several purposes, including\n1. Identifying by number a particular virtual circuit with which this data is to be\n2. Providing sequence numbers that can be used for flow and error control on a\nvirtual circuit basis\nThe entire X.25 packet is then passed down to the LAPB entity, which\nappends control information at the front and back of the packet, forming a LAPB\nframe (see Figure 7.7).Again, the control information in the frame is needed for the\noperation of the LAPB protocol.\nThe operation of the X.25 packet level is similar to that of HDLC as described\nin Chapter 7. Each X.25 data packet includes send and receive sequence numbers.\nThe send sequence number, P(S), is used to number sequentially all outgoing data\npackets on a particular virtual circuit. The receive sequence number, P(R), is an\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nThe traditional approach to packet switching makes use of X.25, which not only\ndetermines the user-network interface but also influences the internal design of the\nnetwork.The following are key features of the X.25 approach:\n• Call control packets, used for setting up and clearing virtual circuits, are car-\nried on the same channel and same virtual circuit as data packets.\n• Multiplexing of virtual circuits takes place at layer 3.\n• Both layer 2 and layer 3 include flow control and error control mechanisms.\nThe X.25 approach results in considerable overhead.At each hop through the\nnetwork, the data link control protocol involves the exchange of a data frame and\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\n• Frame delimiting, alignment, and transparency\n• Frame multiplexing/demultiplexing using the address field\n• Inspection of the frame to ensure that it consists of an integral number of\noctets prior to zero bit insertion or following zero bit extraction\n• Inspection of the frame to ensure that it is neither too long nor too short\n• Detection of transmission errors\n• Congestion control functions\nThe last function listed is new to LAPF. The remaining functions listed are\nalso functions of LAPD.\nThe core functions of LAPF in the user plane constitute a sublayer of the data\nlink layer. This provides the bare service of transferring data link frames from one\nsubscriber to another, with no flow control or error control.Above this, the user may\nchoose to select additional data link or network-layer end-to-end functions. These\nare not part of the frame relay service. Based on the core functions, a network offers\nframe relay as a connection-oriented link layer service with the following properties:\n• Preservation of the order of frame transfer from one edge of the network to\n• A small probability of frame loss\nAs with X.25, frame relay involves the use of logical connections, in this case\ncalled data link connections rather than virtual circuits.The frames transmitted over\nthese data link connections are not protected by a data link control pipe with flow\nand error control. Another difference between X.25 and frame relay is that the lat-\nter devotes a separate data link connection to call control. The setting up and tear-\ning down of data link connections is done over this permanent control-oriented data\nlink connection.\nThe frame relay architecture significantly reduces the amount of work\nrequired of the network. User data are transmitted in frames with virtually no pro-\ncessing by the intermediate network nodes, other than to check for errors and to\nroute based on connection number. A frame in error is simply discarded, leaving\nerror recovery to higher layers.\nUser Data Transfer\nThe operation of frame relay for user data transfer is best explained by considering\nthe frame format, illustrated in Figure 10.16a.This is the format defined for the min-\nimum-function LAPF protocol (known as LAPF core protocol).The format is simi-\nlar to that of LAPD and LAPB with one obvious omission: There is no Control\nfield.This has the following implications:\n• There is only one frame type, used for carrying user data.There are no control\n• It is not possible to perform all control on the connection; a logical connection\ncan only carry user data.\n• It is not possible to perform flow control and error control, because there are\nno sequence numbers.\n10.7 / FRAME RELAY\n(a) Frame format\n(b) Address field—2 octets (default)\n(c) Address field—3 octets \nLower DLCI or DL-CORE control\nLower DLCI or DL-CORE control\n(d) Address field—4 octets \nAddress field extension bit\nCommand/response bit\nForward explicit congestion\nnotification\nBackward explicit congestion\nnotification\nData link connection identifier\nDLCI or DL-CORE control indicator\nDiscard eligibility\nInformation\nThe Flag and Frame Check Sequence (FCS) fields function as in HDLC.\nThe information field carries higher-layer data. If the user selects to implement\nadditional data link control functions end to end, then a data link frame can be\ncarried in this field. Specifically, a common selection will be to use the full LAPF\nprotocol (known as LAPF control protocol), to perform functions above the\nLAPF core functions. Note that the protocol implemented in this fashion is\nstrictly between the end subscribers and is transparent to the frame relay net-\nThe address field has a default length of 2 octets and may be extended to 3\nor 4 octets. It carries a data link connection identifier (DLCI) of 10, 16, or 23 bits.\nThe DLCI serves the same function as the virtual circuit number in X.25: It\nallows multiple logical frame relay connections to be multiplexed over a single\nchannel. As in X.25, the connection identifier has only local significance: Each\nend of the logical connection assigns its own DLCI from the pool of locally\nunused numbers, and the network must map from one to the other. The alterna-\ntive, using the same DLCI on both ends, would require some sort of global man-\nagement of DLCI values.\nThe length of the Address field, and hence of the DLCI, is determined by the\nAddress field extension (EA) bits. The C/R bit is application specific and not used\nby the standard frame relay protocol.The remaining bits in the address field have to\ndo with congestion control and are discussed in Chapter 13.\nFigure 10.16\nLAPF-Core Formats\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\n10.8 RECOMMENDED READING AND WEB SITES\nAs befits its age, circuit switching has inspired a voluminous literature. Two good books on\nthe subject are [BELL00] and [FREE04].\nThe literature on packet switching is enormous. Books with good treatments of this\nsubject include [BERT92] and [SPRA91]. [ROBE78] is a classic paper on how packet switch-\ning technology evolved. [RYBZ80] is a good tutorial on X.25. [BARA02] and [HEGG84] are\nalso interesting.\nA more in-depth treatment of frame relay can be found in [STAL99]. An excellent\nbook-length treatment is [BUCK00]. [CHER89] is a good tutorial on frame relay.\nBaran, P. “The Beginnings of Packet Switching: Some Underlying Concepts.”\nIEEE Communications Magazine, July 2002.\nBellamy, J. Digital Telephony. New York:Wiley, 2000.\nBertsekas, D., and Gallager, R. Data Networks. Englewood Cliffs, NJ: Prentice\nHall, 1992.\nBuckwalter, J. Frame Relay: Technology and Practice. Reading, MA: Addison-\nWesley, 2000.\nCherukuri, R., and Derby, R. “Frame Relay: Protocols and Private Network\nApplications.” Proceedings, IEEE INFOCOM ’89, 1989.\nFreeman, R. Telecommunication System Engineering. New York: Wiley, 1996.\nHeggestad, H. “An Overview of Packet Switching Communications.” IEEE\nCommunications Magazine, April 1984.\nRoberts, L. “The Evolution of Packet Switching.” Proceedings of the IEEE,\nNovember 1978.\nRybzzynski, A. “X.25 Interface and End-to-End Virtual Circuit Characteris-\ntics.” IEEE Transactions on Communications, April 1980.\nSpragins, J,; Hammond, J.; and Pawlikowski, K. Telecommunications Protocols\nand Design. Reading, MA:Addison-Wesley, 1991.\nStallings, W. ISDN and Broadband ISDN, with Frame Relay and ATM. Upper\nSaddle River, NJ: Prentice Hall, 1999.\nRecommended Web sites:\n• International Packet Communications Consortium: News, technical information, and\nvendor information on softswitch technology and products\n• Media Gateway Control Working Group: Chartered by IETF to develop the media\ngateway control protocol and related standards\n• Frame Relay Resource: Good source of tutorials, service providers, and other links\n• Frame Relay Resource Center: Good source of information on frame relay\n10.9 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\n10.9 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nReview Questions\nWhy is it useful to have more than one possible path through a network for each pair\nof stations?\nWhat are the four generic architectural components of a public communications net-\nwork? Define each term.\nWhat is the principal application that has driven the design of circuit-switching net-\nWhat are the advantages of packet switching compared to circuit switching?\nExplain the difference between datagram and virtual circuit operation.\nWhat is the significance of packet size in a packet-switching network?\nWhat types of delay are significant in assessing the performance of a packet-switching\nHow does frame relay differ from X.25?\nWhat are the relative advantages and disadvantages of frame relay compared to\nConsider a simple telephone network consisting of two end offices and one interme-\ndiate switch with a 1-MHz full-duplex trunk between each end office and the inter-\nmediate switch.Assume a 4-kHz channel for each voice call.The average telephone is\nused to make four calls per 8-hour workday, with a mean call duration of six minutes.\nTen percent of the calls are long distance. What is the maximum number of tele-\nphones an end office can support?\nIf a crossbar matrix has n input lines and m output lines, how many crosspoints are\nb. How many crosspoints would be required if there were no distinction between\ninput and output lines (i.e., if any line could be interconnected to any other line\nserviced by the crossbar)?\nShow the minimum configuration.\nConsider a three-stage switch such as Figure 10.6. Assume that there are a total of N\ninput lines and N output lines for the overall three-stage switch. If n is the number of\ninput lines to a stage 1 crossbar and the number of output lines to a stage 3 crossbar,\nthen there are N/n stage 1 crossbars and N/n stage 3 crossbars. Assume each stage 1\ncrossbar has one output line going to each stage 2 crossbar, and each stage 2 crossbar\ncircuit switching\ncircuit-switching network\ncontrol signaling\ncrossbar matrix\ndigital switch\nspace division switching\nframe relay\nmedia gateway controller\npacket switching\nsubscriber line\nsubscriber loop\ntime division switching\nvirtual circuit\nCHAPTER 10 / CIRCUIT SWITCHING AND PACKET SWITCHING\nhas one output line going to each stage 3 crossbar. For such a configuration it can be\nshown that, for the switch to be nonblocking, the number of stage 2 crossbar matrices\nWhat is the total number of crosspoints among all the crossbar switches?\nb. For a given value of N, the total number of crosspoints depends on the value of n.\nThat is, the value depends on how many crossbars are used in the first stage to\nhandle the total number of input lines. Assuming a large number of input lines to\neach crossbar (large value of n), what is the minimum number of crosspoints for a\nnonblocking configuration as a function of n?\nFor a range of N from \nplot the number of crosspoints for a single-stage\nswitch and an optimum three-stage crossbar switch.\nExplain the flaw in the following reasoning: Packet switching requires control and\naddress bits to be added to each packet. This introduces considerable overhead in\npacket switching. In circuit switching, a transparent circuit is established. No extra bits\nare needed. Therefore, there is no overhead in circuit switching. Because there is no\noverhead in circuit switching, line utilization must be more efficient than in packet\nDefine the following parameters for a switching network:\npute the end-to-end delay for circuit switching, virtual circuit packet switching,\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nPhysical layer\nATM adaptation layer (AAL)\nHigher layer\nControl plane\nManagement plane\nLayer management\nPlane management\nFigure 11.1\nATM Protocol Architecture\ncase of ATM, the information flow on each logical connection is organized into fixed-\nsize packets, called cells.\nATM is a streamlined protocol with minimal error and flow control capabili-\nties. This reduces the overhead of processing ATM cells and reduces the number of\noverhead bits required with each cell, thus enabling ATM to operate at high data\nrates. Further, the use of fixed-size cells simplifies the processing required at each\nATM node, again supporting the use of ATM at high data rates.\nThe standards issued for ATM by ITU-T are based on the protocol architec-\nture shown in Figure 11.1, which illustrates the basic architecture for an interface\nbetween user and network. The physical layer involves the specification of a trans-\nmission medium and a signal encoding scheme.The data rates specified at the phys-\nical layer range from 25.6 Mbps to 622.08 Mbps. Other data rates, both higher and\nlower, are possible.\nTwo layers of the protocol architecture relate to ATM functions. There is an\nATM layer common to all services that provides packet transfer capabilities, and an\nATM adaptation layer (AAL) that is service dependent.The ATM layer defines the\ntransmission of data in fixed-size cells and defines the use of logical connections.\nThe use of ATM creates the need for an adaptation layer to support information\ntransfer protocols not based on ATM.The AAL maps higher-layer information into\nATM cells to be transported over an ATM network, then collects information from\nATM cells for delivery to higher layers.\nThe protocol reference model involves three separate planes:\n• User plane: Provides for user information transfer, along with associated con-\ntrols (e.g., flow control, error control)\n• Control plane: Performs call control and connection control functions\n• Management plane: Includes plane management, which performs manage-\nment functions related to a system as a whole and provides coordination\nbetween all the planes, and layer management, which performs management\nfunctions relating to resources and parameters residing in its protocol entities\n11.2 / ATM LOGICAL CONNECTIONS\n11.2 ATM LOGICAL CONNECTIONS\nLogical connections in ATM are referred to as virtual channel connections (VCCs).\nA VCC is analogous to a virtual circuit in X.25; it is the basic unit of switching in an\nATM network. A VCC is set up between two end users through the network and a\nvariable-rate, full-duplex flow of fixed-size cells is exchanged over the connection.\nVCCs are also used for user-network exchange (control signaling) and network-net-\nwork exchange (network management and routing).\nFor ATM, a second sublayer of processing has been introduced that deals with\nthe concept of virtual path (Figure 11.2). A virtual path connection (VPC) is a bun-\ndle of VCCs that have the same endpoints. Thus, all of the cells flowing over all of\nthe VCCs in a single VPC are switched together.\nThe virtual path concept was developed in response to a trend in high-speed\nnetworking in which the control cost of the network is becoming an increasingly\nhigher proportion of the overall network cost. The virtual path technique helps\ncontain the control cost by grouping connections sharing common paths through\nthe network into a single unit. Network management actions can then be applied to\na small number of groups of connections instead of a large number of individual\nconnections.\nSeveral advantages can be listed for the use of virtual paths:\n• Simplified network architecture: Network transport functions can be sepa-\nrated into those related to an individual logical connection (virtual channel)\nand those related to a group of logical connections (virtual path).\n• Increased network performance and reliability: The network deals with fewer,\naggregated entities.\n• Reduced processing and short connection setup time: Much of the work is\ndone when the virtual path is set up. By reserving capacity on a virtual path\nconnection in anticipation of later call arrivals, new virtual channel connec-\ntions can be established by executing simple control functions at the endpoints\nof the virtual path connection; no call processing is required at transit nodes.\nThus, the addition of new virtual channels to an existing virtual path involves\nminimal processing.\n• Enhanced network services: The virtual path is used internal to the network\nbut is also visible to the end user.Thus, the user may define closed user groups\nor closed networks of virtual channel bundles.\ntransmission path\nVirtual path\nVirtual channels\nFigure 11.2\nATM Connection Relationships\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nRequest for\nVCC originates\nEstablish a\nBlock VCC or\nrequest more\nVPC exists?\nCan quality\nbe satisfied?\nFigure 11.3\nCall Establishent Using Virtual Paths\nFigure 11.3 suggests in a general way the call establishment process using\nvirtual channels and virtual paths. The process of setting up a virtual path con-\nnection is decoupled from the process of setting up an individual virtual channel\nconnection:\n• The virtual path control mechanisms include calculating routes, allocating\ncapacity, and storing connection state information.\n• To set up a virtual channel, there must first be a virtual path connection to the\nrequired destination node with sufficient available capacity to support the vir-\ntual channel, with the appropriate quality of service. A virtual channel is set up\nby storing the required state information (virtual channel/virtual path mapping).\n11.2 / ATM LOGICAL CONNECTIONS\nVirtual Path/Virtual Channel Terminology\nVirtual Channel (VC)\nA generic term used to describe unidirectional transport of ATM cells\nassociated by a common unique identifier value.\nVirtual Channel Link\nA means of unidirectional transport of ATM cells between a point where a\nVCI value is assigned and the point where that value is translated or termi-\nVirtual Channel Identifier \nA unique numerical tag that identifies a particular VC link for a given \nVirtual Channel Connection \nA concatenation of VC links that extends between two points where ATM\nservice users access the ATM layer.VCCs are provided for the purpose of\nuser-user, user-network, or network-network information transfer. Cell\nsequence integrity is preserved for cells belonging to the same VCC.\nVirtual Path\nA generic term used to describe unidirectional transport of ATM cells\nbelonging to virtual channels that are associated by a common unique\nidentifier value.\nVirtual Path Link\nA group of VC links, identified by a common value of VPI, between a point\nwhere a VPI value is assigned and the point where that value is translated\nor terminated.\nVirtual Path Identifier (VPI)\nIdentifies a particular VP link.\nVirtual Path Connection (VPC)\nA concatenation of VP links that extends between the point where the VCI\nvalues are assigned and the point where those values are translated or\nremoved, i.e., extending the length of a bundle of VC links that share the\nsame VPI.VPCs are provided for the purpose of user-user, user-network,\nor network-network information transfer.\nThe terminology of virtual paths and virtual channels used in the standard is a\nbit confusing and is summarized in Table 11.1. Whereas most of the network-layer\nprotocols that we deal with in this book relate only to the user-network interface, the\nconcepts of virtual path and virtual channel are defined in the ITU-T Recommenda-\ntions with reference to both the user-network interface and the internal network\nVirtual Channel Connection Uses\nThe endpoints of a VCC may be end users, network entities, or an end user and a\nnetwork entity. In all cases, cell sequence integrity is preserved within a VCC: that is,\ncells are delivered in the same order in which they are sent. Let us consider exam-\nples of the three uses of a VCC:\n• Between end users: Can be used to carry end-to-end user data; can also be\nused to carry control signaling between end users, as explained later. A VPC\nbetween end users provides them with an overall capacity; the VCC organiza-\ntion of the VPC is up to the two end users, provided the set of VCCs does not\nexceed the VPC capacity.\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\n• Between an end user and a network entity: Used for user-to-network control\nsignaling, as discussed subsequently. A user-to-network VPC can be used to\naggregate traffic from an end user to a network exchange or network server.\n• Between two network entities: Used for network traffic management and\nrouting functions. A network-to-network VPC can be used to define a com-\nmon route for the exchange of network management information.\nVirtual Path/Virtual Channel Characteristics\nITU-T Recommendation I.150 lists the following as characteristics of virtual chan-\nnel connections:\n• Quality of service (QoS): A user of a VCC is provided with a QoS specified by\nparameters such as cell loss ratio (ratio of cells lost to cells transmitted) and\ncell delay variation.\n• Switched and semipermanent virtual channel connections: A switched VCC is\nan on-demand connection, which requires a call control signaling for setup and\ntearing down.A semipermanent VCC is one that is of long duration and is set\nup by configuration or network management action.\n• Cell sequence integrity: The sequence of transmitted cells within a VCC is pre-\n• Traffic parameter negotiation and usage monitoring: Traffic parameters can\nbe negotiated between a user and the network for each VCC. The network\nmonitors the input of cells to the VCC, to ensure that the negotiated parame-\nters are not violated.\nThe types of traffic parameters that can be negotiated include average rate,peak\nrate, burstiness, and peak duration. The network may need a number of strategies to\ndeal with congestion and to manage existing and requested VCCs.At the crudest level,\nthe network may simply deny new requests for VCCs to prevent congestion. Addi-\ntionally, cells may be discarded if negotiated parameters are violated or if congestion\nbecomes severe. In an extreme situation, existing connections might be terminated.\nI.150 also lists characteristics of VPCs. The first four characteristics listed are\nidentical to those for VCCs. That is, QoS; switched and semipermanent VPCs; cell\nsequence integrity; and traffic parameter negotiation and usage monitoring are all\nalso characteristics of a VPC. There are a number of reasons for this duplication.\nFirst, this provides some flexibility in how the network service manages the require-\nments placed upon it. Second, the network must be concerned with the overall\nrequirements for a VPC, and within a VPC may negotiate the establishment of vir-\ntual channels with given characteristics. Finally, once a VPC is set up, it is possible\nfor the end users to negotiate the creation of new VCCs. The VPC characteristics\nimpose a discipline on the choices that the end users may make.\nIn addition, a fifth characteristic is listed for VPCs:\n• Virtual channel identifier restriction within a VPC: One or more virtual chan-\nnel identifiers, or numbers, may not be available to the user of the VPC but\nmay be reserved for network use. Examples include VCCs used for network\nmanagement.\n11.3 / ATM CELLS\nControl Signaling\nIn ATM, a mechanism is needed for the establishment and release of VPCs and\nVCCs. The exchange of information involved in this process is referred to as con-\ntrol signaling and takes place on separate connections from those that are being\nFor VCCs, I.150 specifies four methods for providing an establishment/release\nfacility. One or a combination of these methods will be used in any particular net-\n1. Semipermanent VCCs may be used for user-to-user exchange. In this case, no\ncontrol signaling is required.\n2. If there is no preestablished call control signaling channel, then one must be set\nup. For that purpose, a control signaling exchange must take place between the\nuser and the network on some channel. Hence we need a permanent channel,\nprobably of low data rate, that can be used to set up VCCs that can be used for\ncall control. Such a channel is called a meta-signaling channel, as the channel is\nused to set up signaling channels.\n3. The meta-signaling channel can be used to set up a VCC between the user and\nthe network for call control signaling. This user-to-network signaling virtual\nchannel can then be used to set up VCCs to carry user data.\n4. The meta-signaling channel can also be used to set up a user-to-user signaling\nvirtual channel. Such a channel must be set up within a preestablished VPC. It\ncan then be used to allow the two end users, without network intervention, to\nestablish and release user-to-user VCCs to carry user data.\nFor VPCs, three methods are defined in I.150:\n1. A VPC can be established on a semipermanent basis by prior agreement. In\nthis case, no control signaling is required.\n2. VPC establishment/release may be customer controlled. In this case, the cus-\ntomer uses a signaling VCC to request the VPC from the network.\n3. VPC establishment/release may be network controlled. In this case, the net-\nwork establishes a VPC for its own convenience.The path may be network-to-\nnetwork, user-to-network, or user-to-user.\n11.3 ATM CELLS\nThe asynchronous transfer mode makes use of fixed-size cells, consisting of a 5-octet\nheader and a 48-octet information field. There are several advantages to the use of\nsmall, fixed-size cells. First, the use of small cells may reduce queuing delay for a\nhigh-priority cell, because it waits less if it arrives slightly behind a lower-priority\ncell that has gained access to a resource (e.g., the transmitter). Second, it appears\nthat fixed-size cells can be switched more efficiently, which is important for the very\nhigh data rates of ATM [PARE88].With fixed-size cells, it is easier to implement the\nswitching mechanism in hardware.\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nGeneric Flow Control Virtual Path Identifier\nVirtual Path Identifier\nVirtual Channel Identifier\nPayload Type\nHeader Error Control\n(a) User-network interface\nInformation field\n(48 octets)\n(b) Network-network interface\nVirtual Path Identifier\nVirtual Channel Identifier\nHeader Error Control\nInformation field\n(48 octets)\nPayload Type\nFigure 11.4\nATM Cell Format\nHeader Format\nFigure 11.4a shows the cell header format at the user-network interface. Figure\n11.4b shows the cell header format internal to the network.\nThe Generic Flow Control (GFC) field does not appear in the cell header internal\nto the network, but only at the user-network interface. Hence, it can be used for control\nof cell flow only at the local user-network interface.The field could be used to assist the\ncustomer in controlling the flow of traffic for different qualities of service. In any case,\nthe GFC mechanism is used to alleviate short-term overload conditions in the network.\nI.150 lists as a requirement for the GFC mechanism that all terminals be able\nto get access to their assured capacities. This includes all constant-bit-rate (CBR)\nterminals as well as the variable-bit-rate (VBR) terminals that have an element of\nguaranteed capacity (CBR and VBR are explained in Section 11.5). The current\nGFC mechanism is described in a subsequent subsection.\nThe Virtual Path Identifier (VPI) constitutes a routing field for the network.It is\n8 bits at the user-network interface and 12 bits at the network-network interface.The\n11.3 / ATM CELLS\nPayload Type (PT) Field Coding\nInterpretation\nUser data cell,\ncongestion not experienced,\nUser data cell,\ncongestion not experienced,\nUser data cell,\ncongestion experienced,\nUser data cell,\ncongestion experienced,\nOAM segment associated cell\nOAM end-to-end associated cell\nResource management cell\nReserved for future function\nOAM = Operations, Administration, and Maintenance\nSDU = Service Data Unit\nSDU-type = 1\nSDU-type = 0\nSDU-type = 1\nSDU-type = 0\n1This is the term used in ATM Forum documents. In ITU-T documents, this bit is referred to as the \nATM-user-to-ATM-user (AAU) indication bit.The meaning is the same.\nlatter allows support for an expanded number of VPCs internal to the network, to\ninclude those supporting subscribers and those required for network management.\nThe Virtual Channel Identifier (VCI) is used for routing to and from the end user.\nThe Payload Type (PT) field indicates the type of information in the information\nfield.Table 11.2 shows the interpretation of the PT bits.A value of 0 in the first bit indi-\ncates user information (that is, information from the next higher layer). In this case, the\nsecond bit indicates whether congestion has been experienced; the third bit, known as\nthe Service Data Unit (SDU)1 type bit, is a one-bit field that can be used to discrimi-\nnate two types of ATM SDUs associated with a connection.The term SDU refers to the\n48-octet payload of the cell.A value of 1 in the first bit of the Payload Type field indi-\ncates that this cell carries network management or maintenance information.This indi-\ncation allows the insertion of network-management cells onto a user’s VCC without\nimpacting the user’s data. Thus, the PT field can provide inband control information.\nThe Cell Loss Priority (CLP) bit is used to provide guidance to the network\nin the event of congestion. A value of 0 indicates a cell of relatively higher prior-\nity, which should not be discarded unless no other alternative is available.A value\nof 1 indicates that this cell is subject to discard within the network. The user\nmight employ this field so that extra cells (beyond the negotiated rate) may be\ninserted into the network, with a CLP of 1, and delivered to the destination if the\nnetwork is not congested. The network may set this field to 1 for any data cell\nthat is in violation of an agreement concerning traffic parameters between the\nuser and the network. In this case, the switch that does the setting realizes that\nthe cell exceeds the agreed traffic parameters but that the switch is capable of\nhandling the cell. At a later point in the network, if congestion is encountered,\nthis cell has been marked for discard in preference to cells that fall within agreed\ntraffic limits.\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nThe Header Error Control (HEC) field is used for both error control and syn-\nchronization, as explained subsequently.\nGeneric Flow Control\nI.150 specifies the use of the GFC field to control traffic flow at the user-network\ninterface (UNI) in order to alleviate short-term overload conditions.The actual flow\ncontrol mechanism is defined in I.361. GFC flow control is part of a proposed con-\ntrolled cell transfer (CCT) capability intended to meet the requirements of non-\nATM LANs connected to a wide area ATM network [LUIN97]. In particular, CCT\nis intended to provide good service for high-volume bursty traffic with variable-\nlength messages. In the remainder of this subsection, we examine the GFC mecha-\nnism, as so far standardized.\nWhen the equipment at the UNI is configured to support the GFC mecha-\nnism, two sets of procedures are used: uncontrolled transmission and controlled\ntransmission. In essence, every connection is identified as either subject to flow\ncontrol or not. Of those subject to flow control, there may be one group of con-\ntrolled connections (Group A) that is the default, or controlled traffic may be\nclassified into two groups of controlled connections (Group A and Group B);\nthese are known, respectively, as the one-queue and two-queue models. Flow\ncontrol is exercised in the direction from the subscriber to the network by the\nnetwork side.\nFirst, we consider the operation of the GFC mechanism when there is only\none group of controlled connections. The controlled equipment, called terminal\nequipment (TE), initializes two variables: TRANSMIT is a flag initialized to\nSET (1), and GO_CNTR, which is a credit counter, is initialized to 0. A third\nvariable, GO_VALUE, is either initialized to 1 or set to some larger value at\nconfiguration time. The rules for transmission by the controlled device are as\ncells on uncontrolled connections may be sent at any\nno cells may be sent on either controlled or uncon-\ntrolled connections.\n2. If a HALT signal is received from the controlling equipment,TRANSMIT is set\nto 0 and remains at zero until a NO_HALT signal is received, at which time\nTRANSMIT is set to 1.\nand there is no cell to transmit on any uncontrolled connec-\ntions, then\nthen the TE may send a cell on a controlled connec-\ntion.The TE marks that cell as a cell on a controlled connection and decre-\nments GO_CNTR.\nthen the TE may not send a cell on a controlled con-\n4. The TE sets GO_CNTR to GO_VALUE upon receiving a SET signal; a null\nsignal has no effect on GO_CNTR.\nGO_CNTR = 0,\nGO_CNTR 7 0,\nTRANSMIT = 1\nTRANSMIT = 0,\nTRANSMIT = 1,\n11.3 / ATM CELLS\nGeneric Flow Control (GFC) Field Coding\nUncontrolled\n1-Queue Model\n2-Queue Model\n1-Queue Model\n2-Queue Model\nSET(1)/NULL(0)\nSET(1)/NULL(0)\ncell belongs to \ncell belongs to\nfor Group A\ncontrolled(1)/\nGroup A(1)/\nuncontrolled(0)\nSET(1)/NULL(0) \ncell belongs to \nfor Group B\nGroup B(1)/\nequipment is \nequipment is\nuncontrolled(0)/ \nuncontrolled(0)/\ncontrolled(1)\ncontrolled(1)\nControlled : controlling\nControlling : controlled\nThe HALT signal is used logically to limit the effective ATM data rate and\nshould be cyclic. For example, to reduce the data rate over a link by half, the HALT\ncommand is issued by the controlling equipment so as to be in effect 50% of the\ntime. This is done in a predictable, regular pattern over the lifetime of the physical\nconnection.\nFor the two-queue model, there are two counters, each with a current counter\nvalue and an initialization value: GO_CNTR_A, GO_VALUE_A, GO_CNTR_B,\nand GO_VALUE_B. This enables the network to control two separate groups of\nconnections.\nTable 11.3 summarizes the rules for setting GFC bits.\nHeader Error Control\nEach ATM cell includes an 8-bit HEC field that is calculated based on the remain-\ning 32 bits of the header. The polynomial used to generate the code is\nIn most existing protocols that include an error control field,\nsuch as HDLC, the data that serve as input to the error code calculation are in gen-\neral much longer than the size of the resulting error code. This allows for error\ndetection. In the case of ATM, the input to the calculation is only 32 bits, compared\nto 8 bits for the code.The fact that the input is relatively short allows the code to be\nused not only for error detection but also, in some cases, for actual error correction.\nThis is because there is sufficient redundancy in the code to recover from certain\nerror patterns.\nFigure 11.5 depicts the operation of the HEC algorithm at the receiver.At ini-\ntialization, the receiver’s error correction algorithm is in the default mode for sin-\ngle-bit error correction. As each cell is received, the HEC calculation and\ncomparison is performed.As long as no errors are detected, the receiver remains in\nerror correction mode.When an error is detected, the receiver will correct the error\nif it is a single-bit error or will detect that a multibit error has occurred. In either\ncase, the receiver now moves to detection mode. In this mode, no attempt is made to\nX8 + X2 + X + 1.\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nNo error detected\n(no action)\nMultibit error detected\n(cell discarded)\nSingle-bit error detected\n(correction)\nNo error detected\n(no action)\nError detected\n(cell discarded)\nFigure 11.5\nHEC Operation at Receiver\n2The SDH-based approach is not defined for 25.6 Mbps.\ncorrect errors.The reason for this change is a recognition that a noise burst or other\nevent might cause a sequence of errors, a condition for which the HEC is insuffi-\ncient for error correction.The receiver remains in detection mode as long as errored\ncells are received. When a header is examined and found not to be in error, the\nreceiver switches back to correction mode. The flowchart of Figure 11.6 shows the\nconsequence of errors in the cell header.\nThe error protection function provides both recovery from single-bit header\nerrors and a low probability of the delivery of cells with errored headers under\nbursty error conditions. The error characteristics of fiber-based transmission sys-\ntems appear to be a mix of single-bit errors and relatively large burst errors. For\nsome transmission systems, the error correction capability, which is more time-con-\nsuming, might not be invoked.\nFigure 11.7, based on one in ITU-T I.432, indicates how random bit errors\nimpact the probability of occurrence of discarded cells and valid cells with errored\nheaders when HEC is employed.\n11.4 TRANSMISSION OF ATM CELLS\nI.432 specifies that ATM cells may be transmitted at one of several data rates:\n622.08 Mbps, 155.52 Mbps, 51.84 Mbps, or 25.6 Mbps. We need to specify the trans-\nmission structure that will be used to carry this payload. Two approaches are de-\nfined in I.432: a cell-based physical layer and an SDH-based physical layer.2 We\nexamine each of these approaches in turn.\nCell-Based Physical Layer\nFor the cell-based physical layer,no framing is imposed.The interface structure consists\nof a continuous stream of 53-octet cells. Because there is no external frame imposed in\nthe cell-based approach, some form of synchronization is needed. Synchronization is\n11.4 / TRANSMISSION OF ATM CELLS\n(intended service)\nApparently valid cell with\nerrored header\n(unintended service)\ntermined to be\nincorrectable?\nUnsuccessful\nFigure 11.6\nEffect of Error in Cell Header\nachieved on the basis of the HEC field in the cell header. The procedure is as follows\n(Figure 11.8):\n1. In the HUNT state, a cell delineation algorithm is performed bit by bit to\ndetermine if the HEC coding law is observed (i.e., match between received\nHEC and calculated HEC). Once a match is achieved, it is assumed that one\nheader has been found, and the method enters the PRESYNC state.\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nDiscarded cell\nprobability\nProbability of\nvalid cells with\nerrored headers\nOutcome probability\nBit error probability\nFigure 11.7\nImpact of Random Bit Errors on HEC Performance\nCell by cell\nCell by cell\nCorrect HEC\nA consecutive\nincorrect HEC\nD consecutive\ncorrect HEC\nFigure 11.8\nCell Delineation State Diagram\n11.4 / TRANSMISSION OF ATM CELLS\nfor 155.52 Mbps\nBit error probability (Re)\nIn-sync time Td(A) in cell units\nFigure 11.9\nImpact of Random Bit Errors on Cell-Delineation Performance\n2. In the PRESYNC state,a cell structure is now assumed.The cell delineation algo-\nrithm is performed cell by cell until the encoding law has been confirmed consec-\nutively times.\n3. In the SYNC state, the HEC is used for error detection and correction (see\nFigure 11.5). Cell delineation is assumed to be lost if the HEC coding law is\nrecognized consecutively as incorrect \nThe values of \nare design parameters. Greater values of \nlonger delays in establishing synchronization but in greater robustness against false\ndelineation. Greater values of \nresult in longer delays in recognizing a misalign-\nment but in greater robustness against false misalignment. Figures 11.9 and 11.10,\nbased on I.432, show the impact of random bit errors on cell delineation perfor-\nmance for various values of \nThe first figure shows the average amount of\ntime that the receiver will maintain synchronization in the face of errors, with \na parameter. The second figure shows the average amount of time to acquire syn-\nchronization as a function of error rate, with as a parameter.\nThe advantage of using a cell-based transmission scheme is the simplified\ninterface that results when both transmission and transfer mode functions are based\non a common structure.\nSDH-Based Physical Layer\nThe SDH-based physical layer imposes a structure on the ATM cell stream. In this\nsection, we look at the I.432 specification for 155.52 Mbps; similar structures are\nused at other data rates. For the SDH-based physical layer, framing is imposed\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nFor 155.52 Mbps\nBit error probability (Re)\nAcquisition time Ta(D) in cell units\nFigure 11.10\nAcquisition Time versus Bit-Error Probability\nusing the STM-1 (STS-3) frame. Figure 11.11 shows the payload portion of an\nSTM-1 frame (see Figure 8.11). This payload may be offset from the beginning of\nthe frame, as indicated by the pointer in the section overhead of the frame. As can\nbe seen, the payload consists of a 9-octet path overhead portion and the remainder,\nwhich contains ATM cells. Because the payload capacity (2340 octets) is not an\ninteger multiple of the cell length (53 octets), a cell may cross a payload boundary.\n260 columns\nFigure 11.11\nSTM-1 Payload for SDH-Based ATM Cell Transmission\n11.5 / ATM SERVICE CATEGORIES\nThe H4 octet in the path overhead is set at the sending side to indicate the next\noccurrence of a cell boundary.That is, the value in the H4 field indicates the number\nof octets to the first cell boundary following the H4 octet. The permissible range of\nvalues is 0 to 52.\nThe advantages of the SDH-based approach include the following:\n• It can be used to carry either ATM-based or STM-based (synchronous trans-\nfer mode) payloads, making it possible to initially deploy a high-capacity fiber-\nbased transmission infrastructure for a variety of circuit-switched and\ndedicated applications and then readily migrate to the support of ATM.\n• Some specific connections can be circuit switched using an SDH channel. For\nexample, a connection carrying constant-bit-rate video traffic can be mapped\ninto its own exclusive payload envelope of the STM-1 signal, which can be cir-\ncuit switched.This may be more efficient than ATM switching.\n• Using SDH synchronous multiplexing techniques, several ATM streams can\nbe combined to build interfaces with higher bit rates than those supported by\nthe ATM layer at a particular site. For example, four separate ATM streams,\neach with a bit rate of 155 Mbps (STM-1), can be combined to build a 622-\nMbps (STM-4) interface. This arrangement may be more cost effective than\none using a single 622-Mbps ATM stream.\n11.5 ATM SERVICE CATEGORIES\nAn ATM network is designed to be able to transfer many different types of traffic simul-\ntaneously,including real-time flows such as voice,video,and bursty TCP flows.Although\neach such traffic flow is handled as a stream of 53-octet cells traveling through a virtual\nchannel, the way in which each data flow is handled within the network depends on the\ncharacteristics of the traffic flow and the requirements of the application. For example,\nreal-time video traffic must be delivered within minimum variation in delay.\nWe examine the way in which an ATM network handles different types of traf-\nfic flows in Chapter 13. In this section, we summarize ATM service categories, which\nare used by an end system to identify the type of service required.The following ser-\nvice categories have been defined by the ATM Forum:\n• Real-Time Service\n—Constant bit rate (CBR)\n—Real-time variable bit rate (rt-VBR)\n• Non-Real-Time Service\n—Non-real-time variable bit rate (nrt-VBR)\n—Available bit rate (ABR)\n—Unspecified bit rate (UBR)\n—Guaranteed frame rate (GFR)\nReal-Time Services\nThe most important distinction among applications concerns the amount of delay and\nthe variability of delay,referred to as jitter,that the application can tolerate.Real-time\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\napplications typically involve a flow of information to a user that is intended to repro-\nduce that flow at a source. For example, a user expects a flow of audio or video infor-\nmation to be presented in a continuous, smooth fashion. A lack of continuity or\nexcessive loss results in significant loss of quality.Applications that involve interaction\nbetween people have tight constraints on delay.Typically, any delay above a few hun-\ndred milliseconds becomes noticeable and annoying.Accordingly, the demands in the\nATM network for switching and delivery of real-time data are high.\nConstant Bit Rate (CBR) The CBR service is perhaps the simplest service to\ndefine. It is used by applications that require a fixed data rate that is continuously\navailable during the connection lifetime and a relatively tight upper bound on trans-\nfer delay. CBR is commonly used for uncompressed audio and video information.\nExample of CBR applications include\n• Videoconferencing\n• Interactive audio (e.g., telephony)\n• Audio/video distribution (e.g., television, distance learning, pay-per-view)\n• Audio/video retrieval (e.g., video-on-demand, audio library)\nReal-Time Variable Bit Rate (rt-VBR)\nThe rt-VBR category is\nintended for time-sensitive applications; that is, those requiring tightly con-\nstrained delay and delay variation. The principal difference between applica-\ntions appropriate for rt-VBR and those appropriate for CBR is that rt-VBR\napplications transmit at a rate that varies with time. Equivalently, an rt-VBR\nsource can be characterized as somewhat bursty. For example, the standard\napproach to video compression results in a sequence of image frames of varying\nsizes. Because real-time video requires a uniform frame transmission rate, the\nactual data rate varies.\nThe rt-VBR service allows the network more flexibility than CBR. The net-\nwork is able to statistically multiplex a number of connections over the same dedi-\ncated capacity and still provide the required service to each connection.\nNon-Real-Time Services\nNon-real-time services are intended for applications that have bursty traffic charac-\nteristics and do not have tight constraints on delay and delay variation.Accordingly,\nthe network has greater flexibility in handling such traffic flows and can make\ngreater use of statistical multiplexing to increase network efficiency.\nNon-Real-Time Variable Bit Rate (nrt-VBR) For some non-real-time\napplications, it is possible to characterize the expected traffic flow so that the net-\nwork can provide substantially improved QoS in the areas of loss and delay. Such\napplications can use the nrt-VBR service.With this service, the end system specifies\na peak cell rate, a sustainable or average cell rate, and a measure of how bursty or\nclumped the cells may be.With this information, the network can allocate resources\nto provide relatively low delay and minimal cell loss.\nThe nrt-VBR service can be used for data transfers that have critical response-\ntime requirements. Examples include airline reservations, banking transactions, and\nprocess monitoring.\n11.5 / ATM SERVICE CATEGORIES\nUnspecified Bit Rate (UBR) At any given time, a certain amount of the capac-\nity of an ATM network is consumed in carrying CBR and the two types of VBR traf-\nfic. Additional capacity is available for one or both of the following reasons: (1) Not\nall of the total resources have been committed to CBR and VBR traffic, and (2) the\nbursty nature of VBR traffic means that at some times less than the committed\ncapacity is being used. All of this unused capacity could be made available for the\nUBR service.This service is suitable for applications that can tolerate variable delays\nand some cell losses, which is typically true of TCP-based traffic.With UBR, cells are\nforwarded on a first-in-first-out (FIFO) basis using the capacity not consumed by\nother services; both delays and variable losses are possible. No initial commitment is\nmade to a UBR source and no feedback concerning congestion is provided; this is\nreferred to as a best-effort service. Examples of UBR applications include\n• Text/data/image transfer, messaging, distribution, retrieval\n• Remote terminal (e.g., telecommuting)\nAvailable Bit Rate (ABR) Bursty applications that use a reliable end-to-end pro-\ntocol such as TCP can detect congestion in a network by means of increased round-\ntrip delays and packet discarding. This is discussed in Chapter 20. However, TCP has\nno mechanism for causing the resources within the network to be shared fairly among\nmany TCP connections. Further,TCP does not minimize congestion as efficiently as is\npossible using explicit information from congested nodes within the network.\nTo improve the service provided to bursty sources that would otherwise use UBR,\nthe ABR service has been defined.An application using ABR specifies a peak cell rate\n(PCR) that it will use and a minimum cell rate (MCR) that it requires.The network allo-\ncates resources so that all ABR applications receive at least their MCR capacity. Any\nunused capacity is then shared in a fair and controlled fashion among all ABR sources.\nThe ABR mechanism uses explicit feedback to sources to assure that capacity is fairly\nallocated.Any capacity not used by ABR sources remains available for UBR traffic.\nAn example of an application using ABR is LAN interconnection. In this case,\nthe end systems attached to the ATM network are routers.\nFigure 11.12 suggests how a network allocates resources during a steady-state\nperiod of time (no additions or deletions of virtual channels).\nAvailable bit rate\nunspecified bit rate\nVariable bit rate\nConstant bit rate\nFigure 11.12\nATM Bit Rate Services\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nGuaranteed Frame Rate (GFR) The most recent addition to the set of ATM\nservice categories is GFR, which is designed specifically to support IP backbone\nsubnetworks. GFR provides better service than UBR for frame-based traffic,\nincluding IP and Ethernet. A major goal of GFR is to optimize the handling of\nframe-based traffic that passes from a LAN through a router onto an ATM back-\nbone network. Such ATM networks are increasingly being used in large enterprise,\ncarrier, and Internet service provider networks to consolidate and extend IP ser-\nvices over the wide area. While ABR is also an ATM service meant to provide a\ngreater measure of guaranteed packet performance over ATM backbones, ABR is\nrelatively difficult to implement between routers over an ATM network. With the\nincreased emphasis on using ATM to support IP-based traffic, especially traffic that\noriginates on Ethernet LANs, GFR may offer the most attractive alternative for\nproviding ATM service.\nOne of the techniques used by GFR to provide improved performance com-\npared to UBR is to require that network elements be aware of frame or packet bound-\naries. Thus, when congestion requires the discard of cells, network elements must\ndiscard all of the cells that comprise a single frame. GFR also allows a user to reserve\ncapacity for each GFR VC.The user is guaranteed that this minimum capacity will be\nsupported.Additional frames may be transmitted if the network is not congested.\n11.6 RECOMMENDED READING AND WEB SITES\n[MCDY99] and [BLAC99a] provide good coverage of ATM.The virtual path/virtual channel\napproach of ATM is examined in [SATO90], [SATO91], and [BURG91].\n[GARR96] provides a rationale for the ATM service categories and discuss the traffic\nmanagement implications of each. [ARMI93] and [SUZU94] discuss AAL and compare\ntypes 3/4 and 5.\nArmitage, G., and Adams, K. “Packet Reassembly During Cell Loss.” IEEE\nNetwork, September 1995.\nBlack, U. ATM Volume I: Foundation for Broadband Networks. Upper Saddle\nRiver, NJ: Prentice Hall, 1992.\nBurg, J., and Dorman, D.“Broadband ISDN Resource Management:The Role\nof Virtual Paths.” IEEE Communications Magazine, September 1991.\nGarrett,M.“A Service Architecture for ATM:From Applications to Scheduling.”\nIEEE Network, May/June 1996.\nMcDysan, D., and Spohn, D. ATM: Theory and Application. New York:\nMcGraw-Hill, 1999.\nSato, K.; Ohta, S.; and Tokizawa, I. “Broad-Band ATM Network Architecture\nBased on Virtual Paths.” IEEE Transactions on Communications,August 1990.\nSato, K.; Ueda, H.; and Yoshikai, M.“The Role of Virtual Path Crossconnection.”\nIEEE LTS,August 1991.\nSuzuki, T. “ATM Adaptation Layer Protocol.” IEEE Communications \nMagazine,April 1995.\n11.7 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nRecommended Web sites:\n• ATM Hot Links: Excellent collection of white papers and links maintained by the\nUniversity of Minnesota.\n• MFA Forum: An industry forum that promotes ATM and other packet-based tech-\nnologies. Contains white papers, vendor information, and links.\n• Cell Relay Retreat: Contains archives of the cell-relay mailing list, links to numerous\nATM-related documents, and links to many ATM-related Web sites.\n11.7 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nReview Questions\nHow does ATM differ from frame relay?\nWhat are the relative advantages and disadvantages of ATM compared to frame relay?\nWhat is the difference between a virtual channel and a virtual path?\nWhat are the advantages of the use of virtual paths?\nWhat are the characteristics of a virtual channel connection?\nWhat are the characteristics of a virtual path connection?\nList and briefly explain the fields in an ATM cell.\nBriefly explain two methods for transmitting ATM cells.\nList and briefly define the ATM service categories.\nList all 16 possible values of the GFC field and the interpretation of each value (some\nvalues are illegal).\nOne key design decision for ATM was whether to use fixed or variable length cells.\nLet us consider this decision from the point of view of efficiency.We can define trans-\nmission efficiency as\nNumber of information octets\nNumber of information octets + Number of overhead octets\nasynchronous transfer mode\nATM adaptation layer \navailable bit rate (ABR)\ncell loss priority (CLP)\nconstant bit rate (CBR)\ngeneric flow control (GFR)\nguaranteed frame rate\nheader error control (HEC)\nnon-real-time variable bit \nrate (nrt-VBR)\npayload type\nreal-time variable bit rate \nservice data unit (SDU)\nunspecified bit rate (UBR)\nvariable bit rate (VBR)\nvirtual channel\nvirtual path\nCHAPTER 11 / ASYNCHRONOUS TRANSFER MODE\nConsider the use of fixed-length packets. In this case the overhead consists of the\nheader octets. Define\nDerive an expression for N. Hint: The expression will need to use the operator\ninteger greater than or equal to Y.\nb. If cells have variable length, then overhead is determined by the header, plus the\nflags to delimit the cells or an additional length field in the header. Let \noverhead octets required to enable the use of variable-length cells.\nDerive an expression for N in terms of X, H, and Hv.\nPlot N versus message size for fixed- and\nvariable-length cells. Comment on the results.\nAnother key design decision for ATM is the size of the data field for fixed-size cells.\nLet us consider this decision from the point of view of efficiency and delay.\nAssume that an extended transmission takes place, so that all cells are completely\nfilled. Derive an expression for the efficiency N as a function of H and L.\nb. Packetization delay is the delay introduced into a transmission stream by the need\nto buffer bits until an entire packet is filled before transmission. Derive an expres-\nsion for this delay as a function of L and the data rate R of the source.\nCommon data rates for voice coding are 32 kbps and 64 kbps. Plot packetization\ndelay as a function of L for these two data rates; use a left-hand y-axis with a maxi-\nmum value of 2 ms. On the same graph, plot transmission efficiency as a function of\nL; use a right-hand y-axis with a maximum value of 100%. Comment on the results.\nConsider compressed video transmission in an ATM network. Suppose standard\nATM cells must be transmitted through five switches.The data rate is 43 Mbps.\nWhat is the transmission time for one cell through one switch?\nb. Each switch may be transmitting a cell from other traffic all of which we assume\nto have lower (non-preemptive for the cell) priority. If the switch is busy trans-\nmitting a cell, our cell has to wait until the other cell completes transmission. If\nthe switch is free our cell is transmitted immediately.What is the maximum time\nfrom when a typical video cell arrives at the first switch (and possibly waits)\nuntil it is finished being transmitted by the fifth and last one? Assume that you\ncan ignore propagation time, switching time, and everything else but the trans-\nmission time and the time spent waiting for another cell to clear a switch.\nNow suppose we know that each switch is utilized 60% of the time with the other low\npriority traffic. By this we mean that with probability 0.6 when we look at a switch it\nis busy.Suppose that if there is a cell being transmitted by a switch,the average delay\nspent waiting for a cell to finish transmission is one-half a cell transmission time.\nWhat is the average time from the input of the first switch to clearing the fifth?\nd. However, the measure of most interest is not delay but jitter, which is the variabil-\nity in the delay. Use parts (b) and (c) to calculate the maximum and average vari-\nability, respectively, in the delay.\nIn all cases assume that the various random events are independent of one another;\nfor example, we ignore the burstiness typical of such traffic.\nIn order to support IP service over an ATM network, IP datagrams must first be seg-\nmented into a number of ATM cells before sending them over the ATM network. As\nATM does not provide cell loss recovery, the loss of any of these cells will result in the\nloss of the entire IP packet. Given\nDerive an expression for PP, and comment on the resulting expression.\nb. What ATM service would you use to get the best possible performance?\nPP = IP-packet loss rate\nn = number of cells required to transmit a single IP datagram\nPC = cell loss rate in the ATM network\nL = 48, H = 5,\n<Y= = the smallest\nX = Number of information octets to be transmitted as a single message\nH = Header size of the cell in octets\nL = Data field size of the cell in octets\nRouting in Packet-Switching Networks\nExamples: Routing in Arpanet\nLeast-Cost Algorithms\nRecommended Reading\nKey Terms, Review Questions, and Problems\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\n“I tell you,” went on Syme with passion,“that every time a train comes in I feel that it\nhas broken past batteries of besiegers, and that man has won a battle against chaos.\nYou say contemptuously that when one has left Sloane Square one must come to Vic-\ntoria. I say that one might do a thousand things instead, and that whenever I really\ncome there I have the sense of hairbreadth escape. And when I hear the guard shout\nout the word ‘Victoria’, it is not an unmeaning word. It is to me the cry of a herald\nannouncing conquest. It is to me indeed ‘Victoria’; it is the victory of Adam.”\n—The Man Who Was Thursday, G.K. Chesterton\nA variety of routing algorithms have been developed for packet-\nswitching, frame relay, and ATM networks, and for the Internet and\ninternetworks.These algorithms share many common principles.\nRouting schemes can be categorized based on a number of factors, such\nas what criterion is used to determine the best route between two\nnodes, what strategy is used for obtaining information needed to deter-\nmine route, and whether a distributed or centralized algorithm is used.\nThe routing function attempts to find the least-cost route through the\nnetwork, with cost based on number of hops, expected delay, or other\nmetrics.Adaptive routing algorithms typically rely on the exchange of\ninformation about traffic conditions among nodes.\nA key design issue in switched networks, including packet-switching, frame relay,\nand ATM networks, and with internets, is that of routing. In general terms, the\nrouting function seeks to design routes through the network for individual pairs\nof communicating end nodes such that the network is used efficiently.\nThis chapter begins with a brief overview of issues involved in routing\ndesign. Next, we look at the routing function in packet-switching networks and\nthen examine least-cost algorithms that are a central part of routing in switched\nnetworks. These topics cover issues that are relevant to routing in internets as\nwell as packet-switching networks.\n12.1 ROUTING IN PACKET-SWITCHING NETWORKS\nOne of the most complex and crucial design aspects of switched data networks is\nrouting. This section surveys key characteristic that can be used to classify routing\nstrategies. The principles described in this section are also applicable to internet-\nwork routing, discussed in Part Five.\n12.1 / ROUTING IN PACKET-SWITCHING NETWORKS\n1The term hop is used somewhat loosely in the literature. The more common definition, which we use, is\nthat the number of hops along a path from a given source to a given destination is the number of links\nbetween network nodes (packet-switching nodes, ATM switches, routers, etc.) that a packet traverses\nalong that path. Sometimes the number of hops is defined to include the link between the source station\nand the network and the link between the destination station and the network.This latter definition pro-\nduces a value two greater than the definition we use.\nCharacteristics\nThe primary function of a packet-switching network is to accept packets from a source\nstation and deliver them to a destination station. To accomplish this, a path or route\nthrough the network must be determined; generally, more than one route is possible.\nThus, a routing function must be performed. The requirements for this function\n• Correctness\n• Simplicity\n• Optimality\n• Robustness\n• Efficiency\n• Stability\nThe first two items on the list, correctness and simplicity, are self-explanatory.\nRobustness has to do with the ability of the network to deliver packets via some route\nin the face of localized failures and overloads.Ideally,the network can react to such con-\ntingencies without the loss of packets or the breaking of virtual circuits. The designer\nwho seeks robustness must cope with the competing requirement for stability. Tech-\nniques that react to changing conditions have an unfortunate tendency to either react\ntoo slowly to events or to experience unstable swings from one extreme to another. For\nexample, the network may react to congestion in one area by shifting most of the load\nto a second area. Now the second area is overloaded and the first is underutilized, caus-\ning a second shift. During these shifts, packets may travel in loops through the network.\nA tradeoff also exists between fairness and optimality. Some performance cri-\nteria may give higher priority to the exchange of packets between nearby stations\ncompared to an exchange between distant stations. This policy may maximize aver-\nage throughput but will appear unfair to the station that primarily needs to commu-\nnicate with distant stations.\nFinally, any routing technique involves some processing overhead at each\nnode and often a transmission overhead as well, both of which impair network effi-\nciency.The penalty of such overhead needs to be less than the benefit accrued based\non some reasonable metric, such as increased robustness or fairness.\nWith these requirements in mind, we are in a position to assess the various\ndesign elements that contribute to a routing strategy. Table 12.1 lists these elements.\nSome of these categories overlap or are dependent on one another. Nevertheless, an\nexamination of this list serves to clarify and organize routing concepts.\nPerformance Criteria The selection of a route is generally based on some per-\nformance criterion.The simplest criterion is to choose the minimum-hop route (one\nthat passes through the least number of nodes) through the network.1 This is an\neasily measured criterion and should minimize the consumption of network\nresources. A generalization of the minimum-hop criterion is least-cost routing. In\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nElements of Routing Techniques for Packet-Switching Networks\nPerformance Criteria\nNetwork Information Source\nNumber of hops\nAdjacent node\nNodes along route\nDecision Time\nPacket (datagram)\nNetwork Information Update Timing\nSession (virtual circuit)\nDecision Place\nMajor load change\nEach node (distributed)\nTopology change\nCentral node (centralized)\nOriginating node (source)\nFigure 12.1\nExample Network Configuration\nthis case, a cost is associated with each link, and, for any pair of attached stations, the\nroute through the network that accumulates the least cost is sought. For example,\nFigure 12.1 illustrates a network in which the two arrowed lines between a pair of\nnodes represent a link between these nodes, and the corresponding numbers repre-\nsent the current link cost in each direction. The shortest path (fewest hops) from\nnode 1 to node 6 is 1-3-6 \nbut the least-cost path is 1-4-5-6\nCosts are assigned to links to support one or more design\nobjectives. For example, the cost could be inversely related to the data rate (i.e., the\nhigher the data rate on a link, the lower the assigned cost of the link) or the current\nqueuing delay on the link. In the first case, the least-cost route should provide the\nhighest throughput. In the second case, the least-cost route should minimize delay.\n1cost = 1 + 1 + 2 = 42.\n1cost = 5 + 5 = 102,\n12.1 / ROUTING IN PACKET-SWITCHING NETWORKS\nIn either the minimum-hop or least-cost approach, the algorithm for determin-\ning the optimum route for any pair of stations is relatively straightforward, and the\nprocessing time would be about the same for either computation. Because the least-\ncost criterion is more flexible, this is more common than the minimum-hop criterion.\nSeveral least-cost routing algorithms are in common use. These are described\nin Section 12.3.\nDecision Time and Place Routing decisions are made on the basis of some\nperformance criterion.Two key characteristics of the decision are the time and place\nthat the decision is made.\nDecision time is determined by whether the routing decision is made on a\npacket or virtual circuit basis. When the internal operation of the network is data-\ngram, a routing decision is made individually for each packet. For internal virtual\ncircuit operation, a routing decision is made at the time the virtual circuit is estab-\nlished. In the simplest case, all subsequent packets using that virtual circuit follow\nthe same route. In more sophisticated network designs, the network may dynami-\ncally change the route assigned to a particular virtual circuit in response to changing\nconditions (e.g., overload or failure of a portion of the network).\nThe term decision place refers to which node or nodes in the network are\nresponsible for the routing decision. Most common is distributed routing, in which\neach node has the responsibility of selecting an output link for routing packets as\nthey arrive. For centralized routing, the decision is made by some designated node,\nsuch as a network control center. The danger of this latter approach is that the loss\nof the network control center may block operation of the network. The distributed\napproach is perhaps more complex but is also more robust.A third alternative, used\nin some networks, is source routing. In this case, the routing decision is actually\nmade by the source station rather than by a network node and is then communi-\ncated to the network. This allows the user to dictate a route through the network\nthat meets criteria local to that user.\nThe decision time and decision place are independent design variables. For\nexample, in Figure 12.1, suppose that the decision place is each node and that the\nvalues depicted are the costs at a given instant in time: the costs may change. If a\npacket is to be delivered from node 1 to node 6, it might follow the route 1-4-5-6,\nwith each leg of the route determined locally by the transmitting node. Now let the\nvalues change such that 1-4-5-6 is no longer the optimum route. In a datagram net-\nwork, the next packet may follow a different route, again determined by each node\nalong the way. In a virtual circuit network, each node will remember the routing\ndecision that was made when the virtual circuit was established, and simply pass on\nthe packets without making a new decision.\nNetwork Information Source and Update Timing Most routing strate-\ngies require that decisions be based on knowledge of the topology of the network,\ntraffic load, and link cost. Surprisingly, some strategies use no such information and\nyet manage to get packets through; flooding and some random strategies (discussed\nlater) are in this category.\nWith distributed routing,in which the routing decision is made by each node,the\nindividual node may make use of only local information, such as the cost of each out-\ngoing link. Each node might also collect information from adjacent (directly\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nconnected) nodes, such as the amount of congestion experienced at that node. Finally,\nthere are algorithms in common use that allow the node to gain information from all\nnodes on any potential route of interest. In the case of centralized routing, the central\nnode typically makes use of information obtained from all nodes.\nA related concept is that of information update timing, which is a function of\nboth the information source and the routing strategy. Clearly, if no information is\nused (as in flooding), there is no information to update. If only local information is\nused, the update is essentially continuous. That is, an individual node always knows\nits local conditions. For all other information source categories (adjacent nodes, all\nnodes), update timing depends on the routing strategy. For a fixed strategy, the\ninformation is never updated. For an adaptive strategy, information is updated from\ntime to time to enable the routing decision to adapt to changing conditions.\nAs you might expect, the more information available, and the more frequently\nit is updated, the more likely the network is to make good routing decisions. On the\nother hand, the transmission of that information consumes network resources.\nRouting Strategies\nA large number of routing strategies have evolved for dealing with the routing\nrequirements of packet-switching networks. Many of these strategies are also\napplied to internetwork routing, which we cover in Part Five. In this section, we sur-\nvey four key strategies: fixed, flooding, random, and adaptive.\nFixed Routing For fixed routing, a single, permanent route is configured for each\nsource-destination pair of nodes in the network. Either of the least-cost routing\nalgorithms described in Section 12.3 could be used. The routes are fixed, or at least\nonly change when there is a change in the topology of the network. Thus, the link\ncosts used in designing routes cannot be based on any dynamic variable such as traf-\nfic.They could, however, be based on expected traffic or capacity.\nFigure 12.2 suggests how fixed routing might be implemented. A central rout-\ning matrix is created, to be stored perhaps at a network control center. The matrix\nshows, for each source-destination pair of nodes, the identity of the next node on the\nNote that it is not necessary to store the complete route for each possible\npair of nodes. Rather, it is sufficient to know, for each pair of nodes, the identity of\nthe first node on the route. To see this, suppose that the least-cost route from X to\nY begins with the X-A link. Call the remainder of the route \nthis is the part\nfrom A to Y. Define \nas the least-cost route from A to Y. Now, if the cost of \ngreater than that of \nthen the X-Y route can be improved by using \nIf the cost of \nis less than \nis not the least-cost route from A to Y.\nThus, at each point along a route, it is only necessary to know\nthe identity of the next node, not the entire route. In our example, the route from\nnode 1 to node 6 begins by going through node 4.Again consulting the matrix, the\nroute from node 4 to node 6 goes through node 5. Finally, the route from node 5 to\nnode 6 is a direct link to node 6.Thus, the complete route from node 1 to node 6 is\nFrom this overall matrix, routing tables can be developed and stored at each\nnode. From the reasoning in the preceding paragraph, it follows that each node need\n12.1 / ROUTING IN PACKET-SWITCHING NETWORKS\nNode 4 Directory\nDestination\nNode 5 Directory\nDestination\nNode 6 Directory\nDestination\nNode 3 Directory\nDestination\nNode 2 Directory\nDestination\nNode 1 Directory\nDestination\nFigure 12.2\nFixed Routing (using Figure 12.1)\nonly store a single column of the routing directory. The node’s directory shows the\nnext node to take for each destination.\nWith fixed routing, there is no difference between routing for datagrams and\nvirtual circuits. All packets from a given source to a given destination follow the\nsame route. The advantage of fixed routing is its simplicity, and it should work well\nin a reliable network with a stable load. Its disadvantage is its lack of flexibility. It\ndoes not react to network congestion or failures.\nA refinement to fixed routing that would accommodate link and node outages\nwould be to supply the nodes with an alternate next node for each destination. For\nexample, the alternate next nodes in the node 1 directory might be 4, 3, 2, 3, 3.\nFlooding Another simple routing technique is flooding. This technique requires\nno network information whatsoever and works as follows. A packet is sent by a\nsource node to every one of its neighbors. At each node, an incoming packet is\nretransmitted on all outgoing links except for the link on which it arrived. For exam-\nple, if node 1 in Figure 12.1 has a packet to send to node 6, it send a copy of that\npacket (with a destination address of 6), to nodes 2, 3, and 4. Node 2 will send a copy\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\n2For each pair of end systems attached to the network, there is a minimum-hop path. The length of the\nlongest such minimum-hop path is the diameter of the network.\nto nodes 3 and 4. Node 4 will send a copy to nodes 2, 3, and 5. And so it goes. Even-\ntually, a number of copies of the packet will arrive at node 6. The packet must have\nsome unique identifier (e.g., source node and sequence number, or virtual circuit\nnumber and sequence number) so that node 6 knows to discard all but the first copy.\nUnless something is done to stop the incessant retransmission of packets, the\nnumber of packets in circulation just from a single source packet grows without\nbound. One way to prevent this is for each node to remember the identity of those\npackets it has already retransmitted. When duplicate copies of the packet arrive,\nthey are discarded. A simpler technique is to include a hop count field with each\npacket. The count can originally be set to some maximum value, such as the diame-\nter (length of the longest minimum-hop path through the network)2 of the network.\nEach time a node passes on a packet, it decrements the count by one. When the\ncount reaches zero, the packet is discarded.\nAn example of the latter tactic is shown in Figure 12.3. The label on each\npacket in the figure indicates the current value of the hop count field in that packet.\nA packet is to be sent from node 1 to node 6 and is assigned a hop count of 3. On the\nfirst hop, three copies of the packet are created, and the hop count is decrement to\n2. For the second hop of all these copies, a total of nine copies are created. One of\nthese copies reaches node 6, which recognizes that it is the intended destination and\ndoes not retransmit. However, the other nodes generate a total of 22 new copies for\ntheir third and final hop. Each packet now has a hope count of 1. Note that if a node\nis not keeping track of packet identifier, it may generate multiple copies at this third\nstage.All packets received from the third hop are discarded, because the hop count\nis exhausted. In all, node 6 has received four additional copies of the packet.\nThe flooding technique has three remarkable properties:\n• All possible routes between source and destination are tried. Thus, no matter\nwhat link or node outages have occurred, a packet will always get through if at\nleast one path between source and destination exists.\n• Because all routes are tried, at least one copy of the packet to arrive at the des-\ntination will have used a minimum-hop route.\n• All nodes that are directly or indirectly connected to the source node are\nBecause of the first property, the flooding technique is highly robust and\ncould be used to send emergency messages. An example application is a military\nnetwork that is subject to extensive damage. Because of the second property, flood-\ning might be used initially to set up the route for a virtual circuit.The third property\nsuggests that flooding can be useful for the dissemination of important information\nto all nodes; we will see that it is used in some schemes to disseminate routing\ninformation.\nThe principal disadvantage of flooding is the high traffic load that it generates,\nwhich is directly proportional to the connectivity of the network.\n12.1 / ROUTING IN PACKET-SWITCHING NETWORKS\n(a) First hop\n(b) Second hop\n(c) Third hop\nFigure 12.3\nFlooding Example 1hop count = 32\nRandom Routing Random routing has the simplicity and robustness of\nflooding with far less traffic load. With random routing, a node selects only\none outgoing path for retransmission of an incoming packet. The outgoing\nlink is chosen at random, excluding the link on which the packet arrived. If\nall links are equally likely to be chosen, then a node may simply utilize out-\ngoing links in a round-robin fashion.\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nA refinement of this technique is to assign a probability to each outgoing link and to\nselect the link based on that probability.The probability could be based on data rate,\nin which case we have\nThe sum is taken over all candidate outgoing links. This scheme should pro-\nvide good traffic distribution. Note that the probabilities could also be based on\nfixed link costs.\nLike flooding, random routing requires the use of no network information.\nBecause the route taken is random, the actual route will typically not be the least-\ncost route nor the minimum-hop route. Thus, the network must carry a higher than\noptimum traffic load, although not nearly as high as for flooding.\nAdaptive Routing In virtually all packet-switching networks, some sort of adap-\ntive routing technique is used.That is, the routing decisions that are made change as\nconditions on the network change. The principal conditions that influence routing\ndecisions are\n• Failure: When a node or link fails, it can no longer be used as part of a route.\n• Congestion: When a particular portion of the network is heavily congested, it is\ndesirable to route packets around rather than through the area of congestion.\nFor adaptive routing to be possible, information about the state of the network\nmust be exchanged among the nodes. There are several drawbacks associated with\nthe use of adaptive routing, compared to fixed routing:\n• The routing decision is more complex; therefore, the processing burden on\nnetwork nodes increases.\n• In most cases, adaptive strategies depend on status information that is collected\nat one place but used at another.There is a tradeoff here between the quality of\nthe information and the amount of overhead. The more information that is\nexchanged, and the more frequently it is exchanged, the better will be the rout-\ning decisions that each node makes. On the other hand, this information is itself\na load on the constituent networks, causing a performance degradation.\n• An adaptive strategy may react too quickly, causing congestion-producing\noscillation, or too slowly, being irrelevant.\nDespite these real dangers, adaptive routing strategies are by far the most\nprevalent, for two reasons:\n• An adaptive routing strategy can improve performance, as seen by the net-\nRi = data rate on link i\nPi = probability of selecting link i\n12.1 / ROUTING IN PACKET-SWITCHING NETWORKS\n• An adaptive routing strategy can aid in congestion control, which is discussed\nin Chapter 13. Because an adaptive routing strategy tends to balance loads, it\ncan delay the onset of severe congestion.\nThese benefits may or may not be realized, depending on the soundness of the\ndesign and the nature of the load. By and large, adaptive routing is an extraordinar-\nily complex task to perform properly. As demonstration of this, most major packet-\nswitching networks, such as ARPANET and its successors, and many commercial\nnetworks, have endured at least one major overhaul of their routing strategy.\nA convenient way to classify adaptive routing strategies is on the basis of\ninformation source: local, adjacent nodes, all nodes.An example of an adaptive rout-\ning strategy that relies only on local information is one in which a node routes each\npacket to the outgoing link with the shortest queue length, Q. This would have the\neffect of balancing the load on outgoing links. However, some outgoing links may\nnot be headed in the correct general direction.We can improve matters by also tak-\ning into account preferred direction, much as with random routing. In this case, each\nlink emanating from the node would have a bias \nfor each destination i, such that\nlower values of \nindicate more preferred directions. For each incoming packet\nheaded for node i, the node would choose the outgoing link that minimizes \nThus a node would tend to send packets in the right direction, with a concession\nmade to current traffic delays.\nAs an example, Figure 12.4 show the status of node 4 of Figure 12.1 at a certain\npoint in time. Node 4 has links to four other nodes. Packets have been arriving and a\nbacklog has built up, with a queue of packets waiting for each of the outgoing links.\nA packet arrives from node 1 destined for node 6.To which outgoing link should the\npacket be routed? Based on current queue lengths and the values of bias \nNode 4's Bias\nDestination 6\nNext Node Bias\nFigure 12.4\nExample of Isolated Adaptive Routing\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\neach outgoing link, the minimum value of \nis 4, on the link to node 3. Thus,\nnode 4 routes the packet through node 3.\nAdaptive schemes based only on local information are rarely used because\nthey do not exploit easily available information. Strategies based on information\nfrom adjacent nodes or all nodes are commonly found. Both take advantage of\ninformation that each node has about delays and outages that it experiences. Such\nadaptive strategies can be either distributed or centralized. In the distributed case,\neach node exchanges delay information with other nodes. Based on incoming infor-\nmation, a node tries to estimate the delay situation throughout the network, and\napplies a least-cost routing algorithm. In the centralized case, each node reports its\nlink delay status to a central node, which designs routes based on this incoming\ninformation and sends the routing information back to the nodes.\n12.2 EXAMPLES: ROUTING IN ARPANET\nIn this section, we look at several examples of routing strategies. All of these were\ninitially developed for ARPANET, which is a packet-switching network that was the\nfoundation of the present-day Internet. It is instructive to examine these strategies\nfor several reasons. First, these strategies and similar ones are also used in other\npacket-switching networks, including a number of networks on the Internet. Second,\nrouting schemes based on the ARPANET work have also been used for internet-\nwork routing in the Internet and in private internetworks. And finally, the\nARPANET routing scheme evolved in a way that illuminates some of the key\ndesign issues related to routing algorithms.\nFirst Generation\nThe original routing algorithm, designed in 1969, was a distributed adaptive algo-\nrithm using estimated delay as the performance criterion and a version of the Bell-\nman-Ford algorithm (Section 12.3). For this algorithm, each node maintains two\nsij = the next node in the current minimum-delay route from i to j\nSi = successor node vector for node i\nN = number of nodes in the network\ndij = current estimate of minimum delay from node i to node j 1dii = 02\nDi = delay vector for node i\n12.2 / EXAMPLES: ROUTING IN ARPANET\nPeriodically (every 128 ms), each node exchanges its delay vector with all of its\nneighbors. On the basis of all incoming delay vectors, a node k updates both of its\nvectors as follows:\nFigure 12.5 provides an example of the original ARPANET algorithm, using the\nnetwork of Figure 12.6. This is the same network as that of Figure 12.1, with some of\nthe link costs having different values (and assuming the same cost in both directions).\nFigure 12.5a shows the routing table for node 1 at an instant in time that reflects the\nlki = current estimate of delay from k to i\nA = set of neighbor nodes for k\nusing i that minimizes the preceding expression\ni H A [dij + lki]\n(c) Node 1's routing table\nafter update and link\ncosts used in update\n(b) Delay vectors sent to node 1\nfrom neighbor nodes\n(a) Node 1's routing\ntable before update\nFigure 12.5\nOriginal ARPANET Routing Algorithm\nFigure 12.6\nNetwork for Example of Figure 12.5a\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nlink costs of Figure 12.6. For each destination, a delay is specified, and the next node\non the route that produces that delay.At some point, the link costs change to those of\nFigure 12.1. Assume that node 1’s neighbors (nodes 2, 3, and 4) learn of the change\nbefore node 1. Each of these nodes updates its delay vector and sends a copy to all of\nits neighbors,including node 1 (Figure 12.5b).Node 1 discards its current routing table\nand builds a new one, based solely on the incoming delay vector and its own estimate\nof link delay to each of its neighbors.The result is shown in Figure 12.5c.\nThe estimated link delay is simply the queue length for that link.Thus, in build-\ning a new routing table, the node will tend to favor outgoing links with shorter queues.\nThis tends to balance the load on outgoing links. However, because queue lengths\nvary rapidly with time, the distributed perception of the shortest route could change\nwhile a packet is en route. This could lead to a thrashing situation in which a packet\ncontinues to seek out areas of low congestion rather than aiming at the destination.\nSecond Generation\nAfter some years of experience and several minor modifications, the original rout-\ning algorithm was replaced by a quite different one in 1979 [MCQU80]. The major\nshortcomings of the old algorithm were as follows:\n• The algorithm did not consider line speed, merely queue length. Thus, higher-\ncapacity links were not given the favored status they deserved.\n• Queue length is, in any case, an artificial measure of delay, because some vari-\nable amount of processing time elapses between the arrival of a packet at a\nnode and its placement in an outbound queue.\n• The algorithm was not very accurate. In particular, it responded slowly to con-\ngestion and delay increases.\nThe new algorithm is also a distributed adaptive one, using delay as the perfor-\nmance criterion, but the differences are significant. Rather than using queue length as\na surrogate for delay, the delay is measured directly.At a node, each incoming packet\nis timestamped with an arrival time.A departure time is recorded when the packet is\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\n• The overutilization of some links can lead to the spread of congestion within\nthe network (this will be seen in the discussion of congestion in Chapter 13).\n• The large swings in measured delay values result in the need for more fre-\nquent routing update messages. This increases the load on the network at just\nthe time when the network is already stressed.\nThe ARPANET designers concluded that the essence of the problem was that\nevery node was trying to obtain the best route for all destinations,and that these efforts\nconflicted. It was concluded that under heavy loads, the goal of routing should be to\ngive the average route a good path instead of attempting to give all routes the best path.\nThe designers decided that it was unnecessary to change the overall routing\nalgorithm. Rather, it was sufficient to change the function that calculates link costs.\nThis was done in such a way as to damp routing oscillations and reduce routing\noverhead. The calculation begins with measuring the average delay over the last\n10 seconds.This value is then transformed with the following steps:\n1. Using a simple single-server queuing model, the measured delay is trans-\nformed into an estimate of link utilization. From queuing theory, utilization\ncan be expressed as a function of delay as follows:\nThe service time was set at the network-wide average packet size (600 bits)\ndivided by the data rate of the link.\n2. The result is then smoothed by averaging it with the previous estimate of utilization:\nAveraging increases the period of routing oscillations, thus reducing routing\n3. The link cost is then set as a function of average utilization that is designed to\nprovide a reasonable estimate of cost while avoiding oscillation. Figure 12.8\nindicates the way in which the estimate of utilization is converted into a cost\nvalue.The final cost value is, in effect, a transformed value of delay.\nIn Figure 12.8, delay is normalized to the value achieved on an idle line, which is\njust propagation delay plus transmission time. One curve on the figure indicates the\nway in which the actual delay rises as a function of utilization; the increase in delay is\ndue to queuing delay at the node. For the revised algorithm, the cost value is kept at\nr1n2 = link utilization measured at sampling time n\nU1n2 = average utilization calculated at sampling time n\nU1n + 12 = 0.5 * r1n + 12 + 0.5 * U1n2\nTs = service time\nT = measured delay\nr = link utilization\n12.3 / LEAST-COST ALGORITHMS\nFigure 12.8\nARPANET Delay Metrics\nthe minimum value until a given level of utilization is reached. This feature has the\neffect of reducing routing overhead at low traffic levels. Above a certain level of uti-\nlization, the cost level is allowed to rise to a maximum value that is equal to three\ntimes the minimum value. The effect of this maximum value is to dictate that traffic\nshould not be routed around a heavily utilized line by more than two additional hops.\nNote that the minimum threshold is set higher for satellite links. This encour-\nages the use of terrestrial links under conditions of light traffic, because the terres-\ntrial links have much lower propagation delay. Note also that the actual delay curve\nis much steeper than the transformation curves at high utilization levels. It is this\nsteep rise in link cost that causes all of the traffic on a link to be shed, which in turn\ncauses routing oscillations.\nIn summary, the revised cost function is keyed to utilization rather than delay.\nThe function acts similar to a delay-based metric under light loads and to a capacity-\nbased metric under heavy loads.\n12.3 LEAST-COST ALGORITHMS\nVirtually all packet-switching networks and all internets base their routing decision\non some form of least-cost criterion. If the criterion is to minimize the number of\nhops, each link has a value of 1. More typically, the link value is inversely propor-\ntional to the link capacity, proportional to the current load on the link, or some com-\nbination. In any case, these link or hop costs are used as input to a least-cost routing\nalgorithm, which can be simply stated as follows:\nGiven a network of nodes connected by bidirectional links, where each\nlink has a cost associated with it in each direction, define the cost of a path\nEstimated utilization\nTheoretical\nqueueing delay\nterrestrial link\nsatellite link\nDelay (hops)\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nbetween two nodes as the sum of the costs of the links traversed. For each\npair of nodes, find a path with the least cost.\nNote that the cost of a link may differ in its two directions.This would be true,\nfor example, if the cost of a link equaled the length of the queue of packets awaiting\ntransmission from each of the two nodes on the link.\nMost least-cost routing algorithms in use in packet-switching networks and\ninternets are variations of one of two common algorithms, known as Dijkstra’s algo-\nrithm and the Bellman-Ford algorithm. This section provides a summary of these\ntwo algorithms.\nDijkstra’s Algorithm\nDijkstra’s algorithm [DIJK59] can be stated as: Find the shortest paths from a\ngiven source node to all other nodes by developing the paths in order of increas-\ning path length. The algorithm proceeds in stages. By the kth stage, the shortest\npaths to the k nodes closest to (least cost away from) the source node have been\ndetermined; these nodes are in a set T. At stage \nthe node not in T that has\nthe shortest path from the source node is added to T. As each node is added to T,\nits path from the source is defined. The algorithm can be formally described as\nfollows. Define:\nN \u0001 set of nodes in the network\ns \u0001 source node\nT \u0001 set of nodes so far incorporated by the algorithm\nw(i, j) \u0001 rom cost from node i to node j; w(i, i) \u0001 0; w(i, j) \u0001 \f if the two nodes are\nnot directly connected; w(i, j) \n 0 if the two nodes are directly connected\nL(n) \u0001 cost of the least-cost path from node s to node n that is currently known\nto the algorithm; at termination, this is the cost of the least-cost path in\nthe graph from s to n\nThe algorithm has three steps; steps 2 and 3 are repeated until \nsteps 2 and 3 are repeated until final paths have been assigned to all nodes in the\n1. [Initialization]\ni.e., the set of nodes so far incorporated consists\nof only the source node\ni.e., the initial path costs to neighboring nodes are\nsimply the link costs\n2. [Get Next Node] Find the neighboring node not in T that has the least-cost path\nfrom node s and incorporate that node into T: Also incorporate the edge that is\nincident on that node and a node in T that contributes to the path. This can be\nexpressed as\nFind x x T such that L(x) = min\nL(n) = w(s, n)\n12.3 / LEAST-COST ALGORITHMS\nAdd x to T; add to T the edge that is incident on x and that contributes the least\ncost component to L(x), that is, the last hop in the path.\n3. [Update Least-Cost Paths]\nIf the latter term is the minimum, the path from s to n is now the path from s to\nx concatenated with the edge from x to n.\nThe algorithm terminates when all nodes have been added to T. At termina-\ntion, the value L(x) associated with each node x is the cost (length) of the least-cost\npath from s to x. In addition, T defines the least-cost path from s to each other node.\nOne iteration of steps 2 and 3 adds one new node to T and defines the least-\ncost path from s to that node. That path passes only through nodes that are in T. To\nsee this, consider the following line of reasoning.After k iterations, there are k nodes\nin T, and the least-cost path from s to each of these nodes has been defined. Now\nconsider all possible paths from s to nodes not in T. Among those paths, there is one\nof least cost that passes exclusively through nodes in T (see Problem 12.4), ending\nwith a direct link from some node in T to a node not in T. This node is added to T\nand the associated path is defined as the least-cost path for that node.\nTable 12.2a and Figure 12.9 show the result of applying this algorithm to the\ngraph of Figure 12.1, using \nThe shaded edges define the spanning tree for the\ngraph.The values in each circle are the current estimates of L(x) for each node x. A\nL(n) = min[L(n), L(x) + w(x, n)] for all n x T\nExample of Least-Cost Routing Algorithms (using Figure 12.1)\n(a) Dijkstra’a Algorithm \n51, 2, 3, 4, 5, 66\n51, 2, 3, 4, 56\n51, 2, 4, 56\n(b) Bellman-Ford Algorithm \nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nnode is shaded when it is added to T. Note that at each step the path to each node\nplus the total cost of that path is generated. After the final iteration, the least-cost\npath to each node and the cost of that path have been developed. The same proce-\ndure can be used with node 2 as source node, and so on.\nBellman-Ford Algorithm\nThe Bellman-Ford algorithm [FORD62] can be stated as follows: Find the shortest\npaths from a given source node subject to the constraint that the paths contain at\nmost one link, then find the shortest paths with a constraint of paths of at most two\nFigure 12.9\nDijkstra’s Algorithm Applied to Graph of Figure 12.1\nT \u0001 {1, 2, 4}\nT \u0001 {1, 2, 4, 5}\nT \u0001 {1, 2, 3, 4, 5}\nT \u0001 {1, 2, 3, 4, 5, 6}\n12.3 / LEAST-COST ALGORITHMS\nlinks, and so on. This algorithm also proceeds in stages. The algorithm can be for-\nmally described as follows. Define\ns \u0001 source node\nw(i, j) \u0001 link cost from node i to node j; w(i, i) = 0; w(i, j) \u0001 \f if the two nodes are\nnot directly connected;\nif the two nodes are directly connected\nh \u0001 maximum number of links in a path at the current stage of the algorithm\nLh(n) \u0001 cost of the least-cost path from node s to node n under the constraint of no\nmore than h links\n1. [Initialization]\n2. [Update]\nFor each successive \nConnect n with the predecessor node j that achieves the minimum, and elimi-\nnate any connection of n with a different predecessor node formed during an\nearlier iteration.The path from s to n terminates with the link from j to n.\nFor the iteration of step 2 with \nand for each destination node n, the\nalgorithm compares potential paths from s to n of length \nwith the path that\nexisted at the end of the previous iteration. If the previous, shorter, path has less\ncost, then that path is retained. Otherwise a new path with length \nfrom s to n; this path consists of a path of length K from s to some node j, plus a\ndirect hop from node j to node n. In this case, the path from s to j that is used is the\nK-hop path for j defined in the previous iteration (see Problem 12.5).\nTable 12.2b and Figure 12.10 show the result of applying this algorithm to\nFigure 12.1, using \nAt each step, the least-cost paths with a maximum number\nof links equal to h are found. After the final iteration, the least-cost path to each\nnode and the cost of that path have been developed. The same procedure can be\nused with node 2 as source node, and so on. Note that the results agree with those\nobtained using Dijkstra’s algorithm.\nOne interesting comparison can be made between these two algorithms, having to\ndo with what information needs to be gathered. Consider first the Bellman-Ford\nalgorithm. In step 2, the calculation for node n involves knowledge of the link cost to\nall neighboring nodes to node n [i.e., w(j, n)] plus the total path cost to each of those\nneighboring nodes from a particular source node s [i.e.,\n]. Each node can main-\ntain a set of costs and associated paths for every other node in the network and\nexchange this information with its direct neighbors from time to time. Each node\ncan therefore use the expression in step 2 of the Bellman-Ford algorithm, based\nonly on information from its neighbors and knowledge of its link costs, to update its\nLh+11n2 = min\nj [Lh1j2 + w1j, n2]\nLh1s2 = 0, for all h\nL01n2 = q, for all n Z s\nw1i, j2 Ú 0\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\ncosts and paths. On the other hand, consider Dijkstra’s algorithm. Step 3 appears to\nrequire that each node must have complete topological information about the net-\nwork. That is, each node must know the link costs of all links in the network. Thus,\nfor this algorithm, information must be exchanged with all other nodes.\nIn general, evaluation of the relative merits of the two algorithms should con-\nsider the processing time of the algorithms and the amount of information that must\nbe collected from other nodes in the network or internet. The evaluation will\ndepend on the implementation approach and the specific implementation.\nA final point: Both algorithms are known to converge under static conditions\nof topology, and link costs and will converge to the same solution. If the link costs\nchange over time, the algorithm will attempt to catch up with these changes. How-\never, if the link cost depends on traffic, which in turn depends on the routes chosen,\nthen a feedback condition exists, and instabilities may result.\n12.4 RECOMMENDED READING\n[MAXE90] is a useful survey of routing algorithms.Another survey, with numerous examples,\nis [SCHW80].\n[CORM01] contains a detailed analysis of the least-cost algorithms discussed in this\nchapter. [BERT92] also discusses these algorithms in detail.\nFigure 12.10\nBellman-Ford Algorithm Applied to Graph of Figure 12.1\n12.5 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nBertsekas, D., and Gallager, R. Data Networks. Upper Saddle River, NJ: Prentice\nHall, 1992.\nCormen,T., et al. Introduction to Algorithms. Cambridge, MA: MIT Press, 2001.\nMaxemchuk, N., and Zarki, M. “Routing and Flow Control in High-Speed\nWide-Area Networks.” Proceedings of the IEEE, January 1990.\nSchwartz, M., and Stern, T. “Routing Techniques Used in Computer Commu-\nnication Networks.” IEEE Transactions on Communications, April 1980.\n12.5 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nadaptive routing\nalternate routing\nBellman-Ford algorithm\nDijkstra’s algorithm\nfixed routing\nleast-cost algorithms\nrandom routing\nReview Questions\nWhat are the key requirements for a routing function for a packet-switching\nWhat is fixed routing?\nWhat is flooding?\nWhat are the advantages and disadvantages of adaptive routing?\nWhat is a least-cost algorithm?\nWhat is the essential difference between Dijkstra’s algorithm and the Bellman-Ford\nConsider a packet-switching network of N nodes, connected by the following\ntopologies:\nStar: one central node with no attached station; all other nodes attach to the cen-\nb. Loop: each node connects to two other nodes to form a closed loop.\nFully connected: each node is directly connected to all other nodes.\nFor each case, give the average number of hops between stations.\nConsider a binary tree topology for a packet-switching network. The root node con-\nnects to two other nodes.All intermediate nodes connect to one node in the direction\ntoward the root, and two in the direction away from the root.At the bottom are nodes\nwith just one link back toward the root. If there are \nnodes, derive an expres-\nsion for the mean number of hops per packet for large N, assuming that trips between\nall node pairs are equally likely. Hint: You will find the following equalities useful:\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nDijkstra’s algorithm, for finding the least-cost path from a specified node s to a speci-\nfied node t, can be expressed in the following program:\nfor n :\u0001 1 to N do\nL[n] :\u0001 \f; final[n] :\u0001 false; {all nodes are temporarily labeled with\n\f} pred[n] :\u0001 1\nL[s] :\u0001 0; final[s] :\u0001 true;\n{node s is permanently labeled with 0}\nrecent :\u0001 s;\n{the most recent node to be permanently labeled is s}\npath :=  true;\n{initialization over }\nwhile final[t] = false do\nfor n :\u0001 1 to N do\n{find new label}\nif (w[recent, n] < \f) AND (NOT final[n]) then\n{for every immediate successor of recent that is not permanently labeled, do }\nbegin {update temporary labels}\nnewlabel :\u0001 L[recent] \u0004 w[recent,n];\nif newlabel \bL[n] then\nbegin L[n] :\u0001 newlabel; pred[n] :\u0001 recent end\n{re-label n if there is a shorter path via node recent and make\nrecent the predecessor of n on the shortest path from s}\nfor x :\u0001 1 to N do {find node with smallest temporary label}\nif (NOT final[x]) AND (L[x] \b temp) then\nbegin y :\u0001 x; temp :\u0001L[x] end;\nif temp < 8 then {there is a path} then\nbegin final[y] :\u0001 true; recent :\u0001 y end\n{y, the next closest node to s gets permanently labeled}\nelse begin path :\u0001 false; final[t] :\u0001 true end\nIn this program, each node is assigned a temporary label initially. As a final path to a\nnode is determined, it is assigned a permanent label equal to the cost of the path from\ns. Write a similar program for the Bellman-Ford algorithm. Hint: The Bellman-Ford\nalgorithm is often called a label-correcting method, in contrast to Dijkstra’s label-\nsetting method.\nIn the discussion of Dijkstra’s algorithm in Section 12.3, it is asserted that at each iter-\nation, a new node is added to T and that the least-cost path for that new node passes\nonly through nodes already in T. Demonstrate that this is true. Hint: Begin at the\nbeginning. Show that the first node added to T must have a direct link to the source\nnode. Then show that the second node to T must either have a direct link to the\nsource node or a direct link to the first node added to T, and so on. Remember that all\nlink costs are assumed nonnegative.\nIn the discussion of the Bellman-Ford algorithm, it is asserted that at the iteration for\nif any path of length \nis defined, the first K hops of that path\nform a path defined in the previous iteration. Demonstrate that this is true.\nIn step 3 of Dijkstra’s algorithm, the least-cost path values are only updated for nodes\nnot yet in T. Is it possible that a lower-cost path could be found to a node already in\nT? If so, demonstrate by example. If not, provide reasoning as to why not.\nUsing Dijkstra’s algorithm, generate a least-cost route to all other nodes for nodes 2\nthrough 6 of Figure 12.1. Display the results as in Table 12.2a.\nRepeat Problem 12.7 using the Bellman-Ford algorithm.\n12.5 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nApply Dijkstra’s routing algorithm to the networks in Figure 12.11. Provide a table\nsimilar to Table 12.2a and a figure similar to Figure 12.9.\nRepeat Problem 12.9 using the Bellman-Ford algorithm.\nWill Dijkstra’s algorithm and the Bellman-Ford algorithm always yield the same solu-\ntions? Why or why not?\nBoth Dijkstra’s algorithm and the Bellman-Ford algorithm find the least-cost paths\nfrom one node to all other nodes. The Floyd-Warshall algorithm finds the least-cost\npaths between all pairs of nodes together. Define\nN \u0001 set of nodes in the network\nw(i, j) \u0001 link cost from node i to node j; w(i, i) \u0001 0; w(i, j) \u0001 \f if the two nodes are\nnot directly connected\nLn(i, j) \u0001 cost of the least-cost path from node i to node j with the constraint that\nonly nodes 1, 2, . . . , n can be used as intermediate nodes on paths\nThe algorithm has the following steps:\nInitialize:\nExplain the algorithm in words. Use induction to demonstrate that the algorithm\nLn+11i, j2 = min[Ln1i, j2, Ln1i, n + 12 + Ln1n + 1, j2] for all i Z j\nn = 0, 1, Á , N - 1\nL01i, j2 = w1i, j2, for all i, j, i Z j\nFigure 12.11\nPacket-Switching Networks with Link Costs\nCHAPTER 12 / ROUTING IN SWITCHED NETWORKS\nIn Figure 12.3, node 1 sends a packet to node 6 using flooding. Counting the transmis-\nsion of one packet across one link as a load of one, what is the total load generated if\nEach node discards duplicate incoming packets?\nb. A hop count field is used and is initially set to 5, and no duplicate is discarded?\nIt was shown that flooding can be used to determine the minimum-hop route. Can it\nbe used to determine the minimum delay route?\nWith random routing, only one copy of the packet is in existence at a time. Neverthe-\nless, it would be wise to utilize a hop count field.Why?\nAnother adaptive routing scheme is known as backward learning. As a packet is\nrouted through the network, it carries not only the destination address, but the source\naddress plus a running hop count that is incremented for each hop. Each node builds\na routing table that gives the next node and hop count for each destination. How is\nthe packet information used to build the table? What are the advantages and disad-\nvantages of this technique?\nBuild a centralized routing directory for the networks of Figure 12.11.\nConsider a system using flooding with a hop counter. Suppose that the hop counter is\noriginally set to the ”diameter” of the network.When the hop count reaches zero, the\npacket is discarded except at its destination. Does this always ensure that a packet\nwill reach its destination if there exists at least one operable path? Why or why not?\nEffects of Congestion\nCongestion Control\nTraffic Management\nCongestion Control In Packet-Switching Networks\nFrame Relay Congestion Control\nATM Traffic Management\nATM-GFR Traffic Management\nRecommended Reading\nKey Terms, Review Questions, and Problems\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\n1In this chapter we use the term packet in a broad sense, to include packets in a packet-switching \nnetwork, frames in a frame relay network, cells in an ATM network, or IP datagrams in an internet.\n3In the case of a switch of a packet-switching, frame relay, or ATM network, each I/O port connects to a\ntransmission link that connects to another node or end system. In the case of a router of an internet, each\nI/O port connects to either a direct link to another node or to a subnetwork.\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nOutput queue\nInput queue\nOutput queue\nInput queue\nFigure 13.2\nInteraction of Queues in a Data Network\nTo subscriber\nTo subscriber\nInput buffer\nOutput buffer\nFigure 13.1\nInput and Output Queues at Node\nis also managing a number of queues. If node 6 restrains the flow of packets from\nnode 5, this causes the output buffer in node 5 for the port to node 6 to fill up.Thus,\ncongestion at one point in the network can quickly propagate throughout a region\nor the entire network.While flow control is indeed a powerful tool, we need to use it\nin such a way as to manage the traffic on the entire network.\n13.1 / EFFECTS OF CONGESTION\nNormalized load\nNormalized throughput\nNormalized load\nFigure 13.3\nIdeal Network Utilization\nIdeal Performance\nFigure 13.3 suggests the ideal goal for network utilization. The top graph plots the\nsteady-state total throughput (number of packets delivered to destination end systems)\nthrough the network as a function of the offered load (number of packets transmitted\nby source end systems), both normalized to the maximum theoretical throughput of\nthe network. For example, if a network consists of a single node with two full-duplex \n1-Mbps links, then the theoretical capacity of the network is 2 Mbps, consisting of a \n1-Mbps flow in each direction.In the ideal case,the throughput of the network increases\nto accommodate load up to an offered load equal to the full capacity of the network;\nthen normalized throughput remains at 1.0 at higher input loads. Note, however, what\nhappens to the end-to-end delay experienced by the average packet even with this\nassumption of ideal performance. At negligible load, there is some small constant\namount of delay that consists of the propagation delay through the network from\nsource to destination plus processing delay at each node. As the load on the network\nincreases, queuing delays at each node are added to this fixed amount of delay. When\nthe load exceeds the network capacity, delays increase without bound.\nHere is a simple intuitive explanation of why delay must go to infinity. Sup-\npose that each node in the network is equipped with buffers of infinite size and\nsuppose that the input load exceeds network capacity. Under ideal conditions, the\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nnetwork will continue to sustain a normalized throughput of 1.0.Therefore, the rate\nof packets leaving the network is 1.0. Because the rate of packets entering the net-\nwork is greater than 1.0, internal queue sizes grow. In the steady state, with input\ngreater than output, these queue sizes grow without bound and therefore queuing\ndelays grow without bound.\nIt is important to grasp the meaning of Figure 13.3 before looking at real-world\nconditions.This figure represents the ideal, but unattainable, goal of all traffic and con-\ngestion control schemes.No scheme can exceed the performance depicted in Figure 13.3.\nPractical Performance\nThe ideal case reflected in Figure 13.3 assumes infinite buffers and no overhead\nrelated to congestion control. In practice, buffers are finite, leading to buffer over-\nflow, and attempts to control congestion consume network capacity in the exchange\nof control signals.\nLet us consider what happens in a network with finite buffers if no attempt is\nmade to control congestion or to restrain input from end systems.The details will, of\ncourse, differ depending on network configuration and on the statistics of the pre-\nsented traffic. However, the graphs in Figure 13.4 depict the devastating outcome in\ngeneral terms.\nAt light loads,throughput and hence network utilization increases as the offered\nload increases. As the load continues to increase, a point is reached (point A in the\nplot) beyond which the throughput of the network increases at a rate slower than the\nrate at which offered load is increased. This is due to network entry into a moder-\nate congestion state. In this region, the network continues to cope with the load,\nalthough with increased delays. The departure of throughput from the ideal is\naccounted for by a number of factors. For one thing, the load is unlikely to be spread\nuniformly throughout the network. Therefore, while some nodes may experience\nmoderate congestion, others may be experiencing severe congestion and may need to\ndiscard traffic. In addition, as the load increases, the network will attempt to balance\nthe load by routing packets through areas of lower congestion.For the routing function\nto work, an increased number of routing messages must be exchanged between nodes\nto alert each other to areas of congestion; this overhead reduces the capacity available\nfor data packets.\nAs the load on the network continues to increase, the queue lengths of the\nvarious nodes continue to grow. Eventually, a point is reached (point B in the plot)\nbeyond which throughput actually drops with increased offered load. The reason\nfor this is that the buffers at each node are of finite size. When the buffers at a\nnode become full, the node must discard packets. Thus, the sources must retrans-\nmit the discarded packets in addition to new packets. This only exacerbates the\nsituation: As more and more packets are retransmitted, the load on the system\ngrows, and more buffers become saturated. While the system is trying desperately\nto clear the backlog, users are pumping old and new packets into the system.\nEven successfully delivered packets may be retransmitted because it takes too\nlong, at a higher layer (e.g., transport layer), to acknowledge them: The sender\nassumes the packet did not get through and retransmits. Under these circum-\nstances, the effective capacity of the system declines to zero.\n13.2 / CONGESTION CONTROL\n13.2 CONGESTION CONTROL\nIn this book, we discuss various techniques for controlling congestion in packet-\nswitching, frame relay, and ATM networks, and in IP-based internets.To give context\nto this discussion, Figure 13.5 provides a general depiction of important congestion\ncontrol techniques.\nNormalized throughput\nAverage delay\nfor packets that\nare delivered\nAverage delay\nfor all packets\nNo congestion\nSevere congestion\nFigure 13.4\nThe Effects of Congestion\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nBackpressure\nWe have already made reference to backpressure as a technique for congestion con-\ntrol. This technique produces an effect similar to backpressure in fluids flowing\ndown a pipe. When the end of a pipe is closed (or restricted), the fluid pressure\nbacks up the pipe to the point of origin, where the flow is stopped (or slowed).\nBackpressure can be exerted on the basis of links or logical connections (e.g.,\nvirtual circuits). Referring again to Figure 13.2, if node 6 becomes congested\n(buffers fill up), then node 6 can slow down or halt the flow of all packets from node\n5 (or node 3, or both nodes 5 and 3). If this restriction persists, node 5 will need to\nslow down or halt traffic on its incoming links.This flow restriction propagates back-\nward (against the flow of data traffic) to sources, which are restricted in the flow of\nnew packets into the network.\nBackpressure can be selectively applied to logical connections, so that the flow\nfrom one node to the next is only restricted or halted on some connections, gener-\nally the ones with the most traffic. In this case, the restriction propagates back along\nthe connection to the source.\nBackpressure is of limited utility. It can be used in a connection-oriented net-\nwork that allows hop-by-hop (from one node to the next) flow control. X.25-based\npacket-switching networks typically provide this feature. However, neither frame\nrelay nor ATM has any capability for restricting flow on a hop-by-hop basis. In the\ncase of IP-based internets, there have traditionally been no built-in facilities for reg-\nulating the flow of data from one router to the next along a path through the inter-\nnet. Recently, some flow-based schemes have been developed; this topic is\nintroduced in Part Five.\nChoke Packet\nA choke packet is a control packet generated at a congested node and transmitted\nback to a source node to restrict traffic flow. An example of a choke packet is the\nICMP (Internet Control Message Protocol) Source Quench packet. Either a router\nor a destination end system may send this message to a source end system, request-\ning that it reduce the rate at which it is sending traffic to the internet destination. On\nBackpressure\n(delay, discard)\nDestination\nChoke packet\n(binary, credit, rate)\nFigure 13.5\nMechanisms for Congestion Control\n13.2 / CONGESTION CONTROL\nreceipt of a source quench message, the source host should cut back the rate at\nwhich it is sending traffic to the specified destination until it no longer receives\nsource quench messages.The source quench message can be used by a router or host\nthat must discard IP datagrams because of a full buffer. In that case, the router or\nhost will issue a source quench message for every datagram that it discards. In addi-\ntion, a system may anticipate congestion and issue source quench messages when its\nbuffers approach capacity. In that case, the datagram referred to in the source\nquench message may well be delivered. Thus, receipt of a source quench message\ndoes not imply delivery or nondelivery of the corresponding datagram.\nThe choke package is a relatively crude technique for controlling congestion.\nMore sophisticated forms of explicit congestion signaling are discussed subsequently.\nImplicit Congestion Signaling\nWhen network congestion occurs, two things may happen: (1) The transmission\ndelay for an individual packet from source to destination increases, so that it is\nnoticeably longer than the fixed propagation delay, and (2) packets are discarded. If\na source is able to detect increased delays and packet discards, then it has implicit\nevidence of network congestion. If all sources can detect congestion and, in\nresponse, reduce flow on the basis of congestion, then the network congestion will\nbe relieved.Thus, congestion control on the basis of implicit signaling is the respon-\nsibility of end systems and does not require action on the part of network nodes.\nImplicit signaling is an effective congestion control technique in connectionless,\nor datagram, configurations, such as datagram packet-switching networks and IP-\nbased internets. In such cases, there are no logical connections through the internet\non which flow can be regulated. However, between the two end systems, logical con-\nnections can be established at the TCP level.TCP includes mechanisms for acknowl-\nedging receipt of TCP segments and for regulating the flow of data between source\nand destination on a TCP connection. TCP congestion control techniques based on\nthe ability to detect increased delay and segment loss are discussed in Chapter 20.\nImplicit signaling can also be used in connection-oriented networks. For exam-\nple, in frame relay networks, the LAPF control protocol, which is end to end,\nincludes facilities similar to those of TCP for flow and error control. LAPF control\nis capable of detecting lost frames and adjusting the flow of data accordingly.\nExplicit Congestion Signaling\nIt is desirable to use as much of the available capacity in a network as possible but\nstill react to congestion in a controlled and fair manner. This is the purpose of\nexplicit congestion avoidance techniques. In general terms, for explicit congestion\navoidance, the network alerts end systems to growing congestion within the network\nand the end systems take steps to reduce the offered load to the network.\nTypically, explicit congestion control techniques operate over connection-ori-\nented networks and control the flow of packets over individual connections. Explicit\ncongestion signaling approaches can work in one of two directions:\n• Backward: Notifies the source that congestion avoidance procedures should be\ninitiated where applicable for traffic in the opposite direction of the received\nnotification. It indicates that the packets that the user transmits on this logical\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nconnection may encounter congested resources.Backward information is trans-\nmitted either by altering bits in a header of a data packet headed for the source\nto be controlled or by transmitting separate control packets to the source.\n• Forward: Notifies the user that congestion avoidance procedures should be initi-\nated where applicable for traffic in the same direction as the received notifica-\ntion. It indicates that this packet, on this logical connection, has encountered\ncongested resources.Again,this information may be transmitted either as altered\nbits in data packets or in separate control packets. In some schemes, when a for-\nward signal is received by an end system, it echoes the signal back along the logi-\ncal connection to the source. In other schemes, the end system is expected to\nexercise flow control upon the source end system at a higher layer (e.g.,TCP).\nWe can divide explicit congestion signaling approaches into three general\ncategories:\n• Binary: A bit is set in a data packet as it is forwarded by the congested node.\nWhen a source receives a binary indication of congestion on a logical connec-\ntion, it may reduce its traffic flow.\n• Credit based: These schemes are based on providing an explicit credit to a\nsource over a logical connection. The credit indicates how many octets or how\nmany packets the source may transmit.When the credit is exhausted,the source\nmust await additional credit before sending additional data. Credit-based\nschemes are common for end-to-end flow control, in which a destination sys-\ntem uses credit to prevent the source from overflowing the destination buffers,\nbut credit-based schemes have also been considered for congestion control.\n• Rate based: These schemes are based on providing an explicit data rate limit\nto the source over a logical connection.The source may transmit data at a rate\nup to the set limit. To control congestion, any node along the path of the con-\nnection can reduce the data rate limit in a control message to the source.\n13.3 TRAFFIC MANAGEMENT\nThere are a number of issues related to congestion control that might be included\nunder the general category of traffic management. In its simplest form, congestion\ncontrol is concerned with efficient use of a network at high load.The various mech-\nanisms discussed in the previous section can be applied as the situation arises, with-\nout regard to the particular source or destination affected.When a node is saturated\nand must discard packets, it can apply some simple rule, such as discard the most\nrecent arrival. However, other considerations can be used to refine the application\nof congestion control techniques and discard policy. We briefly introduce several of\nthose areas here.\nAs congestion develops, flows of packets between sources and destinations will experi-\nence increased delays and, with high congestion, packet losses. In the absence of other\nrequirements, we would like to assure that the various flows suffer from congestion\n13.4 / CONGESTION CONTROL IN POCKET-SWITCHING NETWORKS\nequally.Simply to discard on a last-in-first-discarded basis may not be fair.As an exam-\nple of a technique that might promote fairness, a node can maintain a separate queue\nfor each logical connection or for each source-destination pair. If all of the queue\nbuffers are of equal length, then the queues with the highest traffic load will suffer dis-\ncards more often, allowing lower-traffic connections a fair share of the capacity.\nQuality of Service\nWe might wish to treat different traffic flows differently. For example, as [JAIN92]\npoints out, some applications, such as voice and video, are delay sensitive but loss\ninsensitive. Others, such as file transfer and electronic mail, are delay insensitive but\nloss sensitive. Still others, such as interactive graphics or interactive computing\napplications, are delay sensitive and loss sensitive. Also, different traffic flows have\ndifferent priorities; for example, network management traffic, particularly during\ntimes of congestion or failure, is more important than application traffic.\nIt is particularly important during periods of congestion that traffic flows with\ndifferent requirements be treated differently and provided a different quality of ser-\nvice (QoS). For example, a node might transmit higher-priority packets ahead of\nlower-priority packets in the same queue. Or a node might maintain different\nqueues for different QoS levels and give preferential treatment to the higher levels.\nReservations\nOne way to avoid congestion and also to provide assured service to applications is\nto use a reservation scheme. Such a scheme is an integral part of ATM networks.\nWhen a logical connection is established, the network and the user enter into a traf-\nfic contract, which specifies a data rate and other characteristics of the traffic flow.\nThe network agrees to give a defined QoS so long as the traffic flow is within con-\ntract parameters; excess traffic is either discarded or handled on a best-effort basis,\nsubject to discard. If the current outstanding reservations are such that the network\nresources are inadequate to meet the new reservation, then the new reservation is\ndenied. A similar type of scheme has now been developed for IP-based internets\n(RSVP, which is discussed in Chapter 19).\nOne aspect of a reservation scheme is traffic policing (Figure 13.5). A node in\nthe network, typically the node to which the end system attaches, monitors the traf-\nfic flow and compares it to the traffic contract. Excess traffic is either discarded or\nmarked to indicate that it is liable to discard or delay.\n13.4 CONGESTION CONTROL IN PACKET-SWITCHING\nA number of control mechanisms for congestion control in packet-switching net-\nworks have been suggested and tried.The following are examples:\n1. Send a control packet from a congested node to some or all source nodes. This\nchoke packet will have the effect of stopping or slowing the rate of transmission\nfrom sources and hence limit the total number of packets in the network. This\napproach requires additional traffic on the network during a period of congestion.\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\n2. Rely on routing information. Routing algorithms, such as ARPANET’s, provide\nlink delay information to other nodes, which influences routing decisions. This\ninformation could also be used to influence the rate at which new packets are\nproduced. Because these delays are being influenced by the routing decision,\nthey may vary too rapidly to be used effectively for congestion control.\n3. Make use of an end-to-end probe packet.Such a packet could be timestamped to\nmeasure the delay between two particular endpoints. This has the disadvantage\nof adding overhead to the network.\n4. Allow packet-switching nodes to add congestion information to packets as\nthey go by. There are two possible approaches here. A node could add such\ninformation to packets going in the direction opposite of the congestion. This\ninformation quickly reaches the source node, which can reduce the flow of\npackets into the network. Alternatively, a node could add such information to\npackets going in the same direction as the congestion. The destination either\nasks the source to adjust the load or returns the signal back to the source in the\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nqueue to overflow, necessitating the discard of either the most recently arrived\nframe or some other frame.\nThe simplest way to cope with congestion is for the frame-relaying network to\ndiscard frames arbitrarily, with no regard to the source of a particular frame. In that\ncase, because there is no reward for restraint, the best strategy for any individual\nend system is to transmit frames as rapidly as possible. This, of course, exacerbates\nthe congestion problem.\nTo provide for a fairer allocation of resources, the frame relay bearer service\nincludes the concept of a committed information rate (CIR). This is a rate, in bits\nper second, that the network agrees to support for a particular frame-mode con-\nnection.Any data transmitted in excess of the CIR are vulnerable to discard in the\nevent of congestion. Despite the use of the term committed, there is no guarantee\nthat even the CIR will be met. In cases of extreme congestion, the network may be\nforced to provide a service at less than the CIR for a given connection. However,\nwhen it comes time to discard frames, the network will choose to discard frames on\nconnections that are exceeding their CIR before discarding frames that are within\nIn theory, each frame-relaying node should manage its affairs so that \nthe aggregate of CIRs of all the connections of all the end systems attached to the\nnode does not exceed the capacity of the node. In addition, the aggregate of the\nCIRs should not exceed the physical data rate across the user-network interface,\nknown as the access rate. The limitation imposed by access rate can be expressed\nas follows:\nConsiderations of node capacity may result in the selection of lower values for some\nof the CIRs.\nFor permanent frame relay connections, the CIR for each connection must be\nestablished at the time the connection is agreed between user and network. For\nswitched connections, the CIR parameter is negotiated; this is done in the setup\nphase of the call control protocol.\nThe CIR provides a way of discriminating among frames in determining\nwhich frames to discard in the face of congestion. Discrimination is indicated by\nmeans of the discard eligibility (DE) bit in the LAPF frame (Figure 10.16). The\nframe handler to which the user’s station attaches performs a metering function\n(Figure 13.6). If the user is sending data at less than the CIR, the incoming frame\nhandler does not alter the DE bit. If the rate exceeds the CIR, the incoming frame\nhandler will set the DE bit on the excess frames and then forward them; such\nframes may get through or may be discarded if congestion is encountered. Finally, a\nmaximum rate is defined, such that any frames above the maximum are discarded\nat the entry frame handler.\ndata-rate TDM channel between the user and the network\n AccessRatej = Data rate of user access channel j; a channel is a fixed-\n CIRi,j = Committed information rate for connection i on channel j\nCIRi,j … AccessRatej\n13.5 / FRAME RELAY CONGESTION CONTROL\nThe CIR, by itself, does not provide much flexibility in dealing with traffic rates.\nIn practice, a frame handler measures traffic over each logical connection for a time\ninterval specific to that connection and then makes a decision based on the amount\nof data received during that interval.Two additional parameters, assigned on perma-\nnent connections and negotiated on switched connections, are needed.They are\n• Committed burst size\nThe maximum amount data that the network\nagrees to transfer, under normal conditions, over a measurement interval T.\nThese data may or may not be contiguous (i.e., they may appear in one frame\nor in several frames).\n• Excess burst size \nThe maximum amount of data in excess of \nnetwork will attempt to transfer, under normal conditions, over a measurement\ninterval T. These data are uncommitted in the sense that the network does not\ncommit to delivery under normal conditions. Put another way, the data that\nare delivered with lower probability than the data within \nThe quantities \nand CIR are related. Because \nis the amount of committed\ndata that may be transmitted by the user over a time T, and CIR is the rate at which\ncommitted data may be transmitted, we must have\nFigure 13.7, based on a figure in ITU-T Recommendation I.370, illustrates the\nrelationship among these parameters. On each graph, the solid line plots the cumu-\nlative number of information bits transferred over a given connection since time\nThe dashed line labeled Access Rate represents the data rate over the chan-\nnel containing this connection.The dashed line labeled CIR represents the commit-\nted information rate over the measurement interval T. Note that when a frame is\nbeing transmitted, the solid line is parallel to the Access Rate line; when a frame is\ntransmitted on a channel, that channel is dedicated to the transmission of that\nframe.When no frame is being transmitted, the solid line is horizontal.\nFigure 13.7a shows an example in which three frames are transmitted within the\nmeasurement interval and the total number of bits in the three frames is less than\nNote that during the transmission of the first frame, the actual transmission rate\ntransmission\ninformation\nTransmit if\nFigure 13.6\nOperation of the CIR\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\ntemporarily exceeds the CIR.This is of no consequence because the frame handler is\nonly concerned with the cumulative number of bits transmitted over the entire interval.\nIn Figure 13.7b, the last frame transmitted during the interval causes the cumulative\nnumber of bits transmitted to exceed \nAccordingly, the DE bit of that frame is set\nby the frame handler. In Figure 13.7c, the third frame exceeds \nand so is labeled for\npotential discard.The fourth frame exceeds \nand is discarded.\nDiscard region\nDE \u0001 1 region\nDE \u0001 0 region\nAccess rate\nbits transmitted\n(a) All frames within CIR\nbits transmitted\nDiscard region\nDE \u0001 1 region\nDE \u0001 0 region\nAccess rate\n(b) One frame marked DE\nbits transmitted\nDiscard region\nDE \u0001 1 region\nDE \u0001 0 region\nAccess rate\n(c) One frame marked DE; one frame discarded\nFigure 13.7\nIllustration of Relationships among Congestion Parameters\n13.5 / FRAME RELAY CONGESTION CONTROL\nCongestion Avoidance with Explicit Signaling\nIt is desirable to use as much of the available capacity in a frame relay network as\npossible but still react to congestion in a controlled and fair manner.This is the pur-\npose of explicit congestion avoidance techniques. In general terms, for explicit con-\ngestion avoidance, the network alerts end systems to growing congestion within the\nnetwork and the end systems take steps to reduce the offered load to the network.\nAs the standards for explicit congestion avoidance were being developed, two\ngeneral strategies were considered [BERG91]. One group believed that congestion\nalways occurred slowly and almost always in the network egress nodes. Another\ngroup had seen cases in which congestion grew very quickly in the internal nodes\nand required quick decisive action to prevent network congestion. We will see that\nthese two approaches are reflected in the forward and backward explicit congestion\navoidance techniques, respectively.\nFor explicit signaling, two bits in the address field of each frame are provided.\nEither bit may be set by any frame handler that detects congestion. If a frame han-\ndler receives a frame in which one or both of these bits are set, it must not clear the\nbits before forwarding the frame. Thus, the bits constitute signals from the network\nto the end user.The two bits are\n• Backward explicit congestion notification (BECN): Notifies the user that con-\ngestion avoidance procedures should be initiated where applicable for traffic in\nthe opposite direction of the received frame.It indicates that the frames that the\nuser transmits on this logical connection may encounter congested resources.\n• Forward explicit congestion notification (FECN): Notifies the user that con-\ngestion avoidance procedures should be initiated where applicable for traffic\nin the same direction as the received frame. It indicates that this frame, on this\nlogical connection, has encountered congested resources.\nLet us consider how these bits are used by the network and the user. First, for\nthe network response, it is necessary for each frame handler to monitor its queuing\nbehavior. If queue lengths begin to grow to a dangerous level, then either FECN or\nBECN bits or a combination should be set to try to reduce the flow of frames\nthrough that frame handler. The choice of FECN or BECN may be determined by\nwhether the end users on a given logical connection are prepared to respond to one\nor the other of these bits.This may be determined at configuration time. In any case,\nthe frame handler has some choice as to which logical connections should be alerted\nto congestion. If congestion is becoming quite serious, all logical connections through\na frame handler might be notified. In the early stages of congestion, the frame han-\ndler might just notify users for those connections that are generating the most traffic.\nThe user response is determined by the receipt of BECN or FECN signals.The\nsimplest procedure is the response to a BECN signal: The user simply reduces the\nrate at which frames are transmitted until the signal ceases. The response to an\nFECN is more complex, as it requires the user to notify its peer user of this connec-\ntion to restrict its flow of frames.The core functions used in the frame relay protocol\ndo not support this notification; therefore, it must be done at a higher layer, such as\nthe transport layer. The flow control could also be accomplished by the LAPF con-\ntrol protocol or some other link control protocol implemented above the frame\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nrelay sublayer. The LAPF control protocol is particularly useful because it includes\nan enhancement to LAPD that permits the user to adjust window size.\n13.6 ATM TRAFFIC MANAGEMENT\nBecause of their high speed and small cell size,ATM networks present difficulties in\neffectively controlling congestion not found in other types of data networks. The\ncomplexity of the problem is compounded by the limited number of overhead bits\navailable for exerting control over the flow of user cells. This area is currently the\nsubject of intense research, and approaches to traffic and congestion control are still\nevolving. ITU-T has defined a restricted initial set of traffic and congestion control\ncapabilities aiming at simple mechanisms and realistic network efficiency; these are\nspecified in I.371. The ATM Forum has published a somewhat more advanced ver-\nsion of this set in its Traffic Management Specification 4.0. This section focuses on\nthe ATM Forum specifications.\nWe begin with an overview of the congestion problem and the framework\nadopted by ITU-T and the ATM Forum. We then discuss some of the specific tech-\nniques that have been developed for traffic management and congestion control.\nRequirements for ATM Traffic and Congestion Control\nBoth the types of traffic patterns imposed on ATM networks and the transmission\ncharacteristics of those networks differ markedly from those of other switching net-\nworks. Most packet-switching and frame relay networks carry non-real-time data\ntraffic. Typically, the traffic on individual virtual circuits or frame relay connections\nis bursty in nature, and the receiving system expects to receive incoming traffic on\neach connection in a bursty fashion.As a result,\n• The network does not need to replicate the exact timing pattern of incoming\ntraffic at the re exit node.\n• Therefore, simple statistical multiplexing can be used to accommodate multi-\nple logical connections over the physical interface between user and network.\nThe average data rate required by each connection is less than the burst rate\nfor that connection, and the user-network interface (UNI) need only be\ndesigned for a capacity somewhat greater than the sum of the average data\nrates for all connections.\nA number of tools are available for control of congestion in packet-switched\nand frame relay networks, some of which are discussed elsewhere in this chapter.\nThese types of congestion control schemes are inadequate for ATM networks.\n[GERS91] cites the following reasons:\n• The majority of traffic is not amenable to flow control. For example, voice and\nvideo traffic sources cannot stop generating cells even when the network is\n• Feedback is slow due to the drastically reduced cell transmission time com-\npared to propagation delays across the network.\n13.6 / ATM TRAFFIC MANAGEMENT\n• ATM networks typically support a wide range of applications requiring capac-\nity ranging from a few kbps to several hundred Mbps. Relatively simple-\nminded congestion control schemes generally end up penalizing one end or\nthe other of that spectrum.\n• Applications on ATM networks may generate very different traffic patterns\n(e.g., constant bit rate versus variable bit rate sources).Again, it is difficult for\nconventional congestion control techniques to handle fairly such variety.\n• Different applications on ATM networks require different network services\n(e.g., delay-sensitive service for voice and video, and loss-sensitive service\n• The very high speeds in switching and transmission make ATM networks\nmore volatile in terms of congestion and traffic control. A scheme that relies\nheavily on reacting to changing conditions will produce extreme and wasteful\nfluctuations in routing policy and flow control.\nTwo key performance issues that relate to the preceding points are latency/speed\neffects and cell delay variation, topics to which we now turn.\nLatency/Speed Effects\nConsider the transfer of ATM cells over a network at a data rate of 150 Mbps. At\nthat rate, it takes \nseconds to insert a\nsingle cell onto the network.The time it takes to transfer the cell from the source to\nthe destination user will depend on the number of intermediate ATM switches, the\nswitching time at each switch, and the propagation time along all links in the path\nfrom source to destination. For simplicity, ignore ATM switching delays and assume\npropagation at the two-thirds the speed of light. Then, if source and destination are\non opposite coasts of the United States, the round-trip propagation delay is about\nWith these conditions in place, suppose that source A is performing a long file\ntransfer to destination B and that implicit congestion control is being used (i.e.,\nthere are no explicit congestion notifications; the source deduces the presence of\ncongestion by the loss of data). If the network drops a cell due to congestion, B can\nreturn a reject message to A, which must then retransmit the dropped cell and pos-\nsibly all subsequent cells. But by the time the notification gets back to A, it has trans-\nmitted an additional N cells, where\nOver 7 megabits have been transmitted before A can react to the congestion \nindication.\nThis calculation helps to explain why the techniques that are satisfactory for\nmore traditional networks break down when dealing with ATM WANs.\nCell Delay Variation\nFor an ATM network, voice and video signals can be digitized and transmitted as\na stream of cells. A key requirement, especially for voice, is that the delay across\n48 * 10-3 seconds\n2.8 * 10-6 seconds/cell = 1.7 * 104 cells = 7.2 * 106 bits\n153 * 8 bits2/1150 * 106 bps2 L 2.8 * 10-6\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nthe network be short. Generally, this will be the case for ATM networks. As we\nhave discussed, ATM is designed to minimize the processing and transmission\noverhead internal to the network so that very fast cell switching and routing is\nThere is another important requirement that to some extent conflicts with the\npreceding requirement, namely that the rate of delivery of cells to the destination\nuser must be constant. It is inevitable that there will be some variability in the rate of\ndelivery of cells due both to effects within the network and at the source UNI; we\nsummarize these effects presently. First, let us consider how the destination user\nmight cope with variations in the delay of cells as they transit from source user to\ndestination user.\nA general procedure for achieving a constant bit rate (CBR) is illustrated in\nFigure 13.8. Let D(i) represent the end-to-end delay experienced by the ith cell.The\ndestination system does not know the exact amount of this delay: there is no time-\nstamp information associated with each cell and, even if there were, it is impossible to\nkeep source and destination clocks perfectly synchronized. When the first cell on a\nconnection arrives at time \nthe target user delays the cell an additional amount\nV(0) prior to delivery to the application. V(0) is an estimate of the amount of cell\ndelay variation that this application can tolerate and that is likely to be produced by\nthe network.\nSubsequent cells are delayed so that they are delivered to the user at a con-\nstant rate of R cells per second. The time between delivery of cells to the target\napplication (time between the start of delivery of one cell and the start of delivery of\nthe next cell) is therefore \nTo achieve a constant rate, the next cell is\ndelayed a variable amount V(1) to satisfy\nSuccessive cells\nCell arrives\nlate; discarded\nSlope \u0001 R cells/s\nFigure 13.8\nTime Reassembly of CBR Cells\n13.6 / ATM TRAFFIC MANAGEMENT\nIn general,\nwhich can also be expressed as\nIf the computed value of V(i) is negative, then that cell is discarded.The result is that\ndata is delivered to the higher layer at a constant bit rate, with occasional gaps due\nto dropped cells.\nThe amount of the initial delay V(0), which is also the average delay applied to\nall incoming cells, is a function of the anticipated cell delay variation. To minimize\nthis delay, a subscriber will therefore request a minimal cell delay variation from the\nnetwork provider. This leads to a tradeoff: Cell delay variation can be reduced by\nincreasing the data rate at the UNI relative to the load and by increasing resources\nwithin the network.\nNetwork Contribution to Cell Delay Variation One component of cell\ndelay variation is due to events within the network. For packet-switching networks,\npacket delay variation can be considerable due to queuing effects at each of the\nintermediate switching nodes and the processing time required to analyze packet\nheaders and perform routing.To a much lesser extent, this is also true of frame delay\nvariation in frame relay networks. In the case of ATM networks, cell delay variations\ndue to network effects are likely to be even less than for frame relay. The principal\nreasons for this are the following:\n• The ATM protocol is designed to minimize processing overhead at intermedi-\nate switching nodes. The cells are fixed size with fixed header formats, and\nthere is no flow control or error control processing required.\n• To accommodate the high speeds of ATM networks, ATM switches have had\nto be designed to provide extremely high throughput. Thus, the processing\ntime for an individual cell at a node is negligible.\nThe only factor that could lead to noticeable cell delay variation within the\nnetwork is congestion. If the network begins to become congested, either cells must\nbe discarded or there will be a buildup of queuing delays at affected switches. Thus,\nit is important that the total load accepted by the network at any time not be such as\nto cause congestion.\nCell Delay Variation at the UNI Even if an application generates data for\ntransmission at a constant bit rate, cell delay variation can occur at the source due to\nthe processing that takes place at the three layers of the ATM model.\nFigure 13.9 illustrates the potential causes of cell delay variation. In this exam-\nple,ATM connections A and B support user data rates of X and Y Mbps, respectively\nAt the AAL level, data are segmented into 48-octet blocks. Note that on a\nV1i2 = V1i - 12 - [ti - 1ti-1 + d2]\nV1i2 = V102 - [ti - 1t0 + i * d2]\nV112 = V102 - [t1 - 1t0 + d2]\nt1 + V112 = t0 + V102 + d\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\ntime diagram, the blocks appear to be of different sizes for the two connections;\nspecifically, the time required to generate a 48-octet block of data, in microseconds, is\nThe ATM layer encapsulates each segment into a 53-octet cell. These cells must\nbe interleaved and delivered to the physical layer to be transmitted at the data rate of\nthe physical link. Delay is introduced into this interleaving process: If two cells from\ndifferent connections arrive at the ATM layer at overlapping times, one of the cells\nmust be delayed by the amount of the overlap.In addition,the ATM layer is generating\nOAM (operation and maintenance) cells that must also be interleaved with user cells.\nAt the physical layer, there is opportunity for the introduction of further cell\ndelays. For example, if cells are transmitted in SDH (synchronous digital hierarchy)\nframes, overhead bits for those frames will be inserted onto the physical link, delay-\ning bits from the ATM layer.\nNone of the delays just listed can be predicted in any detail, and none follow\nany repetitive pattern. Accordingly, there is a random element to the time interval\nbetween reception of data at the ATM layer from the AAL and the transmission of\nthat data in a cell across the UNI.\nConnection B: 48 * 8\nConnection A: 48 * 8\nPhysical layer SAP\nPhysical layer\nATM layer SAP\n(Connection A, X Mbps)\n(Connection B, Y Mbps)\n48 octets, Y Mbps\n48 octets, X Mbps\nFigure 13.9\nOrigins of Cell Delay Variation (I.371)\n13.6 / ATM TRAFFIC MANAGEMENT\nTraffic Control and Congestion Control Functions\nResponse Time\nTraffic Control Functions\nCongestion Control Functions\n• Resource management \nusing virtual paths\nConnection Duration\n• Connection admission \ncontrol (CAC)\n• Fast resource management\n• Explicit forward congestion \nPropagation Time\nindication (EFCI)\n• ABR flow control\nCell Insertion Time\n• Usage parameter control \n• Selective cell discard\n• Priority control\n• Traffic shaping\nTraffic and Congestion Control Framework\nI.371 lists the following objectives of ATM layer traffic and congestion control:\n• ATM layer traffic and congestion control should support a set of ATM layer\nQoS classes sufficient for all foreseeable network services; the specification of\nthese QoS classes should be consistent with network performance parameters\ncurrently under study.\n• ATM layer traffic and congestion control should not rely on AAL protocols\nthat are network service specific, nor on higher-layer protocols that are appli-\ncation specific. Protocol layers above the ATM layer may make use of infor-\nmation provided by the ATM layer to improve the utility those protocols can\nderive from the network.\n• The design of an optimum set of ATM layer traffic controls and congestion\ncontrols should minimize network and end-system complexity while maximiz-\ning network utilization.\nTo meet these objectives, ITU-T and the ATM Forum have defined a collec-\ntion of traffic and congestion control functions that operate across a spectrum of\ntiming intervals. Table 13.2 lists these functions with respect to the response times\nwithin which they operate. Four levels of timing are considered:\n• Cell insertion time: Functions at this level react immediately to cells as they\nare transmitted.\n• Round-trip propagation time: At this level, the network responds within the life-\ntime of a cell in the network and may provide feedback indications to the source.\n• Connection duration: At this level, the network determines whether a new\nconnection at a given QoS can be accommodated and what performance lev-\nels will be agreed to.\n• Long term: These are controls that affect more than one ATM connection and\nare established for long-term use.\nThe essence of the traffic control strategy is based on (1) determining whether a\ngiven new ATM connection can be accommodated and (2) agreeing with the subscriber\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\non the performance parameters that will be supported. In effect, the subscriber and the\nnetwork enter into a traffic contract: the network agrees to support traffic at a certain\nlevel of performance on this connection, and the subscriber agrees not to exceed traffic\nparameter limits.Traffic control functions are concerned with establishing these traffic\nparameters and enforcing them.Thus,they are concerned with congestion avoidance.If\ntraffic control fails in certain instances, then congestion may occur. At this point, con-\ngestion control functions are invoked to respond to and recover from the congestion.\nTraffic Management and Congestion Control Techniques\nITU-T and the ATM Forum have defined a range of traffic management functions\nto maintain the quality of service (QoS) of ATM connections. ATM traffic manage-\nment function refers to the set of actions taken by the network to avoid congestion\nconditions or to minimize congestion effects. In this subsection, we highlight the fol-\nlowing techniques:\n• Resource management using virtual paths\n• Connection admission control\n• Usage parameter control\n• Selective cell discard\n• Traffic shaping\nResource Management Using Virtual Paths The essential concept behind\nnetwork resource management is to allocate network resources in such a way as to\nseparate traffic flows according to service characteristics. So far, the only specific\ntraffic control function based on network resource management defined by the\nATM Forum deals with the use of virtual paths.\nAs discussed in Chapter 11, a virtual path connection (VPC) provides a conve-\nnient means of grouping similar virtual channel connections (VCCs). The network\nprovides aggregate capacity and performance characteristics on the virtual path,\nand these are shared by the virtual connections. There are three cases to consider:\n• User-to-user application: The VPC extends between a pair of UNIs. In this\ncase the network has no knowledge of the QoS of the individual VCCs within\na VPC. It is the user’s responsibility to assure that the aggregate demand from\nthe VCCs can be accommodated by the VPC.\n• User-to-network application: The VPC extends between a UNI and a network\nnode. In this case, the network is aware of the QoS of the VCCs within the\nVPC and has to accommodate them.\n• Network-to-network application: The VPC extends between two network\nnodes.Again, in this case, the network is aware of the QoS of the VCCs within\nthe VPC and has to accommodate them.\nThe QoS parameters that are of primary concern for network resource manage-\nment are cell loss ratio, cell transfer delay, and cell delay variation, all of which are\naffected by the amount of resources devoted to the VPC by the network. If a VCC\nextends through multiple VPCs, then the performance on that VCC depends on the\nperformances of the consecutive VPCs and on how the connection is handled at any\n13.6 / ATM TRAFFIC MANAGEMENT\n\u0001 Virtual path connection\n\u0001 Virtual channel connection\n\u0001 Virtual path switching function\n\u0001 Virtual channel switching function\nFigure 13.10\nConfiguration of VCCs and VPCs\nnode that performs VCC-related functions. Such a node may be a switch, concentrator,\nor other network equipment.The performance of each VPC depends on the capacity of\nthat VPC and the traffic characteristics of the VCCs contained within the VPC. The\nperformance of each VCC-related function depends on the switching/processing speed\nat the node and on the relative priority with which various cells are handled.\nFigure 13.10 gives an example. VCCs 1 and 2 experience a performance that\ndepends on VPCs b and c and on how these VCCs are handled by the intermediate\nnodes.This may differ from the performance experienced by VCCs 3, 4, and 5.\nThere are a number of alternatives for the way in which VCCs are grouped\nand the type of performance they experience. If all of the VCCs within a VPC are\nhandled similarly, then they should experience similar expected network perfor-\nmance, in terms of cell loss ratio, cell transfer delay, and cell delay variation. Alter-\nnatively, when different VCCs within the same VPC require different QoS, the VPC\nperformance objective agreed by network and subscriber should be set suitably for\nthe most demanding VCC requirement.\nIn either case, with multiple VCCs within the same VPC, the network has two\ngeneral options for allocating capacity to the VPC:\n• Aggregate peak demand: The network may set the capacity (data rate) of the\nVPC equal to the total of the peak data rates of all of the VCCs within the\nVPC. The advantage of this approach is that each VCC can be given a QoS\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nthat accommodates its peak demand. The disadvantage is that most of the\ntime, the VPC capacity will not be fully utilized and therefore the network will\nhave underutilized resources.\n• Statistical multiplexing: If the network sets the capacity of the VPC to be\ngreater than or equal to the average data rates of all the VCCs but less than\nthe aggregate peak demand, then a statistical multiplexing service is supplied.\nWith statistical multiplexing,VCCs experience greater cell delay variation and\ngreater cell transfer delay. Depending on the size of buffers used to queue cells\nfor transmission, VCCs may also experience greater cell loss ratio. This\napproach has the advantage of more efficient utilization of capacity and is\nattractive if the VCCs can tolerate the lower QoS.\nWhen statistical multiplexing is used, it is preferable to group VCCs into\nVPCs on the basis of similar traffic characteristics and similar QoS requirements. If\ndissimilar VCCs share the same VPC and statistical multiplexing is used, it is diffi-\ncult to provide fair access to both high-demand and low-demand traffic streams.\nConnection Admission Control Connection admission control is the first line\nof defense for the network in protecting itself from excessive loads. In essence, when\na user requests a new VPC or VCC, the user must specify (implicitly or explicitly) the\ntraffic characteristics in both directions for that connection. The user selects traffic\ncharacteristics by selecting a QoS from among the QoS classes that the network pro-\nvides.The network accepts the connection only if it can commit the resources neces-\nsary to support that traffic level while at the same time maintaining the agreed QoS\nof existing connections. By accepting the connection, the network forms a traffic con-\ntract with the user. Once the connection is accepted, the network continues to pro-\nvide the agreed QoS as long as the user complies with the traffic contract.\nThe traffic contract may consist of the four parameters defined in Table 13.3:\npeak cell rate (PCR), cell delay variation (CDV), sustainable cell rate (SCR), and\nburst tolerance. Only the first two parameters are relevant for a constant-bit-rate\n(CBR) source; all four parameters may be used for variable-bit-rate (VBR) sources.\nAs the name suggests, the peak cell rate is the maximum rate at which cells are\ngenerated by the source on this connection.However,we need to take into account the\ncell delay variation.Although a source may be generating cells at a constant peak rate,\ncell delay variations introduced by various factors (see Figure 13.9) will affect the tim-\ning,causing cells to clump up and gaps to occur.Thus,a source may temporarily exceed\nthe peak cell rate due to clumping. For the network to properly allocate resources to\nthis connection, it must know not only the peak cell rate but also the CDV.\nThe exact relationship between peak cell rate and CDV depends on the oper-\national definitions of these two terms. The standards provide these definitions in\nterms of a cell rate algorithm. Because this algorithm can be used for usage parame-\nter control, we defer a discussion until the next subsection.\nThe PCR and CDV must be specified for every connection. As an option for \nvariable-bit rate sources, the user may also specify a sustainable cell rate and burst\ntolerance.These parameters are analogous to PCR and CDV, respectively, but apply to\nan average rate of cell generation rather than a peak rate. The user can describe the\nfuture flow of cells in greater detail by using the SCR and burst tolerance as well as the\nPCR and CDV.With this additional information,the network may be able to utilize the\n13.6 / ATM TRAFFIC MANAGEMENT\nProcedures Used to Set Values of Traffic Contract Parameters\nExplicitly Specified Parameters\nImplicitly Specified \nParameter Values Set at \nParameter Values\nParameter Values Set\nConnection-Setup Time\nSpecified at Subscription \nUsing Default Rules\nRequested by User/NMS\nAssigned by Network Operator\nby subscription\nnetwork-operator default \nby subscription\nnetwork-operator default \n NMS = network management system\n PVC = permanent virtual connection\n SVC = switched virtual connection\nTraffic Parameters Used in Defining VCC/VPC Quality of Service\nDescription\nTraffic Type\nPeak Cell Rate (PCR)\nAn upper bound on the traffic \nthat can be submitted on an \nATM connection.\nCell Delay Variation (CDV)\nAn upper bound on the \nvariability in the pattern of \ncell arrivals observed at a \nsingle measurement point with \nreference to the peak cell rate.\nSustainable Cell Rate (SCR)\nAn upper bound on the \naverage rate of an ATM \nconnection, calculated over \nthe duration of the connection.\nBurst Tolerance\nAn upper bound on the \nvariability in the pattern of \ncell arrivals observed at a single \nmeasurement point with reference\nto the sustainable cell rate.\n VBR = variable bit rate\n CBR = constant bit rate\nnetwork resources more efficiently. For example, if a number of VCCs are statistically\nmultiplexed over a VPC,knowledge of both average and peak cell rates enables the net-\nwork to allocate buffers of sufficient size to handle the traffic efficiently without cell loss.\nFor a given connection (VPC or VCC) the four traffic parameters may be\nspecified in several ways, as illustrated in Table 13.4. Parameter values may be\nimplicitly defined by default rules set by the network operator. In this case, all con-\nnections are assigned the same values, or all connections of a given class are assigned\nthe same values for that class. The network operator may also associate parameter\nvalues with a given subscriber and assign these at the time of subscription. Finally,\nparameter values tailored to a particular connection may be assigned at connection\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\ntime. In the case of a permanent virtual connection, these values are assigned by the\nnetwork when the connection is set up. For a switched virtual connection, the para-\nmeters are negotiated between the user and the network via a signaling protocol.\nAnother aspect of quality of service that may be requested or assigned for a\nconnection is cell loss priority.A user may request two levels of cell loss priority for\nan ATM connection; the priority of an individual cell is indicated by the user\nthrough the CLP bit in the cell header (Figure 11.4). When two priority levels are\nused, the traffic parameters for both cell flows must be specified. Typically, this is\ndone by specifying a set of traffic parameters for high-priority traffic \nand a set of traffic parameters for all traffic (\nor 1). Based on this break-\ndown, the network may be able to allocate resources more efficiently.\nUsage Parameter Control Once a connection has been accepted by the connec-\ntion admission control function,the usage parameter control (UPC) function of the net-\nwork monitors the connection to determine whether the traffic conforms to the traffic\ncontract.The main purpose of usage parameter control is to protect network resources\nfrom an overload on one connection that would adversely affect the QoS on other con-\nnections by detecting violations of assigned parameters and taking appropriate actions.\nUsage parameter control can be done at both the virtual path and virtual channel\nlevels. Of these, the more important is VPC-level control, because network\nresources are, in general, initially allocated on the basis of virtual paths, with the vir-\ntual path capacity shared among the member virtual channels.\nThere are two separate functions encompassed by usage parameter control:\n• Control of peak cell rate and the associated cell delay variation (CDV)\n• Control of sustainable cell rate and the associated burst tolerance\nLet us first consider the peak cell rate and the associated cell delay variation.\nIn simple terms, a traffic flow is compliant if the peak rate of cell transmission does\nnot exceed the agreed peak cell rate, subject to the possibility of cell delay variation\nwithin the agreed bound. I.371 defines an algorithm, the peak cell rate algorithm,\nthat monitors compliance.The algorithm operates on the basis of two parameters: a\npeak cell rate R and a CDV tolerance limit of \nis the interarrival\ntime between cells if there were no CDV. With CDV, T is the average interarrival\ntime at the peak rate. The algorithm has been defined to monitor the rate at which\ncells arrive and to assure that the interarrival time is not too short to cause the flow\nto exceed the peak cell rate by an amount greater than the tolerance limit.\nThe same algorithm, with different parameters, can be used to monitor the sus-\ntainable cell rate and the associated burst tolerance. In this case, the parameters are\nthe sustainable cell rate \nand a burst tolerance \nThe cell rate algorithm is rather complex; details can be found in [STAL99].\nThe algorithm simply defines a way to monitor compliance with the traffic contract.\nTo perform usage parameter control, the network must act on the results of the\nalgorithm. The simplest strategy is that compliant cells are passed along and non-\ncompliant cells are discarded at the point of the UPC function.\nAt the network’s option, cell tagging may also be used for noncompliant cells.\nIn this case, a noncompliant cell may be tagged with \n(low priority) and\npassed. Such cells are then subject to discard at a later point in the network, should\ncongestion be encountered.\n13.6 / ATM TRAFFIC MANAGEMENT\nFigure 13.11\nToken Bucket for Traffic Shaping\nIf the user has negotiated two levels of cell loss priority for a network, then the\nsituation is more complex. Recall that the user may negotiate a traffic contract for\nhigh priority traffic \nand a separate contract for aggregate traffic (CLP 0\nor 1).The following rules apply:\n1. A cell with \nthat conforms to the traffic contract for \n2. A cell with \nthat is noncompliant for \ntraffic but compliant\nfor (CLP 0 or 1) traffic is tagged and passed.\n3. A cell with \nthat is noncompliant for \ntraffic and non-\ncompliant for (CLP 0 or 1) traffic is discarded.\n4. A cell with \nthat is compliant for (\nor 1) traffic is passed.\n5. A cell with \nthat is noncompliant for (CLP 0 or 1) traffic is discarded.\nSelective Cell Discard Selective cell discard comes into play when the network,\nat some point beyond the UPC function, discards \ncells.The objective is to \ndiscard lower-priority cells during congestion to protect the performance for higher-\npriority cells.Note that the network has no way to discriminate between cells that were\nlabeled as lower priority by the source and cells that were tagged by the UPC function.\nTraffic Shaping The UPC algorithm is referred to as a form of traffic policing.\nTraffic policing occurs when a flow of data is regulated so that cells (or frames or\npackets) that exceed a certain performance level are discarded or tagged. It may be\ndesirable to supplement a traffic-policing policy with a traffic-shaping policy. Traf-\nfic shaping is used to smooth out a traffic flow and reduce cell clumping. This can\nresult in a fairer allocation of resources and a reduced average delay time.\nA simple approach to traffic shaping is to use a form of the UPC algorithm\nknown as token bucket. In contrast to the UPC algorithm, which simply monitors\nthe traffic and tags or discards noncompliant cells, a traffic-shaping token bucket\ncontrols the flow of compliant cells.\nFigure 13.11 illustrates the basic principle of the token bucket.A token gener-\nator produces tokens at a rate of \ntokens per second and places these in the token\nbucket, which has a maximum capacity of \ntokens. Cells arriving from the source\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nare placed in a buffer with a maximum capacity of K cells.To transmit a cell through\nthe server, one token must be removed from the bucket. If the token bucket is\nempty, the cell is queued waiting for the next token.The result of this scheme is that\nif there is a backlog of cells and an empty bucket, then cells are emitted at a smooth\ncells per second with no cell delay variation until the backlog is cleared.\nThus, the token bucket smoothes out bursts of cells.\n13.7 ATM-GFR TRAFFIC MANAGEMENT\nGFR (guaranteed frame rate) provides a service that is as simple as UBR (unspecified\nbit rate) from the end system’s point of view while placing a relatively modest require-\nment on the ATM network elements in terms of processing complexity and overhead.\nIn essence, with GFR, an end system does no policing or shaping of the traffic it trans-\nmits but may transmit at the line rate of the ATM adapter. As with UBR, there is no\nguarantee of frame delivery. It is up to a higher layer, such as TCP, to react to conges-\ntion that results in dropped frames by employing the window management and con-\ngestion control techniques discussed in Part Five.Unlike UBR,GFR allows the user to\nreserve a certain amount of capacity, in terms of a cell rate, for each GFR VC.A GFR\nreservation assures an application that it may transmit at a minimum rate without\nlosses. If the network is not congested, the user will be able to transmit at a higher rate.\nA distinctive characteristic of GFR is that it requires the network to recognize\nframes as well as cells. When congestion occurs, the network discards entire frames\nrather than individual cells. Further, GFR requires that all of the cells of a frame\nhave the same CLP bit setting. The \nAAL5 frames are treated as lower-\npriority frames that are to be transmitted on a best-effort basis. The minimum\nguaranteed capacity applies to the \nThe GFR traffic contract consists of the following parameters:\n• Peak cell rate (PCR)\n• Minimum cell rate (MCR)\n• Maximum burst size (MBS)\n• Maximum frame size (MFS)\n• Cell delay variation tolerance (CDVT)\nMechanisms for Supporting Rate Guarantees\nThere are three basic approaches that can be used by the network to provide per-\nVC guarantees for GFR and to enable a number of users to efficiently use and fairly\nshare the available network capacity [GOYA98]:\n• Tagging and policing\n• Buffer management\n• Scheduling\nThese approaches can be combined in various ways in an ATM network ele-\nments to yield a number of possible GFR implementations. Figure 13.12 illustrates\n13.7 / ATM-GFR TRAFFIC MANAGEMENT\nconformance\nQoS eligibility\ntest mechanism\nFrame forwarding\nFigure 13.12\nThe Fundamental Components of a GFR Mechanism [ANDR99]\nTagging and Policing Tagging is used to discriminate between frames that\nconform to the GFR traffic contract and those that do not. The network element\ndoing the conformance checking sets \non all cells of each frame that does\nnot conform. Because tagged cells are assumed to be in violation of the traffic con-\ntract, they are given a lower quality of service than untagged cells by subsequent\nmechanisms, such as buffer management and scheduling. Tagging can be done by\nthe network, especially the network element at the ingress to the ATM network.\nBut tagging may also be done by the source end system to indicate less important\nThe network, at either the ingress network element or at other ATM switching\nelements, may also choose to discard cells of nonconforming frames (i.e., cells with\n). Cell discard is considered a policing function.\nBuffer Management Buffer management mechanisms have to do with the\nway in which cells are treated that have been buffered at a network switch or that\narrive at a network switch and must be buffered prior to forwarding. When a con-\ngestion condition exists, as reflected by high buffer occupancy, a network element\nwill discard tagged cells in preference to untagged cells. In particular, a network\nelement may discard a tagged cell that is already in a buffer to make room for an\nincoming untagged cell.To provide fair and efficient use of buffer resources, a net-\nwork element may perform per-VC buffering, dedicating a certain amount of\nbuffer space to individual VCs. Then, on the basis of the traffic contracts for each\nVC and the buffer occupancy per VC, the network element can make decisions\nconcerning cell discard. That is, cell discard can be based on queue-specific occu-\npancy thresholds.\nScheduling A scheduling function, at minimum, can give preferential treatment\nto untagged cells over tagged cells.A network can also maintain separate queues for\neach VC and make per-VC scheduling decisions. Thus, within each queue, a first-\ncome, first-served discipline can be used, perhaps modified to give higher priority\nfor scheduling to \nScheduling among the queues enables the net-\nwork element to control the outgoing rate of individual VCs and thus ensure that\nCLP = 0 frames.\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nindividual VCs receive a fair allocation of capacity while meeting traffic contract\nrequirements for minimum cell rate for each VC.\nGFR Conformance Definition\nThe first function indicated in Figure 13.12 is a UPC function. UPC monitors each\nactive VC to ensure that the traffic on each connection conforms to the traffic con-\ntract, and tags or discards nonconforming cells.\nA frame is conforming if all of its cells are conforming, and is nonconforming\nif one or more cells are nonconforming. Three conditions must be met for a cell to\nbe conforming:\n1. The rate of cells must be within the cell rate contract.\n2. All cells of a frame must have the same CLP value.Thus, the CLP bit of the cur-\nrent cell must have the same value as the CLP bit of the first cell of the frame.\n3. The frame containing this cell must satisfy the MFS parameter.This condition\ncan be met by performing the following test on each cell:The cell either is the\nlast cell of the frame or the number of cells in the frame up to and including\nthis cell is less than MFS.\nQoS Eligibility Test Mechanism\nThe first two boxes in Figure 13.12 show what amounts to a two-stage filtering\nprocess. First, frames are tested for conformance to the traffic contract. Frames\nthat do not conform may be discarded immediately. If a nonconforming frame \nis not discarded, its cells are tagged \nmaking them vulnerable to dis-\ncard later on in the network. This first stage is therefore looking at an upper\nbound on traffic and penalizing cells that push the traffic flow above the upper\nThe second stage of filtering determines which frames are eligible for QoS\nguarantees under the GFR contract for a given VC. This stage is looking at a\nlower bound on traffic; over a given period of time, those frames that constitute \na traffic flow below the defined threshold are designated as eligible for QoS \nTherefore, the frames transmitted on a GFR VC fall into three categories:\n• Nonconforming frame: Cells of this frame will be tagged or discarded.\n• Conforming but ineligible frames: Cells will receive a best-effort service.\n• Conforming and eligible frames: Cells will receive a guarantee of delivery.\nTo determine eligibility, a form of the cell rate algorithm referred to in\nSection 13.6 is used. A network may discard or tag any cells that are not eligible.\nHowever,TM 4.1 states that it is expected that an implementation will attempt to\ndeliver conforming but ineligible traffic is on the basis of available resources,\nwith each GFR connection being provided at each link with a fair share of the\nlocal residual bandwidth. The specification does not attempt to define a criterion\nby which to determine if a given implementation meets the aforementioned\nexpectation.\n13.8 / RECOMMENDED READING\n13.8 RECOMMENDED READING\n[YANG95] is a comprehensive survey of congestion control techniques. [JAIN90] and\n[JAIN92] provide excellent discussions of the requirements for congestion control, the various\napproaches that can be taken, and performance considerations. An excellent discussion of\ndata network performance issues is provided by [KLEI93].While somewhat dated, the defin-\nitive reference on flow control is [GERL80].\n[GARR96] provides a rationale for the ATM service categories and discusses the traf-\nfic management implications of each. [MCDY99] contains a thorough discussion of ATM\ntraffic control for CBR and VBR.Two excellent treatments of ATM traffic characteristics and\nperformance are [GIRO99] and [SCHW96].\n[ANDR99] provides a clear, detailed description of GFR.Another useful description is\nInteresting examinations of frame relay congestion control issues are found in\n[CHEN89] and [DOSH88]. Good treatments are also found in [BUCK00] and [GORA99].\nAndrikopoulos, I.; Liakopoulous,A.; Pavlou, G.; and Sun, Z.“Providing Rate\nGuarantees for Internet Application Traffic Across ATM Networks.”IEEE Com-\nmunications Surveys, Third Quarter 1999. http://www.comsoc.org/pubs/surveys\nBonaventure, O., and Nelissen, J.“Guaranteed Frame Rate:A Better Service\nfor TCP/IP in ATM Networks.” IEEE Network, January/February 2001.\nBuckwalter, J. Frame Relay: Technology and Practice. Reading, MA: Addison-\nWesley, 2000.\nChen, K.; Ho, K.; and Saksena, V. “Analysis and Design of a Highly Reliable\nTransport Architecture for ISDN Frame-Relay Networks.” IEEE Journal on\nSelected Areas in Communications, October 1989.\nDoshi, B., and Nguyen, H. “Congestion Control in ISDN Frame-Relay Net-\nworks.” AT&T Technical Journal, November/December 1988.\nGarrett, M. “A Service Architecture for ATM: From Applications to Schedul-\ning.” IEEE Network, May/June 1996.\nGerla, M., and Kleinrock, L. “Flow Control: A Comparative Survey.” IEEE\nTransactions on Communications, April 1980.\nGiroux, N., and Ganti, S. Quality of Service in ATM Networks. Upper Saddle\nRiver, NJ: Prentice Hall, 1999.\nGoralski, W. Frame Relay for High-Speed Networks. New York: Wiley 1999.\nJain, R. “Congestion Control in Computer Networks: Issues and Trends.” IEEE\nNetwork Magazine, May 1990.\nJain, R. “Myths About Congestion Management in High-Speed Networks.”\nInternetworking: Research and Experience, Volume 3, 1992.\nKLEI93 Kleinrock, L. “On the Modeling and Analysis of Computer Networks.”\nProceedings of the IEEE, August 1993.\nMCDY99 McDysan, D., and Spohn, D. ATM: Theory and Application. New York:\nMcGraw-Hill, 1999.\nSchwartz, M. Broadband Integrated Networks. Upper Saddle River, NJ: Prentice\nHall PTR, 1996.\nYang, C., and Reddy, A. “A Taxonomy for Congestion Control Algorithms in\nPacket Switching Networks.” IEEE Network, July/August 1995.\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\n13.9 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nbackpressure\ncell delay variation\nchoke packet\ncongestion control\nexplicit congestion signaling\nimplicit congestion signaling\nquality of service (QoS)\nreservations\ntraffic management\nReview Questions\nWhen a node experiences saturation with respect to incoming packets, what general\nstrategies may be used?\nWhy is it that when the load exceeds the network capacity, delay tends to infinity?\nGive a brief explanation of each of the congestion control techniques illustrated in\nFigure 13.5.\nWhat is the difference between backward and forward explicit congestion signaling?\nBriefly explain the three general approaches to explicit congestion signaling.\nExplain the concept of committed information rate (CIR) in frame relay networks\nWhat is the significance of cell delay variation in an ATM network?\nWhat functions are included in ATM usage parameter control?\nWhat is the difference between traffic policing and traffic shaping?\nA proposed congestion control technique is known as isarithmic control. In this\nmethod, the total number of frames in transit is fixed by inserting a fixed number of\npermits into the network. These permits circulate at random through the frame relay\nnetwork. Whenever a frame handler wants to relay a frame just given to it by an\nattached user, it must first capture and destroy a permit.When the frame is delivered\nto the destination user by the frame handler to which it attaches, that frame handler\nreissues the permit. List three potential problems with this technique.\nIn the discussion of latency/speed effects in Section 13.5, an example was given in\nwhich over 7 megabits were transmitted before the source could react. But isn’t a slid-\ning-window flow control technique, such as described for HDLC, designed to cope\nwith long propagation delays?\nWhen the sustained traffic through a packet-switching node exceeds the node’s\ncapacity, the node must discard packets. Buffers only defer the congestion problem;\nthey do not solve it. Consider the packet-switching network in Figure 13.13. Five sta-\ntions attach to one of the network’s nodes.The node has a single link to the rest of the\nnetwork with a normalized throughput capacity of \nSenders 1 through 5 are\nsending at average sustained rates of of 0.1, 0.2, 0.3, 0.4, and 0.5, respectively. Clearly\nthe node is overloaded. To deal with the congestion, the node discards packets from\nsender i with a probability of \nShow the relationship among \nand C so that the rate of undiscarded packets\ndoes not exceed C.\nThe node establishes a discard policy by assigning values to the \nsuch that the\nrelationship derived in part (a) of this problem is satisfied. For each of the following\npolicies, verify that the relationship is satisfied and describe in words the policy from\nthe point of view of the senders.\n13.9 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nConsider the frame relay network depicted in Figure 13.14. C is the capacity of a link in\nframes per second. Node A presents a constant load of 0.8 frames per second destined\np1 = 0.0; p2 = 0.0; p3 = 0.0; p4 = 0.0; p5 = 1.0\np1 = 0.0; p2 = 0.0; p3 = 0.222; p4 = 0.417; p5 = 0.533\np1 = 0.091; p2 = 0.182; p3 = 0.273; p4 = 0.364; p5 = 0.455\np1 = 0.333; p2 = 0.333; p3 = 0.333; p4 = 0.333; p5 = 0.333\nFigure 13.13\nStations Attached to a Packet-Switching Node\nFigure 13.14\nNetwork of Frame Relay Nodes\nCHAPTER 13 / CONGESTION CONTROL IN DATA NETWORKS\nNode B presents a load destined for \nNode S has a common pool of buffers\nthat it uses for traffic both to \nWhen the buffer is full, frames are discarded,\nand are later retransmitted by the source user. S has a throughput capacity of 2. Plot the\ntotal throughput (i.e., the sum of \ndelivered traffic) as a function of \nWhat fraction of the throughput is \ntraffic for \nFor a frame relay network to be able to detect and then signal congestion,it is necessary\nfor each frame handler to monitor its queuing behavior. If queue lengths begin to grow\nto a dangerous level, then either forward or backward explicit notification or a combi-\nnation should be set to try to reduce the flow of frames through that frame handler.The\nframe handler has some choice as to which logical connections should be alerted to con-\ngestion. If congestion is becoming quite serious, all logical connections through a frame\nhandler might be notified. In the early stages of congestion, the frame handler might\njust notify users for those connections that are generating the most traffic.\nIn one of the frame relay specifications, an algorithm for monitoring queue lengths\nis suggested; this is shown in Figure 13.15. A cycle begins when the outgoing circuit\ngoes from idle (queue empty) to busy (nonzero queue size, including the current\nframe). If a threshold value is exceeded, then the circuit is in a state of incipient con-\ngestion, and the congestion avoidance bits should be set on some or all logical connec-\ntions that use that circuit. Describe the algorithm in words and explain its advantages.\nThe algorithm makes use of the following variables:\nThe algorithm consists of three components:\nt \u0001 current time\nti \u0001 time of ith arrival or departure event\nqi \u0001 number of frames in the system after the event\nT0 \u0001 time at the beginning of the previous cycle\nT1 \u0001 time at the beginning of the current cycle\nUpdate: Beginning with q0 :\u0001 0\nIf the ith event is an arrival event, qi :\u0001 qi\u00031 \u0004 1\nIf the ith event is a departure event, qi :\u0001 qi\u00031 \u0003 1\nL \u0001 Ai \u0004 Ai\u00031\nqi\u00031(ti \u0003 ti\u00031)\nti\r[T0, T1)\nqi\u00031(ti \u0003 ti\u00031)\nFigure 13.15\nA Frame Relay Algorithm\nCompare sustainable cell rate and burst tolerance, as used in ATM networks, with\ncommitted information rate and excess burst size, as used in frame relay networks. Do\nthe respective terms represent the same concepts?\nPrinciples of Cellular Networks\nFirst-Generation Analog\nSecond-Generation CDMA\nThird-Generation Systems\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nAfter the fire of 1805, Judge Woodward was the central figure involved in reestab-\nlishing the town. Influenced by Major Pierre L’Enfant’s plans for Washington, DC,\nJudge Woodward envisioned a modern series of hexagons with major diagonal\navenues centered on circular parks, or circuses, in the center of the hexagons. Fred-\nerick Law Olmstead said, ”nearly all of the most serious mistakes of Detroit’s past\nhave arisen from a disregard of the spirit of Woodward’s plan.”\n—Endangered Detroit, Friends of the Book-Cadillac Hotel\nThe essence of a cellular network is the use of multiple low-power\ntransmitters. The area to be covered is divided into cells in a hexago-\nnal tile pattern that provide full coverage of the area.\nA major technical problem for cellular networks is fading, which\nrefers to the time variation of received signal power caused by\nchanges in the transmission medium or path(s).\nFirst-generation cellular networks were analog,making use of frequency\ndivision multiplexing.\nSecond-generation cellular networks are digital. One technique in\nwidespread use is based on code division multiple access (CDMA).\nThe objective of the third-generation (3G) of wireless communication\nis to provide fairly high-speed wireless communications to support\nmultimedia, data, and video in addition to voice.\nOf all the tremendous advances in data communications and telecommunica-\ntions, perhaps the most revolutionary is the development of cellular networks.\nCellular technology is the foundation of mobile wireless communications and\nsupports users in locations that are not easily served by wired networks. Cellu-\nlar technology is the underlying technology for mobile telephones, personal\ncommunications systems, wireless Internet and wireless Web applications, and\nWe begin this chapter with a look at the basic principles used in all cellular\nnetworks.Then we look at specific cellular technologies and standards, which are\nconveniently grouped into three generations.The first generation is analog based\nand, while still widely used, is passing from the scene. The dominant technology\ntoday is the digital second-generation systems. Finally, third-generation high-\nspeed digital systems have begun to emerge.\n14.1 / PRINCIPLES OF CELLULAR NETWORKS\n(a) Square pattern\n(b) Hexagonal pattern\nFigure 14.1\nCellular Geometries\n14.1 PRINCIPLES OF CELLULAR NETWORKS\nCellular radio is a technique that was developed to increase the capacity available for\nmobile radio telephone service. Prior to the introduction of cellular radio, mobile\nradio telephone service was only provided by a high-power transmitter/receiver. A\ntypical system would support about 25 channels with an effective radius of about \n80 km. The way to increase the capacity of the system is to use lower-power systems\nwith shorter radius and to use numerous transmitters/receivers.We begin this section\nwith a look at the organization of cellular systems and then examine some of the\ndetails of their implementation.\nCellular Network Organization\nThe essence of a cellular network is the use of multiple low-power transmitters, on\nthe order of 100 W or less. Because the range of such a transmitter is small, an area\ncan be divided into cells, each one served by its own antenna. Each cell is allocated\na band of frequencies and is served by a base station, consisting of transmitter,\nreceiver, and control unit.Adjacent cells are assigned different frequencies to avoid\ninterference or crosstalk. However, cells sufficiently distant from each other can use\nthe same frequency band.\nThe first design decision to make is the shape of cells to cover an area. A\nmatrix of square cells would be the simplest layout to define (Figure 14.1a).\nHowever, this geometry is not ideal. If the width of a square cell is d, then a cell\nhas four neighbors at a distance d and four neighbors at a distance \nmobile user within a cell moves toward the cell’s boundaries, it is best if all of the\nadjacent antennas are equidistant.This simplifies the task of determining when to\nswitch the user to an adjacent antenna and which antenna to choose.A hexagonal\npattern provides for equidistant antennas (Figure 14.1b).The radius of a hexagon\nis defined to be the radius of the circle that circumscribes it (equivalently, the \ndistance from the center to each vertex; also equal to the length of a side of a\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\n(c) Black cells indicate a frequency reuse for N \u0001 19\n(a) Frequency reuse pattern for N \u0001 4\n(b) Frequency reuse pattern for N \u0001 7\nCircle with\nFigure 14.2\nFrequency Reuse Patterns\nhexagon). For a cell radius R, the distance between the cell center and each adja-\ncent cell center is \nIn practice, a precise hexagonal pattern is not used. Variations from the ideal\nare due to topographical limitations, local signal propagation conditions, and practi-\ncal limitation on siting antennas.\nA wireless cellular system limits the opportunity to use the same frequency for\ndifferent communications because the signals,not being constrained,can interfere with\none another even if geographically separated. Systems supporting a large number of\ncommunications simultaneously need mechanisms to conserve spectrum.\nFrequency Reuse In a cellular system, each cell has a base transceiver.The trans-\nmission power is carefully controlled (to the extent that it is possible in the highly vari-\nable mobile communication environment) to allow communication within the cell\nusing a given frequency while limiting the power at that frequency that escapes the cell\ninto adjacent ones. The objective is to use the same frequency in other nearby cells,\nthus allowing the frequency to be used for multiple simultaneous conversations. Gen-\nerally,10 to 50 frequencies are assigned to each cell,depending on the traffic expected.\nThe essential issue is to determine how many cells must intervene between two\ncells using the same frequency so that the two cells do not interfere with each other.\nVarious patterns of frequency reuse are possible. Figure 14.2 shows some examples. If\n14.1 / PRINCIPLES OF CELLULAR NETWORKS\nthe pattern consists of N cells and each cell is assigned the same number of frequen-\ncies, each cell can have K/N frequencies, where K is the total number of frequencies\nallotted to the system. For AMPS (Section 14.2),\nis the smallest\npattern that can provide sufficient isolation between two uses of the same frequency.\nThis implies that there can be at most 57 frequencies per cell on average.\nIn characterizing frequency reuse, the following parameters are commonly\nD = minimum distance between centers of cells that use the same band\nof frequencies (called cochannels)\nR = radius of a cell\nd = distance between centers of adjacent cells \nN = number of cells in a repetitious pattern (each cell in the pattern uses\na unique band of frequencies), termed the reuse factor\nIn a hexagonal cell pattern, only the following values of N are possible:\nHence, possible values of N are 1, 3, 4, 7, 9, 12, 13, 16, 19, 21, and so on.The fol-\nlowing relationship holds:\nThis can also be expressed as \nIncreasing Capacity In time, as more customers use the system, traffic may build\nup so that there are not enough frequencies assigned to a cell to handle its calls.A num-\nber of approaches have been used to cope with this situation, including the following:\n• Adding new channels: Typically, when a system is set up in a region, not all of\nthe channels are used, and growth and expansion can be managed in an\norderly fashion by adding new channels.\n• Frequency borrowing: In the simplest case, frequencies are taken from adjacent\ncells by congested cells.The frequencies can also be assigned to cells dynamically.\n• Cell splitting: In practice, the distribution of traffic and topographic features is\nnot uniform, and this presents opportunities for capacity increase. Cells in\nareas of high usage can be split into smaller cells. Generally, the original cells\nare about 6.5 to 13 km in size. The smaller cells can themselves be split; how-\never, 1.5-km cells are close to the practical minimum size as a general solution\n(but see the subsequent discussion of microcells). To use a smaller cell, the\npower level used must be reduced to keep the signal within the cell. Also, as\nthe mobile units move, they pass from cell to cell, which requires transferring\nof the call from one base transceiver to another. This process is called a\nhandoff.As the cells get smaller, these handoffs become much more frequent.\nFigure 14.3 indicates schematically how cells can be divided to provide more\ncapacity. A radius reduction by a factor of F reduces the coverage area and\nincreases the required number of base stations by a factor of F2.\nN = I2 + J2 + 1I * J2, I, J = 0, 1, 2, 3, Á\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nFigure 14.3\nCell Splitting\nTypical Parameters for Macrocells and Microcells\nCell radius\n0.1 to 1 km\nTransmission power\nAverage delay spread\n10 to 100 ns\nMaximum bit rate\n• Cell sectoring: With cell sectoring, a cell is divided into a number of wedge-\nshaped sectors, each with its own set of channels, typically three or six \nsectors per cell. Each sector is assigned a separate subset of the cell’s chan-\nnels, and directional antennas at the base station are used to focus on each\n• Microcells: As cells become smaller, antennas move from the tops of \ntall buildings or hills, to the tops of small buildings or the sides of large\nbuildings, and finally to lamp posts, where they form microcells. Each\ndecrease in cell size is accompanied by a reduction in the radiated power\nlevels from the base stations and the mobile units. Microcells are useful \nin city streets in congested areas, along highways, and inside large public\nTable 14.1 suggests typical parameters for traditional cells,called macrocells,and\nmicrocells with current technology.The average delay spread refers to multipath delay\nspread (i.e., the same signal follows different paths and there is a time delay between\nthe earliest and latest arrival of the signal at the receiver). As indicated, the use \nof smaller cells enables the use of lower power and provides superior propagation\nconditions.\n14.1 / PRINCIPLES OF CELLULAR NETWORKS\n(b) Cell radius \u0001 0.8 km\n(a) Cell radius \u0001 1.6 km\nWidth \u0001 21 \u0007 0.8 \u0001 16.8 km\nWidth \u0001 11 \u0007 1.6 \u0001 17.6 km\nHeight \u0001 5 \u0007\u00013 \u0007 1.6 \u0001 13.9 km\nHeight \u0001 10 \u0007\u00013 \u0007 0.8 \u0001 13.9 km\nFigure 14.4\nFrequency Reuse Example\nEXAMPLE [HAAS00].\nAssume a system of 32 cells with a cell radius of 1.6 km,\na total of 32 cells, a total frequency bandwidth that supports 336 traffic channels,\nand a reuse factor of \nIf there are 32 total cells, what geographic area is cov-\nered, how many channels are there per cell, and what is the total number of con-\ncurrent calls that can be handled? Repeat for a cell radius of 0.8 km and 128 cells.\nFigure 14.4a shows an approximately square pattern.The area of a hexagon\nof radius R is \nA hexagon of radius 1.6 km has an area of \nthe total area covered is \nthe number of chan-\nnels per cell is \nfor a total channel capacity of \nnels. For the layout of Figure 14.4b, the area covered is \nThe number of channels per cell is \nfor a total channel capacity of\n48 * 128 = 6144\n336/7 = 48,\n1.66 * 128 = 213 km2.\n48 * 32 = 1536\n336/7 = 48,\n6.65 * 32 = 213 km2.\nOperation of Cellular Systems\nFigure 14.5 shows the principal elements of a cellular system. In the approximate\ncenter of each cell is a base station (BS). The BS includes an antenna, a controller,\nand a number of transceivers, for communicating on the channels assigned to that\ncell.The controller is used to handle the call process between the mobile unit and the\nrest of the network. At any time, a number of mobile user units may be active and\nmoving about within a cell, communicating with the BS. Each BS is connected to a\nmobile telecommunications switching office (MTSO),with one MTSO serving multiple\nBSs. Typically, the link between an MTSO and a BS is by a wire line, although a\nwireless link is also possible. The MTSO connects calls between mobile units. The\nMTSO is also connected to the public telephone or telecommunications network and\ncan make a connection between a fixed subscriber to the public network and a\nmobile subscriber to the cellular network. The MTSO assigns the voice channel to\neach call, performs handoffs, and monitors the call for billing information.\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\ntransceiver\ntransceiver\ntelecommuni-\ntelecommunications\ntransceiver\nFigure 14.5\nOverview of Cellular System\n1Usually, but not always, the antenna and therefore the base station selected is the closest one to the\nmobile unit. However, because of propagation anomalies, this is not always the case.\nThe use of a cellular system is fully automated and requires no action on the\npart of the user other than placing or answering a call. Two types of channels are\navailable between the mobile unit and the base station (BS): control channels and\ntraffic channels. Control channels are used to exchange information having to do\nwith setting up and maintaining calls and with establishing a relationship between a\nmobile unit and the nearest BS. Traffic channels carry a voice or data connection\nbetween users. Figure 14.6 illustrates the steps in a typical call between two mobile\nusers within an area controlled by a single MTSO:\n• Mobile unit initialization: When the mobile unit is turned on, it scans and\nselects the strongest setup control channel used for this system (Figure 14.6a).\nCells with different frequency bands repetitively broadcast on different setup\nchannels. The receiver selects the strongest setup channel and monitors that\nchannel. The effect of this procedure is that the mobile unit has automatically\nselected the BS antenna of the cell within which it will operate.1 Then a hand-\nshake takes place between the mobile unit and the MTSO controlling this cell,\nthrough the BS in this cell.The handshake is used to identify the user and reg-\nister its location. As long as the mobile unit is on, this scanning procedure is\nrepeated periodically to account for the motion of the unit. If the unit enters a\nnew cell, then a new BS is selected. In addition, the mobile unit is monitoring\nfor pages, discussed subsequently.\n• Mobile-originated call: A mobile unit originates a call by sending the number\nof the called unit on the preselected setup channel (Figure 14.6b).The receiver\nat the mobile unit first checks that the setup channel is idle by examining\ninformation in the forward (from the BS) channel. When an idle is detected,\nthe mobile may transmit on the corresponding reverse (to BS) channel. The\nBS sends the request to the MTSO.\n14.1 / PRINCIPLES OF CELLULAR NETWORKS\n(a) Monitor for strongest signal\n(b) Request for connection\n(d) Call accepted\n(e) Ongoing call\n(f) Handoff\nFigure 14.6\nExample of Mobile Cellular Call\n• Paging: The MTSO then attempts to complete the connection to the called\nunit. The MTSO sends a paging message to certain BSs depending on the\ncalled mobile number (Figure 14.6c). Each BS transmits the paging signal on\nits own assigned setup channel.\n• Call accepted: The called mobile unit recognizes its number on the setup chan-\nnel being monitored and responds to that BS, which sends the response to the\nMTSO.The MTSO sets up a circuit between the calling and called BSs.At the\nsame time, the MTSO selects an available traffic channel within each BS’s cell\nand notifies each BS, which in turn notifies its mobile unit (Figure 14.6d). The\ntwo mobile units tune to their respective assigned channels.\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\n• Ongoing call: While the connection is maintained, the two mobile units\nexchange voice or data signals, going through their respective BSs and the\nMTSO (Figure 14.6e).\n• Handoff: If a mobile unit moves out of range of one cell and into the range of\nanother during a connection, the traffic channel has to change to one assigned\nto the BS in the new cell (Figure 14.6f).The system makes this change without\neither interrupting the call or alerting the user.\nOther functions performed by the system but not illustrated in Figure 14.6\ninclude the following:\n• Call blocking: During the mobile-initiated call stage, if all the traffic channels\nassigned to the nearest BS are busy, then the mobile unit makes a preconfig-\nured number of repeated attempts. After a certain number of failed tries, a\nbusy tone is returned to the user.\n• Call termination: When one of the two users hangs up, the MTSO is informed\nand the traffic channels at the two BSs are released.\n• Call drop: During a connection, because of interference or weak signal spots\nin certain areas, if the BS cannot maintain the minimum required signal\nstrength for a certain period of time, the traffic channel to the user is dropped\nand the MTSO is informed.\n• Calls to/from fixed and remote mobile subscriber: The MTSO connects to the\npublic switched telephone network. Thus, the MTSO can set up a connection\nbetween a mobile user in its area and a fixed subscriber via the telephone net-\nwork. Further, the MTSO can connect to a remote MTSO via the telephone\nnetwork or via dedicated lines and set up a connection between a mobile user\nin its area and a remote mobile user.\nMobile Radio Propagation Effects\nMobile radio communication introduces complexities not found in wire communi-\ncation or in fixed wireless communication. Two general areas of concern are signal\nstrength and signal propagation effects.\n• Signal strength: The strength of the signal between the base station and the\nmobile unit must be strong enough to maintain signal quality at the receiver but\nno so strong as to create too much cochannel interference with channels in\nanother cell using the same frequency band. Several complicating factors exist.\nHuman-made noise varies considerably, resulting in a variable noise level. For\nexample, automobile ignition noise in the cellular frequency range is greater in\nthe city than in a suburban area. Other signal sources vary from place to place.\nThe signal strength varies as a function of distance from the BS to a point within\nits cell.Moreover,the signal strength varies dynamically as the mobile unit moves.\n• Fading: Even if signal strength is within an effective range, signal propagation\neffects may disrupt the signal and cause errors. Fading is discussed subse-\nquently in this section.\nIn designing a cellular layout, the communications engineer must take account\nof these various propagation effects, the desired maximum transmit power level at the\n14.1 / PRINCIPLES OF CELLULAR NETWORKS\nbase station and the mobile units, the typical height of the mobile unit antenna, and\nthe available height of the BS antenna. These factors will determine the size of the\nindividual cell. Unfortunately, as just described, the propagation effects are dynamic\nand difficult to predict.The best that can be done is to come up with a model based on\nempirical data and to apply that model to a given environment to develop guidelines\nfor cell size. One of the most widely used models was developed by Okumura et al.\n[OKUM68] and subsequently refined by Hata [HATA80].The original was a detailed\nanalysis of the Tokyo area and produced path loss information for an urban environ-\nment. Hata’s model is an empirical formulation that takes into account a variety of\nenvironments and conditions. For an urban environment, predicted path loss is\nLdB = 69.55 + 26.16 log fc - 13.82 log ht - A(hr) + (44.9 - 6.55 log ht) log d (14.1)\nFor a small- or medium-sized city, the correction factor is given by\nAnd for a large city it is given by\nTo estimate the path loss in a suburban area, the formula for urban path loss in\nEquation (14.1) is modified as\nAnd for the path loss in open areas, the formula is modified as\nThe Okumura/Hata model is considered to be among the best in terms of\naccuracy in path loss prediction and provides a practical means of estimating path\nloss in a wide variety of situations [FREE97, RAPP97].\nLdB1open2 = LdB1urban2 - 4.781log fc22 - 18.7331log fc2 - 40.98\nLdB1suburban2 = LdB1urban2 - 2[log1fc/282]2 - 5.4\n A1hr2 = 3.2[log111.75hr2]2 - 4.97 dB\n for fc Ú 300 MHz\n A1hr2 = 8.29[log11.54hr2]2 - 1.1 dB\n for fc … 300 MHz\nA1hr2 = 11.1 log fc - 0.72hr - 11.56 log fc - 0.82 dB\n A1hr2 = correction factor for mobile antenna height\nd = propagation distance between antennas in km, from 1 to 20 km\nhr = height of receiving antenna 1mobile station2 in m, from 1 to 10 m\nht = height of transmitting antenna 1base station2 in m, from 30 to 300 m\nfc = carrier frequency in MHz from 150 to 1500 MHz\nEXAMPLE [FREE97].\nEstimate the path loss for a medium-size city.\n= 69.55 + 77.28 - 22.14 - 8.95 + 34.4 = 150.14 dB\nLdB = 69.55 + 26.16 log 900 - 13.82 log 40 - 8.95 + 144.9 - 6.55 log 402 log 10\n= 12.75 - 3.8 = 8.95 dB\n A1hr2 = 11.1 log 900 - 0.725 - 11.56 log 900 - 0.82 dB\nfc = 900 MHz,\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nFigure 14.7\nSketch of Three Important Propagation Mechanisms: Reflection (R), Scatter-\ning (S), Diffraction (D) [ANDE95]\n2On the other hand, the reflected signal has a longer path, which creates a phase shift due to delay\nrelative to the unreflected signal. When this delay is equivalent to half a wavelength, the two signals\nare back in phase.\nFading in the Mobile Environment\nPerhaps the most challenging technical problem facing communications systems\nengineers is fading in a mobile environment.The term fading refers to the time vari-\nation of received signal power caused by changes in the transmission medium or\npath(s). In a fixed environment, fading is affected by changes in atmospheric condi-\ntions, such as rainfall. But in a mobile environment, where one of the two antennas\nis moving relative to the other, the relative location of various obstacles changes\nover time, creating complex transmission effects.\nMultipath Propagation Three propagation mechanisms, illustrated in Figure 14.7,\nplay a role. Reflection occurs when an electromagnetic signal encounters a surface that\nis large relative to the wavelength of the signal.For example,suppose a ground-reflected\nwave near the mobile unit is received. Because the ground-reflected wave has a 180°\nphase shift after reflection,the ground wave and the line-of-sight (LOS) wave may tend\nto cancel, resulting in high signal loss.2 Further, because the mobile antenna is lower\nthan most human-made structures in the area, multipath interference occurs. These\nreflected waves may interfere constructively or destructively at the receiver.\n14.1 / PRINCIPLES OF CELLULAR NETWORKS\nDiffraction occurs at the edge of an impenetrable body that is large compared\nto the wavelength of the radio wave. When a radio wave encounters such an edge,\nwaves propagate in different directions with the edge as the source.Thus, signals can\nbe received even when there is no unobstructed LOS from the transmitter.\nIf the size of an obstacle is on the order of the wavelength of the signal or less,\nscattering occurs.An incoming signal is scattered into several weaker outgoing sig-\nnals.At typical cellular microwave frequencies, there are numerous objects, such as\nlamp posts and traffic signs, that can cause scattering. Thus, scattering effects are\ndifficult to predict.\nThese three propagation effects influence system performance in various ways\ndepending on local conditions and as the mobile unit moves within a cell. If a mobile\nunit has a clear LOS to the transmitter, then diffraction and scattering are generally\nminor effects, although reflection may have a significant impact. If there is no clear\nLOS, such as in an urban area at street level, then diffraction and scattering are the\nprimary means of signal reception.\nThe Effects of Multipath Propagation As just noted, one unwanted effect\nof multipath propagation is that multiple copies of a signal may arrive at different\nphases. If these phases add destructively, the signal level relative to noise declines,\nmaking signal detection at the receiver more difficult.\nA second phenomenon, of particular importance for digital transmission, is intersym-\nbol interference (ISI). Consider that we are sending a narrow pulse at a given fre-\nquency across a link between a fixed antenna and a mobile unit.Figure 14.8 shows what\nthe channel may deliver to the receiver if the impulse is sent at two different times.The\nupper line shows two pulses at the time of transmission. The lower line shows the\nresulting pulses at the receiver. In each case the first received pulse is the desired LOS\nsignal. The magnitude of that pulse may change because of changes in atmospheric\nattenuation.Further,as the mobile unit moves farther away from the fixed antenna,the\namount of LOS attenuation increases. But in addition to this primary pulse, there may\nTransmitted\nTransmitted\nFigure 14.8\nTwo Pulses in Time-Variant Multipath\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nbe multiple secondary pulses due to reflection,diffraction,and scattering.Now suppose\nthat this pulse encodes one or more bits of data. In that case, one or more delayed\ncopies of a pulse may arrive at the same time as the primary pulse for a subsequent bit.\nThese delayed pulses act as a form of noise to the subsequent primary pulse, making\nrecovery of the bit information more difficult.\nAs the mobile antenna moves, the location of various obstacles changes; hence\nthe number, magnitude, and timing of the secondary pulses change. This makes it\ndifficult to design signal processing techniques that will filter out multipath effects\nso that the intended signal is recovered with fidelity.\nTypes of Fading Fading effects in a mobile environment can be classified as\neither fast or slow. Referring to Figure 14.7, as the mobile unit moves down a street\nin an urban environment, rapid variations in signal strength occur over distances of\nabout one-half a wavelength.At a frequency of 900 MHz, which is typical for mobile\ncellular applications, a wavelength is 0.33 m. Changes of amplitude can be as much\nas 20 or 30 dB over a short distance.This type of rapidly changing fading phenome-\nnon, known as fast fading, affects not only mobile phones in automobiles, but even a\nmobile phone user walking down an urban street.\nAs the mobile user covers distances well in excess of a wavelength,the urban envi-\nronment changes, as the user passes buildings of different heights, vacant lots, intersec-\ntions,and so forth.Over these longer distances,there is a change in the average received\npower level about which the rapid fluctuations occur.This is referred to as slow fading.\nFading effects can also be classified as flat or selective. Flat fading, or nonselec-\ntive fading,is that type of fading in which all frequency components of the received sig-\nnal fluctuate in the same proportions simultaneously.Selective fading affects unequally\nthe different spectral components of a radio signal.The term selective fading is usually\nsignificant only relative to the bandwidth of the overall communications channel. If\nattenuation occurs over a portion of the bandwidth of the signal, the fading is consid-\nered to be selective; nonselective fading implies that the signal bandwidth of interest is\nnarrower than, and completely covered by, the spectrum affected by the fading.\nError Compensation Mechanisms The efforts to compensate for the errors\nand distortions introduced by multipath fading fall into three general categories:\nforward error correction, adaptive equalization, and diversity techniques. In the typ-\nical mobile wireless environment, techniques from all three categories are combined\nto combat the error rates encountered.\nForward error correction is applicable in digital transmission applications:\nthose in which the transmitted signal carries digital data or digitized voice or video\ndata.Typically in mobile wireless applications, the ratio of total bits sent to data bits\nsent is between 2 and 3. This may seem an extravagant amount of overhead, in that\nthe capacity of the system is cut to one-half or one-third of its potential,but the mobile\nwireless environment is so difficult that such levels of redundancy are necessary.\nChapter 6 discusses forward error correction.\nAdaptive equalization can be applied to transmissions that carry analog infor-\nmation (e.g., analog voice or video) or digital information (e.g., digital data, digitized\nvoice or video) and is used to combat intersymbol interference. The process of\nequalization involves some method of gathering the dispersed symbol energy back\ntogether into its original time interval. Equalization is a broad topic; techniques\n14.2 / FIRST-GENERATION ANALOG\ninclude the use of so-called lumped analog circuits as well as sophisticated digital\nsignal processing algorithms.\nDiversity is based on the fact that individual channels experience independent\nfading events. We can therefore compensate for error effects by providing multiple\nlogical channels in some sense between transmitter and receiver and sending part of\nthe signal over each channel. This technique does not eliminate errors but it does\nreduce the error rate, since we have spread the transmission out to avoid being sub-\njected to the highest error rate that might occur.The other techniques (equalization,\nforward error correction) can then cope with the reduced error rate.\nSome diversity techniques involve the physical transmission path and are\nreferred to as space diversity. For example, multiple nearby antennas may be used to\nreceive the message, with the signals combined in some fashion to reconstruct the\nmost likely transmitted signal. Another example is the use of collocated multiple\ndirectional antennas, each oriented to a different reception angle with the incoming\nsignals again combined to reconstitute the transmitted signal.\nMore commonly, the term diversity refers to frequency diversity or time diver-\nsity techniques. With frequency diversity, the signal is spread out over a larger fre-\nquency bandwidth or carried on multiple frequency carriers. The most important\nexample of this approach is spread spectrum, which is examined in Chapter 9.\n14.2 FIRST-GENERATION ANALOG\nThe original cellular telephone networks provided analog traffic channels; these are\nnow referred to as first-generation systems. Since the early 1980s the most common\nfirst-generation system in North America has been the Advanced Mobile Phone\nService (AMPS) developed by AT&T. This approach is also common in South\nAmerica,Australia, and China.Although gradually being replaced by second-gener-\nation systems, AMPS is still in common use. In this section, we provide an overview\nSpectral Allocation\nIn North America, two 25-MHz bands are allocated to AMPS (Table 14.2), one for\ntransmission from the base station to the mobile unit (869–894 MHz), the other for\ntransmission from the mobile to the base station (824–849 MHz). Each of these\nbands is split in two to encourage competition (i.e., so that in each market two oper-\nators can be accommodated).An operator is allocated only 12.5 MHz in each direc-\ntion for its system.The channels are spaced 30 kHz apart, which allows a total of 416\nchannels per operator.Twenty-one channels are allocated for control, leaving 395 to\ncarry calls.The control channels are data channels operating at 10 kbps.The conver-\nsation channels carry the conversations in analog using frequency modulation. Con-\ntrol information is also sent on the conversation channels in bursts as data. This\nnumber of channels is inadequate for most major markets, so some way must be\nfound either to use less bandwidth per conversation or to reuse frequencies. Both\napproaches have been taken in the various approaches to mobile telephony. For\nAMPS, frequency reuse is exploited.\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nAMPS Parameters\nBase station transmission band\n869 to 894 MHz\nMobile unit transmission band\n824 to 849 MHz\nSpacing between forward and reverse channels\nChannel bandwidth\nNumber of full-duplex voice channels\nNumber of full-duplex control channels\nMobile unit maximum power\nCell size, radius\nModulation, voice channel\nFM, 12-kHz peak deviation\nModulation, control channel\nFSK, 8-kHz peak deviation\nData transmission rate\nError control coding\nBCH (48, 36,5) and (40, 28,5)\nEach AMPS-capable cellular telephone includes a numeric assignment module (NAM)\nin read-only memory.The NAM contains the telephone number of the phone, which is\nassigned by the service provider, and the serial number of the phone, which is assigned\nby the manufacturer. When the phone is turned on, it transmits its serial number and\nphone number to the MTSO (Figure 14.5); the MTSO maintains a database with infor-\nmation about mobile units that have been reported stolen and uses serial number to\nlock out stolen units. The MTSO uses the phone number for billing purposes. If the\nphone is used in a remote city,the service is still billed to the user’s local service provider.\nWhen a call is placed, the following sequence of events occurs [COUC01]:\n1. The subscriber initiates a call by keying in the telephone number of the called\nparty and presses the send key.\n2. The MTSO verifies that the telephone number is valid and that the user is autho-\nrized to place the call; some service providers require the user to enter a PIN\n(personal identification number) as well as the called number to counter theft.\n3. The MTSO issues a message to the user’s cell phone indicating which traffic\nchannels to use for sending and receiving.\n4. The MTSO sends out a ringing signal to the called party.All of these operations\n(steps 2 through 4) occur within 10 s of initiating the call.\n5. When the called party answers, the MTSO establishes a circuit between the two\nparties and initiates billing information.\n6. When one party hangs up, the MTSO releases the circuit, frees the radio chan-\nnels, and completes the billing information.\nAMPS Control Channels\nEach AMPS service includes 21 full-duplex 30-kHz control channels, consisting of\n21 reverse control channels (RCCs) from subscriber to base station, and 21 forward\n14.3 / SECOND-GENERATION CDMA\nchannels from base station to subscriber. These channels transmit digital data using\nFSK. In both channels, data are transmitted in frames.\nControl information can be transmitted over a voice channel during a conver-\nsation. The mobile unit or the base station can insert a burst of data by turning off\nthe voice FM transmission for about 100 ms and replacing it with an FSK-encoded\nmessage. These messages are used to exchange urgent messages, such as change\npower level and handoff.\n14.3 SECOND-GENERATION CDMA\nThis section begins with an overview and then looks in detail at one type of second-\ngeneration cellular system.\nFirst- and Second-Generation Cellular Systems\nFirst-generation cellular networks, such as AMPS, quickly became highly popular,\nthreatening to swamp available capacity. Second-generation systems have been\ndeveloped to provide higher quality signals, higher data rates for support of digital\nservices, and greater capacity. [BLAC99b] lists the following as the key differences\nbetween the two generations:\n• Digital traffic channels: The most notable difference between the two \ngenerations is that first-generation systems are almost purely analog,\nwhereas second-generation systems are digital. In particular, the first-gen-\neration systems are designed to support voice channels using FM; digital\ntraffic is supported only by the use of a modem that converts the digital\ndata into analog form. Second-generation systems provide digital traffic\nchannels. These readily support digital data; voice traffic is first encoded in\ndigital form before transmitting. Of course, for second-generation systems,\nthe user traffic (data or digitized voice) must be converted to an analog \nsignal for transmission between the mobile unit and the base station (e.g.,\nsee Figure 5.15).\n• Encryption: Because all of the user traffic, as well as control traffic, is digitized\nin second-generation systems, it is a relatively simple matter to encrypt all of\nthe traffic to prevent eavesdropping. All second-generation systems provide\nthis capability, whereas first-generation systems send user traffic in the clear,\nproviding no security.\n• Error detection and correction: The digital traffic stream of second-generation\nsystems also lends itself to the use of error detection and correction tech-\nniques, such as those discussed in Chapter 6.The result can be very clear voice\n• Channel access: In first-generation systems, each cell supports a number of\nchannels. At any given time a channel is allocated to only one user. Second-\ngeneration systems also provide multiple channels per cell, but each channel is\ndynamically shared by a number of users using time division multiple access\n(TDMA) or code division multiple access (CDMA).We look at CDMA-based\nsystems in this section.\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nchipping codes (defined in Chapter 9) to multiple users, so that the receiver can\nrecover the transmission of an individual unit from multiple transmissions.\nCDMA has a number of advantages for a cellular network:\n• Frequency diversity: Because the transmission is spread out over a larger\nbandwidth, frequency-dependent transmission impairments, such as noise\nbursts and selective fading, have less effect on the signal.\n• Multipath resistance: In addition to the ability of DS-SS to overcome multipath\nfading by frequency diversity, the chipping codes used for CDMA not only\nexhibit low cross correlation but also low autocorrelation.3 Therefore, a ver-\nsion of the signal that is delayed by more than one chip interval does not inter-\nfere with the dominant signal as much as in other multipath environments.\n• Privacy: Because spread spectrum is obtained by the use of noiselike signals,\nwhere each user has a unique code, privacy is inherent.\n• Graceful degradation: With FDMA or TDMA, a fixed number of users can\naccess the system simultaneously. However, with CDMA, as more users access\nthe system simultaneously,the noise level and hence the error rate increases;only\ngradually does the system degrade to the point of an unacceptable error rate.\nTwo drawbacks of CDMA cellular should also be mentioned:\n• Self-jamming: Unless all of the mobile users are perfectly synchronized, the\narriving transmissions from multiple users will not be perfectly aligned on chip\nboundaries. Thus the spreading sequences of the different users are not\northogonal and there is some level of cross correlation. This is distinct from\neither TDMA or FDMA, in which for reasonable time or frequency guard-\nbands, respectively, the received signals are orthogonal or nearly so.\n• Near-far problem: Signals closer to the receiver are received with less attenuation\nthan signals farther away.Given the lack of complete orthogonality,the transmis-\nsions from the more remote mobile units may be more difficult to recover.\nMobile Wireless CDMA Design Considerations\nBefore turning to the specific example of IS-95, it will be useful to consider some\ngeneral design elements of a CDMA cellular system.\n14.3 / SECOND-GENERATION CDMA\nRAKE Receiver In a multipath environment, which is common in cellular sys-\ntems, if the multiple versions of a signal arrive more than one chip interval apart\nfrom each other, the receiver can recover the signal by correlating the chip sequence\nwith the dominant incoming signal.The remaining signals are treated as noise. How-\never, even better performance can be achieved if the receiver attempts to recover\nthe signals from multiple paths and then combine them, with suitable delays. This\nprinciple is used in the RAKE receiver.\nFigure 14.9 illustrates the principle of the RAKE receiver. The original binary\nsignal to be transmitted is spread by the exclusive-OR (XOR) operation with\nthe transmitter’s chipping code. The spread sequence is then modulated for\ntransmission over the wireless channel. Because of multipath effects, the chan-\nnel generates multiple copies of the signal, each with a different amount of time\netc.), and each with a different attenuation factors (\nAt the receiver, the combined signal is demodulated. The demodulated chip\nstream is then fed into multiple correlators, each delayed by a different amount.\nThese signals are then combined using weighting factors estimated from the\nDemodulator\nMultipath channel\nRAKE receiver\nFigure 14.9\nPrinciple of RAKE Receiver [PRAS98]\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nIS-95 Forward Link Channel Parameters\nTraffic Rate Set 1\nTraffic Rate Set 2\nData rate (bps)\nCode repetition\nModulation symbol\n19,200 19,200\n19,200 19,200\nPN chips/modulation\nPN chips/bit\nThe most widely used second-generation CDMA scheme is IS-95, which is primarily\ndeployed in North America.The transmission structures on the forward and reverse\nlinks differ and are described separately.\nIS-95 Forward Link\nTable 14.3 lists forward link channel parameters. The forward link consists of up\nto 64 logical CDMA channels each occupying the same 1228-kHz bandwidth\n(Figure 14.10a). The forward link supports four types of channels:\nPilot channel\nPaging channel\nPaging channel\nTraffic channel 1\nTraffic channel 2\nTraffic channel 24\nSynchronization channel\nTraffic channel 25\nTraffic channel 55\n(a) Forward channels\nAccess channel\nAccess channel\nAccess channel\nTraffic channel\nTraffic channel\nTraffic channel\nTraffic channel\n(b) Reverse channels\nDistinct long code\nUser-specific long code\nFigure 14.10\nIS-95 Channel Structure\n14.3 / SECOND-GENERATION CDMA\n• Pilot (channel 0): A continuous signal on a single channel. This channel\nallows the mobile unit to acquire timing information, provides phase refer-\nence for the demodulation process, and provides a means for signal strength\ncomparison for the purpose of handoff determination. The pilot channel\nconsists of all zeros.\n• Synchronization (channel 32): A 1200-bps channel used by the mobile station\nto obtain identification information about the cellular system (system time,\nlong code state, protocol revision, etc.).\n• Paging (channels 1 to 7): Contain messages for one or more mobile stations.\n• Traffic (channels 8 to 31 and 33 to 63): The forward channel supports \n55 traffic channels. The original specification supported data rates of up \nto 9600 bps. A subsequent revision added a second set of rates up to \n14,400 bps.\nNote that all of these channels use the same bandwidth. The chipping code is\nused to distinguish among the different channels. For the forward channel, the chip-\nping codes are the 64 orthogonal 64-bit codes derived from a \nmatrix known\nas the Walsh matrix (discussed in [STAL05]).\nFigure 14.11 shows the processing steps for transmission on a forward traffic\nchannel using rate set 1. For voice traffic, the speech is encoded at a data rate of 8550\nbps.After additional bits are added for error detection, the rate is 9600 bps.The full\nchannel capacity is not used when the user is not speaking. During quiet periods the\ndata rate is lowered to as low as 1200 bps.The 2400-bps rate is used to transmit tran-\nsients in the background noise, and the 4800-bps rate is used to mix digitized speech\nand signaling data.\nThe data or digitized speech is transmitted in 20-ms blocks with forward error\ncorrection provided by a convolutional encoder with rate 1/2, thus doubling the\neffective data rate to a maximum of 19.2 kbps. For lower data rates, the encoder\noutput bits (called code symbols) are replicated to yield the 19.2-kbps rate. The\ndata are then interleaved in blocks to reduce the effects of errors by spreading\nFollowing the interleaver, the data bits are scrambled.The purpose of this is to\nserve as a privacy mask and to prevent the sending of repetitive patterns, which in\nturn reduces the probability of users sending at peak power at the same time. The\nscrambling is accomplished by means of a long code that is generated as a pseudo-\nrandom number from a 42-bit-long shift register.The shift register is initialized with\nthe user’s electronic serial number.The output of the long code generator is at a rate\nof 1.2288 Mbps, which is 64 times the rate of 19.2 kbps, so only one bit in 64 is\nselected (by the decimator function). The resulting stream is XORed with the out-\nput of the block interleaver.\nThe next step in the processing inserts power control information in the traffic\nchannel, to control the power output of the antenna. The power control function of\nthe base station robs the traffic channel of bits at a rate of 800 bps. These are\ninserted by stealing code bits.The 800-bps channel carries information directing the\nmobile unit to increment, decrement, or keep stable its current output level. This\npower control stream is multiplexed into the 19.2 kbps by replacing some of the\ncode bits, using the long code generator to encode the bits.\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nquality indica-\ntors for 9600 &\n4800 bps rates\nencoder tail\nConvolutional\n(n, k, K) \u0001 (2, 1, 9)\nLong code mask\nPower control\nbits 800 bps\nTransmitted\n1.2288 Mbps\n1.2288 Mbps\ninterleaver\nForward traffic channel\ninformation bits\n(172, 80, 40, or 16 b/frame)\nMultiplexor\nFigure 14.11\nIS-95 Forward Link Transmission\nThe next step in the process is the DS-SS function, which spreads the \n19.2 kbps to a rate of 1.2288 Mbps using one row of the \nWalsh matrix.\nOne row of the matrix is assigned to a mobile station during call setup. If a 0 bit\nis presented to the XOR function, then the 64 bits of the assigned row are sent.\nIf a 1 is presented, then the bitwise XOR of the row is sent. Thus, the final bit\nrate is 1.2288 Mbps. This digital bit stream is then modulated onto the carrier\nusing a QPSK modulation scheme. Recall from Chapter 5 that QPSK involves\ncreating two bit streams that are separately modulated (see Figure 5.11).\nIn the IS-95 scheme, the data are split into I and Q (in-phase and quadrature)\nchannels and the data in each channel are XORed with a unique short code. The\nshort codes are generated as pseudorandom numbers from a 15-bit-long shift\n14.3 / SECOND-GENERATION CDMA\nIS-95 Reverse Link Channel Parameters\nTraffic-Rate Set 1\nTraffic-Rate Set 2\nData rate (bps)\nSymbol rate before \nrepetition (sps)\nSymbol repetition\nSymbol rate after\nrepetition (sps)\nTransmit duty cycle\nCode symbols/\nmodulation symbol\nmodulation symbol\nPN chips/bit\nIS-95 Reverse Link\nTable 14.4 lists reverse link channel parameters. The reverse link consists of up to\n94 logical CDMA channels each occupying the same 1228-kHz bandwidth (Figure\n14.10b). The reverse link supports up to 32 access channels and up to 62 traffic\nThe traffic channels in the reverse link are mobile unique. Each station has a\nunique long code mask based on its electronic serial number.The long code mask is\na 42-bit number, so there are \ndifferent masks.The access channel is used by\na mobile to initiate a call, to respond to a paging channel message from the base sta-\ntion, and for a location update.\nFigure 14.12 shows the processing steps for transmission on a reverse traffic\nchannel using rate set 1. The first few steps are the same as for the forward chan-\nnel. For the reverse channel, the convolutional encoder has a rate of 1/3, thus\ntripling the effective data rate to a maximum of 28.8 kbps.The data are then block\ninterleaved.\nThe next step is a spreading of the data using the Walsh matrix. The way in\nwhich the matrix is used, and its purpose, are different from that of the forward\nchannel. In the reverse channel, the data coming out of the block interleaver are\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nquality indica-\ntors for 9600 &\n4800 bps rates\n(307.2 kcps)\n(Walsh chip)\nencoder tail\nConvolutional\n(n, k, K) \u0001 (3, 1, 9)\nTransmitted\n1.2288 Mbps\ninterleaver\nReverse traffic channel\ninformation bits\n(172, 80, 40, or 16 b/frame)\nFigure 14.12\nIS-95 Reverse Link Transmission\ninvolves using the long code mask to smooth the data out over each 20-ms\nThe next step in the process is the DS-SS function. In the case of the reverse\nchannel, the long code unique to the mobile is XORed with the output of the ran-\ndomizer to produce the 1.2288-Mbps final data stream.This digital bit stream is then\nmodulated onto the carrier using an offset QPSK modulation scheme. This differs\nfrom the forward channel in the use of a delay element in the modulator (Figure\n5.11) to produce orthogonality.The reason the modulators are different is that in the\nforward channel, the spreading codes are orthogonal, all coming from the Walsh\nmatrix, whereas in the reverse channel, orthogonality of the spreading codes is not\nguaranteed.\n14.4 / THIRD-GENERATION SYSTEMS\n14.4 THIRD-GENERATION SYSTEMS\nThe objective of the third generation (3G) of wireless communication is to provide\nfairly high-speed wireless communications to support multimedia, data, and video in\naddition to voice. The ITU’s International Mobile Telecommunications for the year\n2000 (IMT-2000) initiative has defined the ITU’s view of third-generation capabili-\n• Voice quality comparable to the public switched telephone network\n• 144-kbps data rate available to users in high-speed motor vehicles over large\n• 384 kbps available to pedestrians standing or moving slowly over small areas\n• Support (to be phased in) for 2.048 Mbps for office use\n• Symmetrical and asymmetrical data transmission rates\n• Support for both packet-switched and circuit-switched data services\n• An adaptive interface to the Internet to reflect efficiently the common asym-\nmetry between inbound and outbound traffic\n• More efficient use of the available spectrum in general\n• Support for a wide variety of mobile equipment\n• Flexibility to allow the introduction of new services and technologies\nMore generally, one of the driving forces of modern communication technology is\nthe trend toward universal personal telecommunications and universal communi-\ncations access. The first concept refers to the ability of a person to identify \nhimself or herself easily and use conveniently any communication system in an\nentire country, over a continent, or even globally, in terms of a single account.The\nsecond refers to the capability of using one’s terminal in a wide variety of envi-\nronments to connect to information services (e.g., to have a portable terminal\nthat will work in the office, on the street, and on airplanes equally well). This \nrevolution in personal computing will obviously involve wireless communication\nin a fundamental way.\nPersonal communications services (PCSs) and personal communication net-\nworks (PCNs) are names attached to these concepts of global wireless communica-\ntions, and they also form objectives for third-generation wireless.\nGenerally, the technology planned is digital using time division multiple access\nor code division multiple access to provide efficient use of the spectrum and high\nPCS handsets are designed to be low power and relatively small and light.\nEfforts are being made internationally to allow the same terminals to be used\nAlternative Interfaces\nFigure 14.13 shows the alternative schemes that have been adopted as part of \nIMT-2000.The specification covers a set of radio interfaces for optimized performance\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nRadio interface\nDirect spread\nMulticarrier\nSingle carrier\nFrequency-time\nFigure 14.13\nIMT-2000 Terrestrial Radio Interfaces\nin different radio environments. A major reason for the inclusion of five alterna-\ntives was to enable a smooth evolution from existing first- and second-generation\nThe five alternatives reflect the evolution from the second generation.Two of\nthe specifications grow out of the work at the European Telecommunications Stan-\ndards Institute (ETSI) to develop a UMTS (universal mobile telecommunications\nsystem) as Europe’s 3G wireless standard. UMTS includes two standards. One of\nthese is known as wideband CDMA, or W-CDMA. This scheme fully exploits\nCDMA technology to provide high data rates with efficient use of bandwidth.\nTable 14.5 shows some of the key parameters of W-CDMA. The other European\neffort under UMTS is known as IMT-TC, or TD-CDMA.This approach is a combi-\nnation of W-CDMA and TDMA technology. IMT-TC is intended to provide an\nupgrade path for the TDMA-based GSM systems.\nAnother CDMA-based system, known as cdma2000, has a North American\norigin. This scheme is similar to, but incompatible with, W-CDMA, in part because\nthe standards use different chip rates. Also, cdma2000 uses a technique known as\nmulticarrier, not used with W-CDMA.\nTwo other interface specifications are shown in Figure 14.13. IMT-SC is pri-\nmarily designed for TDMA-only networks. IMT-FT can be used by both TDMA and\nFDMA carriers to provide some 3G services; it is an outgrowth of the Digital Euro-\npean Cordless Telecommunications (DECT) standard.\nCDMA Design Considerations\nThe dominant technology for 3G systems is CDMA. Although three different\nCDMA schemes have been adopted, they share some common design issues.\n[OJAN98] lists the following:\n14.4 / THIRD-GENERATION SYSTEMS\nW-CDMA Parameters\nChannel bandwidth\nForward RF channel structure\nDirect spread\nFrame length\nNumber of slots/frame\nSpreading modulation\nBalanced QPSK (forward)\nDual channel QPSK (reverse)\nComplex spreading circuit\nData modulation\nQPSK (forward)\nBPSK (reverse)\nCoherent detection\nPilot symbols\nReverse channel multiplexing\nControl and pilot channel time multiplexed. I and Q multiplexing\nfor data and control channels\nVarious spreading and multicode\nSpreading factors\nPower control\nOpen and fast closed loop (1.6 kHz)\nSpreading (forward)\nVariable length orthogonal sequences for channel separation.\nGold sequences \nfor cell and user separation.\nSpreading (reverse)\nSame as forward, different time shifts in I and Q channels.\n• Bandwidth: An important design goal for all 3G systems is to limit channel\nusage to 5 MHz. There are several reasons for this goal. On the one hand, a\nbandwidth of 5 MHz or more improves the receiver’s ability to resolve\nmultipath when compared to narrower bandwidths. On the other hand,\navailable spectrum is limited by competing needs, and 5 MHz is a reason-\nable upper limit on what can be allocated for 3G. Finally, 5 MHz is \nadequate for supporting data rates of 144 and 384 kHz, the main targets for\n3G services.\n• Chip rate: Given the bandwidth, the chip rate depends on desired data rate,\nthe need for error control, and bandwidth limitations.A chip rate of 3 Mcps or\nmore is reasonable given these design parameters.\n• Multirate: The term multirate refers to the provision of multiple fixed-data-\nrate logical channels to a given user, in which different data rates are pro-\nvided on different logical channels. Further, the traffic on each logical\nchannel can be switched independently through the wireless and fixed net-\nworks to different destinations. The advantage of multirate is that the sys-\ntem can flexibly support multiple simultaneous applications from a given\nuser and can efficiently use available capacity by only providing the capac-\nity required for each service. Multirate can be achieved with a TDMA\nscheme within a single CDMA channel, in which a different number of slots\nper frame are assigned to achieve different data rates. All the subchannels\nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nFigure 14.14\nTime and Code Multiplexing Principles [OJAN98]\n(a) Time multiplexing\n(b) Code multiplexing\ninterleaving\ninterleaving\ninterleaving\ninterleaving\ninterleaving\nat a given data rate would be protected by error correction and interleaving\ntechniques (Figure 14.14a). An alternative is to use multiple CDMA codes,\nwith separate coding and interleaving, and map them to separate CDMA\nchannels (Figure 14.14b).\n14.5 RECOMMENDED READING AND WEB SITES\n[BERT94] and [ANDE95] are instructive surveys of cellular wireless propagation\neffects. [BLAC99b] is one of the best technical treatments of second-generation cellular\n[TANT98] contains reprints of numerous important papers dealing with CDMA in cel-\nlular networks. [DINA98] provides an overview of both PN and orthogonal spreading codes\nfor cellular CDMA networks.\n[OJAN98] provides an overview of key technical design considerations for 3G \nsystems. Another useful survey is [ZENG00]. [PRAS00] is a much more detailed \n14.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nAnderson, J.; Rappaport, T.; and Yoshida, S. “Propagation Measurements and\nModels for Wireless Communications Channels.” IEEE Communications Maga-\nzine, January 1995.\nBertoni, H.; Honcharenko, W.; Maciel, L.; and Xia, H. “UHF Propagation Predic-\ntion for Wireless Personal Communications.”Proceedings of the IEEE, September 1994.\nBlack, U. Second-Generation Mobile and Wireless Networks. Upper Saddle\nRiver, NJ: Prentice Hall, 1999.\nDinan, E., and Jabbari, B. “Spreading Codes for Direct Sequence CDMA and\nWideband CDMA Cellular Networks.” IEEE Communications Magazine, Septem-\nOjanpera,T., and Prasad, G.“An Overview of Air Interface Multiple Access for\nIMT-2000/UMTS.” IEEE Communications Magazine, September 1998.\nPrasad, R.; Mohr, W.; and Konhauser, W., eds. Third-Generation Mobile Com-\nmunication Systems. Boston:Artech House, 2000.\nTantaratana, S, and Ahmed, K., eds. Wireless Applications of Spread Spectrum\nSystems: Selected Readings. Piscataway, NJ: IEEE Press, 1998.\nZeng, M.; Annamalai, A.; and Bhargava, V. “Harmonization of Global Third-\ngeneration Mobile Systems. IEEE Communications Magazine, December 2000.\nRecommended Web sites:\n• Cellular Telecommunications and Internet Association: An industry consortium\nthat provides information on successful applications of wireless technology.\n• CDMA Development Group: Information and links for IS-95 and CDMA generally.\n• 3G Americas: A trade group of Western Hemisphere companies supporting a variety\nof second- and third-generation schemes. Includes industry news, white papers, and\nother technical information.\n14.6 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nadaptive equalization\nAdvanced Mobile Phone \nService (AMPS)\nbase station\ncellular network\ncode division multiple access\ndiffraction\nfast fading\nflat fading\nfirst-generation (1G) network\nforward channel\nfrequency diversity\nfrequency reuse\nmobile radio\npower control\nreuse factor\nreverse channel\nsecond-generation (2G) \nselective fading\nslow fading\nspace diversity\nthird-generation (3G) \nCHAPTER 14 / CELLULAR WIRELESS NETWORKS\nReview Questions\nWhat geometric shape is used in cellular system design?\nWhat is the principle of frequency reuse in the context of a cellular network?\nList five ways of increasing the capacity of a cellular system.\nExplain the paging function of a cellular system.\nWhat is fading?\nWhat is the difference between diffraction and scattering?\nWhat is the difference between fast and slow fading?\nWhat is the difference between flat and selective fading?\nWhat are the key differences between first- and second-generation cellular systems?\nWhat are the advantages of using CDMA for a cellular network?\nWhat are the disadvantages of using CDMA for a cellular network?\nWhat are some key characteristics that distinguish third-generation cellular systems\nfrom second-generation cellular systems?\nConsider four different cellular systems that share the following characteristics. The\nfrequency bands are 825 to 845 MHz for mobile unit transmission and 870 to 890\nMHz for base station transmission.A duplex circuit consists of one 30-kHz channel in\neach direction.The systems are distinguished by the reuse factor, which is 4, 7, 12, and\n19, respectively.\nSuppose that in each of the systems, the cluster of cells (4, 7, 12, 19) is duplicated\n16 times. Find the number of simultaneous communications that can be supported\nby each system.\nb. Find the number of simultaneous communications that can be supported by a sin-\ngle cell in each system.\nWhat is the area covered, in cells, by each system?\nd. Suppose the cell size is the same in all four systems and a fixed area of 100 cells is\ncovered by each system. Find the number of simultaneous communications that\ncan be supported by each system.\nDescribe a sequence of events similar to that of Figure 14.6 for\na call from a mobile unit to a fixed subscriber\nb. a call from a fixed subscriber to a mobile unit\nAn analog cellular system has a total of 33 MHz of bandwidth and uses two 25-kHz\nsimplex (one-way) channels to provide full duplex voice and control channels.\nWhat is the number of channels available per cell for a frequency reuse factor of\n(1) 4 cells, (2) 7 cells, and (3) 12 cells?\nb. Assume that 1 MHz is dedicated to control channels but that only one control\nchannel is needed per cell. Determine a reasonable distribution of control channels\nand voice channels in each cell for the three frequency reuse factors of part (a).\nA cellular system uses FDMA with a spectrum allocation of 12.5 MHz in each direc-\ntion, a guard band at the edge of the allocated spectrum of 10 kHz, and a channel\nbandwidth of 30 kHz.What is the number of available channels?\nFor a cellular system, FDMA spectral efficiency is defined as \nWhat is an upper bound on ha?\nNT = total number of voice channels in the covered area\nBw = total bandwidth in one direction\nBc = channel bandwidth\n14.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nWalsh codes are the most common orthogonal codes used in CDMA applications. A\nset of Walsh codes of length n consists of the n rows of an \nWalsh matrix.That is,\nthere are n codes, each of length n.The matrix is defined recursively as follows:\nwhere n is the dimension of the matrix and the overscore denotes the logical NOT of\nthe bits in the matrix.The Walsh matrix has the property that every row is orthogonal\nto every other row and to the logical NOT of every other row. Show the Walsh matri-\nces of dimensions 2, 4, and 8.\nDemonstrate that the codes in an \nWalsh matrix are orthogonal to each other by\nshowing that multiplying any code by any other code produces a result of zero.\nConsider a CDMA system in which users A and B have the Walsh codes\nrespectively.\nShow the output at the receiver if A transmits a data bit 1 and B does not transmit.\nb. Show the output at the receiver if A transmits a data bit 0 and B does not transmit.\nShow the output at the receiver if A transmits a data bit 1 and B transmits a data\nbit 1.Assume the received power from both A and B is the same.\nd. Show the output at the receiver if A transmits a data bit 0 and B transmits a data\nbit 1.Assume the received power from both A and B is the same.\nShow the output at the receiver if A transmits a data bit 1 and B transmits a data\nbit 0.Assume the received power from both A and B is the same.\nShow the output at the receiver if A transmits a data bit 0 and B transmits a data\nbit 0.Assume the received power from both A and B is the same.\nShow the output at the receiver if A transmits a data bit 1 and B transmits a data\nbit 1.Assume the received power from B is twice the received power from A.This\ncan be represented by showing the received signal component from A as consist-\ning of elements of magnitude \nand the received signal component from\nB as consisting of elements of magnitude \nh. Show the output at the receiver if A transmits a data bit 0 and B transmits a data\nbit 1.Assume the received power from B is twice the received power from A.\n1-1 -1 1 1 -1 -1 1 12,\n1-1 1 -1 1 -1 1 -1 12\nhe trend in local area networks (LANs) involves the use of shared trans-\nmission media or shared switching capacity to achieve high data rates\nover relatively short distances. Several key issues present themselves.\nOne is the choice of transmission medium. Whereas coaxial cable was com-\nmonly used in traditional LANs, contemporary LAN installations emphasize\nthe use of twisted pair or optical fiber. In the case of twisted pair, efficient\nencoding schemes are needed to enable high data rates over the medium.Wire-\nless LANs have also assumed increased importance. Another design issue is\nthat of access control.\nLocal Area Networks\nChapter 15 Local Area Network Overview\nThe essential technology underlying all forms of LANs comprises\ntopology, transmission medium, and medium access control technique.\nChapter 15 examines the first two of these elements. Four topologies\nare in common use: bus, tree, ring, and star. The most common transmis-\nsion media for local networking are twisted pair (unshielded and\nshielded), coaxial cable (baseband and broadband), optical fiber, and\nwireless (microwave and infrared). These topologies and transmission\nmedia are discussed, with the exception of wireless, which is covered in\nChapter 17.\nThe increasing deployment of LANs has led to an increased need\nto interconnect LANs with each other and with WANs. Chapter 15 also\ndiscusses a key device used in interconnecting LANs: the bridge.\nChapter 16 High-Speed LANs\nChapter 16 looks in detail at the topologies, transmission media, and MAC\nprotocols of the most important LAN systems in current use; all of these\nhave been defined in standards documents.The most important of these is\nEthernet, which has been deployed in versions at 10 Mbps, 100 Mbps,\n1 Gbps, and 10 Gbps.Then the chapter looks at Fibre Channel.\nChapter 17 Wireless LANs\nWireless LANs use one of three transmission techniques: spread spec-\ntrum, narrowband microwave, and infrared. Chapter 17 provides an\noverview wireless LAN technology and applications. The most significant\nset of standards defining wireless LANs are those defined by the IEEE\n802.11 committee. Chapter 17 examines this set of standards in depth.\nTopologies and Transmission Media\nLAN Protocol Architecture\nLayer 2 and Layer 3 Switches\nRecommended Reading and Web Site\nKey Terms, Review Questions, and Problems\nThe whole of this operation is described in minute detail in the official British Naval\nHistory, and should be studied with its excellent charts by those who are interested in\nits technical aspect. So complicated is the full story that the lay reader cannot see the\nwood for the trees. I have endeavored to render intelligible the broad effects.\n—The World Crisis, Winston Churchill\nA LAN consists of a shared transmission medium and a set of hard-\nware and software for interfacing devices to the medium and regulat-\ning the orderly access to the medium.\nThe topologies that have been used for LANs are ring, bus, tree, and\nstar.A ring LAN consists of a closed loop of repeaters that allow data\nto circulate around the ring. A repeater may also function as a device\nattachment point.Transmission is generally in the form of frames.The\nbus and tree topologies are passive sections of cable to which stations\nare attached. A transmission of a frame by any one station can be\nheard by any other station. A star LAN includes a central node to\nwhich stations are attached.\nA set of standards has been defined for LANs that specifies a range of\ndata rates and encompasses a variety of topologies and transmission\nIn most cases, an organization will have multiple LANs that need to be\ninterconnected. The simplest approach to meeting this requirement is\nthe bridge.\nHubs and switches form the basic building blocks of most LANs.\nWe turn now to a discussion of local area networks (LANs). Whereas wide\narea networks may be public or private, LANs usually are owned by the orga-\nnization that is using the network to interconnect equipment. LANs have\nmuch greater capacity than wide area networks, to carry what is generally a\ngreater internal communications load.\nIn this chapter we look at the underlying technology and protocol archi-\ntecture of LANs. Chapters 16 and 17 are devoted to a discussion of specific\nLAN systems.\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\n15.1 BACKGROUND\nThe variety of applications for LANs is wide.To provide some insight into the types\nof requirements that LANs are intended to meet, this section provides a brief dis-\ncussion of some of the most important general application areas for these networks.\nPersonal Computer LANs\nA common LAN configuration is one that supports personal computers. With the\nrelatively low cost of such systems, individual managers within organizations often\nindependently procure personal computers for departmental applications, such as\nspreadsheet and project management tools, and Internet access.\nBut a collection of department-level processors will not meet all of an orga-\nnization’s needs; central processing facilities are still required. Some programs,\nsuch as econometric forecasting models, are too big to run on a small computer.\nCorporate-wide data files, such as accounting and payroll, require a centralized\nfacility but should be accessible to a number of users. In addition, there are other\nkinds of files that, although specialized, must be shared by a number of users. Fur-\nther, there are sound reasons for connecting individual intelligent workstations\nnot only to a central facility but to each other as well. Members of a project or\norganization team need to share work and information. By far the most efficient\nway to do so is digitally.\nCertain expensive resources, such as a disk or a laser printer, can be shared by\nall users of the departmental LAN. In addition, the network can tie into larger cor-\nporate network facilities. For example, the corporation may have a building-wide\nLAN and a wide area private network. A communications server can provide con-\ntrolled access to these resources.\nLANs for the support of personal computers and workstations have become\nnearly universal in organizations of all sizes. Even those sites that still depend heav-\nily on the mainframe have transferred much of the processing load to networks of\npersonal computers. Perhaps the prime example of the way in which personal com-\nputers are being used is to implement client/server applications.\nFor personal computer networks, a key requirement is low cost. In particular,\nthe cost of attachment to the network must be significantly less than the cost of the\nattached device.Thus, for the ordinary personal computer, an attachment cost in the\nhundreds of dollars is desirable. For more expensive, high-performance worksta-\ntions, higher attachment costs can be tolerated.\nBackend Networks and Storage Area Networks\nBackend networks are used to interconnect large systems such as mainframes,\nsupercomputers, and mass storage devices. The key requirement here is for bulk\ndata transfer among a limited number of devices in a small area. High reliability is\ngenerally also a requirement.Typical characteristics include the following:\n• High data rate: To satisfy the high-volume demand, data rates of 100 Mbps or\nmore are required.\n15.1 / BACKGROUND\n• High-speed interface: Data transfer operations between a large host system\nand a mass storage device are typically performed through high-speed parallel\nI/O interfaces, rather than slower communications interfaces. Thus, the physi-\ncal link between station and network must be high speed.\n• Distributed access: Some sort of distributed medium access control (MAC)\ntechnique is needed to enable a number of devices to share the transmission\nmedium with efficient and reliable access.\n• Limited distance: Typically, a backend network will be employed in a com-\nputer room or a small number of contiguous rooms.\n• Limited number of devices: The number of expensive mainframes and mass\nstorage devices found in the computer room generally numbers in the tens of\nTypically, backend networks are found at sites of large companies or research\ninstallations with large data processing budgets. Because of the scale involved, a\nsmall difference in productivity can translate into a sizable difference in cost.\nConsider a site that uses a dedicated mainframe computer.This implies a fairly\nlarge application or set of applications. As the load at the site grows, the existing\nmainframe may be replaced by a more powerful one, perhaps a multiprocessor sys-\ntem. At some sites, a single-system replacement will not be able to keep up; equip-\nment performance growth rates will be exceeded by demand growth rates. The\nfacility will eventually require multiple independent computers. Again, there are\ncompelling reasons for interconnecting these systems.The cost of system interrupt is\nhigh, so it should be possible, easily and quickly, to shift applications to backup sys-\ntems. It must be possible to test new procedures and applications without degrading\nthe production system. Large bulk storage files must be accessible from more than\none computer. Load leveling should be possible to maximize utilization and perfor-\nIt can be seen that some key requirements for backend networks differ from\nthose for personal computer LANs. High data rates are required to keep up with the\nwork, which typically involves the transfer of large blocks of data. The equipment\nfor achieving high speeds is expensive. Fortunately, given the much higher cost of\nthe attached devices, such costs are reasonable.\nA concept related to that of the backend network is the storage area network\n(SAN). A SAN is a separate network to handle storage needs. The SAN detaches\nstorage tasks from specific servers and creates a shared storage facility across a\nhigh-speed network. The collection of networked storage devices can include hard\ndisks, tape libraries, and CD arrays. Most SANs use Fibre Channel, which is\ndescribed in Chapter 16. Figure 15.1 contrasts the SAN with the traditional server-\nbased means of supporting shared storage. In a typical large LAN installation, a\nnumber of servers and perhaps mainframes each has its own dedicated storage\ndevices. If a client needs access to a particular storage device, it must go through\nthe server that controls that device. In a SAN, no server sits between the storage\ndevices and the network; instead, the storage devices and servers are linked\ndirectly to the network. The SAN arrangement improves client-to-storage access\nefficiency, as well as direct storage-to-storage communications for backup and\nreplication functions.\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\n(a) Server-based storage\nStorage devices\n(b) Storage area network\nStorage devices\nFigure 15.1\nThe Use of Storage Area Networks [HURW98]\n1A picture element, or pel, is the smallest discrete scanning-line sample of a facsimile system, which con-\ntains only black-white information (no gray scales). A pixel is a picture element that contains gray-scale\ninformation.\nHigh-Speed Office Networks\nTraditionally, the office environment has included a variety of devices with low- to\nmedium-speed data transfer requirements. However, applications in today’s office\nenvironment would overwhelm the limited speeds (up to 10 Mbps) of traditional\nLAN. Desktop image processors have increased network data flow by an unprece-\ndented amount. Examples of these applications include fax machines, document\nimage processors, and graphics programs on personal computers and workstations.\nConsider that a typical page with 200 picture elements, or pels1 (black or white\npoints), per inch resolution (which is adequate but not high resolution) generates\n3,740,000 bits \ncompression techniques, this will generate a tremendous load. In addition, disk tech-\nnology and price/performance have evolved so that desktop storage capacities of\nmultiple gigabytes are common.These new demands require LANs with high speed\nthat can support the larger numbers and greater geographic extent of office systems\nas compared to backend systems.\nBackbone LANs\nThe increasing use of distributed processing applications and personal computers has\nled to a need for a flexible strategy for local networking. Support of premises-wide\ndata communications requires a networking service that is capable of spanning the dis-\ntances involved and that interconnects equipment in a single (perhaps large) building\n18.5 inches * 11 inches * 40,000 pels per square inch2.\n15.2 / TOPOLOGIES AND TRANSMISSION MEDIA\nor a cluster of buildings. Although it is possible to develop a single LAN to intercon-\nnect all the data processing equipment of a premises, this is probably not a practical\nalternative in most cases.There are several drawbacks to a single-LAN strategy:\n• Reliability: With a single LAN, a service interruption, even of short duration,\ncould result in a major disruption for users.\n• Capacity: A single LAN could be saturated as the number of devices attached\nto the network grows over time.\n• Cost: A single LAN technology is not optimized for the diverse requirements\nof interconnection and communication.The presence of large numbers of low-\ncost microcomputers dictates that network support for these devices be pro-\nvided at low cost. LANs that support very-low-cost attachment will not be\nsuitable for meeting the overall requirement.\nA more attractive alternative is to employ lower-cost,lower-capacity LANs within\nbuildings or departments and to interconnect these networks with a higher-capacity\nLAN.This latter network is referred to as a backbone LAN.If confined to a single build-\ning or cluster of buildings, a high-capacity LAN can perform the backbone function.\n15.2 TOPOLOGIES AND TRANSMISSION MEDIA\nThe key elements of a LAN are\n• Transmission medium\n• Wiring layout\n• Medium access control\nTogether, these elements determine not only the cost and capacity of the LAN, but\nalso the type of data that may be transmitted, the speed and efficiency of communi-\ncations, and even the kinds of applications that can be supported.\nThis section provides a survey of the major technologies in the first two of\nthese categories. It will be seen that there is an interdependence among the choices\nin different categories.Accordingly, a discussion of pros and cons relative to specific\napplications is best done by looking at preferred combinations. This, in turn, is best\ndone in the context of standards, which is a subject of a later section.\nIn the context of a communication network, the term topology refers to the way in\nwhich the end points, or stations, attached to the network are interconnected. The\ncommon topologies for LANs are bus, tree, ring, and star (Figure 15.2). The bus is a\nspecial case of the tree, with only one trunk and no branches.\nBus and Tree Topologies Both bus and tree topologies are characterized by the\nuse of a multipoint medium. For the bus, all stations attach, through appropriate hard-\nware interfacing known as a tap, directly to a linear transmission medium, or bus. Full-\nduplex operation between the station and the tap allows data to be transmitted onto\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\nCentral hub, switch,\nor repeater\nTerminating\nFlow of data\nFigure 15.2\nLAN Topologies \n15.2 / TOPOLOGIES AND TRANSMISSION MEDIA\nthe bus and received from the bus. A transmission from any station propagates the\nlength of the medium in both directions and can be received by all other stations. At\neach end of the bus is a terminator,which absorbs any signal,removing it from the bus.\nThe tree topology is a generalization of the bus topology. The transmission\nmedium is a branching cable with no closed loops. The tree layout begins at a point\nknown as the headend. One or more cables start at the headend, and each of these\nmay have branches. The branches in turn may have additional branches to allow\nquite complex layouts. Again, a transmission from any station propagates through-\nout the medium and can be received by all other stations.\nTwo problems present themselves in this arrangement. First, because a trans-\nmission from any one station can be received by all other stations, there needs to be\nsome way of indicating for whom the transmission is intended. Second, a mechanism\nis needed to regulate transmission.To see the reason for this, consider that if two sta-\ntions on the bus attempt to transmit at the same time, their signals will overlap and\nbecome garbled. Or consider that one station decides to transmit continuously for a\nlong period of time.\nTo solve these problems, stations transmit data in small blocks, known as\nframes. Each frame consists of a portion of the data that a station wishes to transmit,\nplus a frame header that contains control information. Each station on the bus is\nassigned a unique address, or identifier, and the destination address for a frame is\nincluded in its header.\nFigure 15.3 illustrates the scheme. In this example, station C wishes to transmit\na frame of data to A. The frame header includes A’s address. As the frame propa-\ngates along the bus, it passes B. B observes the address and ignores the frame.A, on\nthe other hand, sees that the frame is addressed to itself and therefore copies the\ndata from the frame as it goes by.\nSo the frame structure solves the first problem mentioned previously: It pro-\nvides a mechanism for indicating the intended recipient of data. It also provides the\nbasic tool for solving the second problem, the regulation of access. In particular, the\nstations take turns sending frames in some cooperative fashion. This involves\nputting additional control information into the frame header, as discussed later.\nWith the bus or tree, no special action needs to be taken to remove frames\nfrom the medium. When a signal reaches the end of the medium, it is absorbed by\nthe terminator.\nRing Topology In the ring topology, the network consists of a set of repeaters\njoined by point-to-point links in a closed loop. The repeater is a comparatively sim-\nple device, capable of receiving data on one link and transmitting them, bit by bit, on\nthe other link as fast as they are received. The links are unidirectional; that is, data\nare transmitted in one direction only, so that data circulate around the ring in one\ndirection (clockwise or counterclockwise).\nEach station attaches to the network at a repeater and can transmit data onto\nthe network through the repeater. As with the bus and tree, data are transmitted in\nframes. As a frame circulates past all the other stations, the destination station rec-\nognizes its address and copies the frame into a local buffer as it goes by. The frame\ncontinues to circulate until it returns to the source station, where it is removed\n(Figure 15.4). Because multiple stations share the ring, medium access control is\nneeded to determine at what time each station may insert frames.\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\nC transmits frame addressed to A\nFrame is not addressed to B; B ignores it\nA copies frame as it goes by\nFigure 15.3\nFrame Transmission on a Bus LAN\nStar Topology In the star LAN topology, each station is directly connected to a\ncommon central node. Typically, each station attaches to a central node via two\npoint-to-point links, one for transmission and one for reception.\nIn general, there are two alternatives for the operation of the central node.\nOne approach is for the central node to operate in a broadcast fashion. A trans-\nmission of a frame from one station to the node is retransmitted on all of the out-\ngoing links. In this case, although the arrangement is physically a star, it is logically\na bus: A transmission from any station is received by all other stations, and only\none station at a time may successfully transmit. In this case, the central element is\nreferred to as a hub. Another approach is for the central node to act as a frame-\nswitching device. An incoming frame is buffered in the node and then retransmit-\nted on an outgoing link to the destination station. These approaches are explored\nin Section 15.5.\n15.2 / TOPOLOGIES AND TRANSMISSION MEDIA\n(a) C transmits frame\naddressed to A\n(b) Frame is not addressed\n      to B; B ignores it\n(c) A copies frame\n     as it goes by\n(d) C absorbs\nreturning frame\nFigure 15.4\nFrame Transmission on a Ring LAN\nChoice of Topology The choice of topology depends on a variety of factors,\nincluding reliability, expandability, and performance.This choice is part of the over-\nall task of designing a LAN and thus cannot be made in isolation, independent of\nthe choice of transmission medium, wiring layout, and access control technique. A\nfew general remarks can be made at this point.There are four alternative media that\ncan be used for a bus LAN:\n• Twisted pair: In the early days of LAN development, voice-grade twisted pair\nwas used to provide an inexpensive, easily installed bus LAN. A number of\nsystems operating at 1 Mbps were implemented. Scaling twisted pair up to\nhigher data rates in a shared-medium bus configuration is not practical, so this\napproach was dropped long ago.\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\n• Baseband coaxial cable: A baseband coaxial cable is one that makes use of digi-\ntal signaling.The original Ethernet scheme makes use of baseband coaxial cable.\n• Broadband coaxial cable: Broadband coaxial cable is the type of cable used in\ncable television systems. Analog signaling is used at radio and television fre-\nquencies. This type of system is more expensive and more difficult to install\nand maintain than baseband coaxial cable.This approach never achieved pop-\nularity and such LANs are no longer made.\n• Optical fiber: There has been considerable research relating to this alternative\nover the years, but the expense of the optical fiber taps and the availability of\nbetter alternatives have resulted in the demise of this option as well.\nThus, for a bus topology, only baseband coaxial cable has achieved widespread\nuse, primarily for Ethernet systems. Compared to a star-topology twisted pair or\noptical fiber installation, the bus topology using baseband coaxial cable is difficult to\nwork with. Even simple changes may require access to the coaxial cable, movement\nof taps, and rerouting of cable segments. Accordingly, few if any new installations\nare being attempted. Despite its limitations, there is a considerable installed base of\nbaseband coaxial cable bus LANs.\nVery-high-speed links over considerable distances can be used for the ring\ntopology. Hence, the ring has the potential of providing the best throughput of any\ntopology. One disadvantage of the ring is that a single link or repeater failure could\ndisable the entire network.\nThe star topology takes advantage of the natural layout of wiring in a building.\nIt is generally best for short distances and can support a small number of devices at\nhigh data rates.\nChoice of Transmission Medium The choice of transmission medium is\ndetermined by a number of factors. It is, we shall see, constrained by the topology of\nthe LAN. Other factors come into play, including\n• Capacity: to support the expected network traffic\n• Reliability: to meet requirements for availability\n• Types of data supported: tailored to the application\n• Environmental scope: to provide service over the range of environments\nThe choice is part of the overall task of designing a local network, which is\naddressed in Chapter 16. Here we can make a few general observations.\nVoice-grade unshielded twisted pair (UTP) is an inexpensive, well-understood\nmedium; this is the Category 3 UTP referred to in Chapter 4.Typically, office build-\nings are wired to meet the anticipated telephone system demand plus a healthy mar-\ngin; thus, there are no cable installation costs in the use of Category 3 UTP.\nHowever, the data rate that can be supported is generally quite limited, with the\nexception of very small LAN. Category 3 UTP is likely to be the most cost-effective\nfor a single-building, low-traffic LAN installation.\nShielded twisted pair and baseband coaxial cable are more expensive than\nCategory 3 UTP but provide greater capacity. Broadband cable is even more expen-\nsive but provides even greater capacity. However, in recent years, the trend has been\n15.3 / LAN PROTOCOL ARCHITECTURE\nChapter 16.\nOptical fiber has a number of attractive features, such as electromagnetic iso-\nlation, high capacity, and small size, which have attracted a great deal of interest.As\nyet the market penetration of fiber LANs is low; this is primarily due to the high\ncost of fiber components and the lack of skilled personnel to install and maintain\nfiber systems. This situation is beginning to change rapidly as more products using\nfiber are introduced.\n15.3 LAN PROTOCOL ARCHITECTURE\nThe architecture of a LAN is best described in terms of a layering of protocols that\norganize the basic functions of a LAN. This section opens with a description of the\nstandardized protocol architecture for LANs, which encompasses physical, medium\naccess control (MAC), and logical link control (LLC) layers. The physical layer\nencompasses topology and transmission medium, and is covered in Section 15.2.\nThis section provides an overview of the MAC and LLC layers.\nIEEE 802 Reference Model\nProtocols defined specifically for LAN and MAN transmission address issues relat-\ning to the transmission of blocks of data over the network. In OSI terms, higher\nlayer protocols (layer 3 or 4 and above) are independent of network architecture\nand are applicable to LANs, MANs, and WANs.Thus, a discussion of LAN protocols\nis concerned principally with lower layers of the OSI model.\nFigure 15.5 relates the LAN protocols to the OSI architecture (Figure 2.11).\nThis architecture was developed by the IEEE 802 LAN standards committee2 and\nhas been adopted by all organizations working on the specification of LAN stan-\ndards. It is generally referred to as the IEEE 802 reference model.\nWorking from the bottom up,the lowest layer of the IEEE 802 reference model\ncorresponds to the physical layer of the OSI model and includes such functions as\n• Encoding/decoding of signals\n• Preamble generation/removal (for synchronization)\n• Bit transmission/reception\nIn addition, the physical layer of the 802 model includes a specification of the trans-\nmission medium and the topology. Generally, this is considered “below” the lowest\nlayer of the OSI model. However, the choice of transmission medium and topology\nis critical in LAN design, and so a specification of the medium is included.\nAbove the physical layer are the functions associated with providing service to\nLAN users.These include\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\n• On transmission, assemble data into a frame with address and error detection\n• On reception, disassemble frame, and perform address recognition and error\n• Govern access to the LAN transmission medium.\n• Provide an interface to higher layers and perform flow and error control.\nThese are functions typically associated with OSI layer 2. The set of functions\nin the last bullet item are grouped into a logical link control (LLC) layer. The func-\ntions in the first three bullet items are treated as a separate layer, called medium\naccess control (MAC). The separation is done for the following reasons:\n• The logic required to manage access to a shared-access medium is not found in\ntraditional layer 2 data link control.\n• For the same LLC, several MAC options may be provided.\nFigure 15.6 illustrates the relationship between the levels of the architecture\n(compare Figure 2.9). Higher-level data are passed down to LLC, which appends\nOSI reference\nMedium access\nLogical link control\nLLC service\naccess point\nPresentation\nApplication\nFigure 15.5\nIEEE 802 Protocol Layers Compared to OSI Model\n15.3 / LAN PROTOCOL ARCHITECTURE\nTCP segment\nIP datagram\nLLC protocol data unit\nApplication data\nApplication layer\nFigure 15.6\nLAN Protocols in Context\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\ncontrol information as a header,creating an LLC protocol data unit (PDU).This con-\ntrol information is used in the operation of the LLC protocol. The entire LLC PDU\nis then passed down to the MAC layer, which appends control information at the\nfront and back of the packet, forming a MAC frame. Again, the control information\nin the frame is needed for the operation of the MAC protocol. For context, the figure\nalso shows the use of TCP/IP and an application layer above the LAN protocols.\nLogical Link Control\nThe LLC layer for LANs is similar in many respects to other link layers in common\nuse. Like all link layers, LLC is concerned with the transmission of a link-level PDU\nbetween two stations, without the necessity of an intermediate switching node. LLC\nhas two characteristics not shared by most other link control protocols:\n1. It must support the multiaccess, shared-medium nature of the link (this differs\nfrom a multidrop line in that there is no primary node).\n2. It is relieved of some details of link access by the MAC layer.\nAddressing in LLC involves specifying the source and destination LLC users.\nTypically, a user is a higher-layer protocol or a network management function in the\nstation.These LLC user addresses are referred to as service access points (SAPs), in\nkeeping with OSI terminology for the user of a protocol layer.\nWe look first at the services that LLC provides to a higher-level user, and then\nat the LLC protocol.\nLLC Services LLC specifies the mechanisms for addressing stations across the\nmedium and for controlling the exchange of data between two users. The operation\nand format of this standard is based on HDLC.Three services are provided as alter-\nnatives for attached devices using LLC:\n• Unacknowledged connectionless service: This service is a datagram-style ser-\nvice. It is a very simple service that does not involve any of the flow- and error-\ncontrol mechanisms. Thus, the delivery of data is not guaranteed. However, in\nmost devices, there will be some higher layer of software that deals with relia-\nbility issues.\n• Connection-mode service: This service is similar to that offered by HDLC. A\nlogical connection is set up between two users exchanging data, and flow con-\ntrol and error control are provided.\n• Acknowledged connectionless service: This is a cross between the previous\ntwo services. It provides that datagrams are to be acknowledged, but no prior\nlogical connection is set up.\nTypically, a vendor will provide these services as options that the customer can\nselect when purchasing the equipment. Alternatively, the customer can purchase\nequipment that provides two or all three services and select a specific service based\non application.\nThe unacknowledged connectionless service requires minimum logic and is\nuseful in two contexts. First, it will often be the case that higher layers of software\nwill provide the necessary reliability and flow-control mechanism, and it is efficient\n15.3 / LAN PROTOCOL ARCHITECTURE\nto avoid duplicating them. For example, TCP could provide the mechanisms\nneeded to ensure that data is delivered reliably. Second, there are instances in\nwhich the overhead of connection establishment and maintenance is unjustified or\neven counterproductive (for example, data collection activities that involve the\nperiodic sampling of data sources, such as sensors and automatic self-test reports\nfrom security equipment or network components). In a monitoring application, the\nloss of an occasional data unit would not cause distress, as the next report should\narrive shortly. Thus, in most cases, the unacknowledged connectionless service is\nthe preferred option.\nThe connection-mode service could be used in very simple devices, such as ter-\nminal controllers, that have little software operating above this level. In these cases,\nit would provide the flow control and reliability mechanisms normally implemented\nat higher layers of the communications software.\nThe acknowledged connectionless service is useful in several contexts.With the\nconnection-mode service, the logical link control software must maintain some sort\nof table for each active connection, to keep track of the status of that connection. If\nthe user needs guaranteed delivery but there are a large number of destinations for\ndata,then the connection-mode service may be impractical because of the large num-\nber of tables required. An example is a process control or automated factory envi-\nronment where a central site may need to communicate with a large number of\nprocessors and programmable controllers. Another use of this is the handling of\nimportant and time-critical alarm or emergency control signals in a factory. Because\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\naddress fields\nI/G \u0001 Individual/Group\nC/R \u0001 Command/Response\nDestination\nMAC address\nMAC address\nLLC control\nInformation\nFigure 15.7\nLLC PDU in a Generic MAC Frame Format\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\nRound Robin With round robin, each station in turn is given the opportunity to\ntransmit. During that opportunity, the station may decline to transmit or may transmit\nsubject to a specified upper bound, usually expressed as a maximum amount of data\ntransmitted or time for this opportunity. In any case, the station, when it is finished,\nrelinquishes its turn, and the right to transmit passes to the next station in logical\nsequence. Control of sequence may be centralized or distributed. Polling is an exam-\nple of a centralized technique.\nWhen many stations have data to transmit over an extended period of time,\nround-robin techniques can be very efficient. If only a few stations have data to\ntransmit over an extended period of time, then there is a considerable overhead in\npassing the turn from station to station, because most of the stations will not trans-\nmit but simply pass their turns. Under such circumstances other techniques may be\npreferable, largely depending on whether the data traffic has a stream or bursty\ncharacteristic. Stream traffic is characterized by lengthy and fairly continuous trans-\nmissions; examples are voice communication, telemetry, and bulk file transfer.\nBursty traffic is characterized by short, sporadic transmissions; interactive terminal-\nhost traffic fits this description.\nReservation For stream traffic, reservation techniques are well suited. In general,\nfor these techniques, time on the medium is divided into slots, much as with syn-\nchronous TDM. A station wishing to transmit reserves future slots for an extended\nor even an indefinite period. Again, reservations may be made in a centralized or\ndistributed fashion.\nContention For bursty traffic, contention techniques are usually appropriate.With\nthese techniques, no control is exercised to determine whose turn it is; all stations\ncontend for time in a way that can be, as we shall see, rather rough and tumble.These\ntechniques are of necessity distributed in nature. Their principal advantage is that\nthey are simple to implement and, under light to moderate load, efficient. For some\nof these techniques, however, performance tends to collapse under heavy load.\nAlthough both centralized and distributed reservation techniques have been\nimplemented in some LAN products, round-robin and contention techniques are\nthe most common.\nMAC Frame Format The MAC layer receives a block of data from the LLC\nlayer and is responsible for performing functions related to medium access and for\ntransmitting the data. As with other protocol layers, MAC implements these func-\ntions making use of a protocol data unit at its layer. In this case, the PDU is referred\nto as a MAC frame.\nThe exact format of the MAC frame differs somewhat for the various MAC\nprotocols in use. In general, all of the MAC frames have a format similar to that of\nFigure 15.7.The fields of this frame are\n• MAC Control: This field contains any protocol control information needed for\nthe functioning of the MAC protocol. For example, a priority level could be\nindicated here.\n• Destination MAC Address: The destination physical attachment point on the\nLAN for this frame.\n15.4 / BRIDGES\n• Source MAC Address: The source physical attachment point on the LAN for\nthis frame.\n• LLC: The LLC data from the next higher layer.\n• CRC: The Cyclic Redundancy Check field (also known as the frame check\nsequence, FCS, field). This is an error-detecting code, as we have seen in\nHDLC and other data link control protocols (Chapter 7).\nIn most data link control protocols, the data link protocol entity is responsible\nnot only for detecting errors using the CRC, but for recovering from those errors by\nretransmitting damaged frames. In the LAN protocol architecture, these two func-\ntions are split between the MAC and LLC layers. The MAC layer is responsible for\ndetecting errors and discarding any frames that are in error. The LLC layer option-\nally keeps track of which frames have been successfully received and retransmits\nunsuccessful frames.\n15.4 BRIDGES\nIn virtually all cases, there is a need to expand beyond the confines of a single LAN,\nto provide interconnection to other LANs and to wide area networks. Two general\napproaches are used for this purpose: bridges and routers.The bridge is the simpler\nof the two devices and provides a means of interconnecting similar LANs. The\nrouter is a more general-purpose device, capable of interconnecting a variety of\nLANs and WANs. We explore bridges in this section and look at routers in Part\nThe bridge is designed for use between local area networks (LANs) that use\nidentical protocols for the physical and link layers (e.g., all conforming to IEEE\n802.3). Because the devices all use the same protocols, the amount of processing\nrequired at the bridge is minimal. More sophisticated bridges are capable of map-\nping from one MAC format to another (e.g., to interconnect an Ethernet and a\ntoken ring LAN).\nBecause the bridge is used in a situation in which all the LANs have the same\ncharacteristics, the reader may ask, why not simply have one large LAN? Depend-\ning on circumstance, there are several reasons for the use of multiple LANs con-\nnected by bridges:\n• Reliability: The danger in connecting all data processing devices in an organi-\nzation to one network is that a fault on the network may disable communica-\ntion for all devices. By using bridges, the network can be partitioned into\nself-contained units.\n• Performance: In general, performance on a LAN declines with an increase in\nthe number of devices or the length of the wire. A number of smaller LANs\nwill often give improved performance if devices can be clustered so that\nintranetwork traffic significantly exceeds internetwork traffic.\n• Security: The establishment of multiple LANs may improve security of com-\nmunications. It is desirable to keep different types of traffic (e.g., accounting,\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\npersonnel, strategic planning) that have different security needs on physically\nseparate media. At the same time, the different types of users with different\nlevels of security need to communicate through controlled and monitored\nmechanisms.\n• Geography: Clearly, two separate LANs are needed to support devices clus-\ntered in two geographically distant locations. Even in the case of two buildings\nseparated by a highway, it may be far easier to use a microwave bridge link\nthan to attempt to string coaxial cable between the two buildings.\nFunctions of a Bridge\nFigure 15.8 illustrates the action of a bridge connecting two LANs, A and B, using\nthe same MAC protocol. In this example, a single bridge attaches to both LANs; fre-\nquently, the bridge function is performed by two “half-bridges,” one on each LAN.\nThe functions of the bridge are few and simple:\n• Read all frames transmitted on A and accept those addressed to any station on B.\n• Using the medium access control protocol for B, retransmit each frame on B.\n• Do the same for B-to-A traffic.\nSeveral design aspects of a bridge are worth highlighting:\n• The bridge makes no modification to the content or format of the frames it\nreceives, nor does it encapsulate them with an additional header. Each frame\nto be transferred is simply copied from one LAN and repeated with exactly\nthe same bit pattern on the other LAN. Because the two LANs use the same\nLAN protocols, it is permissible to do this.\n• The bridge should contain enough buffer space to meet peak demands. Over a\nshort period of time, frames may arrive faster than they can be retransmitted.\n• The bridge must contain addressing and routing intelligence. At a minimum,\nthe bridge must know which addresses are on each network to know which\nframes to pass. Further, there may be more than two LANs interconnected by\na number of bridges. In that case, a frame may have to be routed through sev-\neral bridges in its journey from source to destination.\n• A bridge may connect more than two LANs.\nIn summary, the bridge provides an extension to the LAN that requires no\nmodification to the communications software in the stations attached to the LANs.\nIt appears to all stations on the two (or more) LANs that there is a single LAN on\nwhich each station has a unique address. The station uses that unique address and\nneed not explicitly discriminate between stations on the same LAN and stations on\nother LANs; the bridge takes care of that.\nBridge Protocol Architecture\nThe IEEE 802.1D specification defines the protocol architecture for MAC bridges.\nWithin the 802 architecture, the endpoint or station address is designated at the\n15.4 / BRIDGES\nFrames with\naddresses 11 through\n20 are accepted and\nrepeated on LAN B\nFrames with\naddresses 1 through\n10 are accepted and\nrepeated on LAN A\nFigure 15.8\nBridge Operation\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\n(a) Architecture\n(b) Operation\nMAC-H LLC-H\nt3, t4, t5, t6\nFigure 15.9\nConnection of Two LANs by a Bridge\nMAC level.Thus, it is at the MAC level that a bridge can function. Figure 15.9 shows\nthe simplest case, which consists of two LANs connected by a single bridge. The\nLANs employ the same MAC and LLC protocols.The bridge operates as previously\ndescribed. A MAC frame whose destination is not on the immediate LAN is cap-\ntured by the bridge, buffered briefly, and then transmitted on the other LAN.As far\nas the LLC layer is concerned, there is a dialogue between peer LLC entities in the\ntwo endpoint stations. The bridge need not contain an LLC layer because it is\nmerely serving to relay the MAC frames.\nFigure 15.9b indicates the way in which data are encapsulated using a bridge.\nData are provided by some user to LLC. The LLC entity appends a header and\npasses the resulting data unit to the MAC entity, which appends a header and a\ntrailer to form a MAC frame. On the basis of the destination MAC address in the\nframe, it is captured by the bridge.The bridge does not strip off the MAC fields; its\nfunction is to relay the MAC frame intact to the destination LAN. Thus, the frame\nis deposited on the destination LAN and captured by the destination station.\nThe concept of a MAC relay bridge is not limited to the use of a single bridge\nto connect two nearby LANs. If the LANs are some distance apart, then they can be\nconnected by two bridges that are in turn connected by a communications facility.\nThe intervening communications facility can be a network, such as a wide area\npacket-switching network, or a point-to-point link. In such cases, when a bridge cap-\ntures a MAC frame, it must encapsulate the frame in the appropriate packaging and\ntransmit it over the communications facility to a target bridge. The target bridge\nstrips off these extra fields and transmits the original, unmodified MAC frame to the\ndestination station.\nFixed Routing\nThere is a trend within many organizations to an increasing number of LANs inter-\nconnected by bridges. As the number of LANs grows, it becomes important to\n15.4 / BRIDGES\nFigure 15.10\nConfiguration of Bridges and LANs, with Alternate Routes\nprovide alternate paths between LANs via bridges for load balancing and recon-\nfiguration in response to failure. Thus, many organizations will find that static, pre-\nconfigured routing tables are inadequate and that some sort of dynamic routing is\nConsider the configuration of Figure 15.10. Suppose that station 1 transmits a\nframe on LAN A intended for station 6.The frame will be read by bridges 101, 102,\nand 107. For each bridge, the addressed station is not on a LAN to which the bridge\nis attached.Therefore, each bridge must make a decision whether or not to retrans-\nmit the frame on its other LAN, in order to move it closer to its intended destina-\ntion. In this case, bridge 102 should repeat the frame on LAN C, whereas bridges\n101 and 107 should refrain from retransmitting the frame. Once the frame has been\ntransmitted on LAN C, it will be picked up by both bridges 105 and 106. Again,\neach must decide whether or not to forward the frame. In this case, bridge 105\nshould retransmit the frame on LAN F, where it will be received by the destination,\nThus we see that, in the general case, the bridge must be equipped with a\nrouting capability.When a bridge receives a frame, it must decide whether or not to\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\nforward it. If the bridge is attached to two or more networks, then it must decide\nwhether or not to forward the frame and, if so, on which LAN the frame should be\ntransmitted.\nThe routing decision may not always be a simple one. Figure 15.10 also shows\nthat there are two routes between LAN A and LAN E. Such redundancy provides\nfor higher overall Internet availability and creates the possibility for load balanc-\ning. In this case, if station 1 transmits a frame on LAN A intended for station 5 on\nLAN E, then either bridge 101 or bridge 107 could forward the frame. It would\nappear preferable for bridge 107 to forward the frame, since it will involve only one\nhop, whereas if the frame travels through bridge 101, it must suffer two hops.\nAnother consideration is that there may be changes in the configuration. For exam-\nple, bridge 107 may fail, in which case subsequent frames from station 1 to station 5\nshould go through bridge 101. So we can say that the routing capability must take\ninto account the topology of the internet configuration and may need to be dynam-\nically altered.\nA variety of routing strategies have been proposed and implemented in recent\nyears. The simplest and most common strategy is fixed routing. This strategy is suit-\nable for small internets and for internets that are relatively stable. In addition, two\ngroups within the IEEE 802 committee have developed specifications for routing\nstrategies.The IEEE 802.1 group has issued a standard for routing based on the use\nof a spanning tree algorithm. The token ring committee, IEEE 802.5, has issued its\nown specification, referred to as source routing. In the remainder of this section, we\nlook at fixed routing and the spanning tree algorithm, which is the most commonly\nused bridge routing algorithm.\nFor fixed routing, a route is selected for each source-destination pair of LANs\nin the configuration. If alternate routes are available between two LANs, then typi-\ncally the route with the least number of hops is selected. The routes are fixed, or at\nleast only change when there is a change in the topology of the internet.\nThe strategy for developing a fixed routing configuration for bridges is similar\nto that employed in a packet-switching network (Figure 12.2). A central routing\nmatrix is created, to be stored perhaps at a network control center. The matrix\nshows, for each source-destination pair of LANs, the identity of the first bridge on\nthe route. So, for example, the route from LAN E to LAN F begins by going through\nbridge 107 to LAN A. Again consulting the matrix, the route from LAN A to LAN\nF goes through bridge 102 to LAN C. Finally, the route from LAN C to LAN F is\ndirectly through bridge 105. Thus the complete route from LAN E to LAN F is\nbridge 107, LAN A, bridge 102, LAN C, bridge 105.\nFrom this overall matrix, routing tables can be developed and stored at each\nbridge. Each bridge needs one table for each LAN to which it attaches. The infor-\nmation for each table is derived from a single row of the matrix. For example, bridge\n105 has two tables, one for frames arriving from LAN C and one for frames arriving\nfrom LAN F.The table shows, for each possible destination MAC address, the iden-\ntity of the LAN to which the bridge should forward the frame.\nOnce the directories have been established, routing is a simple matter. A\nbridge copies each incoming frame on each of its LANs. If the destination MAC\naddress corresponds to an entry in its routing table, the frame is retransmitted on\nthe appropriate LAN.\n15.4 / BRIDGES\nThe fixed routing strategy is widely used in commercially available products. It\nrequires that a network manager manually load the data into the routing tables. It\nhas the advantage of simplicity and minimal processing requirements. However, in a\ncomplex internet, in which bridges may be dynamically added and in which failures\nmust be allowed for, this strategy is too limited.\nThe Spanning Tree Approach\nThe spanning tree approach is a mechanism in which bridges automatically develop\na routing table and update that table in response to changing topology. The algo-\nrithm consists of three mechanisms: frame forwarding, address learning, and loop\nresolution.\nFrame Forwarding In this scheme, a bridge maintains a forwarding database for\neach port attached to a LAN.The database indicates the station addresses for which\nframes should be forwarded through that port.We can interpret this in the following\nfashion. For each port, a list of stations is maintained.A station is on the list if it is on\nthe “same side” of the bridge as the port. For example, for bridge 102 of Figure\n15.10, stations on LANs C, F, and G are on the same side of the bridge as the LAN\nC port, and stations on LANs A, B, D, and E are on the same side of the bridge as\nthe LAN A port. When a frame is received on any port, the bridge must decide\nwhether that frame is to be forwarded through the bridge and out through one of\nthe bridge’s other ports. Suppose that a bridge receives a MAC frame on port x. The\nfollowing rules are applied:\n1. Search the forwarding database to determine if the MAC address is listed for\nany port except port x.\n2. If the destination MAC address is not found, forward frame out all ports except\nthe one from which is was received.This is part of the learning process described\nsubsequently.\n3. If the destination address is in the forwarding database for some port y, then\ndetermine whether port y is in a blocking or forwarding state. For reasons\nexplained later, a port may sometimes be blocked, which prevents it from receiv-\ning or transmitting frames.\n4. If port y is not blocked, transmit the frame through port y onto the LAN to\nwhich that port attaches.\nAddress Learning The preceding scheme assumes that the bridge is already\nequipped with a forwarding database that indicates the direction, from the bridge, of\neach destination station. This information can be preloaded into the bridge, as in\nfixed routing. However, an effective automatic mechanism for learning the direction\nof each station is desirable.A simple scheme for acquiring this information is based\non the use of the source address field in each MAC frame.\nThe strategy is this. When a frame arrives on a particular port, it clearly has\ncome from the direction of the incoming LAN. The source address field of the\nframe indicates the source station. Thus, a bridge can update its forwarding data-\nbase for that port on the basis of the source address field of each incoming frame.\nTo allow for changes in topology, each element in the database is equipped with a\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\ntimer. When a new element is added to the database, its timer is set. If the timer\nexpires, then the element is eliminated from the database, since the correspond-\ning direction information may no longer be valid. Each time a frame is received,\nits source address is checked against the database. If the element is already in the\ndatabase, the entry is updated (the direction may have changed) and the timer is\nreset. If the element is not in the database, a new entry is created, with its own\nSpanning Tree Algorithm The address learning mechanism described previ-\nously is effective if the topology of the internet is a tree; that is, if there are no alter-\nnate routes in the network. The existence of alternate routes means that there is a\nclosed loop. For example in Figure 15.10, the following is a closed loop: LAN A,\nbridge 101, LAN B, bridge 104, LAN E, bridge 107, LAN A.\nTo see the problem created by a closed loop, consider Figure 15.11.At time \nstation A transmits a frame addressed to station B. The frame is captured by both\nbridges. Each bridge updates its database to indicate that station A is in the direc-\ntion of LAN X, and retransmits the frame on LAN Y. Say that bridge \nretransmits\na short time later \nThus B will receive two copies of the\nframe. Furthermore, each bridge will receive the other’s transmission on LAN Y.\nNote that each transmission is a frame with a source address of A and a destination\naddress of B.Thus each bridge will update its database to indicate that station A is in\nFigure 15.11\nLoop of Bridges\n15.5 / LAYER 2 AND LAYER 3 SWITCHES\nthe direction of LAN Y. Neither bridge is now capable of forwarding a frame\naddressed to station A.\nTo overcome this problem, a simple result from graph theory is used: For any\nconnected graph, consisting of nodes and edges connecting pairs of nodes, there is a\nspanning tree of edges that maintains the connectivity of the graph but contains no\nclosed loops. In terms of internets, each LAN corresponds to a graph node, and each\nbridge corresponds to a graph edge. Thus, in Figure 15.10, the removal of one (and\nonly one) of bridges 107, 101, and 104, results in a spanning tree.What is desired is to\ndevelop a simple algorithm by which the bridges of the internet can exchange suffi-\ncient information to automatically (without user intervention) derive a spanning\ntree. The algorithm must be dynamic. That is, when a topology change occurs, the\nbridges must be able to discover this fact and automatically derive a new spanning\nThe spanning tree algorithm developed by IEEE 802.1, as the name sug-\ngests, is able to develop such a spanning tree. All that is required is that each\nbridge be assigned a unique identifier and that costs be assigned to each bridge\nport. In the absence of any special considerations, all costs could be set equal; this\nproduces a minimum-hop tree. The algorithm involves a brief exchange of mes-\nsages among all of the bridges to discover the minimum-cost spanning tree.\nWhenever there is a change in topology, the bridges automatically recalculate the\nspanning tree.\n15.5 LAYER 2 AND LAYER 3 SWITCHES\nIn recent years, there has been a proliferation of types of devices for interconnecting\nLANs that goes beyond the bridges discussed in Section 15.4 and the routers dis-\ncussed in Part Five. These devices can conveniently be grouped into the categories\nof layer 2 switches and layer 3 switches.We begin with a discussion of hubs and then\nexplore these two concepts.\nEarlier, we used the term hub in reference to a star-topology LAN. The hub is the\nactive central element of the star layout. Each station is connected to the hub by two\nlines (transmit and receive).The hub acts as a repeater:When a single station trans-\nmits, the hub repeats the signal on the outgoing line to each station. Ordinarily, the\nline consists of two unshielded twisted pairs. Because of the high data rate and the\npoor transmission qualities of unshielded twisted pair, the length of a line is limited\nto about 100 m.As an alternative, an optical fiber link may be used. In this case, the\nmaximum length is about 500 m.\nNote that although this scheme is physically a star, it is logically a bus:A trans-\nmission from any one station is received by all other stations, and if two stations\ntransmit at the same time there will be a collision.\nMultiple levels of hubs can be cascaded in a hierarchical configuration.\nFigure 15.12 illustrates a two-level configuration. There is one header hub\n(HHUB) and one or more intermediate hubs (IHUB). Each hub may have a\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\n(twisted pair or\noptical fiber)\nFigure 15.12\nTwo-Level Star Topology\nmixture of stations and other hubs attached to it from below. This layout fits well\nwith building wiring practices.Typically, there is a wiring closet on each floor of an\noffice building, and a hub can be placed in each one. Each hub could service the\nstations on its floor.\nLayer 2 Switches\nIn recent years, a new device, the layer 2 switch, has replaced the hub in popularity,\nparticularly for high-speed LANs.The layer 2 switch is also sometimes referred to as\na switching hub.\nTo clarify the distinction between hubs and switches, Figure 15.13a shows a\ntypical bus layout of a traditional 10-Mbps LAN.A bus is installed that is laid out so\nthat all the devices to be attached are in reasonable proximity to a point on the bus.\nIn the figure, station B is transmitting.This transmission goes from B, across the lead\nfrom B to the bus, along the bus in both directions, and along the access lines of each\nof the other attached stations. In this configuration, all the stations must share the\ntotal capacity of the bus, which is 10 Mbps.\nA hub, often in a building wiring closet, uses a star wiring arrangement to\nattach stations to the hub. In this arrangement, a transmission from any one station\nis received by the hub and retransmitted on all of the outgoing lines. Therefore, to\navoid collision, only one station can transmit at a time. Again, the total capacity of\nthe LAN is 10 Mbps. The hub has several advantages over the simple bus arrange-\nment. It exploits standard building wiring practices in the layout of cable. In addi-\ntion, the hub can be configured to recognize a malfunctioning station that is\n15.5 / LAYER 2 AND LAYER 3 SWITCHES\nFigure 15.13\nLan Hubs and Switches\nShared bus — 10 Mbps\n(a) Shared medium bus\n(b) Shared medium hub\nTotal capacity\nup to 10 Mbps\n(c) Layer 2 switch\nTotal capacity\nN \u0006 10 Mbps\njamming the network and to cut that station out of the network. Figure 15.13b illus-\ntrates the operation of a hub. Here again, station B is transmitting.This transmission\ngoes from B, across the transmit line from B to the hub, and from the hub along the\nreceive lines of each of the other attached stations.\nWe can achieve greater performance with a layer 2 switch. In this case, the cen-\ntral hub acts as a switch, much as a packet switch or circuit switch. With a layer 2\nswitch, an incoming frame from a particular station is switched to the appropriate\noutput line to be delivered to the intended destination. At the same time, other\nunused lines can be used for switching other traffic. Figure 15.13c shows an example\nin which B is transmitting a frame to A and at the same time C is transmitting a\nframe to D. So, in this example, the current throughput on the LAN is 20 Mbps,\nalthough each individual device is limited to 10 Mbps.The layer 2 switch has several\nattractive features:\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\n1. No change is required to the software or hardware of the attached devices to\nconvert a bus LAN or a hub LAN to a switched LAN. In the case of an Ether-\nnet LAN, each attached device continues to use the Ethernet medium access\ncontrol protocol to access the LAN. From the point of view of the attached\ndevices, nothing has changed in the access logic.\n2. Each attached device has a dedicated capacity equal to that of the entire original\nLAN, assuming that the layer 2 switch has sufficient capacity to keep up with all\nattached devices. For example, in Figure 15.13c, if the layer 2 switch can sustain a\nthroughput of 20 Mbps, each attached device appears to have a dedicated capac-\nity for either input or output of 10 Mbps.\n3. The layer 2 switch scales easily.Additional devices can be attached to the layer\n2 switch by increasing the capacity of the layer 2 switch correspondingly.\nTwo types of layer 2 switches are available as commercial products:\n• Store-and-forward switch: The layer 2 switch accepts a frame on an input\nline, buffers it briefly, and then routes it to the appropriate output line.\n• Cut-through switch: The layer 2 switch takes advantage of the fact that the\ndestination address appears at the beginning of the MAC (medium access\ncontrol) frame. The layer 2 switch begins repeating the incoming frame onto\nthe appropriate output line as soon as the layer 2 switch recognizes the desti-\nnation address.\nThe cut-through switch yields the highest possible throughput but at some risk\nof propagating bad frames, because the switch is not able to check the CRC prior to\nretransmission. The store-and-forward switch involves a delay between sender and\nreceiver but boosts the overall integrity of the network.\nA layer 2 switch can be viewed as a full-duplex version of the hub. It can also\nincorporate logic that allows it to function as a multiport bridge. [BREY99] lists the\nfollowing differences between layer 2 switches and bridges:\n• Bridge frame handling is done in software. A layer 2 switch performs the\naddress recognition and frame forwarding functions in hardware.\n• A bridge can typically only analyze and forward one frame at a time, whereas\na layer 2 switch has multiple parallel data paths and can handle multiple\nframes at a time.\n• A bridge uses store-and-forward operation.With a layer 2 switch, it is possible\nto have cut-through instead of store-and-forward operation.\nBecause a layer 2 switch has higher performance and can incorporate the\nfunctions of a bridge, the bridge has suffered commercially. New installations typi-\ncally include layer 2 switches with bridge functionality rather than bridges.\nLayer 3 Switches\nLayer 2 switches provide increased performance to meet the needs of high-volume\ntraffic generated by personal computers, workstations, and servers. However, as the\nnumber of devices in a building or complex of buildings grows, layer 2 switches\n15.5 / LAYER 2 AND LAYER 3 SWITCHES\nreveal some inadequacies.Two problems in particular present themselves: broadcast\noverload and the lack of multiple links.\nA set of devices and LANs connected by layer 2 switches is considered to have\na flat address space.The term flat means that all users share a common MAC broad-\ncast address. Thus, if any device issues a MAC frame with a broadcast address, that\nframe is to be delivered to all devices attached to the overall network connected by\nlayer 2 switches and/or bridges. In a large network, frequent transmission of broad-\ncast frames can create tremendous overhead. Worse, a malfunctioning device can\ncreate a broadcast storm, in which numerous broadcast frames clog the network and\ncrowd out legitimate traffic.\nA second performance-related problem with the use of bridges and/or layer\n2 switches is that the current standards for bridge protocols dictate that there be no\nclosed loops in the network. That is, there can only be one path between any two\ndevices.Thus, it is impossible, in a standards-based implementation, to provide mul-\ntiple paths through multiple switches between devices. This restriction limits both\nperformance and reliability.\nTo overcome these problems, it seems logical to break up a large local network\ninto a number of subnetworks connected by routers. A MAC broadcast frame is\nthen limited to only the devices and switches contained in a single subnetwork. Fur-\nthermore, IP-based routers employ sophisticated routing algorithms that allow the\nuse of multiple paths between subnetworks going through different routers.\nHowever, the problem with using routers to overcome some of the inadequa-\ncies of bridges and layer 2 switches is that routers typically do all of the IP-level\nprocessing involved in the forwarding of IP traffic in software. High-speed LANs\nand high-performance layer 2 switches may pump millions of packets per second,\nwhereas a software-based router may only be able to handle well under a million\npackets per second.To accommodate such a load, a number of vendors have devel-\noped layer 3 switches, which implement the packet-forwarding logic of the router\nin hardware.\nThere are a number of different layer 3 schemes on the market, but funda-\nmentally they fall into two categories: packet by packet and flow based.The packet-\nby-packet switch operates in the identical fashion as a traditional router. Because\nthe forwarding logic is in hardware, the packet-by-packet switch can achieve an\norder of magnitude increase in performance compared to the software-based\nrouter.A flow-based switch tries to enhance performance by identifying flows of IP\npackets that have the same source and destination. This can be done by observing\nongoing traffic or by using a special flow label in the packet header (allowed in IPv6\nbut not IPv4). Once a flow is identified, a predefined route can be established\nthrough the network to speed up the forwarding process. Again, huge performance\nincreases over a pure software-based router are achieved.\nFigure 15.14 is a typical example of the approach taken to local networking in\nan organization with a large number of PCs and workstations (thousands to tens of\nthousands). Desktop systems have links of 10 Mbps to 100 Mbps into a LAN con-\ntrolled by a layer 2 switch. Wireless LAN connectivity is also likely to be available\nfor mobile users. Layer 3 switches are at the local network’s core, forming a local\nbackbone. Typically, these switches are interconnected at 1 Gbps and connect to\nlayer 2 switches at from 100 Mbps to 1 Gbps. Servers connect directly to layer 2 or\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\nLaptop with\nwireless connection\nFigure 15.14\nTypical Premises Network Configuration\nlayer 3 switches at 1 Gbps or possible 100 Mbps.A lower-cost software-based router\nprovides WAN connection. The circles in the figure identify separate LAN subnet-\nworks; a MAC broadcast frame is limited to its own subnetwork.\n15.6 RECOMMENDED READING AND WEB SITE\nThe material in this chapter is covered in much more depth in [STAL00]. [REGA04] and\n[FORO02] also provides extensive coverage. [METZ99] is an excellent treatment of layer 2\nand layer 3 switches, with a detailed discussion of products and case studies. Another com-\nprehensive account is [SEIF00].\n15.7 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nForouzan,B.,and Chung,S.Local Area Networks. New York:McGraw-Hill,2002.\nMetzler, J., and DeNoia, L. Layer 2 Switching. Upper Saddle River, NJ: Pren-\ntice Hall, 1999.\nRegan, P. Local Area Networks. Upper Saddle River, NJ: Prentice Hall, 2004.\nSeifert, R. The Switch Book. New York:Wiley, 2000.\nStallings, W. Local and Metropolitan Area Networks, Sixth Edition. Upper\nSaddle River, NJ: Prentice Hall, 2000.\nRecommended Web site:\n• IEEE 802 LAN/MAN Standards Committee: Status and documents for all of the\nworking groups\n15.7 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nReview Questions\nHow do the key requirements for computer room networks differ from those for per-\nsonal computer local networks?\nWhat are the differences among backend LANs, SANs, and backbone LANs?\nWhat is network topology?\nList four common LAN topologies and briefly describe their methods of operation.\nWhat is the purpose of the IEEE 802 committee?\nWhy are there multiple LAN standards?\nList and briefly define the services provided by LLC.\nList and briefly define the types of operation provided by the LLC protocol.\nList some basic functions performed at the MAC layer.\nWhat functions are performed by a bridge?\nWhat is a spanning tree?\nWhat is the difference between a hub and a layer 2 switch?\nWhat is the difference between a store-and forward switch and a cut-through switch?\nbus topology\nlayer 2 switch\nlayer 3 switch\nlocal area network (LAN)\nlogical link control\nmedium access control (MAC)\nring topology\nspanning tree\nstar topology\ntree topology\nstorage area networks (SAN)\nCHAPTER 15 / LOCAL AREA NETWORK OVERVIEW\nInstead of LLC, could HDLC be used as a data link control protocol for a LAN? If\nnot, what is lacking?\nAn asynchronous device, such as a teletype, transmits characters one at a time with\nunpredictable delays between characters. What problems, if any, do you foresee if\nsuch a device is connected to a LAN and allowed to transmit at will (subject to gain-\ning access to the medium)? How might such problems be resolved?\nConsider the transfer of a file containing one million 8-bit characters from one station\nto another. What is the total elapsed time and effective throughput for the following\nA circuit-switched, star-topology local network. Call setup time is negligible and\nthe data rate on the medium is 64 kbps.\nb. A bus topology local network with two stations a distance D apart, a data rate of\nB bps, and a frame size of P with 80 bits of overhead per frame. Each frame is\nacknowledged with an 88-bit frame before the next is sent.The propagation speed\non the bus is \nA ring topology local network with a total circular length of 2D, with the two sta-\nChapter 17 covers wireless LANs.\n16.1 THE EMERGENCE OF HIGH-SPEED LANS\nPersonal computers and microcomputer workstations began to achieve widespread\nacceptance in business computing in the early 1980s and have now achieved the sta-\ntus of the telephone: an essential tool for office workers. Until relatively recently,\noffice LANs provided basic connectivity services—connecting personal computers\nCHAPTER 16 / HIGH-SPEED LANS\nCharacteristics of Some High-Speed LANs\nFast Ethernet\nGigabit Ethernet\nFibre Channel\nWireless LAN\n1 Gbps, 10 Gbps\n100 Mbps–3.2 Gbps\n1 Mbps–54 Mbps\nTransmission Media\nUTP, shielded cable,\nOptical fiber, coaxial\n2.4-GHz, 5-GHz \noptical Fiber\noptical fiber\nAccess Method\nCSMA/Polling\nSupporting Standard\nFibre Channel\nIEEE 802.11\nAssociation\nand terminals to mainframes and midrange systems that ran corporate applications,\nand providing workgroup connectivity at the departmental or divisional level. In\nboth cases, traffic patterns were relatively light, with an emphasis on file transfer\nand electronic mail. The LANs that were available for this type of workload, pri-\nmarily Ethernet and token ring, are well suited to this environment.\nIn recent years, two significant trends have altered the role of the personal\ncomputer and therefore the requirements on the LAN:\n• The speed and computing power of personal computers has continued to\nenjoy explosive growth. Today’s more powerful platforms support graphics-\nintensive applications and ever more elaborate graphical user interfaces to the\noperating system.\n• MIS organizations have recognized the LAN as a viable and indeed essential\ncomputing platform, resulting in the focus on network computing. This trend\nbegan with client/server computing, which has become a dominant architec-\nture in the business environment and the more recent intranetwork trend.\nBoth of these approaches involve the frequent transfer of potentially large\nvolumes of data in a transaction-oriented environment.\nThe effect of these trends has been to increase the volume of data to be han-\ndled over LANs and, because applications are more interactive, to reduce the accept-\nable delay on data transfers. The earlier generation of 10-Mbps Ethernets and\n16-Mbps token rings are simply not up to the job of supporting these requirements.\nThe following are examples of requirements that call for higher-speed LANs:\n• Centralized server farms: In many applications, there is a need for user, or\nclient, systems to be able to draw huge amounts of data from multiple central-\nized servers, called server farms.An example is a color publishing operation, in\nwhich servers typically contain hundreds of gigabytes of image data that must\nbe downloaded to imaging workstations. As the performance of the servers\nthemselves has increased, the bottleneck has shifted to the network.\n• Power workgroups: These groups typically consist of a small number of cooper-\nating users who need to draw massive data files across the network. Examples\nare a software development group that runs tests on a new software version, or\na computer-aided design (CAD) company that regularly runs simulations of\nnew designs. In such cases, large amounts of data are distributed to several\nworkstations, processed, and updated at very high speed for multiple iterations.\n16.2 / ETHERNET\n• High-speed local backbone: As processing demand grows, LANs proliferate at\na site, and high-speed interconnection is necessary.\n16.2 ETHERNET\nThe most widely used high-speed LANs today are based on Ethernet and were\ndeveloped by the IEEE 802.3 standards committee. As with other LAN standards,\nthere is both a medium access control layer and a physical layer, which are consid-\nered in turn in what follows.\nIEEE 802.3 Medium Access Control\nIt is easier to understand the operation of CSMA/CD if we look first at some earlier\nschemes from which CSMA/CD evolved.\nPrecursors CSMA/CD and its precursors can be termed random access, or con-\ntention, techniques.They are random access in the sense that there is no predictable\nor scheduled time for any station to transmit; station transmissions are ordered ran-\ndomly. They exhibit contention in the sense that stations contend for time on the\nshared medium.\nThe earliest of these techniques, known as ALOHA, was developed for\npacket radio networks. However, it is applicable to any shared transmission\nmedium. ALOHA, or pure ALOHA as it is sometimes called, specifies that a sta-\ntion may transmit a frame at any time. The station then listens for an amount of\ntime equal to the maximum possible round-trip propagation delay on the network\n(twice the time it takes to send a frame between the two most widely separated sta-\nCHAPTER 16 / HIGH-SPEED LANS\nBoth ALOHA and slotted ALOHA exhibit poor utilization. Both fail to take\nadvantage of one of the key properties of both packet radio networks and LANs,\nwhich is that propagation delay between stations may be very small compared to\nframe transmission time. Consider the following observations. If the station-to-station\npropagation time is large compared to the frame transmission time, then, after a sta-\ntion launches a frame, it will be a long time before other stations know about it. Dur-\ning that time, one of the other stations may transmit a frame; the two frames may\ninterfere with each other and neither gets through. Indeed, if the distances are great\nenough, many stations may begin transmitting, one after the other, and none of their\nframes get through unscathed. Suppose, however, that the propagation time is small\ncompared to frame transmission time.In that case,when a station launches a frame,all\nthe other stations know it almost immediately. So, if they had any sense, they would\nnot try transmitting until the first station was done. Collisions would be rare because\nthey would occur only when two stations began to transmit almost simultaneously.\nAnother way to look at it is that a short propagation delay provides the stations with\nbetter feedback about the state of the network; this information can be used to\nimprove efficiency.\nThe foregoing observations led to the development of carrier sense multiple\naccess (CSMA). With CSMA, a station wishing to transmit first listens to the\nmedium to determine if another transmission is in progress (carrier sense). If the\nmedium is in use, the station must wait. If the medium is idle, the station may trans-\nmit. It may happen that two or more stations attempt to transmit at about the same\ntime. If this happens, there will be a collision; the data from both transmissions will\nbe garbled and not received successfully. To account for this, a station waits a rea-\nCHAPTER 16 / HIGH-SPEED LANS\ntransmission, the expected number of stations that will attempt to transmit is\nequal to the number of stations ready to transmit times the probability of trans-\nmitting, or np. If np is greater than 1, on average multiple stations will attempt to\ntransmit and there will be a collision. What is more, as soon as all these stations\nrealize that their transmission suffered a collision, they will be back again,\nalmost guaranteeing more collisions. Worse yet, these retries will compete with\nnew transmissions from other stations, further increasing the probability of col-\nlision. Eventually, all stations will be trying to send, causing continuous colli-\nsions, with throughput dropping to zero. To avoid this catastrophe, np must be\nless than one for the expected peaks of n; therefore, if a heavy load is expected\nto occur with some regularity, p must be small. However, as p is made smaller,\nstations must wait longer to attempt transmission. At low loads, this can result in\nvery long delays. For example, if only a single station desires to transmit, the\nexpected number of iterations of step 1 is 1/p (see Problem 16.2). Thus, if\nat low load, a station will wait an average of 9 time units before trans-\nmitting on an idle line.\nDescription of CSMA/CD CSMA, although more efficient than ALOHA or\nslotted ALOHA, still has one glaring inefficiency. When two frames collide, the\nmedium remains unusable for the duration of transmission of both damaged frames.\nFor long frames, compared to propagation time, the amount of wasted capacity can\nbe considerable. This waste can be reduced if a station continues to listen to the\nmedium while transmitting.This leads to the following rules for CSMA/CD:\n1. If the medium is idle, transmit; otherwise, go to step 2.\n2. If the medium is busy, continue to listen until the channel is idle, then transmit\nimmediately.\n3. If a collision is detected during transmission, transmit a brief jamming signal to\nassure that all stations know that there has been a collision and then cease trans-\n4. After transmitting the jamming signal, wait a random amount of time, referred\nto as the backoff, then attempt to transmit again (repeat from step 1).\nFigure 16.2 illustrates the technique for a baseband bus. The upper part of\nthe figure shows a bus LAN layout. At time \nstation A begins transmitting \na packet addressed to D. At \nboth B and C are ready to transmit. B senses a\ntransmission and so defers. C, however, is still unaware of A’s transmission\n(because the leading edge of A’s transmission has not yet arrived at C) \nand begins its own transmission. When A’s transmission reaches C, at \ndetects the collision and ceases transmission. The effect of the collision propa-\ngates back to A, where it is detected some time later,\nat which time A ceases\ntransmission.\nWith CSMA/CD, the amount of wasted capacity is reduced to the time it takes\nto detect a collision. Question: How long does that take? Let us consider the case of a\nbaseband bus and consider two stations as far apart as possible.For example,in Figure\n16.2, suppose that station A begins a transmission and that just before that transmis-\nsion reaches D, D is ready to transmit. Because D is not yet aware of A’s transmission,\n16.2 / ETHERNET\nit begins to transmit. A collision occurs almost immediately and is recognized by D.\nHowever, the collision must propagate all the way back to A before A is aware of the\ncollision. By this line of reasoning, we conclude that the amount of time that it takes\nto detect a collision is no greater than twice the end-to-end propagation delay.\nAn important rule followed in most CSMA/CD systems, including the IEEE\nstandard, is that frames should be long enough to allow collision detection prior to the\nend of transmission.If shorter frames are used,then collision detection does not occur,\nand CSMA/CD exhibits the same performance as the less efficient CSMA protocol.\nFor a CSMA/CD LAN, the question arises as to which persistence algorithm to\nuse. You may be surprised to learn that the algorithm used in the IEEE 802.3 stan-\ndard is 1-persistent. Recall that both nonpersistent and p-persistent have perfor-\nmance problems. In the nonpersistent case, capacity is wasted because the medium\nwill generally remain idle following the end of a transmission even if there are sta-\ntions waiting to send. In the p-persistent case, p must be set low enough to avoid\nA's transmission\nC's transmission\nSignal on bus\nA's transmission\nC's transmission\nSignal on bus\nA's transmission\nC's transmission\nSignal on bus\nA's transmission\nC's transmission\nSignal on bus\nFigure 16.2\nCSMA/CD Operation\nCHAPTER 16 / HIGH-SPEED LANS\ninstability, with the result of sometimes atrocious delays under light load.The 1-per-\nsistent algorithm, which means, after all, that \nwould seem to be even more\nunstable than p-persistent due to the greed of the stations.What saves the day is that\nthe wasted time due to collisions is mercifully short (if the frames are long relative to\npropagation delay), and with random backoff, the two stations involved in a collision\nare unlikely to collide on their next tries. To ensure that backoff maintains stability,\nIEEE 802.3 and Ethernet use a technique known as binary exponential backoff. A\nstation will attempt to transmit repeatedly in the face of repeated collisions. For the\nfirst 10 retransmission attempts, the mean value of the random delay is doubled.This\nmean value then remains the same for 6 additional attempts. After 16 unsuccessful\nattempts, the station gives up and reports an error.Thus, as congestion increases, sta-\ntions back off by larger and larger amounts to reduce the probability of collision.\nThe beauty of the 1-persistent algorithm with binary exponential backoff is\nthat it is efficient over a wide range of loads. At low loads, 1-persistence guarantees\nthat a station can seize the channel as soon as it goes idle, in contrast to the non- and\np-persistent schemes. At high loads, it is at least as stable as the other techniques.\nHowever, one unfortunate effect of the backoff algorithm is that it has a last-in first-\nout effect; stations with no or few collisions will have a chance to transmit before\nstations that have waited longer.\nFor baseband bus, a collision should produce substantially higher voltage\nswings than those produced by a single transmitter.Accordingly, the IEEE standard\ndictates that the transmitter will detect a collision if the signal on the cable at the\ntransmitter tap point exceeds the maximum that could be produced by the transmit-\nter alone. Because a transmitted signal attenuates as it propagates, there is a poten-\ntial problem: If two stations far apart are transmitting, each station will receive a\ngreatly attenuated signal from the other. The signal strength could be so small that\nwhen it is added to the transmitted signal at the transmitter tap point, the combined\nsignal does not exceed the CD threshold. For this reason, among others, the IEEE\nstandard restricts the maximum length of coaxial cable to 500 m for 10BASE5 and\n200 m for 10BASE2.\nA much simpler collision detection scheme is possible with the twisted-pair\nstar-topology approach (Figure 15.12). In this case, collision detection is based on\nlogic rather than sensing voltage magnitudes. For any hub, if there is activity (signal)\non more than one input, a collision is assumed. A special signal called the collision\npresence signal is generated.This signal is generated and sent out as long as activity\nis sensed on any of the input lines. This signal is interpreted by every node as an\noccurrence of a collision.\nMAC Frame Figure 16.3 depicts the frame format for the 802.3 protocol. It con-\nsists of the following fields:\n• Preamble: A 7-octet pattern of alternating 0s and 1s used by the receiver to\nestablish bit synchronization.\n• Start Frame Delimiter (SFD): The sequence 10101011, which indicates the\nactual start of the frame and enables the receiver to locate the first bit of the\nrest of the frame.\n• Destination Address (DA): Specifies the station(s) for which the frame is\nintended.It may be a unique physical address,a group address,or a global address.\n16.2 / ETHERNET\nFigure 16.3\nIEEE 802.3 Frame Format\n46 to 1500 octets\nSFD = Start of frame delimiter\nDA = Destination address\n= Source address\nFCS = Frame check sequence\nCHAPTER 16 / HIGH-SPEED LANS\n1There is also a 10BROAD36 option, specifying a 10-Mbps broadband bus; this option is rarely used.\n2See Section 5.1.\n• Source Address (SA): Specifies the station that sent the frame.\n• Length/Type: Length of LLC data field in octets, or Ethernet Type field,\ndepending on whether the frame conforms to the IEEE 802.3 standard or the\nearlier Ethernet specification. In either case, the maximum frame size, exclud-\ning the Preamble and SFD, is 1518 octets.\n• LLC Data: Data unit supplied by LLC.\n• Pad: Octets added to ensure that the frame is long enough for proper CD\n• Frame Check Sequence (FCS): A 32-bit cyclic redundancy check, based on all\nfields except preamble, SFD, and FCS.\nIEEE 802.3 10-Mbps Specifications (Ethernet)\nThe IEEE 802.3 committee has defined a number of alternative physical configura-\ntions.This is both good and bad. On the good side, the standard has been responsive\nto evolving technology. On the bad side, the customer, not to mention the potential\nvendor, is faced with a bewildering array of options. However, the committee has\nbeen at pains to ensure that the various options can be easily integrated into a con-\nfiguration that satisfies a variety of needs. Thus, the user that has a complex set of\nrequirements may find the flexibility and variety of the 802.3 standard to be an asset.\nTo distinguish the various implementations that are available, the committee\nhas developed a concise notation:\n6data rate in Mbps6signaling method7maximum segment length in \nhundreds of meters7\nThe defined alternatives for 10-Mbps are as follows:1\n• 10BASE5: Specifies the use of 50-ohm coaxial cable and Manchester digital sig-\nnaling.2 The maximum length of a cable segment is set at 500 meters.The length\nof the network can be extended by the use of repeaters.A repeater is transpar-\nent to the MAC level; as it does no buffering, it does not isolate one segment\nfrom another. So, for example, if two stations on different segments attempt to\ntransmit at the same time, their transmissions will collide.To avoid looping, only\none path of segments and repeaters is allowed between any two stations. The\nstandard allows a maximum of four repeaters in the path between any two\nstations, extending the effective length of the medium to 2.5 kilometers.\n• 10BASE2: Similar to 10BASE5 but uses a thinner cable, which supports fewer\ntaps over a shorter distance than the 10BASE5 cable.This is a lower-cost alter-\nnative to 10BASE5.\n• 10BASE-T: Uses unshielded twisted pair in a star-shaped topology. Because of\nthe high data rate and the poor transmission qualities of unshielded twisted-\npair, the length of a link is limited to 100 meters. As an alternative, an optical\nfiber link may be used. In this case, the maximum length is 500 m.\n16.2 / ETHERNET\nIEEE 802.3 10-Mbps Physical Layer Medium Alternatives\nTransmission medium\nCoaxial cable \nCoaxial cable\n850-nm optical\ntwisted pair\nSignaling technique\nManchester/\n(Manchester)\n(Manchester)\n(Manchester)\nMaximum segment length (m)\nNodes per segment\nCable diameter (mm)\n62.5/125 mm\n3See Chapter 4 for a discussion of Category 3 and Category 5 cable.\n• 10BASE-F: Contains three specifications: a passive-star topology for intercon-\nnecting stations and repeaters with up to 1 km per segment; a point-to-point\nlink that can be used to connect stations or repeaters at up to 2 km; a point-to-\npoint link that can be used to connect repeaters at up to 2 km.\nNote that 10BASE-T and 10-BASE-F do not quite follow the notation: “T”\nstands for twisted pair and “F” stands for optical fiber. Table 16.2 summarizes the\nremaining options. All of the alternatives listed in the table specify a data rate of\nIEEE 802.3 100-Mbps Specifications (Fast Ethernet)\nFast Ethernet refers to a set of specifications developed by the IEEE 802.3 \ncommittee to provide a low-cost, Ethernet-compatible LAN operating at \n100 Mbps. The blanket designation for these standards is 100BASE-T. The com-\nmittee defined a number of alternatives to be used with different transmission\nTable 16.3 summarizes key characteristics of the 100BASE-T options. All of\nthe 100BASE-T options use the IEEE 802.3 MAC protocol and frame format.\n100BASE-X refers to a set of options that use two physical links between nodes; one\nfor transmission and one for reception. 100BASE-TX makes use of shielded twisted\npair (STP) or high-quality (Category 5) unshielded twisted pair (UTP). 100BASE-\nFX uses optical fiber.\nIn many buildings, any of the 100BASE-X options requires the installation\nof new cable. For such cases, 100BASE-T4 defines a lower-cost alternative that can\nuse Category 3, voice-grade UTP in addition to the higher-quality Category 5\nUTP.3 To achieve the 100-Mbps data rate over lower-quality cable, 100BASE-T4 dic-\ntates the use of four twisted-pair lines between nodes, with the data transmission mak-\ning use of three pairs in one direction at a time.\nFor all of the 100BASE-T options, the topology is similar to that of 10BASE-T,\nnamely a star-wire topology.\nCHAPTER 16 / HIGH-SPEED LANS\nIEEE 802.3 100BASE-T Physical Layer Medium Alternatives\nTransmission medium\n2 pair, STP\n2 pair, Category\n2 optical fibers\n4 pair, Category\n3, 4, or 5 UTP\nSignaling technique\nMaximum segment length\nNetwork span\n100BASE-X For all of the transmission media specified under 100BASE-X, a uni-\ndirectional data rate of 100 Mbps is achieved transmitting over a single link (single\ntwisted pair, single optical fiber). For all of these media, an efficient and effective sig-\nnal encoding scheme is required.The one chosen is referred to as 4B/5B-NRZI.This\nCHAPTER 16 / HIGH-SPEED LANS\n100/1000-Mbps Hubs\n100 Mbps link\n1 Gbps link\nFigure 16.4\nExample Gigabit Ethernet Configuration\n• Frame bursting: This feature allows for multiple short frames to be transmit-\nted consecutively, up to a limit, without relinquishing control for CSMA/CD\nbetween frames. Frame bursting avoids the overhead of carrier extension\nwhen a single station has a number of small frames ready to send.\nWith a switching hub (Figure 15.13c), which provides dedicated access to the\nmedium, the carrier extension and frame bursting features are not needed. This is\nbecause data transmission and reception at a station can occur simultaneously with-\nout interference and with no contention for a shared medium.\nPhysical Layer The current 1-Gbps specification for IEEE 802.3 includes the fol-\nlowing physical layer alternatives (Figure 16.5):\n• 1000BASE-SX: This short-wavelength option supports duplex links of up to\n275 m using \nmultimode or up to 550 m using \nmultimode fiber.\nWavelengths are in the range of 770 to 860 nm.\n• 1000BASE-LX: This long-wavelength option supports duplex links of up to\nmultimode fiber or 5 km of \nsingle-mode\nfiber.Wavelengths are in the range of 1270 to 1355 nm.\n16.2 / ETHERNET\n• 1000BASE-CX:\nThis option supports 1-Gbps links among devices \nlocated within a single room or equipment rack, using copper jumpers (spe-\ncialized shielded twisted-pair cable that spans no more than 25 m). Each\nlink is composed of a separate shielded twisted pair running in each \n• 1000BASE-T: This option makes use of four pairs of Category 5 unshielded\ntwisted pair to support devices over a range of up to 100 m.\nThe signal encoding scheme used for the first three Gigabit Ethernet options\nCHAPTER 16 / HIGH-SPEED LANS\nInitially network managers will use 10-Gbps Ethernet to provide high-speed,\nlocal backbone interconnection between large-capacity switches.As the demand for\nbandwidth increases, 10-Gbps Ethernet will be deployed throughout the entire net-\nwork and will include server farm, backbone, and campuswide connectivity. This\ntechnology enables Internet service providers (ISPs) and network service providers\n(NSPs) to create very high-speed links at a low cost, between co-located, carrier-\nclass switches and routers.\nThe technology also allows the construction of metropolitan area networks\n(MANs) and WANs that connect geographically dispersed LANs between cam-\npuses or points of presence (PoPs). Thus, Ethernet begins to compete with ATM\nand other wide area transmission and networking technologies. In most cases\nwhere the customer requirement is data and TCP/IP transport, 10-Gbps Ethernet\nprovides substantial value over ATM transport for both network end users and\nservice providers:\n• No expensive, bandwidth-consuming conversion between Ethernet packets\nand ATM cells is required; the network is Ethernet, end to end.\n• The combination of IP and Ethernet offers quality of service and traffic\npolicing capabilities that approach those provided by ATM, so that\nadvanced traffic engineering technologies are available to users and\n• A wide variety of standard optical interfaces (wavelengths and link distances)\nhave been specified for 10-Gbps Ethernet, optimizing its operation and cost\nfor LAN, MAN, or WAN applications.\nFigure 16.6 illustrates potential uses of 10-Gbps Ethernet. Higher-capacity\nbackbone pipes will help relieve congestion for workgroup switches, where Gigabit\nEthernet uplinks can easily become overloaded, and for server farms, where 1-Gbps\nnetwork interface cards are already in widespread use.\nThe goal for maximum link distances cover a range of applications: from 300 m\nto 40 km.The links operate in full-duplex mode only, using a variety of optical fiber\nphysical media.\nFour physical layer options are defined for 10-Gbps Ethernet (Figure 16.7).\nThe first three of these have two suboptions: an “R” suboption and a “W” subop-\ntion.The R designation refers to a family of physical layer implementations that use\na signal encoding technique known as 64B/66B. The R implementations are\ndesigned for use over dark fiber, meaning a fiber optic cable that is not in use and\nthat is not connected to any other equipment. The W designation refers to a family\nof physical layer implementations that also use 64B/66B signaling but that are then\nencapsulated to connect to SONET equipment.\nThe four physical layer options are\n• 10GBASE-S (short): Designed for 850-nm transmission on multimode fiber.\nThis medium can achieve distances up to 300 m. There are 10GBASE-SR and\n10GBASE-SW versions.\n• 10GBASE-L (long): Designed for 1310-nm transmission on single-mode fiber.\nThis medium can achieve distances up to 10 km.There are 10GBASE-LR and\n10GBASE-LW versions.\n16.2 / ETHERNET\n10/100 Mbps\nWorkstations\nServer farm\nFigure 16.6\nExample 10 Gigabit Ethernet Configuration\nCHAPTER 16 / HIGH-SPEED LANS\nMaximum distance\n50-\u0004m multimode fiber\nSingle-mode fiber\nSingle-mode fiber\nSingle-mode fiber\n50-\u0004m multimode fiber\n10GBASE-LX4\n62.5-\u0004m multimode fiber\n62.5-\u0004m multimode fiber\nFigure 16.7\n10-Gbps Ethernet Distance Options (log scale)\n• 10GBASE-E (extended): Designed for 1550-nm transmission on single-mode\nfiber. This medium can achieve distances up to 40 km. There are 10GBASE-\nER and 10GBASE-EW versions.\n• 10GBASE-LX4: Designed for 1310-nm transmission on single-mode or multi-\nmode fiber.This medium can achieve distances up to 10 km.This medium uses\nwavelength-division multiplexing (WDM) to multiplex the bit stream across\nfour light waves.\nThe success of Fast Ethernet, Gigabit Ethernet, and 10-Gbps Ethernet highlights\nthe importance of network management concerns in choosing a network technology.\nBoth ATM and Fiber Channel,explored later,may be technically superior choices for a\nhigh-speed backbone,because of their flexibility and scalability.However,the Ethernet\nalternatives offer compatibility with existing installed LANs, network management\nsoftware, and applications.This compatibility has accounted for the survival of a nearly\n30-year-old technology (CSMA/CD) in today’s fast-evolving network environment.\n16.3 FIBRE CHANNEL\nAs the speed and memory capacity of personal computers, workstations, and servers\nhave grown, and as applications have become ever more complex with greater\nreliance on graphics and video, the requirement for greater speed in delivering data\nto the processor has grown.This requirement affects two methods of data communi-\ncations with the processor: I/O channel and network communications.\nAn I/O channel is a direct point-to-point or multipoint communications link,\npredominantly hardware based and designed for high speed over very short dis-\ntances. The I/O channel transfers data between a buffer at the source device and a\nCHAPTER 16 / HIGH-SPEED LANS\nFibre Channel\nswitching fabric\nFigure 16.8\nFibre Channel Network\nThe solution was to develop a simple generic transport mechanism based on\npoint-to-point links and a switching network. This underlying infrastructure sup-\nports a simple encoding and framing scheme that in turn supports a variety of chan-\nnel and network protocols.\nFibre Channel Elements\nThe key elements of a Fibre Channel network are the end systems, called nodes, and\nthe network itself, which consists of one or more switching elements. The collection\nof switching elements is referred to as a fabric. These elements are interconnected\nby point-to-point links between ports on the individual nodes and switches. Com-\nmunication consists of the transmission of frames across the point-to-point links.\nEach node includes one or more ports, called N_ports, for interconnection.\nSimilarly, each fabric-switching element includes multiple ports, called F_ports.\nInterconnection is by means of bidirectional links between ports. Any node can\ncommunicate with any other node connected to the same fabric using the services of\nthe fabric.All routing of frames between N_ports is done by the fabric. Frames may\nbe buffered within the fabric, making it possible for different nodes to connect to\nthe fabric at different data rates.\nA fabric can be implemented as a single fabric element with attached nodes (a\nsimple star arrangement) or as a more general network of fabric elements, as shown\nin Figure 16.8. In either case, the fabric is responsible for buffering and for routing\nframes between source and destination nodes.\nThe Fibre Channel network is quite different from the IEEE 802 LANs. Fibre\nChannel is more like a traditional circuit-switching or packet-switching network, in\ncontrast to the typical shared-medium LAN. Thus, Fibre Channel need not be con-\ncerned with medium access control issues.Because it is based on a switching network,\nthe Fibre Channel scales easily in terms of N_ports, data rate, and distance covered.\n16.3 / FIBRE CHANNEL\nMaximum Distance for Fibre Channel Media Types\nSingle mode fiber\nmultimode fiber\nmultimode fiber\nVideo coaxial cable\nMiniature coaxial cable\nShielded twisted pair\nThis approach provides great flexibility.Fibre Channel can readily accommodate new\ntransmission media and data rates by adding new switches and F_ports to an existing\nfabric. Thus, an existing investment is not lost with an upgrade to new technologies\nand equipment. Further, the layered protocol architecture accommodates existing\nI/O interface and networking protocols, preserving the preexisting investment.\nFibre Channel Protocol Architecture\nThe Fibre Channel standard is organized into five levels. Each level defines a func-\ntion or set of related functions. The standard does not dictate a correspondence\nbetween levels and actual implementations, with a specific interface between adja-\ncent levels. Rather, the standard refers to the level as a “document artifice” used to\ngroup related functions.The layers are as follows:\n• FC-0 Physical Media: Includes optical fiber for long-distance applications,\ncoaxial cable for high speeds over short distances, and shielded twisted pair for\nlower speeds over short distances\n• FC-1 Transmission Protocol: Defines the signal encoding scheme\n• FC-2 Framing Protocol: Deals with defining topologies, frame format, flow\nand error control, and grouping of frames into logical entities called sequences\nand exchanges\n• FC-3 Common Services: Includes multicasting\n• FC-4 Mapping: Defines the mapping of various channel and network proto-\ncols to Fibre Channel, including IEEE 802, ATM, IP, and the Small Computer\nSystem Interface (SCSI)\nFibre Channel Physical Media and Topologies\nOne of the major strengths of the Fibre Channel standard is that it provides a range\nof options for the physical medium, the data rate on that medium, and the topology\nof the network (Table 16.4).\nTransmission Media The transmission media options that are available under\nFibre Channel include shielded twisted pair, video coaxial cable, and optical fiber.\nStandardized data rates range from 100 Mbps to 3.2 Gbps. Point-to-point link dis-\ntances range from 33 m to 10 km.\nCHAPTER 16 / HIGH-SPEED LANS\nTopologies The most general topology supported by Fibre Channel is referred to\nas a fabric or switched topology. This is an arbitrary topology that includes at least\none switch to interconnect a number of end systems. The fabric topology may also\nconsist of a number of switches forming a switched network, with some or all of\nthese switches also supporting end nodes.\nRouting in the fabric topology is transparent to the nodes. Each port in the con-\nfiguration has a unique address.When data from a node are transmitted into the fab-\nric, the edge switch to which the node is attached uses the destination port address in\nthe incoming data frame to determine the destination port location.The switch then\neither delivers the frame to another node attached to the same switch or transfers the\nframe to an adjacent switch to begin routing the frame to a remote destination.\nThe fabric topology provides scalability of capacity: As additional ports are\nadded, the aggregate capacity of the network increases, thus minimizing congestion\nand contention and increasing throughput. The fabric is protocol independent and\nlargely distance insensitive.The technology of the switch itself and of the transmis-\nsion links connecting the switch to nodes may be changed without affecting the\noverall configuration. Another advantage of the fabric topology is that the burden\non nodes is minimized. An individual Fibre Channel node (end system) is only\nresponsible for managing a simple point-to-point connection between itself and the\nfabric; the fabric is responsible for routing between ports and error detection.\nIn addition to the fabric topology, the Fibre Channel standard defines two\nother topologies. With the point-to-point topology there are only two ports, and\nthese are directly connected, with no intervening fabric switches. In this case there is\nno routing. The arbitrated loop topology is a simple, low-cost topology for connect-\ning up to 126 nodes in a loop. The arbitrated loop operates in a manner roughly\nequivalent to the token ring protocols that we have seen.\nTopologies, transmission media, and data rates may be combined to provide an\noptimized configuration for a given site. Figure 16.9 is an example that illustrates the\nprincipal applications of Fiber Channel.\nProspects for Fibre Channel\nFibre Channel is backed by an industry interest group known as the Fibre Chan-\nnel Association and a variety of interface cards for different applications are\navailable. Fibre Channel has been most widely accepted as an improved periph-\neral device interconnect, providing services that can eventually replace such\nschemes as SCSI. It is a technically attractive solution to general high-speed LAN\nrequirements but must compete with Ethernet and ATM LANs. Cost and perfor-\nmance issues should dominate the manager’s consideration of these competing\ntechnologies.\n16.4 RECOMMENDED READING AND WEB SITES\n[STAL00] covers in greater detail the LAN systems discussed in this chapter.\n[SPUR00] provides a concise but thorough overview of all of the 10-Mbps through 1-Gbps\n802.3 systems, including configuration guidelines for a single segment of each media type, as well\n16.4 / RECOMMENDED READING AND WEB SITES\nas guidelines for building multisegment Ethernets using a variety of media types.Two excellent\ntreatments of both 100-Mbps and Gigabit Ethernet are [SEIF98] and [KADA98].A good survey\narticle on Gigabit Ethernet is [FRAZ99].\n[SACH96] is a good survey of Fibre Channel. A short but worthwhile treatment is\nLinking high-\nperformance\nworkstation clusters\nConnecting mainframes\nto each other\nLinking LANs and\nWANs to the backbone\nClustering disk farms\nGiving server farms high-speed pipes\nFibre Channel\nswitching fabric\nFigure 16.9\nFive Applications of Fibre Channel\nFibre Channel Industry Association. Fibre Channel Storage Area Networks. San\nFrancisco: Fibre Channel Industry Association, 2001.\nFrazier, H., and Johnson, H. “Gigabit Ethernet: From 100 to 1,000 Mbps.”\nIEEE Internet Computing, January/February 1999.\nKadambi, J.; Crayford, I.; and Kalkunte, M. Gigabit Ethernet. Upper Saddle\nRiver, NJ: Prentice Hall, 1998.\nSachs. M., and Varma,A.“Fibre Channel and Related Standards.” IEEE Com-\nmunications Magazine, August 1996.\nSeifert, R. Gigabit Ethernet. Reading, MA:Addison-Wesley, 1998.\nSpurgeon, C. Ethernet: The Definitive Guide. Cambridge, MA: O’Reilly and\nAssociates, 2000.\nStallings, W. Local and Metropolitan Area Networks, Sixth Edition. Upper Sad-\ndle River, NJ: Prentice Hall, 2000.\nCHAPTER 16 / HIGH-SPEED LANS\nRecommended Web sites:\n• Interoperability Lab: University of New Hampshire site for equipment testing for\nhigh-speed LANs\n• Charles Spurgeon’s Ethernet Web Site: Provides extensive information about Eth-\nernet, including links and documents\n• IEEE 802.3 10-Gbps Ethernet Task Force: Latest documents\n• Fibre Channel Industry Association: Includes tutorials, white papers, links to ven-\ndors, and descriptions of Fibre Channel applications\n• CERN Fibre Channel Site: Includes tutorials, white papers, links to vendors, and\ndescriptions of Fibre Channel applications\n• Storage Network Industry Association: An industry forum of developers, integra-\ntors, and IT professionals who evolve and promote storage networking technology and\n16.5 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\n1-persistent CSMA\nbinary exponential backoff\ncarrier sense multiple access\ncarrier sense multiple access\nwith collision detection\nFibre Channel\nfull-duplex operation\nnonpersistent CSMA\np-persistent CSMA\nslotted ALOHA\nReview Questions\nWhat is a server farm?\nExplain the three persistence protocols that can be used with CSMA.\nWhat is CSMA/CD?\nExplain binary exponential backoff.\nWhat are the transmission medium options for Fast Ethernet?\nHow does Fast Ethernet differ from 10BASE-T, other than the data rate?\nIn the context of Ethernet, what is full-duplex operation?\nList the levels of Fibre Channel and the functions of each level.\nWhat are the topology options for Fibre Channel?\nA disadvantage of the contention approach for LANs, such as CSMA/CD, is the\ncapacity wasted due to multiple stations attempting to access the channel at the\nsame time. Suppose that time is divided into discrete slots, with each of N stations\nattempting to transmit with probability p during each slot. What fraction of slots are\nwasted due to multiple simultaneous transmission attempts?\n16.5 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nFor p-persistent CSMA, consider the following situation. A station is ready to trans-\nmit and is listening to the current transmission. No other station is ready to transmit,\nand there will be no other transmission for an indefinite period. If the time unit used\nin the protocol is T, show that the average number of iterations of step 1 of the proto-\ncol is 1/p and that therefore the expected time that the station will have to wait after\nthe current transmission is\nHint: Use the equality \nThe binary exponential backoff algorithm is defined by IEEE 802 as follows:\nThe delay is an integral multiple of slot time.The number of slot times to delay\nbefore the nth retransmission attempt is chosen as a uniformly distributed ran-\ndom integer r in the range \nSlot time is, roughly, twice the round-trip propagation delay.Assume that two stations\nalways have a frame to send. After a collision, what is the mean number of retrans-\nmission attempts before one station successfully retransmits? What is the answer if\nthree stations always have frames to send?\nDescribe the signal pattern produced on the medium by the Manchester-encoded\npreamble of the IEEE 802.3 MAC frame.\nAnalyze the advantages of having the FCS field of IEEE 802.3 frames in the trailer of\nthe frame rather than in the header of the frame.\nThe most widely used MAC approach for a ring topology is token ring, defined in\nIEEE 802.5. The token ring technique is based on the use of a small frame, called a\ntoken, that circulates when all stations are idle. A station wishing to transmit must\nwait until it detects a token passing by. It then seizes the token by changing one bit in\nthe token, which transforms it from a token to a start-of-frame sequence for a data\nframe. The station then appends and transmits the remainder of the fields needed to\nconstruct a data frame. When a station seizes a token and begins to transmit a data\nframe, there is no token on the ring, so other stations wishing to transmit must wait.\nThe frame on the ring will make a round trip and be absorbed by the transmitting sta-\ntion.The transmitting station will insert a new token on the ring when both of the fol-\nlowing conditions have been met: (1) The station has completed transmission of its\nframe. (2) The leading edge of the transmitted frame has returned (after a complete\ncirculation of the ring) to the station.\nAn option in IEEE 802.5, known as early token release, eliminates the second\ncondition just listed. Under what conditions will early token release result in\nimproved utilization?\nb. Are there any potential disadvantages to early token release? Explain.\nFor a token ring LAN, suppose that the destination station removes the data\nCHAPTER 16 / HIGH-SPEED LANS\nWith 8B6T coding, the DC algorithm sometimes negates all of the ternary symbols in\na code group. How does the receiver recognize this condition? How does the receiver\ndiscriminate between a negated code group and one that has not been negated? For\nexample, the code group for data byte 00 is \nand the code group for data\nbyte 38 is the negation of that, namely,\nDraw the MLT decoder state diagram that corresponds to the encoder state diagram\nof Figure 16.10.\nFor the bit stream 0101110, sketch the waveforms for NRZ-L, NRZI, Manchester, and\nDifferential Manchester, and MLT-3.\nConsider a token ring system with N stations in which a station that has just transmit-\nted a frame releases a new token only after the station has completed transmission of\nits frame and the leading edge of the transmitted frame has returned (after a com-\nplete circulation of the ring) to the station.\nShow that utilization can be approximated by \nb. What is the asymptotic value of utilization as N increases?\nVerify that the division illustrated in Figure 16.18a corresponds to the implemen-\ntation of Figure 16.17a by calculating the result step by step using Equation (16.7).\nb. Verify that the multiplication illustrated in Figure 16.18b corresponds to the\nimplementation of Figure 16.17b by calculating the result step by step using\nEquation (16.8).\nDraw a figure similar to Figure 16.17 for the MLT-3 scrambler and descrambler.\nIn Chapter 5, we looked at some of the common techniques for encoding digital data for trans-\nmission, including Manchester and differential Manchester, which are used in some of the\nferential encoding. Recall from Chapter 5 that in differential encoding, the signal is decoded\nby comparing the polarity of adjacent signal elements rather than the absolute value of a sig-\nnal element.A benefit of this scheme is that it is generally more reliable to detect a transition\nin the presence of noise and distortion than to compare a value to a threshold.\nNow we are in a position to describe the 4B/5B code and to understand the selections\nthat were made.Table 16.5 shows the symbol encoding.Each 5-bit code group pattern is shown,\ntogether with its NRZI realization. Because we are encoding 4 bits with a 5-bit pattern, only\n16 of the 32 possible patterns are needed for data encoding. The codes selected to represent\n4B/5B Code Groups (page 1 of 2)\nNRZI pattern\nInterpretation\nStart of stream \ndelimiter, part 1\nStart of stream \ndelimiter, part 2\nEnd of stream \ndelimiter, part 1\nEnd of stream \ndelimiter, part 2\nTransmit error\nInvalid codes\nCHAPTER 16 / HIGH-SPEED LANS\nthe 16 4-bit data blocks are such that a transition is present at least twice for each 5-code group\ncode. No more than three zeros in a row are allowed across one or more code groups\nThe encoding scheme can be summarized as follows:\n1. A simple NRZ encoding is rejected because it does not provide synchronization; a\nstring of 1s or 0s will have no transitions.\n2. The data to be transmitted must first be encoded to assure transitions.The 4B/5B code is\nchosen over Manchester because it is more efficient.\n3. The 4B/5B code is further encoded using NRZI so that the resulting differential signal will\nimprove reception reliability.\n4. The specific 5-bit patterns for the encoding of the 16 4-bit data patterns are chosen to\nguarantee no more than three zeros in a row to provide for adequate synchronization.\nThose code groups not used to represent data are either declared invalid or assigned\nspecial meaning as control symbols. These assignments are listed in Table 16.5. The nondata\nsymbols fall into the following categories:\n• Idle: The idle code group is transmitted between data transmission sequences. It\nconsists of a constant flow of binary ones, which in NRZI comes out as a continuous\nalternation between the two signal levels. This continuous fill pattern establishes\nand maintains synchronization and is used in the CSMA/CD protocol to indicate\nthat the shared medium is idle.\n• Start of stream delimiter: Used to delineate the starting boundary of a data transmis-\nsion sequence; consists of two different code groups.\n• End of stream delimiter: Used to terminate normal data transmission sequences; con-\nsists of two different code groups.\n• Transmit error: This code group is interpreted as a signaling error. The normal use of\nthis indicator is for repeaters to propagate received errors.\nAlthough 4B/5B-NRZI is effective over optical fiber, it is not suitable as is for use over\ntwisted pair. The reason is that the signal energy is concentrated in such a way as to produce\nundesirable radiated emissions from the wire. MLT-3, which is used on 100BASE-TX, is\ndesigned to overcome this problem.\nThe following steps are involved:\n1. NRZI to NRZ conversion. The 4B/5B NRZI signal of the basic 100BASE-X is con-\nverted back to NRZ.\n2. Scrambling. The bit stream is scrambled to produce a more uniform spectrum distribution\nfor the next stage.\n3. Encoder. The scrambled bit stream is encoded using a scheme known as MLT-3.\n4. Driver. The resulting encoding is transmitted.\nThe effect of the MLT-3 scheme is to concentrate most of the energy in the transmitted\nsignal below 30 MHz, which reduces radiated emissions.This in turn reduces problems due to\ninterference.\nThe MLT-3 encoding produces an output that has a transition for every binary one and\nthat uses three levels: a positive voltage \na negative voltage \nand no voltage (0).\nThe encoding rules are best explained with reference to the encoder state diagram shown in\nFigure 16.10:\nCHAPTER 16 / HIGH-SPEED LANS\nTable 16.6 shows a portion of the 8B6T code table; the full table maps all possible 8-bit\npatterns into a unique code group of 6 ternary symbols. The mapping was chosen with two\nrequirements in mind: synchronization and DC balance. For synchronization, the codes were\nchosen so to maximize the average number of transitions per code group.The second require-\nment is to maintain DC balance, so that the average voltage on the line is zero. For this pur-\npose all of the selected code groups either have an equal number of positive and negative\nsymbols or an excess of one positive symbol. To maintain balance, a DC balancing algorithm\nis used. In essence, this algorithm monitors the cumulative weight of the of all code groups\ntransmitted on a single pair. Each code group has a weight of 0 or 1.To maintain balance, the\n3 = 25 Mbaud\nStream of 8-bit\n8B (100 Mbps)\n6T (25 Mbaud)\n6T (25 MBaud)\n6T (25 MBaud)\nFigure 16.12\n8B6T Transmission Scheme\nPortion of 8B6T Code Table\n+ 0 - + 0 -\n0 - - 0 + +\n0 - + 0 + -\n+ 0 - - 0 +\n- + 0 + 0 -\n- 0 - 0 + +\n0 - + 0 - +\n- + 0 - 0 +\n0 - + + 0 -\n- - 0 0 + +\n0 - + + + -\n0 - + - 0 +\n+ 0 - 0 - +\n0 - - + + 0\n0 - + 0 0 +\n+ 0 - 0 + -\n+ 0 - - + 0\n0 - - + 0 +\n0 + - 0 0 +\n+ 0 - + - 0\n- + 0 - + 0\n- 0 - + 0 +\n0 + - + + -\n- + 0 + - 0\n0 - + - + 0\n- - 0 + 0 +\n0 + - 0 - +\n0 - + + - 0\n- + 0 0 - +\n- 0 - + + 0\n0 + - 0 + -\n- + 0 0 + -\n- 0 + + 0 -\n- - + + + -\n0 + + 0 - -\n- 0 + - 0 +\n+ - 0 + 0 -\n0 0 - 0 0 +\n+ 0 + 0 - -\n+ - 0 - 0 +\n0 + - + 0 -\n0 0 + 0 + -\n+ + 0 0 - -\n0 + - - 0 +\n- 0 + 0 - +\n0 0 + 0 - +\n0 + + - - 0\n- 0 + 0 + -\n- 0 + - + 0\n+ + - 0 - +\n0 + + - 0 -\n- 0 + + - 0\n+ - 0 - + 0\n+ + - 0 + -\n+ 0 + - 0 -\n+ - 0 + - 0\n0 + - - + 0\n- - + 0 0 +\n+ + 0 - 0 -\n0 + - + - 0\n+ - 0 0 - +\n0 0 - + + -\n+ 0 + - - 0\n+ - 0 0 + -\nCHAPTER 16 / HIGH-SPEED LANS\nEthernet frame, including control fields, is considered “data” for this pzrocess. In addition, there\nare nondata symbols, called “control,” and which include those defined for the 4B/5B code dis-\ncussed previously plus a few other symbols. For a 64-bit block consisting only of data octets, the\nentire block is scrambled.Two synchronization bits, with values 01, are prepended to the scram-\nbled block. For a block consisting a mixture of control and data octets, a 56-bit block is used,\nwhich is scrambled;a 66-bit block is formed by prepending two synchronization bits,with values\n10, and an 8-bit control type field, which defines the control functions included with this block.\nIn both cases, scrambling is performed using the polynomial \nIn Chapter 7, we introduced the parameter a, defined as\nIn that context, we were concerned with a point-to-point link, with a given propagation time\nbetween the two endpoints and a transmission time for either a fixed or average frame size. It\nwas shown that a could be expressed as\nThis parameter is also important in the context of LANs and MANs, and in fact deter-\nmines an upper bound on utilization. Consider a perfectly efficient access mechanism that\nallows only one transmission at a time. As soon as one transmission is over, another station\nbegins transmitting. Furthermore, the transmission is pure data; no overhead bits.What is the\nmaximum possible utilization of the network? It can be expressed as the ratio of total\nthroughput of the network to its data rate:\nNow define, as in Chapter 7:\nL = average or fixed frame length\nV = velocity of signal propagation\nd = maximum distance between any two stations\nR = data rate of the channel\nLength of data link in bits\nLength of frame in bits\nPropagation time\nTransmission time\n1 + X39 + X58.\nCHAPTER 16 / HIGH-SPEED LANS\nevents 2 and 3 are interchanged. In both cases, the total time for one “turn”\nbut the transmission time is only 1, for a utilization of \nThe same effect can be seen to apply to a ring network in Figure 16.15. Here we assume\nthat one station transmits and then waits to receive its own transmission before any other sta-\ntion transmits.The identical sequence of events just outlined applies.\nTypical values of a range from about 0.01 to 0.1 for LANs and 0.1 to well over 1.0 for\nMANs. Table 16.7 gives some representative values for a bus topology. As can be seen, for\nlarger and/or higher-speed networks, utilization suffers. For this reason, the restriction of only\none frame at a time is lifted for high-speed LANs.\nFigure 16.15\nThe Effect of a on Utilization for Ring\nRepresentative Values of a\nData Rate (Mbps)\nFrame Size (bits)\nNetwork Length (km)\nCHAPTER 16 / HIGH-SPEED LANS\nThis function takes on a maximum over P when\nWe are interested in the maximum because we want to calculate the maximum throughput of\nthe medium. It should be clear that the maximum throughput will be achieved if we maximize\nthe probability of successful seizure of the medium. Therefore, the following rule should be\nenforced: During periods of heavy usage, a station should restrain its offered load to 1/N.\n(This assumes that each station knows the value of N. To derive an expression for maximum\npossible throughput, we live with this assumption.) On the other hand, during periods of light\nusage, maximum utilization cannot be achieved because the load is too low; this region is not\nof interest here.\nNow we can estimate the mean length of a contention interval, w, in slots:\nThe summation converges to\nWe can now determine the maximum utilization, which is the length of a transmission inter-\nval as a proportion of a cycle consisting of a transmission and a contention interval:\nFigure 16.16 shows normalized throughput as a function of a for two values of N.\nThroughput declines as a increases.This is to be expected. Figure 16.16 also shows throughput\nas a function of N. The performance of CSMA/CD decreases because of the increased likeli-\nhood of collision or no transmission.\nIt is interesting to note the asymptotic value of U as N increases.We need to know that\nThen we have\nCHAPTER 16 / HIGH-SPEED LANS\nAs can be seen, the descrambled output is the original sequence.\nWe can represent this process with the use of polynomials. Thus, for this example, the\npolynomial is \nThe input is divided by this polynomial to produce the\nscrambled sequence. At the receiver the received scrambled signal is multiplied by the same\npolynomial to reproduce the original input. Figure 16.18 is an example using the polynomial\nP(X) and an input of 101010100000111.4 The scrambled transmission, produced by dividing\nby P(X) (100101), is 101110001101001. When this number is multiplied by P(X), we get the\nP1X2 = 1 + X3 + X5.\n= Am1 \u0001 Bm-3 \u0001 Bm-32 \u0001 1Bm-5 \u0001 Bm-52\n= 1Am \u0001 Bm-3 \u0001 Bm-52 \u0001 Bm-3 \u0001 Bm-5\nCm = Bm \u0001 Bm-3 \u0001 Bm-5\n(a) Scrambling\n(b) Descrambling\n1 0 0 1 0 1\n1 0 0 1 0 1\n1 1 1 1 1 0\n1 0 0 1 0 1\n1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 \u0003 \u0003 \u0003 \u0003 \u0003\n1 0 1 1 1 0 0 0 1 1 0 1 0 0 1\n1 1 0 1 1 0\n1 0 0 1 0 1\n1 0 0 1 1 0\n1 0 0 1 0 1\n1 1 0 0 1 1\n1 0 0 1 0 1\n1 0 1 1 0 1\n1 0 0 1 0 1\n1 0 0 0 0 0\n1 0 0 1 0 1\n1 0 1 0 0 0\n1 0 1 1 1 0 0 0 1 1 0 1 0 0 1\n1 0 1 1 1 0 0 0 1 1 0 1 0 0 1\n1 0 1 1 1 0 0 0 1 1 0 1 0 0 1\n1 0 1 1 1 0 0 0 1 1 0 1 0 0 1\n1 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n1 0 0 1 0 1\nFigure 16.18\nExample of Scrambling with P1X2 \u0001 1 \u0002 X\u00033 \u0002 X\u00035\n4We use the convention that the leftmost bit is the first bit presented to the scrambler; thus the bits can be\nSimilarly, the polynomial is converted to a bit string from left to right.The polynomial\nis represented as B0B1B2 Á .\nB0 + B1X + B2X2 + . . .\nCHAPTER 17 / WIRELESS LANS\nfor this included high prices, low data rates, occupational safety concerns, and licens-\ning requirements. As these problems have been addressed, the popularity of wire-\nless LANs has grown rapidly.\nIn this section, we survey the key wireless LAN application areas and then\nlook at the requirements for and advantages of wireless LANs.\nWireless LAN Applications\n[PAHL95] lists four application areas for wireless LANs: LAN extension, cross-\nbuilding interconnect, nomadic access, and ad hoc networks. Let us consider each of\nthese in turn.\nLAN Extension Early wireless LAN products, introduced in the late 1980s, were\nmarketed as substitutes for traditional wired LANs. A wireless LAN saves the cost\nof the installation of LAN cabling and eases the task of relocation and other modi-\nfications to network structure. However, this motivation for wireless LANs was\novertaken by events. First, as awareness of the need for LANs became greater,\narchitects designed new buildings to include extensive prewiring for data applica-\ntions. Second, with advances in data transmission technology, there is an increasing\nreliance on twisted pair cabling for LANs and, in particular, Category 3 and Cate-\ngory 5 unshielded twisted pair. Most older buildings are already wired with an abun-\ndance of Category 3 cable, and many newer buildings are prewired with Category 5.\nThus, the use of a wireless LAN to replace wired LANs has not happened to any\ngreat extent.\nHowever, in a number of environments, there is a role for the wireless LAN as\nan alternative to a wired LAN. Examples include buildings with large open areas,\nsuch as manufacturing plants, stock exchange trading floors, and warehouses; histor-\nical buildings with insufficient twisted pair and where drilling holes for new wiring is\nprohibited; and small offices where installation and maintenance of wired LANs is\nnot economical. In all of these cases, a wireless LAN provides an effective and more\nattractive alternative. In most of these cases, an organization will also have a wired\nLAN to support servers and some stationary workstations. For example, a manufac-\nturing facility typically has an office area that is separate from the factory floor but\nthat must be linked to it for networking purposes. Therefore, typically, a wireless\nLAN will be linked into a wired LAN on the same premises. Thus, this application\narea is referred to as LAN extension.\nFigure 17.1 indicates a simple wireless LAN configuration that is typical of\nmany environments. There is a backbone wired LAN, such as Ethernet, that sup-\nports servers, workstations, and one or more bridges or routers to link with other\nnetworks. In addition, there is a control module (CM) that acts as an interface to a\nwireless LAN. The control module includes either bridge or router functionality to\nlink the wireless LAN to the backbone. It includes some sort of access control logic,\nsuch as a polling or token-passing scheme, to regulate the access from the end sys-\ntems. Note that some of the end systems are standalone devices, such as a worksta-\ntion or a server. Hubs or other user modules (UMs) that control a number of\nstations off a wired LAN may also be part of the wireless LAN configuration.\nThe configuration of Figure 17.1 can be referred to as a single-cell wireless\nLAN; all of the wireless end systems are within range of a single control module.\n17.1 / OVERVIEW\nUM \u0001 user module\nCM \u0001 control module\nEthernet switch\nEthernet switch\nBridge or router\nFigure 17.1\nExample Single-Cell Wireless LAN Configuration\nAnother common configuration, suggested by Figure 17.2, is a multiple-cell wireless\nLAN. In this case, there are multiple control modules interconnected by a wired\nLAN. Each control module supports a number of wireless end systems within its\ntransmission range. For example, with an infrared LAN, transmission is limited to a\nsingle room; therefore, one cell is needed for each room in an office building that\nrequires wireless support.\nCross-Building Interconnect Another use of wireless LAN technology is to\nconnect LANs in nearby buildings, be they wired or wireless LANs. In this case, a\npoint-to-point wireless link is used between two buildings.The devices so connected\nare typically bridges or routers. This single point-to-point link is not a LAN per se,\nbut it is usual to include this application under the heading of wireless LAN.\nNomadic Access Nomadic access provides a wireless link between a LAN hub\nand a mobile data terminal equipped with an antenna, such as a laptop computer or\nnotepad computer. One example of the utility of such a connection is to enable an\nemployee returning from a trip to transfer data from a personal portable computer\nto a server in the office. Nomadic access is also useful in an extended environment\nsuch as a campus or a business operating out of a cluster of buildings. In both of\nCHAPTER 17 / WIRELESS LANS\nFrequency 1\nFrequency 2\nFrequency 3\nEthernet switch\nBridge or router\nFigure 17.2\nExample Multiple-Cell Wireless LAN Configuration\nthese cases, users may move around with their portable computers and may wish\naccess to the servers on a wired LAN from various locations.\nAd Hoc Networking An ad hoc network is a peer-to-peer network (no central-\nized server) set up temporarily to meet some immediate need. For example, a group\nof employees, each with a laptop or palmtop computer, may convene in a confer-\nence room for a business or classroom meeting.The employees link their computers\nin a temporary network just for the duration of the meeting.\nFigure 17.3 suggests the differences between a wireless LAN that supports LAN\nextension and nomadic access requirements and an ad hoc wireless LAN.In the former\ncase,the wireless LAN forms a stationary infrastructure consisting of one or more cells\nwith a control module for each cell.Within a cell, there may be a number of stationary\nend systems. Nomadic stations can move from one cell to another. In contrast, there is\nno infrastructure for an ad hoc network. Rather, a peer collection of stations within\nrange of each other may dynamically configure themselves into a temporary network.\nWireless LAN Requirements\nA wireless LAN must meet the same sort of requirements typical of any LAN,\nincluding high capacity, ability to cover short distances, full connectivity among\nattached stations, and broadcast capability. In addition, there are a number of\n17.1 / OVERVIEW\nHigh-speed backbone wired LAN\n(b) Ad hoc LAN\n(a) Infrastructure wireless LAN\nFigure 17.3\nWireless LAN Configurations\nrequirements specific to the wireless LAN environment. The following are among\nthe most important requirements for wireless LANs:\n• Throughput: The medium access control protocol should make as efficient use\nas possible of the wireless medium to maximize capacity.\n• Number of nodes: Wireless LANs may need to support hundreds of nodes\nacross multiple cells.\n• Connection to backbone LAN: In most cases, interconnection with stations on\na wired backbone LAN is required. For infrastructure wireless LANs, this is\neasily accomplished through the use of control modules that connect to both\nCHAPTER 17 / WIRELESS LANS\ntypes of LANs. There may also need to be accommodation for mobile users\nand ad hoc wireless networks.\n• Service area: A typical coverage area for a wireless LAN has a diameter of\n100 to 300 m.\n• Battery power consumption: Mobile workers use battery-powered worksta-\ntions that need to have a long battery life when used with wireless adapters.\nThis suggests that a MAC protocol that requires mobile nodes to monitor\naccess points constantly or engage in frequent handshakes with a base station\nis inappropriate. Typical wireless LAN implementations have features to\nreduce power consumption while not using the network, such as a sleep mode.\n• Transmission robustness and security: Unless properly designed, a wireless\nLAN may be especially vulnerable to interference and eavesdropping. The\ndesign of a wireless LAN must permit reliable transmission even in a noisy\nenvironment and should provide some level of security from eavesdropping.\n• Collocated network operation: As wireless LANs become more popular, it is\nquite likely for two or more wireless LANs to operate in the same area or in\nsome area where interference between the LANs is possible. Such interfer-\nence may thwart the normal operation of a MAC algorithm and may allow\nunauthorized access to a particular LAN.\n• License-free operation: Users would prefer to buy and operate wireless LAN\nproducts without having to secure a license for the frequency band used by the\n• Handoff/roaming: The MAC protocol used in the wireless LAN should enable\nmobile stations to move from one cell to another.\n• Dynamic configuration: The MAC addressing and network management\naspects of the LAN should permit dynamic and automated addition, deletion,\nand relocation of end systems without disruption to other users.\n17.2 WIRELESS LAN TECHNOLOGY\nWireless LANs are generally categorized according to the transmission technique that\nis used.All current wireless LAN products fall into one of the following categories:\n• Infrared (IR) LANs: An individual cell of an IR LAN is limited to a single\nroom, because infrared light does not penetrate opaque walls.\n• Spread spectrum LANs: This type of LAN makes use of spread spectrum trans-\nmission technology. In most cases, these LANs operate in the ISM (industrial,\nscientific, and medical) microwave bands so that no Federal Communications\nCommission (FCC) licensing is required for their use in the United States.\nInfrared LANs\nOptical wireless communication in the infrared portion of the spectrum is com-\nmonplace in most homes, where it is used for a variety of remote control devices.\n17.2 / WIRELESS LAN TECHNOLOGY\nMore recently, attention has turned to the use of infrared technology to construct\nwireless LANs. In this section, we begin with a comparison of the characteristics of\ninfrared LANs with those of radio LANs and then look at some of the details of\ninfrared LANs.\nStrengths and Weaknesses Infrared offers a number of significant advantages\nover microwave approaches. The spectrum for infrared is virtually unlimited, which\npresents the possibility of achieving extremely high data rates. The infrared spec-\ntrum is unregulated worldwide, which is not true of some portions of the microwave\nIn addition, infrared shares some properties of visible light that make it attrac-\ntive for certain types of LAN configurations. Infrared light is diffusely reflected by\nlight-colored objects; thus it is possible to use ceiling reflection to achieve coverage\nof an entire room. Infrared light does not penetrate walls or other opaque objects.\nThis has two advantages: First, infrared communications can be more easily secured\nagainst eavesdropping than microwave; and second, a separate infrared installation\ncan be operated in every room in a building without interference, enabling the con-\nstruction of very large infrared LANs.\nAnother strength of infrared is that the equipment is relatively inexpensive\nand simple. Infrared data transmission typically uses intensity modulation, so that\nIR receivers need to detect only the amplitude of optical signals, whereas most\nmicrowave receivers must detect frequency or phase.\nThe infrared medium also exhibits some drawbacks. Many indoor environ-\nments experience rather intense infrared background radiation, from sunlight and\nindoor lighting. This ambient radiation appears as noise in an infrared receiver,\nrequiring the use of transmitters of higher power than would otherwise be required\nand also limiting the range. However, increases in transmitter power are limited by\nconcerns of eye safety and excessive power consumption.\nTransmission Techniques Three alternative transmission techniques are in\ncommon use for IR data transmission: the transmitted signal can be focused and\naimed (as in a remote TV control); it can be radiated omnidirectionally; or it can be\nreflected from a light-colored ceiling.\nDirected-beam IR can be used to create point-to-point links. In this mode, the\nrange depends on the emitted power and on the degree of focusing.A focused IR data\nlink can have a range of kilometers.Such ranges are not needed for constructing indoor\nwireless LANs. However, an IR link can be used for cross-building interconnect\nbetween bridges or routers located in buildings within a line of sight of each other.\nOne indoor use of point-to-point IR links is to set up a ring LAN. A set of IR\ntransceivers can be positioned so that data circulate around them in a ring configu-\nration. Each transceiver supports a workstation or a hub of stations, with the hub\nproviding a bridging function.\nAn omnidirectional configuration involves a single base station that is within\nline of sight of all other stations on the LAN.Typically, this station is mounted on the\nceiling. The base station acts as a multiport repeater. The ceiling transmitter broad-\ncasts an omnidirectional signal that can be received by all of the other IR trans-\nceivers in the area. These other transceivers transmit a directional beam aimed at\nthe ceiling base unit.\nCHAPTER 17 / WIRELESS LANS\nIn a diffused configuration, all of the IR transmitters are focused and aimed at\na point on a diffusely reflecting ceiling. IR radiation striking the ceiling is reradiated\nomnidirectionally and picked up by all of the receivers in the area.\nSpread Spectrum LANs\nCurrently, the most popular type of wireless LAN uses spread spectrum techniques.\nConfiguration Except for quite small offices, a spread spectrum wireless LAN\nmakes use of a multiple-cell arrangement, as was illustrated in Figure 17.2.Adjacent\ncells make use of different center frequencies within the same band to avoid inter-\nWithin a given cell, the topology can be either hub or peer to peer. The hub\ntopology is indicated in Figure 17.2. In a hub topology, the hub is typically mounted\non the ceiling and connected to a backbone wired LAN to provide connectivity to\nstations attached to the wired LAN and to stations that are part of wireless LANs in\nother cells. The hub may also control access, as in the IEEE 802.11 point coordina-\ntion function, described subsequently. The hub may also control access by acting as\na multiport repeater with similar functionality to Ethernet multiport repeaters. In\nthis case, all stations in the cell transmit only to the hub and receive only from the\nhub. Alternatively, and regardless of access control mechanism, each station may\nbroadcast using an omnidirectional antenna so that all other stations in the cell may\nreceive; this corresponds to a logical bus configuration.\nOne other potential function of a hub is automatic handoff of mobile stations.\nAt any time, a number of stations are dynamically assigned to a given hub based on\nproximity.When the hub senses a weakening signal, it can automatically hand off to\nthe nearest adjacent hub.\nA peer-to-peer topology is one in which there is no hub.A MAC algorithm such\nas CSMA is used to control access.This topology is appropriate for ad hoc LANs.\nTransmission Issues A desirable,though not necessary,characteristic of a wireless\nLAN is that it be usable without having to go through a licensing procedure. The\nlicensing regulations differ from one country to another,which complicates this objec-\ntive. Within the United States, the FCC has authorized two unlicensed applications\nwithin the ISM band: spread spectrum systems, which can operate at up to 1 watt, and\nvery low power systems, which can operate at up to 0.5 watts. Since the FCC opened\nup this band, its use for spread spectrum wireless LANs has become popular.\nIn the United States, three microwave bands have been set aside for unlicensed\nspread spectrum use:902–928 MHz (915-MHz band),2.4–2.4835 GHz (2.4-GHz band),\nand 5.725–5.825 GHz (5.8-GHz band).Of these,the 2.4 GHz is also used in this manner\nin Europe and Japan.The higher the frequency, the higher the potential bandwidth, so\nthe three bands are of increasing order of attractiveness from a capacity point of view.\nIn addition, the potential for interference must be considered. There are a number of\ndevices that operate at around 900 MHz,including cordless telephones,wireless micro-\nphones, and amateur radio.There are fewer devices operating at 2.4 GHz; one notable\nexample is the microwave oven, which tends to have greater leakage of radiation with\nincreasing age.At present there is little competition at the 5.8-GHz-band; however, the\nhigher the frequency band, in general the more expensive the equipment.\n17.3 / IEEE 802.11 ARCHITECTURE AND SERVICES\n17.3 IEEE 802.11 ARCHITECTURE AND SERVICES\nIn 1990, the IEEE 802 Committee formed a new working group, IEEE 802.11, specifi-\ncally devoted to wireless LANs,with a charter to develop a MAC protocol and physical\nmedium specification.The initial interest was in developing a wireless LAN operating\nin the ISM (industrial, scientific, and medical) band. Since that time, the demand for\nWLANs, at different frequencies and data rates, has exploded. Keeping pace with this\ndemand,the IEEE 802.11 working group has issued an ever-expanding list of standards\n(Table 17.1). Table 17.2 briefly defines key terms used in the IEEE 802.11 standard.\nIEEE 802.11 Standards\nMedium access control (MAC): One common MAC for WLAN applications\nIEEE 802.11\nPhysical layer: Infrared at 1 and 2 Mbps\nPhysical layer: 2.4-GHz FHSS at 1 and 2 Mbps\nPhysical layer: 2.4-GHz DSSS at 1 and 2 Mbps\nIEEE 802.11a\nPhysical layer: 5-GHz OFDM at rates from 6 to 54 Mbps\nIEEE 802.11b\nPhysical layer: 2.4-GHz DSSS at 5.5 and 11 Mbps\nIEEE 802.11c\nBridge operation at 802.11 MAC layer\nIEEE 802.11d\nPhysical layer: Extend operation of 802.11 WLANs to new regulatory domains \n(countries)\nIEEE 802.11e\nMAC: Enhance to improve quality of service and enhance security mechanisms\nIEEE 802.11f\nRecommended practices for multivendor access point interoperability\nIEEE 802.11g\nPhysical layer: Extend 802.11b to data rates \nIEEE 802.11h\nPhysical/MAC: Enhance IEEE 802.11a to add indoor and outdoor channel selection and \nto improve spectrum and transmit power management\nIEEE 802.11i\nMAC: Enhance security and authentication mechanisms\nIEEE 802.11j\nPhysical: Enhance IEEE 802.11a to conform to Japanese requirements\nIEEE 802.11k\nRadio resource measurement enhancements to provide interface to higher layers for \nradio and network measurements\nIEEE 802.11m\nMaintenance of IEEE 802.11-1999 standard with technical and editorial corrections\nIEEE 802.11n\nPhysical/MAC: Enhancements to enable higher throughput\nIEEE 802.11p\nPhysical/MAC:Wireless access in vehicular environments\nIEEE 802.11r\nPhysical/MAC: Fast roaming (fast BSS transition)\nIEEE 802.11s\nPhysical/MAC: ESS mesh networking\nIEEE 802.11,2\nRecommended practice for the evaluation of 802.11 wireless performance\nIEEE 802.11u\nPhysical/MAC: Interworking with external networks\nCHAPTER 17 / WIRELESS LANS\nIEEE 802.11 Terminology\nAccess point (AP)\nAny entity that has station functionality and provides access to the distribution system \nvia the wireless medium for associated stations\nBasic service set\nA set of stations controlled by a single coordination function\nCoordination\nThe logical function that determines when a station operating within a BSS is permitted \nto transmit and may be able to receive PDUs\nDistribution system\nA system used to interconnect a set of BSSs and integrated LANs to create an (ESS)\nExtended service \nA set of one or more interconnected BSSs and integrated LANs that appear as a \nsingle BSS to the LLC layer at any station associated with one of these BSSs\nMAC protocol data\nThe unit of data exchanged between two peer MAC entites using the \nunit (MPDU)\nservices of the physical layer\nMAC service data\nInformation that is delivered as a unit between MAC users\nunit (MSDU)\nAny device that contains an IEEE 802.11 conformant MAC and physical layer\nThe Wi-Fi Alliance\nThe first 802.11 standard to gain broad industry acceptance was 802.11b. Although\n802.11b products are all based on the same standard, there is always a concern\nwhether products from different vendors will successfully interoperate.To meet this\nconcern, the Wireless Ethernet Compatibility Alliance (WECA), an industry con-\nsortium, was formed in 1999. This organization, subsequently renamed the Wi-Fi\n(Wireless Fidelity) Alliance, created a test suite to certify interoperability for\n802.11b products. The term used for certified 802.11b products is Wi-Fi. Wi-Fi certi-\nfication has been extended to 802.11g products,. The Wi-Fi Alliance has also devel-\noped a certification process for 802.11a products, called Wi-Fi5. The Wi-Fi Alliance\nis concerned with a range of market areas for WLANs, including enterprise, home,\nand hot spots.\nIEEE 802.11 Architecture\nFigure 17.4 illustrates the model developed by the 802.11 working group. The\nsmallest building block of a wireless LAN is a basic service set (BSS), which con-\nsists of some number of stations executing the same MAC protocol and competing\nfor access to the same shared wireless medium. A BSS may be isolated or it may\nconnect to a backbone distribution system (DS) through an access point (AP).\nThe AP functions as a bridge and a relay point. In a BSS, client stations do not\ncommunicate directly with one another. Rather, if one station in the BSS wants to\ncommunicate with another station in the same BSS, the MAC frame is first sent\nfrom the originating station to the AP, and then from the AP to the destination\nstation. Similarly, a MAC frame from a station in the BSS to a remote station is\nsent from the local station to the AP and then relayed by the AP over the DS on\nits way to the destination station. The BSS generally corresponds to what is\nreferred to as a cell in the literature.The DS can be a switch, a wired network, or a\nwireless network.\n17.3 / IEEE 802.11 ARCHITECTURE AND SERVICES\nservice set\nservice set\nSTA \u0001 station\nAP \u0001 access point\nservice set\nDistribution system\nIEEE 802.x LAN\nFigure 17.4\nIEEE 802.11 Architecture\nWhen all the stations in the BSS are mobile stations, with no connection to\nother BSSs, the BSS is called an independent BSS (IBSS).An IBSS is typically an ad\nhoc network. In an IBSS, the stations all communicate directly, and no AP is\nA simple configuration is shown in Figure 17.4, in which each station belongs\nto a single BSS; that is, each station is within wireless range only of other stations\nwithin the same BSS. It is also possible for two BSSs to overlap geographically, so\nthat a single station could participate in more than one BSS. Further, the association\nbetween a station and a BSS is dynamic. Stations may turn off, come within range,\nand go out of range.\nAn extended service set (ESS) consists of two or more basic service sets inter-\nconnected by a distribution system. Typically, the distribution system is a wired\nbackbone LAN but can be any communications network. The extended service set\nappears as a single logical LAN to the logical link control (LLC) level.\nFigure 17.4 indicates that an access point (AP) is implemented as part of a sta-\ntion; the AP is the logic within a station that provides access to the DS by providing\nDS services in addition to acting as a station.To integrate the IEEE 802.11 architec-\nture with a traditional wired LAN, a portal is used. The portal logic is implemented\nin a device, such as a bridge or router, that is part of the wired LAN and that is\nattached to the DS.\nIEEE 802.11 Services\nIEEE 802.11 defines nine services that need to be provided by the wireless LAN to\nprovide functionality equivalent to that which is inherent to wired LANs. Table 17.3\nlists the services and indicates two ways of categorizing them.\nCHAPTER 17 / WIRELESS LANS\nIEEE 802.11 Services\nUsed to Support\nAssociation\nDistribution system\nMSDU delivery\nAuthentication\nLAN access and security\nDeauthentication\nLAN access and security\nDissassociation\nDistribution system\nMSDU delivery\nDistribution\nDistribution system\nMSDU delivery\nIntegration\nDistribution system\nMSDU delivery\nMSDU delivery\nMSDU delivery\nLAN access and security\nReassocation\nDistribution system\nMSDU delivery\n1. The service provider can be either the station or the DS. Station services are\nimplemented in every 802.11 station, including AP stations. Distribution ser-\nvices are provided between BSSs; these services may be implemented in an AP\nor in another special-purpose device attached to the distribution system.\n2. Three of the services are used to control IEEE 802.11 LAN access and confi-\ndentiality. Six of the services are used to support delivery of MAC service data\nunits (MSDUs) between stations. The MSDU is a block of data passed down\nfrom the MAC user to the MAC layer; typically this is a LLC PDU. If the\nMSDU is too large to be transmitted in a single MAC frame, it may be frag-\nmented and transmitted in a series of MAC frames. Fragmentation is discussed\nin Section 17.4.\nFollowing the IEEE 802.11 document, we next discuss the services in an order\ndesigned to clarify the operation of an IEEE 802.11 ESS network. MSDU delivery,\nwhich is the basic service, has already been mentioned. Services related to security\nare discussed in Section17.6.\nDistribution of Messages within a DS The two services involved with the dis-\ntribution of messages within a DS are distribution and integration. Distribution is the\nprimary service used by stations to exchange MAC frames when the frame must tra-\nverse the DS to get from a station in one BSS to a station in another BSS.For example,\nsuppose a frame is to be sent from station 2 (STA 2) to STA 7 in Figure 17.4.The frame\nis sent from STA 2 to STA 1, which is the AP for this BSS.The AP gives the frame to\nthe DS,which has the job of directing the frame to the AP associated with STA 5 in the\ntarget BSS. STA 5 receives the frame and forwards it to STA 7. How the message is\ntransported through the DS is beyond the scope of the IEEE 802.11 standard.\nIf the two stations that are communicating are within the same BSS, then the\ndistribution service logically goes through the single AP of that BSS.\nThe integration service enables transfer of data between a station on an IEEE\n802.11 LAN and a station on an integrated IEEE 802.x LAN. The term integrated\n17.4 / IEEE 802.11 MEDIUM ACCESS CONTROL\nrefers to a wired LAN that is physically connected to the DS and whose stations\nmay be logically connected to an IEEE 802.11 LAN via the integration service.The\nintegration service takes care of any address translation and media conversion logic\nrequired for the exchange of data.\nAssociation-Related Services The primary purpose of the MAC layer is to\ntransfer MSDUs between MAC entities; this purpose is fulfilled by the distribution\nservice. For that service to function, it requires information about stations within the\nESS that is provided by the association-related services. Before the distribution ser-\nvice can deliver data to or accept data from a station, that station must be associated.\nBefore looking at the concept of association, we need to describe the concept of\nmobility.The standard defines three transition types, based on mobility:\n• No transition: A station of this type is either stationary or moves only within\nthe direct communication range of the communicating stations of a single BSS.\n• BSS transition: This is defined as a station movement from one BSS to another\nBSS within the same ESS. In this case, delivery of data to the station requires\nthat the addressing capability be able to recognize the new location of the sta-\n• ESS transition: This is defined as a station movement from a BSS in one ESS\nto a BSS within another ESS. This case is supported only in the sense that the\nstation can move. Maintenance of upper-layer connections supported by\n802.11 cannot be guaranteed. In fact, disruption of service is likely to occur.\nTo deliver a message within a DS, the distribution service needs to know\nwhere the destination station is located. Specifically, the DS needs to know the iden-\ntity of the AP to which the message should be delivered in order for that message to\nreach the destination station. To meet this requirement, a station must maintain an\nassociation with the AP within its current BSS.Three services relate to this require-\n• Association: Establishes an initial association between a station and an AP.Before\na station can transmit or receive frames on a wireless LAN,its identity and address\nmust be known. For this purpose, a station must establish an association with an\nAP within a particular BSS. The AP can then communicate this information to\nother APs within the ESS to facilitate routing and delivery of addressed frames.\n• Reassociation: Enables an established association to be transferred from one\nAP to another, allowing a mobile station to move from one BSS to another.\n• Disassociation: A notification from either a station or an AP that an existing\nassociation is terminated.A station should give this notification before leaving\nan ESS or shutting down. However, the MAC management facility protects\nitself against stations that disappear without notification.\n17.4 IEEE 802.11 MEDIUM ACCESS CONTROL\nThe IEEE 802.11 MAC layer covers three functional areas: reliable data delivery,\naccess control, and security.This section covers the first two topics.\nCHAPTER 17 / WIRELESS LANS\nReliable Data Delivery\nAs with any wireless network, a wireless LAN using the IEEE 802.11 physical and\nMAC layers is subject to considerable unreliability. Noise, interference, and other\npropagation effects result in the loss of a significant number of frames. Even with\nerror correction codes, a number of MAC frames may not successfully be received.\nThis situation can be dealt with by reliability mechanisms at a higher layer, such as\nTCP. However, timers used for retransmission at higher layers are typically on the\norder of seconds. It is therefore more efficient to deal with errors at the MAC level.\nFor this purpose, IEEE 802.11 includes a frame exchange protocol. When a station\nCHAPTER 17 / WIRELESS LANS\nWait for frame\nto transmit\nWait until current\ntransmission ends\nExponential backoff\nwhile medium idle\nTransmit frame\nTransmit frame\nFigure 17.6\nIEEE 802.11 Medium Access Control Logic\n2. If the medium is busy (either because the station initially finds the medium busy\nor because the medium becomes busy during the IFS idle time), the station\ndefers transmission and continues to monitor the medium until the current trans-\nmission is over.\n3. Once the current transmission is over, the station delays another IFS. If the\nmedium remains idle for this period, then the station backs off a random amount\nof time and again senses the medium. If the medium is still idle, the station may\ntransmit.During the backoff time,if the medium becomes busy,the backoff timer\nis halted and resumes when the medium becomes idle.\n4. If the transmission is unsuccessful, which is determined by the absence of an\nacknowledgement, then it is assumed that a collision has occurred.\nTo ensure that backoff maintains stability, binary exponential backoff,\ndescribed in Chapter 16, is used. Binary exponential backoff provides a means of\n17.4 / IEEE 802.11 MEDIUM ACCESS CONTROL\nhandling a heavy load. Repeated failed attempts to transmit result in longer and\nlonger backoff times, which helps to smooth out the load. Without such a backoff,\nthe following situation could occur:Two or more stations attempt to transmit at the\nsame time, causing a collision. These stations then immediately attempt to retrans-\nmit, causing a new collision.\nThe preceding scheme is refined for DCF to provide priority-based access by\nthe simple expedient of using three values for IFS:\n• SIFS (short IFS): The shortest IFS, used for all immediate response actions, as\nexplained in the following discussion\n• PIFS (point coordination function IFS): A midlength IFS, used by the central-\nized controller in the PCF scheme when issuing polls\n• DIFS (distributed coordination function IFS): The longest IFS, used as a min-\nimum delay for asynchronous frames contending for access\nFigure 17.7a illustrates the use of these time values. Consider first the SIFS.\nAny station using SIFS to determine transmission opportunity has, in effect, the\nhighest priority, because it will always gain access in preference to a station waiting\nan amount of time equal to PIFS or DIFS.The SIFS is used in the following circum-\nDefer access\nImmediate access\nwhen medium is free\nlonger than DIFS\nBusy Medium\nBackoff window\nContention window\nSelect slot using binary exponential backoff\n(a) Basic access method\nPCF (optional)\nContention-free\nVariable length\n(per superframe)\nBusy medium\nPCF (optional)\nContention period\nSuperframe (fixed nominal length)\nSuperframe (fixed nominal length)\nForeshortened actual\nsuperframe period\nasynchronous\ntraffic defers\n(b) PCF superframe construction\nFigure 17.7\nIEEE 802.11 MAC Timing\nCHAPTER 17 / WIRELESS LANS\nCHAPTER 17 / WIRELESS LANS\nWe now look at the three MAC frame types.\nControl Frames Control frames assist in the reliable delivery of data frames.\nThere are six control frame subtypes:\n• Power Save-Poll (PS-Poll): This frame is sent by any station to the station that\nincludes the AP (access point). Its purpose is to request that the AP transmit a\nframe that has been buffered for this station while the station was in power-\nsaving mode.\n• Request to Send (RTS): This is the first frame in the four-way frame exchange\ndiscussed under the subsection on reliable data delivery at the beginning of\nSection 17.3. The station sending this message is alerting a potential destina-\ntion, and all other stations within reception range, that it intends to send a data\nframe to that destination.\n• Clear to Send (CTS): This is the second frame in the four-way exchange. It is\nsent by the destination station to the source station to grant permission to send\na data frame.\nCHAPTER 17 / WIRELESS LANS\nRecall from Chapter 9 that a DSSS system makes use of a chipping code, or\npseudonoise sequence, to spread the data rate and hence the bandwidth of the sig-\nnal. For IEEE 802.11, a Barker sequence is used.\nA Barker sequence is a binary \nof length n with the\nproperty that its autocorrelation values \nAutocorrelation is defined by the following formula:\nare the bits of the sequence.1\nFurther, the Barker property is preserved under the following transformations:\nas well as under compositions of these transformations. Only the following Barker\nsequences are known:\nIEEE 802.11 DSSS uses the 11-chip Barker sequence. Each data binary 1 is\nmapped into the sequence \nand each binary 0 is\nmapped into the sequence \nImportant characteristics of Barker sequences are their robustness against\ninterference and their insensitivity to multipath propagation.\nFrequency-Hopping Spread Spectrum Recall from Chapter 9 that a FHSS\nsystem makes use of a multiple channels, with the signal hopping from one channel\nto another based on a pseudonoise sequence. In the case of the IEEE 802.11\nscheme, 1-MHz channels are used.\nThe details of the hopping scheme are adjustable. For example, the\nminimum hop rate for the United States is 2.5 hops per second. The minimum\n5- + - - + - - - + + +6.\n5+ - + + - + + + - - -6,\n+ + + + + - - + + - + - +\n+ - + + - + + + - - -\n+ + + - - + -\ns1t2 : -s1t2\ns1t2 : 1-12ts1t2 and s1t2 : -s1n - 1 - t2\nƒ t ƒ … 1n - 12.\nƒ R1t2ƒ … 1\n17.5 / IEEE 802.11 PHYSICAL LAYER\nhop distance in frequency is 6 MHz in North America and most of Europe and \n5 MHz in Japan.\nFor modulation, the FHSS scheme uses two-level Gaussian FSK for the 1-Mbps\nsystem. The bits zero and one are encoded as deviations from the current carrier\nfrequency. For 2 Mbps, a four-level GFSK scheme is used, in which four different\ndeviations from the center frequency define the four 2-bit combinations.\nInfrared The IEEE 802.11 infrared scheme is omnidirectional rather than point to\npoint.A range of up to 20 m is possible.The modulation scheme for the 1-Mbps data\nrate is known as 16-PPM (pulse position modulation). In pulse position modulation\n(PPM), the input value determines the position of a narrow pulse relative to the\nclocking time.The advantage of PPM is that it reduces the output power required of\nthe infrared source. For 16-PPM, each group of 4 data bits is mapped into one of the\n16-PPM symbols; each symbol is a string of 16 pulse positions. Each 16-pulse string\nconsists of fifteen 0s and one binary 1. For the 2-Mbps data rate, each group of 2\ndata bits is mapped into one of four 4-pulse-position sequences. Each sequence con-\nsists of three 0s and one binary 1.The actual transmission uses an intensity modula-\ntion scheme, in which the presence of a signal corresponds to a binary 1 and the\nabsence of a signal corresponds to binary 0.\nIEEE 802.11a\nChannel Structure IEEE 802.11a makes use of the frequency band called the\nUniversal Networking Information Infrastructure (UNNI), which is divided into\nthree parts. The UNNI-1 band (5.15 to 5.25 GHz) is intended for indoor use; the\nUNNI-2 band (5.25 to 5.35 GHz) can be used either indoor or outdoor, and the\nUNNI-3 band (5.725 to 5.825 GHz) is for outdoor use.\nIEEE 80211.a has several advantages over IEEE 802.11b/g:\n• IEEE 802.11a utilizes more available bandwidth than 802.11b/g. Each UNNI\nband provides four nonoverlapping channels for a total of 12 across the allo-\ncated spectrum.\n• IEEE 802.11a provides much higher data rates than 802.11b and the same\nmaximum data rate as 802.11g.\n• IEEE 802.11a uses a different, relatively uncluttered frequency spectrum \nCoding and Modulation Unlike the 2.4-GHz specifications, IEEE 802.11 does\nnot use a spread spectrum scheme but rather uses orthogonal frequency division\nmultiplexing (OFDM). Recall from Section 11.2 that OFDM, also called multicar-\nrier modulation, uses multiple carrier signals at different frequencies, sending some\nof the bits on each channel.This is similar to FDM. However, in the case of OFDM,\nall of the subchannels are dedicated to a single data source.\nTo complement OFDM, the specification supports the use of a variety of\nmodulation and coding alternatives. The system uses up to 48 subcarriers that are\nmodulated using BPSK, QPSK, 16-QAM, or 64-QAM. Subcarrier frequency\nCHAPTER 17 / WIRELESS LANS\nPLCP preamble\nPLCP Header\nPLCP Preamble\n(BPSK, r = 1/2)\n(rate is indicated in signal)\n1 Mbps DBPSK\n2 Mbps DQPSK\nVariable number bits at\n2 Mbps DQPSK; 5.5 Mbps DBPSK; 11 Mbps DQPSK\nVariable number of OFDM symbols\n(a) IEEE 802.11a physical PDU\n(b) IEEE 802.11b physical PDU\n12 OFDM symbols\nFigure 17.9\nIEEE 802 Physical-Level Protocol Data Units\nspacing is 0.3125 MHz., and each subcarrier transmits at a rate of 250 kbaud. A\nconvolutional code at a rate of 1/2, 2/3, or 3/4 provides forward error correction.\nThe combination of modulation technique and coding rate determines the data\nPhysical-Layer Frame Structure The primary purpose of the physical layer is\nto transmit medium access control (MAC) protocol data units (MPDUs) as directed\nby the 802.11 MAC layer. The PLCP sublayer provides the framing and signaling\nbits needed for the OFDM transmission and the PDM sublayer performs the actual\nencoding and transmission operation.\nFigure 17.9a illustrates the physical layer frame format. The PLCP Preamble\nfield enables the receiver to acquire an incoming OFDM signal and synchronize the\ndemodulator. Next is the Signal field, which consists of 24 bits encoded as a single\nOFDM symbol. The Preamble and Signal fields are transmitted at 6 Mbps using\nBPSK.The signal field consists of the following subfields:\n• Rate: Specifies the data rate at which the data field portion of the frame is\ntransmitted\n• r: reserved for future use\n• Length: Number of octets in the MAC PDU\n17.5 / IEEE 802.11 PHYSICAL LAYER\n• P: An even parity bit for the 17 bits in the Rate, r, and Length subfields\n• Tail: Consists of 6 zero bits appended to the symbol to bring the convolutional\nencoder to zero state\nThe Data field consists of a variable number of OFDM symbols transmitted at\nthe data rate specified in the Rate subfield. Prior to transmission, all of the bits of\nCHAPTER 17 / WIRELESS LANS\n(discussed in [STAL05]). The output of the mapping, plus the two additional\nbits, forms the input to a QPSK modulator.\nAn optional alternative to CCK is known as packet binary convolutional cod-\ning (PBCC). PBCC provides for potentially more efficient transmission at the cost\nof increased computation at the receiver. PBCC was incorporated into 802.11b in\nanticipation of its need for higher data rates for future enhancements to the stan-\nPhysical-Layer Frame Structure IEEE 802.11b defines two physical-layer\nframe formats, which differ only in the length of the preamble.The long preamble of\n144 bits is the same as used in the original 802.11 DSSS scheme and allows interop-\nerability with other legacy systems.The short preamble of 72 bits provides improved\nthroughput efficiency. Figure 17.9b illustrates the physical layer frame format with\nthe short preamble. The PLCP Preamble field enables the receiver to acquire an\nincoming signal and synchronize the demodulator. It consists of two subfields: a \n56-bit Sync field for synchronization, and a 16-bit start-of-frame delimiter (SFD).\nThe preamble is transmitted at 1 Mbps using differential BPSK and Barker code\nFollowing the preamble is the PLCP Header, which is transmitted at 2 Mbps\nusing DQPSK. It consists of the following subfields:\n• Signal: Specifies the data rate at which the MPDU portion of the frame is\ntransmitted.\n• Service: Only 3 bits of this 8-bit field are used in 802.11b. One bit indicates\nwhether the transmit frequency and symbol clocks use the same local oscilla-\ntor.Another bit indicates whether CCK or PBCC encoding is used.A third bit\nacts as an extension to the Length subfield.\n• Length: Indicates the length of the MPDU field by specifying the number of\nmicroseconds necessary to transmit the MPDU. Given the data rate, the length\nof the MPDU in octets can be calculated. For any data rate over 8 Mbps, the\nlength extension bit from the Service field is needed to resolve a rounding\n• CRC: A 16-bit error detection code used to protect the Signal, Service, and\nLength fields.\nThe MPDU field consists of a variable number of bits transmitted at the\ndata rate specified in the Signal subfield. Prior to transmission, all of the bits of\nCHAPTER 17 / WIRELESS LANS\nAccess and Privacy Services\nIEEE 802.11 defines three services that provide a wireless LAN with these two fea-\n• Authentication: Used to establish the identity of stations to each other. In a\nwired LAN, it is generally assumed that access to a physical connection con-\nveys authority to connect to the LAN.This is not a valid assumption for a wire-\nless LAN, in which connectivity is achieved simply by having an attached\nantenna that is properly tuned. The authentication service is used by stations\nto establish their identity with stations they wish to communicate with. IEEE\n802.11 supports several authentication schemes and allows for expansion of\nthe functionality of these schemes.The standard does not mandate any partic-\nular authentication scheme, which could range from relatively unsecure hand-\nshaking to public-key encryption schemes. However, IEEE 802.11 requires\nmutually acceptable, successful authentication before a station can establish\nan association with an AP.\n• Deauthentication: This service is invoked whenever an existing authentication\nis to be terminated.\nWPA is examined in Chapter 21.\n17.7 RECOMMENDED READING AND WEB SITES\n[PAHL95] and [BANT94] are detailed survey articles on wireless LANs. [KAHN97] provides\ngood coverage of infrared LANs.\n[ROSH04] provides a good up-to-date technical treatment of IEEE 802.11. Another\nuseful book is [BING02]. [OHAR99] is an excellent technical treatment of IEEE 802.11.\nAnother good treatment is [LARO02]. [CROW97] is a good survey article on the 802.11\nstandards but does not cover IEEE 802.11a and IEEE 802.11b. A brief but useful survey of\n802.11 is [MCFA03]. [GEIE01] has a good discussion of IEEE 802.11a. [PETR00] summa-\nrizes IEEE 802.11b. [SHOE02] provides an overview of IEEE 802.11g. [XIAO04] discusses\n17.8 / KEY TERMS, REVIEW QUESTION, AND PROBLEMS\nBantz, D., and Bauchot, F.“Wireless LAN Design Alternatives.” IEEE Network,\nMarch/April 1994.\nBing, B. Wireless Local Area Networks. New York:Wiley, 2002.\nCrow, B., et al. “IEEE 802.11 Wireless Local Area Networks.” IEEE\nCommunications Magazine, September 1997.\nGeier, J. “Enabling Fast Wireless Networks with OFDM.” Communications\nSystem Design, February 2001. (www.csdmag.com)\nKahn, J., and Barry, J.“Wireless Infrared Communications.” Proceedings of the\nIEEE, February 1997.\nLaRocca, J., and LaRocca, R. 802.11 Demystified. New York: McGraw-Hill, 2002.\nMcFarland, B., and Wong, M. ’The Family Dynamics of 802.11” ACM Queue,\nOhara, B., and Petrick, A. IEEE 802.11 Handbook: A Designer’s Companion.\nNew York: IEEE Press, 1999.\nPahlavan, K.; Probert, T.; and Chase, M. “Trends in Local Wireless Networks.”\nIEEE Communications Magazine, March 1995.\nPetrick, A. “IEEE 802.11b—Wireless Ethernet.” Communications System\nDesign, June 2000. www.commsdesign.com\nRoshan, P., and Leary, J. 802.11 Wireless LAN Fundamentals. Indianapolis:\nCisco Press, 2004.\nShoemake, M. “IEEE 802.11g Jells as Applications Mount.” Communications\nSystem Design, April 2002. www.commsdesign.com.\nXiao, Y. “IEEE 802.11e: QoS Provisioning at the MAC Layer.” IEEE\nCommunications Magazine, June 2004.\nRecommended Web sites:\n• Wireless LAN Association: Gives an introduction to the technology,including a discus-\nsion of implementation considerations and case studies from users. Links to related sites.\n• The IEEE 802.11 Wireless LAN Working Group: Contains working group docu-\nments plus discussion archives.\n• Wi-Fi Alliance: An industry group promoting the interoperability of 802.11 products\nwith each other and with Ethernet.\n17.8 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\naccess point (AP)\nad hoc networking\nBarker sequence\nbasic service set (BSS)\ncomplementary code keying\ncoordination function\ndistributed coordination \nfunction (DCF)\nCHAPTER 17 / WIRELESS LANS\nSubscriber A\nSubscriber B\nSubscriber C\n3. Data \u0002 CF-Ack\n2. Data \u0002 CF-Poll\n4. Data \u0002 CF-Ack \u0002 CF-Poll\n5. Data \u0002 CF-ACK\n7. Data (Null)\nFigure 17.11\nConfiguration for Problem 17.1\nReview Questions\nList and briefly define four application areas for wireless LANs.\nList and briefly define key requirements for wireless LANs.\nWhat is the difference between a single-cell and a multiple-cell wireless LAN?\nWhat are some key advantages of infrared LANs?\nWhat are some key disadvantages of infrared LANs?\nList and briefly define three transmission techniques for infrared LANs.\nWhat is the difference between an access point and a portal?\nIs a distribution system a wireless network?\nList and briefly define IEEE 802.11 services.\nHow is the concept of an association related to that of mobility?\nConsider the sequence of actions within a BSS depicted in Figure 17.11. Draw a time-\nline, beginning with a period during which the medium is busy and ending with a\ndistribution system (DS)\nextended service set (ESS)\ninfrared LAN\nLAN extension\nnarrowband microwave LAN\nnomadic access\npoint coordination function\nspread spectrum LAN\nwireless LAN\n17.8 / KEY TERMS, REVIEW QUESTION, AND PROBLEMS\nperiod in which the CF-End is broadcast from the AP. Show the transmission periods\nand the gaps.\nFind the autocorrelation for the 11-bit Barker sequence as a function of \nFor the 16-PPM scheme used for the 1-Mbps IEEE 802.11 infrared standard,\na1. What is the period of transmission (time between bits)?\nFor the corresponding infrared pulse transmission,\na2. What is the average time between pulses (1 values) and the corresponding\naverage rate of pulse transmission?\na3. What is the minimum time between adjacent pulses?\na4. What is the maximum time between pulses?\nb. Repeat (a) for the 4-PPM scheme used for the 2-Mbps infrared standard.\nFor IEEE 802.11a, show how the modulation technique and coding rate determine\nthe data rate.\nBefore proceeding with Part Five, the reader is advised to revisit Chapter 2,\nwhich introduces the concept of a protocol architecture and discusses the key\nelements of a protocol.\nInternet and Transport\nChapter 18 Internet Protocols\nWith the proliferation of networks, internetworking facilities have become\nessential components of network design. Chapter 18 begins with an exami-\nnation of the requirements for an internetworking facility and the various\ndesign approaches that can be taken to satisfy those requirements. The\nremainder of the chapter deals with the use of routers for internetworking.\nThe Internet Protocol (IP) and the new IPv6 are examined.\nChapter 19 Internetwork Operation\nChapter 19 begins with a discussion of multicasting across an internet.\nThen issues of routing and quality of service are explored.\nThe traffic that the Internet and these private internetworks must\ncarry continues to grow and change.The demand generated by traditional\ndata-based applications, such as electronic mail, Usenet news, file trans-\nfer, and remote logon, is sufficient to challenge these systems. But the dri-\nving factors are the heavy use of the World Wide Web, which demands\nreal-time response, and the increasing use of voice, image, and even video\nover internetwork architectures.\nThese internetwork schemes are essentially datagram packet-\nswitching technology with routers functioning as the switches. This tech-\nnology was not designed to handle voice and video and is straining to\nmeet the demands placed on it. While some foresee the replacement of\nthis conglomeration of Ethernet-based LANs, packet-based WANs, and\nIP-datagram-based routers with a seamless ATM transport service from\ndesktop to backbone, that day is far off. Meanwhile, the internetworking\nand routing functions of these networks must be engineered to meet the\nChapter 19 looks at some of the tools and techniques designed to\nmeet the new demand, beginning with a discussion of routing schemes,\nwhich can help smooth out load surges. The remainder of the chapter\nlooks at recent efforts to provide a given level of quality of service (QoS)\nto various applications. The most important elements of this new\napproach are integrated services and differentiated services.\nChapter 20 Transport Protocols\nThe transport protocol is the keystone of the whole concept of a com-\nputer communications architecture. It can also be one of the most com-\nplex of protocols. Chapter 20 examines in detail transport protocol\nmechanisms and then discusses two important examples, TCP and UDP.\nThe bulk of the chapter is devoted to an analysis of the complex set of\nTCP mechanisms and of TCP congestion control schemes.\n18.1 Basic Protocol Functions\n18.2 Principles of Internetworking\n18.3 Internet Protocol Operation\n18.4 Internet Protocol\n18.6 Virtual Private Networks and IP Security\n18.7 Recommended Reading and Web Sites\n18.8 Key Terms, Review Questions, and Problems\nThe map of the London Underground, which can be seen inside every train, has\nbeen called a model of its kind, a work of art. It presents the underground network\nas a geometric grid. The tube lines do not, of course, lie at right angles to one\nanother like the streets of Manhattan. Nor do they branch off at acute angles or\nform perfect oblongs.\n—King Solomon’s Carpet. Barbara Vine (Ruth Rendell)\nKey functions typically performed by a protocol include encapsula-\ntion, fragmentation and reassembly, connection control, ordered\ndelivery, flow control, error control, addressing, and multiplexing.\nAn internet consists of multiple separate networks that are intercon-\nnected by routers. Data are transmitted in packets from a source sys-\ntem to a destination across a path involving multiple networks and\nrouters. Typically, a connectionless or datagram operation is used. A\nrouter accepts datagrams and relays them on toward their destination\nand is responsible for determining the route, much the same way as\npacket-switching nodes operate.\nThe most widely used protocol for internetworking is the Internet\nProtocol (IP). IP attaches a header to upper-layer (e.g., TCP) data to\nform an IP datagram. The header includes source and destination\naddresses, information used for fragmentation and reassembly, a time-\nto-live field, a type-of-service field, and a checksum.\nA next-generation IP, known as IPv6, has been defined. IPv6 provides\nlonger address fields and more functionality than the current IP.\nThe purpose of this chapter is to examine the Internet Protocol, which is\nthe foundation on which all of the internet-based protocols and on which\ninternetworking is based.First,it will be useful to review the basic functions\nof networking protocols. This review serves to summarize some of the\nmaterial introduced previously and to set the stage for the study of inter-\nnet-based protocols in Parts Five and Six.We then move to a discussion of\ninternetworking. Next, the chapter focuses on the two standard internet\nprotocols: IPv4 and IPv6. Finally, the topic of IP security is introduced.\nRefer to Figure 2.5 to see the position within the TCP/IP suite of the\nprotocols discussed in this chapter.\n18.1 BASIC PROTOCOL FUNCTIONS\nBefore turning to a discussion of internet protocols, let us consider a rather small set\nof functions that form the basis of all protocols. Not all protocols have all functions;\nthis would involve a significant duplication of effort. There are, nevertheless, many\ninstances of the same type of function being present in protocols at different levels.\nWe can group protocol functions into the following categories:\n• Encapsulation\n• Fragmentation and reassembly\n• Connection control\n• Ordered delivery\n• Flow control\n• Error control\n• Addressing\n• Multiplexing\n• Transmission services\nEncapsulation\nFor virtually all protocols, data are transferred in blocks, called protocol data units\n(PDUs). Each PDU contains not only data but also control information. Indeed,\nsome PDUs consist solely of control information and no data. The control informa-\ntion falls into three general categories:\n• Address: The address of the sender and/or receiver may be indicated.\n• Error-detecting code: Some sort of frame check sequence is often included for\nerror detection.\n• Protocol control: Additional information is included to implement the proto-\ncol functions listed in the remainder of this section.\nThe addition of control information to data is referred to as encapsulation.\nData are accepted or generated by an entity and encapsulated into a PDU contain-\ning that data plus control information. Typically, the control information is con-\ntained in a PDU header; some data link layer PDUs include a trailer as well.\nNumerous examples of PDUs appear in the preceding chapters [e.g., TFTP (Figure\n2.13), HDLC (Figure 7.7), frame relay (Figure 10.16),ATM (Figure 11.4), LLC (Fig-\nure 15.7), IEEE 802.3 (Figure 16.3), IEEE 802.11 (Figure 17.8)].\nFragmentation and Reassembly1\nA protocol is concerned with exchanging data between two entities. Usually, the\ntransfer can be characterized as consisting of a sequence of PDUs of some bounded\nsize. Whether the application entity sends data in messages or in a continuous\n1The term segmentation is used in OSI-related documents, but in protocol specifications related to the\nTCP/IP protocol suite, the term fragmentation is used.The meaning is the same.\nCHAPTER 18 / INTERNET PROTOCOLS\n18.1 / BASIC PROTOCOL FUNCTIONS\nstream, lower-level protocols typically organize the data into blocks. Further, a \nprotocol may need to divide a block received from a higher layer into multiple\nblocks of some smaller bounded size.This process is called fragmentation.\nThere are a number of motivations for fragmentation, depending on the \ncontext.Among the typical reasons for fragmentation are the following:\n• The communications network may only accept blocks of data up to a certain\nsize. For example, an ATM network is limited to blocks of 53 octets; Ethernet\nimposes a maximum size of 1526 octets.\n• Error control may be more efficient with a smaller PDU size. With smaller\nPDUs, fewer bits need to be retransmitted when a PDU suffers an error.\n• More equitable access to shared transmission facilities, with shorter delay, can\nbe provided. For example, without a maximum block size, one station could\nmonopolize a multipoint medium.\n• A smaller PDU size may mean that receiving entities can allocate smaller\n• An entity may require that data transfer comes to some sort of “closure” from\ntime to time, for checkpoint and restart/recovery operations.\nThere are several disadvantages to fragmentation that argue for making PDUs\nas large as possible:\n• Because each PDU contains a certain amount of control information, smaller\nblocks have a greater percentage of overhead.\n• PDU arrival may generate an interrupt that must be serviced. Smaller blocks\nresult in more interrupts.\n• More time is spent processing smaller, more numerous PDUs.\nAll of these factors must be taken into account by the protocol designer in\ndetermining minimum and maximum PDU size.\nThe counterpart of fragmentation is reassembly. Eventually, the segmented\ndata must be reassembled into messages appropriate to the application level. If\nPDUs arrive out of order, the task is complicated.\nConnection Control\nAn entity may transmit data to another entity in such a way that each PDU is\ntreated independently of all prior PDUs.This is known as connectionless data trans-\nfer; an example is the use of the datagram, described in Chapter 10.While this mode\nis useful, an equally important technique is connection-oriented data transfer, of\nwhich the virtual circuit, also described in Chapter 10, is an example.\nConnection-oriented data transfer is preferred (even required) if stations\nanticipate a lengthy exchange of data and/or certain details of their protocol must\nbe worked out dynamically. A logical association, or connection, is established\nbetween the entities.Three phases occur (Figure 18.1):\n• Connection establishment\n• Data transfer\n• Connection termination\nWith more sophisticated protocols, there may also be connection interrupt and\nrecovery phases to cope with errors and other sorts of interruptions.\nDuring the connection establishment phase, two entities agree to exchange\ndata. Typically, one station will issue a connection request (in connectionless fash-\nion) to the other.A central authority may or may not be involved. In simpler proto-\ncols, the receiving entity either accepts or rejects the request and, in the former case,\nthe connection is considered to be established. In more complex proposals, this\nphase includes a negotiation concerning the syntax, semantics, and timing of the\nprotocol. Both entities must, of course, be using the same protocol. But the protocol\nmay allow certain optional features and these must be agreed upon by means of\nnegotiation. For example, the protocol may specify a PDU size of up to 8000 octets;\none station may wish to restrict this to 1000 octets.\nFollowing connection establishment, the data transfer phase is entered. Dur-\ning this phase both data and control information (e.g., flow control, error control)\nare exchanged. Figure 18.1 shows a situation in which all of the data flow in one\nCHAPTER 18 / INTERNET PROTOCOLS\n18.1 / BASIC PROTOCOL FUNCTIONS\n2The term host refers to any end system attached to a network, such as a PC, workstation, or server.\nnumber PDUs and keep track of both incoming and outgoing numbers. Sequencing\nsupports three main functions: ordered deliver, flow control, and error control.\nSequencing is not found in all connection-oriented protocols. Examples\ninclude frame relay and ATM. However, all connection-oriented protocols \ninclude in the PDU format some way of identifying the connection, which may be a\nunique connection identifier or a combination of source and destination addresses.\nOrdered Delivery\nIf two communicating entities are in different hosts2connected by a network, there\nis a risk that PDUs will not arrive in the order in which they were sent, because\nthey may traverse different paths through the network. In connection-oriented\nprotocols, it is generally required that PDU order be maintained. For example, if a\nfile is transferred between two systems, we would like to be assured that the\nrecords of the received file are in the same order as those of the transmitted file,\nand not shuffled. If each PDU is given a unique number, and numbers are assigned\nsequentially, then it is a logically simple task for the receiving entity to reorder\nreceived PDUs on the basis of sequence number. A problem with this scheme is\nthat, with a finite sequence number field, sequence numbers repeat (modulo some\nmaximum number). Evidently, the maximum sequence number must be greater\nthan the maximum number of PDUs that could be outstanding at any time. In fact,\nthe maximum number may need to be twice the maximum number of PDUs that\ncould be outstanding (e.g., selective-repeat ARQ; see Chapter 7).\nFlow Control\nFlow control is a function performed by a receiving entity to limit the amount or\nrate of data that is sent by a transmitting entity.\nThe simplest form of flow control is a stop-and-wait procedure, in which each\nPDU must be acknowledged before the next can be sent. More efficient protocols\ninvolve some form of credit provided to the transmitter, which is the amount of data\nis an example of this mechanism (Chapter 7).\nFlow control is a good example of a function that must be implemented in sev-\neral protocols. Consider Figure 18.2, which repeats Figure 2.1.The network will need\nto exercise flow control over host A via the network access protocol, to enforce net-\nwork traffic control. At the same time, B’s network access module has finite buffer\nspace and needs to exercise flow control over A’s transmission; it can do this via the\ntransport protocol. Finally, even though B’s network access module can control its\ndata flow, B’s application may be vulnerable to overflow. For example, the applica-\ntion could be hung up waiting for disk access.Thus, flow control is also needed over\nthe application-oriented protocol.\nError Control\nError control techniques are needed to guard against loss or damage of data and\ncontrol information. Typically, error control is implemented as two separate\nCHAPTER 18 / INTERNET PROTOCOLS\nPhysical Physical\nNetwork access\nprotocol #1\nNetwork access\nprotocol #2\nGlobal internet\nSubnetwork attachment\npoint address\nLogical connection\n(e.g., virtual circuit)\nLogical connection\n(TCP connection)\n(service access point)\nFigure 18.2\nTCP/IP Concepts\nfunctions: error detection and retransmission.To achieve error detection, the sender\ninserts an error-detecting code in the transmitted PDU, which is a function of the\nother bits in the PDU. The receiver checks the value of the code on the incoming\nPDU. If an error is detected, the receiver discards the PDU. Upon failing to receive\nidentifiers. For example, in the frame relay protocol (discussed in Chapter 10),\nconnection request packets contain both source and destination address fields.\nAfter a logical connection, called a data link connection, is established, data\nframes contain a data link connection identifier (DLCI) of 10, 16, or 23 bits.\n• Routing: In setting up a connection, a fixed route may be defined.The connec-\ntion identifier serves to identify the route to intermediate systems, such as\npacket-switching nodes, for handling future PDUs.\n• Multiplexing: We address this function in more general terms later. Here we\nnote that an entity may wish to enjoy more than one connection simultane-\nously.Thus, incoming PDUs must be identified by connection identifier.\n• Use of state information: Once a connection is established, the end systems\ncan maintain state information relating to the connection. This enables such\nfunctions as flow control and error control using sequence numbers. We see\nexamples of this with HDLC (Chapter 7) and IEEE 802.11 (Chapter 17).\nFigure 18.2 shows several examples of connections. The logical connection\nbetween router J and host B is at the network level. For example, if network 2 is a\nframe relay network, then this logical connection would be a data link connection.\nAt a higher level, many transport-level protocols, such as TCP, support logical con-\nnections between users of the transport service. Thus, TCP can maintain a connec-\ntion between two ports on different systems.\nAnother addressing concept is that of addressing mode. Most commonly, an\naddress refers to a single system or port; in this case it is referred to as an individual or\nunicast address. It is also possible for an address to refer to more than one entity or\nport.Such an address identifies multiple simultaneous recipients for data.For example,\na user might wish to send a memo to a number of individuals.The network control cen-\nter may wish to notify all users that the network is going down.An address for multiple\nrecipients may be broadcast, intended for all entities within a domain, or multicast,\nintended for a specific subset of entities. Table 18.1 illustrates the possibilities.\nMultiplexing\nRelated to the concept of addressing is that of multiplexing. One form of multi-\nplexing is supported by means of multiple connections into a single system. For\nCHAPTER 18 / INTERNET PROTOCOLS\n18.1 / BASIC PROTOCOL FUNCTIONS\nexample, with frame relay, there can be multiple data link connections terminating\nin a single end system; we can say that these data link connections are multiplexed\nover the single physical interface between the end system and the network. Multi-\nplexing can also be accomplished via port names, which also permit multiple simul-\ntaneous connections. For example, there can be multiple TCP connections\nterminating in a given system, each connection supporting a different pair of ports.\nMultiplexing is used in another context as well, namely the mapping of connec-\ntions from one level to another. Consider again Figure 18.2. Network 1 might provide\na connection-oriented service. For each process-to-process connection established at\nthe next higher level, a data link connection could be created at the network access\nlevel.This is a one-to-one relationship, but it need not be so. Multiplexing can be used\nin one of two directions. Upward multiplexing, or inward multiplexing, occurs when\nmultiple higher-level connections are multiplexed on, or share, a single lower-level\nconnection.This may be needed to make more efficient use of the lower-level service\nor to provide several higher-level connections in an environment where only a single\nlower-level connection exists. Downward multiplexing, or splitting, means that a sin-\ngle higher-level connection is built on top of multiple lower-level connections, and\nthe traffic on the higher connection is divided among the various lower connections.\nThis technique may be used to provide reliability, performance, or efficiency.\nTransmission Services\nA protocol may provide a variety of additional services to the entities that use it.We\nmention here three common examples:\n• Priority: Certain messages, such as control messages, may need to get through\nto the destination entity with minimum delay. An example would be a termi-\nnate-connection request. Thus, priority could be assigned on a message basis.\nAdditionally, priority could be assigned on a connection basis.\n• Quality of service: Certain classes of data may require a minimum throughput\nor a maximum delay threshold.\n• Security: Security mechanisms, restricting access, may be invoked.\nAll of these services depend on the underlying transmission system and any\nintervening lower-level entities. If it is possible for these services to be provided\nfrom below, the protocol can be used by the two entities to exercise those services.\nAddressing Modes\nDestination\nNetwork Address\nSystem Address\nPort/SAP Address\n18.2 PRINCIPLES OF INTERNETWORKING\nPacket-switching and packet-broadcasting networks grew out of a need to allow the\ncomputer user to have access to resources beyond that available in a single system.\nIn a similar fashion, the resources of a single network are often inadequate to meet\nusers’ needs. Because the networks that might be of interest exhibit so many differ-\nences, it is impractical to consider merging them into a single network. Rather, what\nis needed is the ability to interconnect various networks so that any two stations on\nany of the constituent networks can communicate.\nTable 18.2 lists some commonly used terms relating to the interconnection of\nnetworks, or internetworking. An interconnected set of networks, from a user’s\npoint of view, may appear simply as a larger network. However, if each of the con-\nstituent networks retains its identity and special mechanisms are needed for com-\nmunicating across multiple networks, then the entire configuration is often referred\nto as an internet.\nEach constituent network in an internet supports communication among the\ndevices attached to that network; these devices are referred to as end systems (ESs).\nIn addition, networks are connected by devices referred to in the ISO documents as\nintermediate systems (ISs). Intermediate systems provide a communications path\nCHAPTER 18 / INTERNET PROTOCOLS\nInternetworking Terms\nCommunication Network\nA facility that provides a data transfer service among devices attached to the network.\nA collection of communication networks interconnected by bridges and/or routers.\nAn internet used by a single organization that provides the key Internet applications, especially the World\nWide Web.An intranet operates within the organization for internal purposes and can exist as an isolated,\nself-contained internet, or may have links to the Internet.\nRefers to a constituent network of an internet.This avoids ambiguity because the entire internet, from a user’s\npoint of view, is a single network.\nEnd System (ES)\nA device attached to one of the networks of an internet that is used to support end-user applications or \nIntermediate System (IS)\nA device used to connect two networks and permit communication between end systems attached to different\nAn IS used to connect two LANs that use similar LAN protocols.The bridge acts as an address filter, picking\nup packets from one LAN that are intended for a destination on another LAN and passing those packets on.\nprotocol present in each router and each end system of the network. The router operates at layer 3 of the\n18.2 / PRINCIPLES OF INTERNETWORKING\nand perform the necessary relaying and routing functions so that data can be\nexchanged between devices attached to different networks in the internet.\nTwo types of ISs of particular interest are bridges and routers. The differ-\nences between them have to do with the types of protocols used for the internet-\nworking logic. In essence, a bridge operates at layer 2 of the open systems\ninterconnection (OSI) seven-layer architecture and acts as a relay of frames\nbetween similar networks; bridges are discussed in Chapter 15. A router operates\nat layer 3 of the OSI architecture and routes packets between potentially different\nnetworks. Both the bridge and the router assume that the same upper-layer proto-\ncols are in use.\nWe begin our examination of internetworking with a discussion of the basic\nprinciples of internetworking. We then examine the most important architectural\napproach to internetworking: the connectionless router.\nRequirements\nThe overall requirements for an internetworking facility are as follows (we refer to\nFigure 18.2 as an example throughout):\n1. Provide a link between networks. At minimum, a physical and link control\nconnection is needed. (Router J has physical links to N1 and N2, and on each\nlink there is a data link protocol.)\n2. Provide for the routing and delivery of data between processes on different net-\nworks. (Application X on host A exchanges data with application X on host B.)\n3. Provide an accounting service that keeps track of the use of the various networks\nand routers and maintains status information.\n4. Provide the services just listed in such a way as not to require modifications to the\nnetworking architecture of any of the constituent networks. This means that the\ninternetworking facility must accommodate a number of differences among net-\nworks.These include\n• Different addressing schemes: The networks may use different endpoint\nnames and addresses and directory maintenance schemes. Some form of\nglobal network addressing must be provided, as well as a directory service.\n(Hosts A and B and router J have globally unique IP addresses.)\n• Different maximum packet size: Packets from one network may have to be\nbroken up into smaller pieces for another. This process is referred to as\nfragmentation. (N1 and N2 may set different upper limits on packet sizes.)\n• Different network access mechanisms:\nThe network access mechanism\nbetween station and network may be different for stations on different net-\nworks. (For example, N1 may be a frame relay network and N2 an Ethernet\n• Different timeouts: Typically, a connection-oriented transport service will\nCHAPTER 18 / INTERNET PROTOCOLS\n18.3 / INTERNET PROTOCOL OPERATION\n18.3 INTERNET PROTOCOL OPERATION\nIn this section, we examine the essential functions of an internetwork protocol.\nFor convenience, we refer specifically to the Internet Standard IPv4, but the\nnarrative in this section applies to any connectionless Internet Protocol, such\nFrame relay\nt1, t6, t7, t10, t11, t16\n\u0001 TCP header\n\u0001 IP header\n\u0001 LLC header\nMACi-H \u0001 MAC header\nMACi-T \u0001 MAC trailer\n\u0001 Frame relay header\n\u0001 Frame relay trailer\nMAC1-H LLC1-H\nMAC2-H LLC2-H\nFigure 18.3\nExample of Internet Protocol Operation\nOperation of a Connectionless Internetworking Scheme\nIP provides a connectionless, or datagram, service between end systems.There are a\nnumber of advantages to this approach:\n• A connectionless internet facility is flexible. It can deal with a variety of net-\nworks, some of which are themselves connectionless. In essence, IP requires\nvery little from the constituent networks.\n• A connectionless internet service can be made highly robust. This is basically\nthe same argument made for a datagram network service versus a virtual cir-\ncuit service. For a further discussion, see Section 10.5.\n• A connectionless internet service is best for connectionless transport proto-\ncols, because it does not impose unnecessary overhead.\nFigure 18.3 depicts a typical example using IP, in which two LANs are intercon-\nnected by a frame relay WAN.The figure depicts the operation of the Internet Protocol\nfor data exchange between host A on one LAN (network 1) and host B on another\nLAN (network 2) through the WAN. The figure shows the protocol architecture and\nformat of the data unit at each stage.The end systems and routers must all share a com-\nmon Internet Protocol. In addition, the end systems must share the same protocols\nabove IP.The intermediate routers need only implement up through IP.\nThe IP at A receives blocks of data to be sent to B from a higher layers of soft-\nware in A (e.g., TCP or UDP). IP attaches a header (at time ) specifying, among\nother things, the global internet address of B. That address is logically in two parts:\nnetwork identifier and end system identifier. The combination of IP header and\nupper-level data is called an Internet Protocol data unit (PDU), or simply a data-\ngram.The datagram is then encapsulated with the LAN protocol (LLC header at \nMAC header and trailer at ) and sent to the router, which strips off the LAN fields\nto read the IP header \nThe router then encapsulates the datagram with the\nframe relay protocol fields \nand transmits it across the WAN to another router.\nThis router strips off the frame relay fields and recovers the datagram, which it then\nwraps in LAN fields appropriate to LAN 2 and sends it to B.\nLet us now look at this example in more detail. End system A has a datagram to\ntransmit to end system B;the datagram includes the internet address of B.The IP mod-\nule in A recognizes that the destination (B) is on another network.So the first step is to\nsend the data to a router, in this case router X.To do this, IP passes the datagram down\nto the next lower layer (in this case LLC) with instructions to send it to router X. LLC\nin turn passes this information down to the MAC layer, which inserts the MAC-level\naddress of router X into the MAC header. Thus, the block of data transmitted onto\nLAN 1 includes data from a layer or layers above TCP, plus a TCP header, an IP\nheader, an LLC header, and a MAC header and trailer (time \nin Figure 18.3).\nNext, the packet travels through network 1 to router X. The router removes\nMAC and LLC fields and analyzes the IP header to determine the ultimate destina-\ntion of the data, in this case B. The router must now make a routing decision. There\nare three possibilities:\n1. The destination station B is connected directly to one of the networks to which the\nrouter is attached. If so, the router sends the datagram directly to the destination.\nCHAPTER 18 / INTERNET PROTOCOLS\n18.3 / INTERNET PROTOCOL OPERATION\n2. To reach the destination, one or more additional routers must be traversed.\nIf so, a routing decision must be made: To which router should the datagram\nbe sent? In both cases 1 and 2, the IP module in the router sends the datagram\ndown to the next lower layer with the destination network address. Please\nnote that we are speaking here of a lower-layer address that refers to this\n3. The router does not know the destination address. In this case, the router\nreturns an error message to the source of the datagram.\nIn this example, the data must pass through router Y before reaching the desti-\nnation. So router X constructs a new frame by appending a frame relay (LAPF)\nheader and trailer to the IP datagram. The frame relay header indicates a logical\nconnection to router Y. When this frame arrives at router Y, the frame header and\ntrailer are stripped off. The router determines that this IP data unit is destined for B,\nwhich is connected directly to a network to which this router is attached. The router\ntherefore creates a frame with a layer-2 destination address of B and sends it out\nonto LAN 2. The data finally arrive at B, where the LAN and IP headers can be\nstripped off.\nAt each router, before the data can be forwarded, the router may need to frag-\nment the datagram to accommodate a smaller maximum packet size limitation on\nthe outgoing network. If so, the data unit is split into two or more fragments, each of\nwhich becomes an independent IP datagram. Each new data unit is wrapped in a\nlower-layer packet and queued for transmission. The router may also limit the\nlength of its queue for each network to which it attaches so as to avoid having a slow\nnetwork penalize a faster one. Once the queue limit is reached, additional data units\nare simply dropped.\nThe process just described continues through as many routers as it takes for\nthe data unit to reach its destination. As with a router, the destination end system\nrecovers the IP datagram from its network wrapping. If fragmentation has occurred,\nthe IP module in the destination end system buffers the incoming data until the\nentire original data field can be reassembled. This block of data is then passed to a\nhigher layer in the end system.3\nThis service offered by IP is an unreliable one. That is, IP does not guaran-\ntee that all data will be delivered or that the data that are delivered will arrive in\nthe proper order. It is the responsibility of the next higher layer (e.g., TCP) to\nrecover from any errors that occur. This approach provides for a great deal of\nflexibility.\nWith the Internet Protocol approach, each unit of data is passed from router to\nrouter in an attempt to get from source to destination. Because delivery is not\nguaranteed, there is no particular reliability requirement on any of the networks.\nThus, the protocol will work with any combination of network types. Because the\nsequence of delivery is not guaranteed, successive data units can follow different\npaths through the internet. This allows the protocol to react to both congestion and\nfailure in the internet by changing routes.\nCHAPTER 18 / INTERNET PROTOCOLS\n(a) Packet-switching network architecture\n(b) Internetwork architecture\nFigure 18.4\nThe Internet as a Network (based on [HIND83])\n18.3 / INTERNET PROTOCOL OPERATION\nto the packet-switching nodes (P1, P2, P3) in the network, and the networks (N1, N2,\nN3) in the internet correspond to the transmission links (T1,T2,T3) in the networks.\nThe routers perform essentially the same functions as packet-switching nodes and\nuse the intervening networks in a manner analogous to transmission links.\nRouting For the purpose of routing, each end system and router maintains a rout-\ning table that lists, for each possible destination network, the next router to which\nthe internet datagram should be sent.\nThe routing table may be static or dynamic.A static table, however, could con-\ntain alternate routes if a particular router is unavailable. A dynamic table is more\nflexible in responding to both error and congestion conditions. In the Internet, for\nexample, when a router goes down, all of its neighbors will send out a status report,\nallowing other routers and stations to update their routing tables. A similar scheme\ncan be used to control congestion. Congestion control is particularly important\nbecause of the mismatch in capacity between local and wide area networks. Chapter\n19 discusses routing protocols.\nRouting tables may also be used to support other internetworking services,\nsuch as security and priority. For example, individual networks might be classified to\nhandle data up to a given security classification.The routing mechanism must assure\nthat data of a given security level are not allowed to pass through networks not\ncleared to handle such data.\nAnother routing technique is source routing. The source station specifies the\nroute by including a sequential list of routers in the datagram. This, again, could be\nuseful for security or priority requirements.\nFinally, we mention a service related to routing: route recording. To record a\nroute, each router appends its internet address to a list of addresses in the datagram.\nThis feature is useful for testing and debugging purposes.\nDatagram Lifetime If dynamic or alternate routing is used, the potential exists\nfor a datagram to loop indefinitely through the internet. This is undesirable for two\nreasons. First, an endlessly circulating datagram consumes resources. Second, we will\nsee in Chapter 20 that a transport protocol may depend on the existence of an upper\nbound on datagram lifetime.To avoid these problems, each datagram can be marked\nwith a lifetime. Once the lifetime expires, the datagram is discarded.\nA simple way to implement lifetime is to use a hop count. Each time that a\ndatagram passes through a router, the count is decremented. Alternatively, the life-\ntime could be a true measure of time. This requires that the routers must somehow\nknow how long it has been since the datagram or fragment last crossed a router, to\nknow by how much to decrement the lifetime field. This would seem to require\nsome global clocking mechanism.The advantage of using a true time measure is that\nit can be used in the reassembly algorithm, described next.\nFragmentation and Reassembly Individual networks within an internet may\nspecify different maximum packet sizes. It would be inefficient and unwieldy to try\nto dictate uniform packet size across networks. Thus, routers may need to fragment\nincoming datagrams into smaller pieces, called segments or fragments, before trans-\nmitting on to the next network.\nIf datagrams can be fragmented (perhaps more than once) in the course of\ntheir travels, the question arises as to where they should be reassembled.The easiest\nsolution is to have reassembly performed at the destination only. The principal dis-\nadvantage of this approach is that fragments can only get smaller as data move\nthrough the internet. This may impair the efficiency of some networks. However, if\nintermediate router reassembly is allowed, the following disadvantages result:\n1. Large buffers are required at routers, and there is the risk that all of the buffer\nspace will be used up storing partial datagrams.\n2. All fragments of a datagram must pass through the same router. This inhibits\nthe use of dynamic routing.\nIn IP, datagram fragments are reassembled at the destination end system. The\nIP fragmentation technique uses the following information in the IP header:\n• Data Unit Identifier (ID)\n• Data Length4\n• More Flag\nThe ID is a means of uniquely identifying an end-system-originated datagram.\nIn IP, it consists of the source and destination addresses, a number that corresponds\nto the protocol layer that generated the data (e.g., TCP), and an identification sup-\nplied by that protocol layer. The Data Length is the length of the user data field in\noctets, and the Offset is the position of a fragment of user data in the data field of the\noriginal datagram, in multiples of 64 bits.\nThe source end system creates a datagram with a Data Length equal to the entire\nlength of the data field, with Offset \u0001 0, and a More Flag set to 0 (false).To fragment a\nlong datagram into two pieces, an IP module in a router performs the following tasks:\n1. Create two new datagrams and copy the header fields of the incoming data-\ngram into both.\n2. Divide the incoming user data field into two portions along a 64-bit boundary\n(counting from the beginning), placing one portion in each new datagram. The\nfirst portion must be a multiple of 64 bits (8 octets).\n3. Set the Data Length of the first new datagram to the length of the inserted data,\nand set More Flag to 1 (true).The Offset field is unchanged.\n4. Set the Data Length of the second new datagram to the length of the inserted\ndata, and add the length of the first data portion divided by 8 to the Offset\nfield.The More Flag remains the same.\nFigure 18.5 gives an example in which two fragments are created from an\noriginal IP datagram. The procedure is easily generalized to an n-way split. In this\nexample, the payload of the original IP datagram is a TCP segment, consisting of a\n4In the IPv6 header, there is a Payload Length field that corresponds to Data Length in this discussion. In\nthe IPv4 header, there is Total Length field whose value is the length of the header plus data; the data\nlength must be calculated by subtracting the header length.\nCHAPTER 18 / INTERNET PROTOCOLS\n18.3 / INTERNET PROTOCOL OPERATION\nTCP header and application data.The IP header from the original datagram is used\nin both fragments, with the appropriate changes to the fragmentation-related fields.\nNote that the first fragment contains the TCP header; this header is not replicated in\nthe second fragment, because all of the IP payload, including the TCP header is\nCHAPTER 18 / INTERNET PROTOCOLS\n18.4 / INTERNET PROTOCOL\nIP provides two service primitives at the interface to the next higher layer.The\nSend primitive is used to request transmission of a data unit. The Deliver primitive\nis used by IP to notify a user of the arrival of a data unit.The parameters associated\nwith the two primitives are as follows:\n• Source address: Internetwork address of sending IP entity.\n• Destination address: Internetwork address of destination IP entity.\n• Protocol: Recipient protocol entity (an IP user, such as TCP).\n• Type-of-service indicators: Used to specify the treatment of the data unit in its\ntransmission through component networks.\n• Identification: Used in combination with the source and destination addresses\nand user protocol to identify the data unit uniquely. This parameter is needed\nfor reassembly and error reporting.\n• Don’t fragment identifier: Indicates whether IP can fragment data to accom-\nplish delivery.\n• Time to live: Measured in seconds.\n• Data length: Length of data being transmitted.\n• Option data: Options requested by the IP user.\n• Data: User data to be transmitted.\nThe identification, don’t fragment identifier, and time to live parameters\nare present in the Send primitive but not in the Deliver primitive. These three\nparameters provide instructions to IP that are not of concern to the recipient IP\nThe options parameter allows for future extensibility and for inclusion of\nparameters that are usually not invoked. The currently defined options are as\n• Security: Allows a security label to be attached to a datagram.\n• Source routing: A sequenced list of router addresses that specifies the route to\nbe followed. Routing may be strict (only identified routers may be visited) or\nloose (other intermediate routers may be visited).\n• Route recording: A field is allocated to record the sequence of routers visited\nby the datagram.\n• Stream identification: Names reserved resources used for stream service. This\nservice provides special handling for volatile periodic traffic (e.g., voice).\n• Timestamping: The source IP entity and some or all intermediate routers add\na timestamp (precision to milliseconds) to the data unit as it goes by.\nInternet Protocol\nThe protocol between IP entities is best described with reference to the IP datagram\nformat, shown in Figure 18.6.The fields are as follows:\n• Version (4 bits): Indicates version number, to allow evolution of the protocol;\nthe value is 4.\n• Internet Header Length (IHL) (4 bits): Length of header in 32-bit words. The\nminimum value is five, for a minimum header length of 20 octets.\n• DS/ECN (8 bits): Prior to the introduction of differentiated services, this\nfield was referred to as the Type of Service field and specified reliability,\nprecedence, delay, and throughput parameters. This interpretation has now\nbeen superseded. The first six bits of this field are now referred to as the DS\n(Differentiated Services) field, discussed in Chapter 19. The remaining 2 bits\nare reserved for an ECN (Explicit Congestion Notification) field, currently\nin the process of standardization. The ECN field provides for explicit signal-\ning of congestion in a manner similar to that discussed for frame relay\n(Section 13.5).\n• Total Length (16 bits): Total datagram length,including header plus data,in octets.\n• Identification (16 bits): A sequence number that, together with the source\naddress, destination address, and user protocol, is intended to identify a data-\ngram uniquely. Thus, this number should be unique for the datagram’s source\naddress, destination address, and user protocol for the time during which the\ndatagram will remain in the internet.\n• Flags (3 bits): Only two of the bits are currently defined. The More bit is used\nfor fragmentation and reassembly, as previously explained. The Don’t Frag-\nment bit prohibits fragmentation when set.This bit may be useful if it is known\nthat the destination does not have the capability to reassemble fragments.\nHowever, if this bit is set, the datagram will be discarded if it exceeds the\nmaximum size of an en route network. Therefore, if the bit is set, it may be\nadvisable to use source routing to avoid networks with small maximum\npacket size.\n• Fragment Offset (13 bits): Indicates where in the original datagram this\nfragment belongs, measured in 64-bit units. This implies that fragments other\nTotal Length\nIdentification\nFragment Offset\nTime to Live\nHeader Checksum\nOptions \u0002 Padding\nSource Address\nDestination Address\nFigure 18.6\nIPv4 Header\nCHAPTER 18 / INTERNET PROTOCOLS\n18.4 / INTERNET PROTOCOL\nthan the last fragment must contain a data field that is a multiple of 64 bits\n• Time to Live (8 bits): Specifies how long, in seconds, a datagram is allowed to\nremain in the internet. Every router that processes a datagram must decrease\nthe TTL by at least one, so the TTL is similar to a hop count.\n• Protocol (8 bits): Indicates the next higher level protocol that is to receive the\ndata field at the destination; thus, this field identifies the type of the next header\nin the packet after the IP header. Example values are \nA complete list is maintained at http://www.iana.org/ \nassignments/protocol-numbers.\n• Header Checksum (16 bits): An error-detecting code applied to the header\nonly. Because some header fields may change during transit (e.g.,Time to Live,\nfragmentation-related fields), this is reverified and recomputed at each router.\nThe checksum is formed by taking the ones complement of the 16-bit ones\ncomplement addition of all 16-bit words in the header. For purposes of com-\nputation, the checksum field is itself initialized to a value of zero.5\n• Source Address (32 bits): Coded to allow a variable allocation of bits to\nspecify the network and the end system attached to the specified network, as\ndiscussed subsequently.\n• Destination Address (32 bits): Same characteristics as source address.\n• Options (variable): Encodes the options requested by the sending user.\n• Padding (variable): Used to ensure that the datagram header is a multiple of\n32 bits in length.\n• Data (variable): The data field must be an integer multiple of 8 bits in length.\nThe maximum length of the datagram (data field plus header) is 65,535 octets.\nIt should be clear how the IP services specified in the Send and Deliver primi-\ntives map into the fields of the IP datagram.\nIP Addresses\nThe source and destination address fields in the IP header each contain a 32-bit global\ninternet address, generally consisting of a network identifier and a host identifier.\nNetwork Classes The address is coded to allow a variable allocation of bits to\nspecify network and host, as depicted in Figure 18.7. This encoding provides\nflexibility in assigning addresses to hosts and allows a mix of network sizes on an\ninternet. The three principal network classes are best suited to the following\nconditions:\n• Class A: Few networks, each with many hosts\n• Class B: Medium number of networks, each with a medium number of hosts\n• Class C: Many networks, each with a few hosts\n17; ICMP = 1.\nTCP = 6; UDP =\nCHAPTER 18 / INTERNET PROTOCOLS\n18.4 / INTERNET PROTOCOL\nand routing complexity. One approach to this problem is to assign a single network\nnumber to all of the LANs at a site. From the point of view of the rest of the inter-\nnet, there is a single network at that site, which simplifies addressing and routing.To\nallow the routers within the site to function properly, each LAN is assigned a subnet\nnumber.The host portion of the internet address is partitioned into a subnet number\nand a host number to accommodate this new level of addressing.\nWithin the subnetted network, the local routers must route on the basis of an\nextended network number consisting of the network portion of the IP address and the\nsubnet number. The bit positions containing this extended network number are indi-\ncated by the address mask. The use of the address mask allows the host to determine\nwhether an outgoing datagram is destined for a host on the same LAN (send directly)\nor another LAN (send datagram to router). It is assumed that some other means (e.g.,\nmanual configuration) are used to create address masks and make them known to the\nlocal routers.\nTable 18.3a shows the calculations involved in the use of a subnet mask. Note\nthat the effect of the subnet mask is to erase the portion of the host field that refers\nto an actual host on a subnet. What remains is the network number and the subnet\nnumber. Figure 18.8 shows an example of the use of subnetting. The figure shows a\nlocal complex consisting of three LANs and two routers. To the rest of the internet,\nthis complex is a single network with a Class C address of the form 192.228.17.x,\nwhere the leftmost three octets are the network number and the rightmost octet\ncontains a host number x. Both routers R1 and R2 are configured with a subnet\nIP Addresses and Subnet Masks [STEI95]\n(a) Dotted decimal and binary representations of IP address and subnet masks\nBinary Representation\nDotted Decimal\n11000000.11100100.00010001.00111001\n192.228.17.57\nSubnet mask\n11111111.11111111.11111111.11100000\n255.255.255.224\nBitwise AND of address and mask \n11000000.11100100.00010001.00100000\n192.228.17.32\n(resultant network/subnet number)\nSubnet number\n11000000.11100100.00010001.001\nHost number\n00000000.00000000.00000000.00011001\n(b) Default subnet masks\nBinary Representation\nDotted Decimal\nClass A default mask\n11111111.00000000.00000000.00000000\nExample Class A mask\n11111111.11000000.00000000.00000000\n255.192.0.0\nClass B default mask\n11111111.11111111.00000000.00000000\n255.255.0.0\nExample Class B mask\n11111111.11111111.11111000.00000000\n255.255.248.0\nClass C default mask\n11111111.11111111.11111111.00000000\n255. 255. 255.0\nExample Class C mask\n11111111.11111111.11111111.11111100\n255. 255. 255.252\nmask with the value 255.255.255.224 (see Table 18.3a). For example, if a datagram\nwith the destination address 192.228.17.57 arrives at R1 from either the rest of the\ninternet or from LAN Y, R1 applies the subnet mask to determine that this address\nrefers to subnet 1, which is LAN X, and so forwards the datagram to LAN X. Simi-\nlarly, if a datagram with that destination address arrives at R2 from LAN Z, R2\napplies the mask and then determines from its forwarding database that datagrams\ndestined for subnet 1 should be forwarded to R1. Hosts must also employ a subnet\nmask to make routing decisions.\nThe default subnet mask for a given class of addresses is a null mask (Table\n18.3b), which yields the same network and host number as the non-subnetted address.\nInternet Control Message Protocol (ICMP)\nThe IP standard specifies that a compliant implementation must also implement\nICMP (RFC 792). ICMP provides a means for transferring messages from\nrouters and other hosts to a host. In essence, ICMP provides feedback about\nproblems in the communication environment. Examples of its use are when a\ndatagram cannot reach its destination, when the router does not have the buffer-\ning capacity to forward a datagram, and when the router can direct the station to\nsend traffic on a shorter route. In most cases, an ICMP message is sent in\nresponse to a datagram, either by a router along the datagram’s path or by the\nintended destination host.\nCHAPTER 18 / INTERNET PROTOCOLS\nNet ID/Subnet ID: 192.228.17.32\nSubnet number: 1\nNet ID/Subnet ID: 192.228.17.64\nSubnet number: 2\nNet ID/Subnet ID: 192.228.17.96\nSubnet number: 3\nIP Address: 192.228.17.97\nHost number: 1\nIP Address: 192.228.17.65\nHost number: 1\nIP Address: 192.228.17.33\nHost number: 1\nIP Address: 192.228.17.57\nHost number: 25\nFigure 18.8\nExample of Subnetworking\n18.4 / INTERNET PROTOCOL\nAlthough ICMP is, in effect, at the same level as IP in the TCP/IP architecture,\nit is a user of IP.An ICMP message is constructed and then passed down to IP, which\nencapsulates the message with an IP header and then transmits the resulting data-\ngram in the usual fashion. Because ICMP messages are transmitted in IP datagrams,\ntheir delivery is not guaranteed and their use cannot be considered reliable.\nFigure 18.9 shows the format of the various ICMP message types. An ICMP\nmessage starts with a 64-bit header consisting of the following:\n• Type (8 bits): Specifies the type of ICMP message.\n• Code (8 bits): Used to specify parameters of the message that can be encoded\nin one or a few bits.\n• Checksum (16 bits): Checksum of the entire ICMP message. This is the same\nchecksum algorithm used for IP.\n• Parameters (32 bits): Used to specify more lengthy parameters.\nThese fields are generally followed by additional information fields that fur-\nther specify the content of the message.\nIn those cases in which the ICMP message refers to a prior datagram, the\ninformation field includes the entire IP header plus the first 64 bits of the data field\nof the original datagram.This enables the source host to match the incoming ICMP\nmessage with the prior datagram. The reason for including the first 64 bits of the\ndata field is that this will enable the IP module in the host to determine which\nIPHeader \u0002 64 bits of original datagram\n(a) Destination unreachable; time exceeded; source quench\nGateway Internet address\nIP Header \u0002 64 bits of original datagram\n(c) Redirect\n(b) Parameter problem\nIP Header \u0002 64 bits of original datagram\n(d) Echo, echo reply\nSequence number\nOptional data\n(h) Address mask reply\nSequence number\nAddress mask\n(g) Address mask request\nSequence number\n(e) Timestamp\nSequence number\nOriginate timestamp\n(f) Timestamp reply\nSequence number\nOriginate timestamp\nReceive timestamp\nTransmit timestamp\nFigure 18.9\nICMP Message Formats\nupper-level protocol or protocols were involved. In particular, the first 64 bits would\ninclude a portion of the TCP header or other transport-level header.\nThe destination unreachable message covers a number of contingencies. A\nrouter may return this message if it does not know how to reach the destination net-\nwork. In some networks, an attached router may be able to determine if a particular\nhost is unreachable and returns the message. The destination host itself may return\nthis message if the user protocol or some higher-level service access point is\nunreachable. This could happen if the corresponding field in the IP header was set\nincorrectly. If the datagram specifies a source route that is unusable, a message is\nreturned. Finally, if a router must fragment a datagram but the Don’t Fragment flag\nis set, the datagram is discarded and a message is returned.\nA router will return a time exceeded message if the lifetime of the datagram\nexpires.A host will send this message if it cannot complete reassembly within a time\nA syntactic or semantic error in an IP header will cause a parameter problem\nmessage to be returned by a router or host. For example, an incorrect argument may\nbe provided with an option. The Parameter field contains a pointer to the octet in\nthe original header where the error was detected.\nThe source quench message provides a rudimentary form of flow control.\nEither a router or a destination host may send this message to a source host,\nrequesting that it reduce the rate at which it is sending traffic to the internet des-\ntination. On receipt of a source quench message, the source host should cut back\nthe rate at which it is sending traffic to the specified destination until it no longer\nreceives source quench messages. The source quench message can be used by a\nrouter or host that must discard datagrams because of a full buffer. In that case,\nthe router or host will issue a source quench message for every datagram that it\ndiscards. In addition, a system may anticipate congestion and issue source quench\nmessages when its buffers approach capacity. In that case, the datagram referred\nto in the source quench message may well be delivered. Thus, receipt of a source\nquench message does not imply delivery or nondelivery of the corresponding\nA router sends a redirect message to a host on a directly connected router to\nadvise the host of a better route to a particular destination. The following is an\nexample, using Figure 18.8. Router R1 receives a datagram from host C on network\nY, to which R1 is attached. R1 checks its routing table and obtains the address for\nthe next router, R2, on the route to the datagram’s internet destination network, Z.\nBecause R2 and the host identified by the internet source address of the datagram\nare on the same network, R1 sends a redirect message to C. The redirect message\nadvises the host to send its traffic for network Z directly to router R2, because this is\na shorter path to the destination. The router forwards the original datagram to its\ninternet destination (via R2). The address of R2 is contained in the parameter field\nof the redirect message.\nThe echo and echo reply messages provide a mechanism for testing that\ncommunication is possible between entities. The recipient of an echo message is\nobligated to return the message in an echo reply message. An identifier and\nsequence number are associated with the echo message to be matched in the\necho reply message. The identifier might be used like a service access point to\nCHAPTER 18 / INTERNET PROTOCOLS\n18.4 / INTERNET PROTOCOL\nidentify a particular session, and the sequence number might be incremented on\neach echo request sent.\nThe timestamp and timestamp reply messages provide a mechanism for\nsampling the delay characteristics of the internet.The sender of a timestamp mes-\nsage may include an identifier and sequence number in the parameters field and\ninclude the time that the message is sent (originate timestamp). The receiver\nrecords the time it received the message and the time that it transmits the reply\nmessage in the timestamp reply message. If the timestamp message is sent using\nstrict source routing, then the delay characteristics of a particular route can be\nThe address mask request and address mask reply messages are useful in\nan environment that includes subnets. The address mask request and reply\nmessages allow a host to learn the address mask for the LAN to which it con-\nnects. The host broadcasts an address mask request message on the LAN. The\nrouter on the LAN responds with an address mask reply message that contains\nthe address mask.\nAddress Resolution Protocol (ARP)\nEarlier in this chapter, we referred to the concepts of a global address (IP\naddress) and an address that conforms to the addressing scheme of the network\nto which a host is attached (subnetwork address). For a local area network, the\nlatter address is a MAC address, which provides a physical address for a host port\nattached to the LAN. Clearly, to deliver an IP datagram to a destination host, a\nmapping must be made from the IP address to the subnetwork address for that\nlast hop. If a datagram traverses one or more routers between source and desti-\nnation hosts, then the mapping must be done in the final router, which is attached\nto the same subnetwork as the destination host. If a datagram is sent from one\nhost to another on the same subnetwork, then the source host must do the map-\nping. In the following discussion, we use the term system to refer to the entity that\ndoes the mapping.\nFor mapping from an IP address to a subnetwork address, a number of\napproaches are possible, including\n• Each system can maintain a local table of IP addresses and matching subnet-\nwork addresses for possible correspondents. This approach does not accom-\nmodate easy and automatic additions of new hosts to the subnetwork.\n• The subnetwork address can be a subset of the network portion of the IP\naddress. However, the entire internet address is 32 bits long and for most sub-\nnetwork types (e.g., Ethernet) the Host Address field is longer than 32 bits.\n• A centralized directory can be maintained on each subnetwork that contains the\nIP-subnet address mappings. This is a reasonable solution for many networks.\n• An address resolution protocol can be used. This is a simpler approach than\nthe use of a centralized directory and is well suited to LANs.\nRFC 826 defines an Address Resolution Protocol (ARP), which allows\ndynamic distribution of the information needed to build tables to translate an IP\naddress A into a 48-bit Ethernet address; the protocol can be used for any broadcast\nnetwork. ARP exploits the broadcast property of a LAN; namely, that a transmis-\nsion from any device on the network is received by all other devices on the network.\nARP works as follows:\n1. Each system on the LAN maintains a table of known IP-subnetwork address\n2. When a subnetwork address is needed for an IP address, and the mapping is not\nfound in the system’s table,the system uses ARP directly on top of the LAN pro-\ntocol (e.g.,IEEE 802) to broadcast a request.The broadcast message contains the\nIP address for which a subnetwork address is needed.\n3. Other hosts on the subnetwork listen for ARP messages and reply when a\nmatch occurs.The reply includes both the IP and subnetwork addresses of the\nreplying host.\n4. The original request includes the requesting host’s IP address and subnetwork\naddress.Any interested host can copy this information into its local table, avoid-\ning the need for later ARP messages.\n5. The ARP message can also be used simply to broadcast a host’s IP address and\nsubnetwork address, for the benefit of others on the subnetwork.\nThe Internet Protocol (IP) has been the foundation of the Internet and virtually all\nmultivendor private internetworks.This protocol is reaching the end of its useful life\nand a new protocol, known as IPv6 (IP version 6), has been defined to ultimately\nreplace IP.6\nWe first look at the motivation for developing a new version of IP and then\nexamine some of its details.\nIP Next Generation\nThe driving motivation for the adoption of a new version of IP was the limitation\nimposed by the 32-bit address field in IPv4. With a 32-bit address field, it is possi-\nble in principle to assign \ndifferent addresses, which is over 4 billion possible\naddresses. One might think that this number of addresses was more than ade-\nquate to meet addressing needs on the Internet. However, in the late 1980s it was\nperceived that there would be a problem, and this problem began to manifest\nitself in the early 1990s. Reasons for the inadequacy of 32-bit addresses include\nthe following:\n• The two-level structure of the IP address (network number, host number) is\nconvenient but wasteful of the address space. Once a network number is\nassigned to a network, all of the host-number addresses for that network\nnumber are assigned to that network.The address space for that network may\n6The currently deployed version of IP is IP version 4; previous versions of IP (1 through 3) were succes-\nsively defined and replaced to reach IPv4. Version 5 is the number assigned to the Stream Protocol, a\nconnection-oriented internet-layer protocol; hence the use of the label version 6.\nCHAPTER 18 / INTERNET PROTOCOLS\n18.5 / IPv6\nbe sparsely used, but as far as the effective IP address space is concerned, if a\nnetwork number is used, then all addresses within the network are used.\n• The IP addressing model generally requires that a unique network number be\nassigned to each IP network whether or not it is actually connected to the\n• Networks are proliferating rapidly. Most organizations boast multiple LANs,\nnot just a single LAN system.Wireless networks have rapidly assumed a major\nrole.The Internet itself has grown explosively for years.\n• Growth of TCP/IP usage into new areas will result in a rapid growth in the\ndemand for unique IP addresses. Examples include using TCP/IP to intercon-\nnect electronic point-of-sale terminals and for cable television receivers.\n• Typically, a single IP address is assigned to each host.A more flexible arrange-\nment is to allow multiple IP addresses per host. This, of course, increases the\ndemand for IP addresses.\nSo the need for an increased address space dictated that a new version of IP\nwas needed. In addition, IP is a very old protocol, and new requirements in the\nareas of address configuration, routing flexibility, and traffic support had been\nIn response to these needs, the Internet Engineering Task Force (IETF) issued\na call for proposals for a next generation IP (IPng) in July of 1992. A number of\nproposals were received, and by 1994 the final design for IPng emerged. A major\nmilestone was reached with the publication of RFC 1752, “The Recommendation\nfor the IP Next Generation Protocol,” issued in January 1995. RFC 1752 outlines the\nrequirements for IPng, specifies the PDU formats, and highlights the IPng approach\nin the areas of addressing, routing, and security. A number of other Internet docu-\nments defined details of the protocol, now officially called IPv6; these include an\noverall specification of IPv6 (RFC 2460), an RFC dealing with addressing structure\nof IPv6 (RFC 2373), and numerous others.\nIPv6 includes the following enhancements over IPv4:\n• Expanded address space: IPv6 uses 128-bit addresses instead of the 32-bit\naddresses of IPv4. This is an increase of address space by a factor of \nhas been pointed out [HIND95] that this allows on the order of \nunique addresses per square meter of the surface of the earth. Even if\naddresses are very inefficiently allocated, this address space seems inex-\n• Improved option mechanism: IPv6 options are placed in separate optional\nheaders that are located between the IPv6 header and the transport-layer\nheader. Most of these optional headers are not examined or processed by any\nrouter on the packet’s path.This simplifies and speeds up router processing of\nIPv6 packets compared to IPv4 datagrams.7 It also makes it easier to add\nadditional options.\n7The protocol data unit for IPv6 is referred to as a packet rather than a datagram, which is the term used\nfor IPv4 PDUs.\n• Address autoconfiguration: This capability provides for dynamic assignment\nof IPv6 addresses.\n• Increased addressing flexibility: IPv6 includes the concept of an anycast\naddress, for which a packet is delivered to just one of a set of nodes.The scala-\nbility of multicast routing is improved by adding a scope field to multicast\n• Support for resource allocation: IPv6 enables the labeling of packets\nbelonging to a particular traffic flow for which the sender requests special\nhandling. This aids in the support of specialized traffic such as real-time\nAll of these features are explored in the remainder of this section.\nIPv6 Structure\nAn IPv6 protocol data unit (known as a packet) has the following general form:\nThe only header that is required is referred to simply as the IPv6 header.This is\nof fixed size with a length of 40 octets, compared to 20 octets for the mandatory por-\ntion of the IPv4 header (Figure 18.6). The following extension headers have been\n• Hop-by-Hop Options header: Defines special options that require hop-by-hop\n• Routing header: Provides extended routing, similar to IPv4 source routing\n• Fragment header: Contains fragmentation and reassembly information\n• Authentication header: Provides packet integrity and authentication\n• Encapsulating Security Payload header: Provides privacy\n• Destination Options header: Contains optional information to be examined\nby the destination node\nThe IPv6 standard recommends that, when multiple extension headers are\nused, the IPv6 headers appear in the following order:\n1. IPv6 header: Mandatory, must always appear first\n2. Hop-by-Hop Options header\n3. Destination Options header: For options to be processed by the first destination\nthat appears in the IPv6 Destination Address field plus subsequent destinations\nlisted in the Routing header\n4. Routing header\n5. Fragment header\n6. Authentication header\n7. Encapsulating Security Payload header\nCHAPTER 18 / INTERNET PROTOCOLS\nIPv6 header\nExtension header\nExtension header\nTransport-level PDU\n;!40 octets !:;!!!!!!!!!!!\n!!!!!!!!!!!:\n18.5 / IPv6\n8. Destination Options header: For options to be processed only by the final des-\ntination of the packet\nFigure 18.10 shows an example of an IPv6 packet that includes an instance\nof each header, except those related to security. Note that the IPv6 header and\neach extension header include a Next Header field. This field identifies the type\nof the immediately following header. If the next header is an extension header,\nthen this field contains the type identifier of that header. Otherwise, this field\ncontains the protocol identifier of the upper-layer protocol using IPv6 (typically\na transport-level protocol), using the same values as the IPv4 Protocol field. In\nFigure 18.10, the upper-layer protocol is TCP; thus, the upper-layer data carried\nby the IPv6 packet consist of a TCP header followed by a block of application\nWe first look at the main IPv6 header and then examine each of the extensions\nIPv6 header\noptions header\nDestination options\nRouting header\nApplication data\nFragment header\n\u0001 Next Header field\n20 (optional variable part)\nIPv6 header\nFigure 18.10\nIPv6 Packet with Extension Headers (containing a TCP \nIPv6 Header\nThe IPv6 header has a fixed length of 40 octets, consisting of the following fields\n(Figure 18.11):\n• Version (4 bits): Internet protocol version number; the value is 6.\n• DS/ECN (8 bits): Available for use by originating nodes and/or forwarding\nrouters for differentiated services and congestion functions, as described for\nthe IPv4 DS/ECN field.\n• Flow Label (20 bits): May be used by a host to label those packets for which it is\nrequesting special handling by routers within a network; discussed subsequently.\n• Payload Length (16 bits): Length of the remainder of the IPv6 packet follow-\ning the header, in octets. In other words, this is the total length of all of the\nextension headers plus the transport-level PDU.\n• Next Header (8 bits): Identifies the type of header immediately following the\nIPv6 header; this will either be an IPv6 extension header or a higher-layer\nheader, such as TCP or UDP.\n• Hop Limit (8 bits): The remaining number of allowable hops for this packet.The\nhop limit is set to some desired maximum value by the source and decremented\nby 1 by each node that forwards the packet.The packet is discarded if Hop Limit\nis decremented to zero. This is a simplification over the processing required for\nthe Time to Live field of IPv4.The consensus was that the extra effort in account-\ning for time intervals in IPv4 added no significant value to the protocol. In fact,\nIPv4 routers, as a general rule, treat the Time to Live field as a hop limit field.\n• Source Address (128 bits): The address of the originator of the packet.\nCHAPTER 18 / INTERNET PROTOCOLS\nPayload Length\nNext Header\nSource Address\nDestination Address\n10 \u0006 32 bits \u0001 40 octets\nFigure 18.11\nIPv6 Header\n18.5 / IPv6\n• Destination Address (128 bits): The address of the intended recipient of the\npacket. This may not in fact be the intended ultimate destination if a Routing\nheader is present, as explained subsequently.\nAlthough the IPv6 header is longer than the mandatory portion of the IPv4\nheader (40 octets versus 20 octets), it contains fewer fields (8 versus 12). Thus,\nrouters have less processing to do per header, which should speed up routing.\nFlow Label RFC 3967 defines a flow as a sequence of packets sent from a partic-\nular source to a particular (unicast, anycast, or multicast) destination for which the\nsource desires special handling by the intervening routers.A flow is uniquely identi-\nfied by the combination of a source address, destination address, and a nonzero \n20-bit flow label. Thus, all packets that are to be part of the same flow are assigned\nthe same flow label by the source.\nFrom the source’s point of view, a flow typically will be a sequence of packets\nthat are generated from a single application instance at the source and that have the\nsame transfer service requirements. A flow may comprise a single TCP connection\nor even multiple TCP connections; an example of the latter is a file transfer applica-\ntion, which could have one control connection and multiple data connections.A sin-\ngle application may generate a single flow or multiple flows. An example of the\nlatter is multimedia conferencing, which might have one flow for audio and one for\ngraphic windows, each with different transfer requirements in terms of data rate,\ndelay, and delay variation.\nFrom the router’s point of view, a flow is a sequence of packets that share\nattributes that affect how these packets are handled by the router. These include\npath, resource allocation, discard requirements, accounting, and security attributes.\nThe router may treat packets from different flows differently in a number of ways,\nincluding allocating different buffer sizes, giving different precedence in terms of\nforwarding, and requesting different quality of service from networks.\nThere is no special significance to any particular flow label. Instead the special\nhandling to be provided for a packet flow must be declared in some other way. For\nexample, a source might negotiate or request special handling ahead of time from\nrouters by means of a control protocol, or at transmission time by information in\none of the extension headers in the packet, such as the Hop-by-Hop Options\nheader. Examples of special handling that might be requested include some sort of\nnondefault quality of service and some form of real-time service.\nIn principle, all of a user’s requirements for a particular flow could be defined\nin an extension header and included with each packet. If we wish to leave the con-\ncept of flow open to include a wide variety of requirements, this design approach\ncould result in very large packet headers. The alternative, adopted for IPv6, is the\nflow label, in which the flow requirements are defined prior to flow commencement\nand a unique flow label is assigned to the flow. In this case, the router must save flow\nrequirement information about each flow.\nThe following rules apply to the flow label:\n1. Hosts or routers that do not support the Flow Label field must set the field to\nzero when originating a packet, pass the field unchanged when forwarding a\npacket, and ignore the field when receiving a packet.\n2. All packets originating from a given source with the same nonzero Flow Label\nmust have the same Destination Address, Source Address, Hop-by-Hop\nCHAPTER 18 / INTERNET PROTOCOLS\n8In IPv6, a node is any device that implements IPv6; this includes hosts and routers.\n18.5 / IPv6\n• Anycast: An identifier for a set of interfaces (typically belonging to different\nnodes). A packet sent to an anycast address is delivered to one of the inter-\nfaces identified by that address (the “nearest” one, according to the routing\nprotocols’ measure of distance).\n• Multicast: An identifier for a set of interfaces (typically belonging to different\nnodes).A packet sent to a multicast address is delivered to all interfaces iden-\ntified by that address.\nHop-by-Hop Options Header\nThe Hop-by-Hop Options header carries optional information that, if present, must\nbe examined by every router along the path.This header consists of (Figure 18.12a):\n• Next Header (8 bits): Identifies the type of header immediately following this\n• Header Extension Length (8 bits): Length of this header in 64-bit units, not\nincluding the first 64 bits.\n• Options: A variable-length field consisting of one or more option definitions.\nEach definition is in the form of three subfields: Option Type (8 bits), which\nidentifies the option; Length (8 bits), which specifies the length of the Option\nData field in octets; and Option Data, which is a variable-length specification\nof the option.\nNext header Hdr ext len Routing type Segments left\nType-specific data\n(c) Generic routing header\n(b) Fragment header\nNext header\nFragment offset\nIdentification\n(d) Type 0 routing header\nNext header\nHdr ext len\nSegments left\nHdr ext len\nOne or more options\n(a) Hop-by-Hop options header;\ndestination options header\nNext header\nFigure 18.12\nIPv6 Extension Headers\nIt is actually the lowest-order five bits of the Option Type field that are used to\nspecify a particular option. The high-order two bits indicate that action to be taken\nby a node that does not recognize this option type, as follows:\n• 00—Skip over this option and continue processing the header.\n• 01—Discard the packet.\n• 10—Discard the packet and send an ICMP Parameter Problem message to the\npacket’s Source Address, pointing to the unrecognized Option Type.\n• 11—Discard the packet and, only if the packet’s Destination Address is not a\nmulticast address, send an ICMP Parameter Problem message to the packet’s\nSource Address, pointing to the unrecognized Option Type.\nThe third highest-order bit specifies whether the Option Data field does not\nchange (0) or may change (1) en route from source to destination. Data that may\nchange must be excluded from authentication calculations, as discussed in Chapter 21.\nThese conventions for the Option Type field also apply to the Destination\nOptions header.\nFour hop-by-hop options have been specified so far:\n• Pad1: Used to insert one byte of padding into the Options area of the header.\n• PadN: Used to insert N bytes \nof padding into the Options area of the\nheader.The two padding options ensure that the header is a multiple of 8 bytes\n• Jumbo payload: Used to send IPv6 packets with payloads longer than 65,535\noctets.The Option Data field of this option is 32 bits long and gives the length of\nthe packet in octets, excluding the IPv6 header. For such packets, the Payload\nLength field in the IPv6 header must be set to zero, and there must be no\nFragment header.With this option, IPv6 supports packet sizes up to more than 4\nbillion octets.This facilitates the transmission of large video packets and enables\nIPv6 to make the best use of available capacity over any transmission medium.\nsupport for protocols such as RSVP (Chapter 19) that generate packets that\nneed to be examined by intermediate routers for purposes of traffic control.\nRather than requiring the intermediate routers to look in detail at the extension\nheaders of a packet,this option alerts the router when such attention is required.\nFragment Header\nIn IPv6, fragmentation may only be performed by source nodes, not by routers\nalong a packet’s delivery path. To take full advantage of the internetworking envi-\nronment, a node must perform a path discovery algorithm that enables it to learn\nthe smallest maximum transmission unit (MTU) supported by any network on the\npath.With this knowledge, the source node will fragment, as required, for each given\nCHAPTER 18 / INTERNET PROTOCOLS\n18.5 / IPv6\ndestination address. Otherwise the source must limit all packets to 1280 octets,\nwhich is the minimum MTU that must be supported by each network.\nThe fragment header consists of the following (Figure 18.12b):\n• Next Header (8 bits): Identifies the type of header immediately following this\n• Reserved (8 bits): For future use.\n• Fragment Offset (13 bits): Indicates where in the original packet the payload of\nthis fragment belongs,measured in 64-bit units.This implies that fragments (other\nthan the last fragment) must contain a data field that is a multiple of 64 bits long.\n• Res (2 bits): Reserved for future use.\n• M Flag (1 bit):\n• Identification (32 bits): Intended to uniquely identify the original packet. The\nidentifier must be unique for the packet’s source address and destination\naddress for the time during which the packet will remain in the internet. All\nfragments with the same identifier, source address, and destination address are\nreassembled to form the original packet.\nThe fragmentation algorithm is the same as that described in Section 18.3.\nRouting Header\nThe Routing header contains a list of one or more intermediate nodes to be visited\non the way to a packet’s destination. All routing headers start with a 32-bit block\nconsisting of four 8-bit fields, followed by routing data specific to a given routing\ntype (Figure 18.12c).The four 8-bit fields are as follows:\n• Next Header: Identifies the type of header immediately following this header.\n• Header Extension Length: Length of this header in 64-bit units, not including\nthe first 64 bits.\n• Routing Type: Identifies a particular Routing header variant. If a router does\nnot recognize the Routing Type value, it must discard the packet.\n• Segments Left: Number of route segments remaining;that is,the number of explic-\nitly listed intermediate nodes still to be visited before reaching the final destination.\nThe only specific routing header format defined in RFC 2460 is the Type 0 Rout-\ning header (Figure 18.12d). When using the Type 0 Routing header, the source node\ndoes not place the ultimate destination address in the IPv6 header.Instead,that address\nis the last address listed in the Routing header (Address[n] in Figure 18.12d), and the\nIPv6 header contains the destination address of the first desired router on the path.The\nRouting header will not be examined until the packet reaches the node identified in the\nCHAPTER 18 / INTERNET PROTOCOLS\n18.6 / VIRTUAL PRIVATE NETWORKS AND IP SECURITY\n• Secure remote access over the Internet: An end user whose system is equipped\nwith IP security protocols can make a local call to an Internet service provider\n(ISP) and gain secure access to a company network. This reduces the cost of\ntoll charges for traveling employees and telecommuters.\n• Establishing extranet and intranet connectivity with partners: IPSec can be\nused to secure communication with other organizations, ensuring authentica-\ntion and confidentiality and providing a key exchange mechanism.\n• Enhancing electronic commerce security: Even though some Web and elec-\ntronic commerce applications have built-in security protocols, the use of IPSec\nenhances that security. IPSec guarantees that all traffic designated by the net-\nwork administrator is both encrypted and authenticated, adding an additional\nlayer of security to whatever is provided at the application layer.\nThe principal feature of IPSec that enables it to support these varied applica-\ntions is that it can encrypt and/or authenticate all traffic at the IP level. Thus, all\ndistributed applications, including remote logon, client/server, e-mail, file transfer,\nWeb access, and so on, can be secured.\nFigure 18.13 is a typical scenario of IPSec usage. An organization maintains\nLANs at dispersed locations. Nonsecure IP traffic is conducted on each LAN. For traf-\nfic offsite,through some sort of private or public WAN,IPSec protocols are used.These\nprotocols operate in networking devices, such as a router or firewall, that connect each\nLAN to the outside world. The IPSec networking device will typically encrypt and\ncompress all traffic going into the WAN, and decrypt and decompress traffic coming\nfrom the WAN; these operations are transparent to workstations and servers on the\nLAN.Secure transmission is also possible with individual users who dial into the WAN.\nSuch user workstations must implement the IPSec protocols to provide security.\nBenefits of IPSec\nSome of the benefits of IPSec are as follows:\n• When IPSec is implemented in a firewall or router, it provides strong security\nthat can be applied to all traffic crossing the perimeter. Traffic within a com-\npany or workgroup does not incur the overhead of security-related processing.\n• IPSec in a firewall is resistant to bypass if all traffic from the outside must use\nIP and the firewall is the only means of entrance from the Internet into the\norganization.\n• IPSec is below the transport layer (TCP, UDP) and so is transparent to appli-\ncations.There is no need to change software on a user or server system when\nIPSec is implemented in the firewall or router. Even if IPSec is implemented\nin end systems, upper-layer software, including applications, is not affected.\n• IPSec can be transparent to end users.There is no need to train users on secu-\nrity mechanisms, issue keying material on a per-user basis, or revoke keying\nmaterial when users leave the organization.\n• IPSec can provide security for individual users if needed.This is useful for off-\nsite workers and for setting up a secure virtual subnetwork within an organi-\nzation for sensitive applications.\nNetworking device\nUser system\nNetworking device\nPublic (Internet)\nFigure 18.13\nAn IP Security Scenario\n18.7 / RECOMMENDED READING AND WEB SITES\nIPSec Functions\nIPSec provides three main facilities: an authentication-only function referred to as\nAuthentication Header (AH), a combined authentication/encryption function\ncalled Encapsulating Security Payload (ESP), and a key exchange function. For\nVPNs, both authentication and encryption are generally desired, because it is\nimportant both to (1) assure that unauthorized users do not penetrate the virtual\nprivate network and (2) assure that eavesdroppers on the Internet cannot read mes-\nsages sent over the virtual private network. Because both features are generally\ndesirable, most implementations are likely to use ESP rather than AH. The key\nexchange function allows for manual exchange of keys as well as an automated\nIPSec is explored in Chapter 21.\n18.7 RECOMMENDED READING AND WEB SITES\n[RODR02] provides clear coverage of all of the topics in this chapter. Good coverage of\ninternetworking and IPv4 can be found in [COME06] and [STEV94]. [SHAN02] and\n[KENT87] provide useful discussions of fragmentation. [LEE05] is a thorough technical\ndescription IPv6. [KESH98] provides an instructive look at present and future router\nfunctionality. [METZ02] and [DOI94] describe the IPv6 anycast feature. For the reader\ninterested in a more in-depth discussion of IP addressing, [SPOR03] offers a wealth of\nComer, D. Internetworking with TCP/IP, Volume I: Principles, Protocols, and\nArchitecture. Upper Saddle River, NJ: Prentice Hall, 2006.\nDoi, S., et al. “IPv6 Anycast for Simple and Effective Communications.” IEEE\nCommunications Magazine, May 2004.\nHuitema, C. IPv6:The New Internet Protocol. Upper Saddle River, NJ: Prentice\nHall, 1998.\nKent, C., and Mogul, J.“Fragmentation Considered Harmful.” ACM Computer\nCommunication Review, October 1987.\nKeshav, S., and Sharma, R. “Issues and Trends in Router Design.” IEEE Com-\nmunications Magazine, May 1998.\nLee, H. Understanding IPv6. New York: Springer-Verlag, 2005.\nMetz C.“IP Anycast.” IEEE Internet Computing, March 2002.\nRodriguez, A., et al. TCP/IP Tutorial and Technical Overview. Upper Saddle\nRiver: NJ: Prentice Hall, 2002.\nShannon, C.; Moore, D.; and Claffy, K. “Beyond Folklore: Observations on\nFragmented Traffic.” IEEE/ACM Transactions on Networking, December 2002.\nSportack, M. IP Addressing Fundamentals. Indianapolis, IN: Cisco Press, 2003.\nStevens, W. TCP/IP Illustrated, Volume 1: The Protocols. Reading, MA:\nAddison-Wesley, 1994.\ndatagram lifetime\nfragmentation\nintermediate system\nInternet Control Message \nProtocol (ICMP)\nInternet Protocol (IP)\ninternetworking\nsegmentation\nsubnet mask\ntraffic class\nRecommended Web sites:\n• IPv6: Information about IPv6 and related topics.\n• IPv6 Working Group: Chartered by IETF to develop standards related to IPv6. The\nWeb site includes all relevant RFCs and Internet drafts.\n• IPv6 Forum: An industry consortium that promotes IPv6-related products. Includes a\nnumber of white papers and articles.\n18.8 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nCHAPTER 18 / INTERNET PROTOCOLS\nReview Questions\nGive some reasons for using fragmentation and reassembly.\nList the requirements for an internetworking facility.\nWhat are the pros and cons of limiting reassembly to the endpoint as compared to\nallowing en route reassembly?\nExplain the function of the three flags in the IPv4 header.\nHow is the IPv4 header checksum calculated?\nWhat is the difference between the traffic class and flow label fields in the IPv6 header?\nBriefly explain the three types of IPv6 addresses.\nWhat is the purpose of each of the IPv6 header types?\nAlthough not explicitly stated, the Internet Protocol (IP) specification, RFC 791,\ndefines the minimum packet size a network technology must support to allow IP to\nrun over it.\nRead Section 3.2 of RFC 791 to find out that value.What is it?\nb. Discuss the reasons for adopting that specific value.\nIn the discussion of IP, it was mentioned that the identifier, don’t fragment identifier,\nand time-to-live parameters are present in the Send primitive but not in the Deliver\nprimitive because they are only of concern to IP. For each of these parameters, indi-\ncate whether it is of concern to the IP entity in the source, the IP entities in any inter-\nmediate routers, and the IP entity in the destination end systems. Justify your answer.\n18.8 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nWhat is the header overhead in the IP protocol?\nDescribe some circumstances where it might be desirable to use source routing rather\nthan let the routers make the routing decision.\nBecause of fragmentation, an IP datagram can arrive in several pieces, not necessarily\nin the correct order. The IP entity at the receiving end system must accumulate these\nfragments until the original datagram is reconstituted.\nConsider that the IP entity creates a buffer for assembling the data field in the origi-\nnal datagram.As assembly proceeds,the buffer will contain blocks of data and “holes”\nbetween the data blocks.Describe an algorithm for reassembly based on this concept.\nb. For the algorithm in part (a), it is necessary to keep track of the holes. Describe a\nsimple mechanism for doing this.\nA 4480-octet datagram is to be transmitted and needs to be fragmented because it\nwill pass through an Ethernet with a maximum payload of 1500 octets. Show the\nTotal Length, More Flag, and Fragment Offset values in each of the resulting frag-\nConsider a header that consists of 10 octets, with the checksum in the last two octets\n(this does not correspond to any actual header format) with the following content (in\nhexadecimal): 01 00 F6 F7 F4 F5 F2 03 00 00\nCalculate the checksum. Show your calculation.\nb. Show the resulting packet.\nVerify the checksum.\nThe IP checksum needs to be recalculated at routers because of changes to the IP\nheader, such as the lifetime field. It is possible to recalculate the checksum from\nscratch. Suggest a procedure that involves less calculation. Hint: Suppose that the\nvalue in octet k is changed by \nconsider the effect of this\nchange on the checksum.\nAn IP datagram is to be fragmented. Which options in the option field need to be\ncopied into the header of each fragment, and which need only be retained in the first\nfragment? Justify the handling of each option.\nA transport-layer message consisting of 1500 bits of data and 160 bits of header is sent\nto an internet layer, which appends another 160 bits of header. This is then transmit-\nted through two networks, each of which uses a 24-bit packet header.The destination\nnetwork has a maximum packet size of 800 bits. How many bits, including headers, are\ndelivered to the network-layer protocol at the destination?\nThe architecture suggested by Figure 18.2 is to be used. What functions could be\nadded to the routers to alleviate some of the problems caused by the mismatched\nlocal and long-haul networks?\nShould internetworking be concerned with a network’s internal routing? Why or why not?\nProvide the following parameter values for each of the network classes A, B, and C.\nBe sure to consider any special or reserved addresses in your calculations.\nNumber of bits in network portion of address\nb. Number of bits in host portion of address\nNumber of distinct networks allowed\nd. Number of distinct hosts per network allowed\nInteger range of first octet\nWhat percentage of the total IP address space does each of the network classes rep-\nWhat is the difference between the subnet mask for a Class A address with 16 bits for\nthe subnet ID and a class B address with 8 bits for the subnet ID?\nIs the subnet mask 255.255.0.255 valid for a Class A address?\nGiven a network address of 192.168.100.0 and a subnet mask of 255.255.255.192,\nHow many subnets are created?\nb. How many hosts are there per subnet?\nZ = new_value - old_value;\nGiven a company with six individual departments and each department having ten\ncomputers or networked devices, what mask could be applied to the company net-\nwork to provide the subnetting necessary to divide up the network equally?\nIn contemporary routing and addressing, the notation commonly used is called class-\nless interdomain routing or CIDR.With CIDR, the number of bits in the mask is indi-\ncated in the following fashion: 192.168.100.0/24. This corresponds to a mask of\n255.255.255.0. If this example would provide for 256 host addresses on the network,\nhow many addresses are provided with the following?\n192.168.100.0/23\nb. 192.168.100.0/25\nFind out about your network. Using the command “ipconfig”,“ifconfig”, or “winipcfg”,\nwe can learn not only our IP address but other network parameters as well. Can you\ndetermine your mask,gateway,and the number of addresses available on your network?\nUsing your IP address and your mask, what is your network address? This is deter-\nmined by converting the IP address and the mask to binary and then proceeding with\na bitwise logical AND operation. For example, given the address 172.16.45.0 and the\nmask 255.255.224.0, we would discover that the network address would be 172.16.32.0.\nCompare the individual fields of the IPv4 header with the IPv6 header. Account for\nthe functionality provided by each IPv4 field by showing how the same functionality\nis provided in IPv6.\nJustify the recommended order in which IPv6 extension headers appear (i.e., why is\nthe Hop-by-Hop Options header first, why is the Routing header before the Frag-\nment header, and so on).\nThe IPv6 standard states that if a packet with a nonzero flow label arrives at a router\nand the router has no information for that flow label, the router should ignore the\nflow label and forward the packet.\nWhat are the disadvantages of treating this event as an error, discarding the\npacket, and sending an ICMP message?\nb. Are there situations in which routing the packet as if its flow label were zero will\ncause the wrong result? Explain.\nThe IPv6 flow mechanism assumes that the state associated with a given flow label is\nstored in routers, so they know how to handle packets that carry that flow label. A\ndesign requirement is to flush flow labels that are no longer being used (stale flow\nlabel) from routers.\nAssume that a source always send a control message to all affected routers delet-\ning a flow label when the source finishes with that flow. In that case, how could a\nstale flow label persist?\nb. Suggest router and source mechanisms to overcome the problem of stale flow labels.\nThe question arises as to which packets generated by a source should carry nonzero\nIPv6 flow labels. For some applications, the answer is obvious. Small exchanges of\ndata should have a zero flow label because it is not worth creating a flow for a few\npackets. Real-time flows should have a flow label; such flows are a primary reason\nflow labels were created.A more difficult issue is what to do with peers sending large\namounts of best-effort traffic (e.g., TCP connections). Make a case for assigning a\nunique flow label to each long-term TCP connection. Make a case for not doing this.\nThe original IPv6 specifications combined the Traffic Class and Flow Label fields into\na single 28-bit Flow Label field. This allowed flows to redefine the interpretation of\ndifferent values of priority. Suggest reasons why the final specification includes the\nPriority field as a distinct field.\nFor Type 0 IPv6 routing, specify the algorithm for updating the IPv6 and Routing\nheaders by intermediate nodes.\nCHAPTER 18 / INTERNET PROTOCOLS\n19.1 Multicasting\n19.2 Routing Protocols\n19.3 Integrated Services Architecture\n19.4 Differentiated Services\n19.5 Service Level Agreements\n19.6 IP Performance Metrics\n19.7 Recommended Reading and Web Sites\n19.8 Key Terms, Review Questions, and Problems\nCHAPTER 19 / INTERNETWORK OPERATION\nShe occupied herself with studying a map on the opposite wall because she knew\nshe would have to change trains at some point.Tottenham Court Road must be that\npoint, an interchange from the black line to the red.This train would take her there,\nwas bearing her there rapidly now, and at the station she would follow the signs, for\nsigns there must be, to the Central Line going westward.\n—King Solomon’s Carpet, Barbara Vine (Ruth Rendell)\nThe act of sending a packet from a source to multiple destinations is\nreferred to as multicasting. Multicasting raises design issues in the\nareas of addressing and routing.\nRouting protocols in an internet function in a similar fashion to those\nused in packet-switching networks. An internet routing protocol is\nused to exchange information about reachability and traffic delays,\nallowing each router to construct a next-hop routing table for paths\nthrough the internet. Typically, relatively simple routing protocols are\nused between autonomous systems within a larger internet and more\ncomplex routing protocols are used within each autonomous system.\nThe integrated services architecture is a response to the growing vari-\nety and volume of traffic experienced in the Internet and intranets. It\nprovides a framework for the development of protocols such as RSVP\nto handle multimedia/multicast traffic and provides guidance to\nrouter vendors on the development of efficient techniques for han-\ndling a varied load.\nThe differentiated services architecture is designed to provide a sim-\nple, easy-to-implement, low-overhead tool to support a range of net-\nwork services that are differentiated on the basis of performance.\nDifferentiated services are provided on the basis of a 6-bit label in the\nIP header, which classifies traffic in terms of the type of service to be\ngiven by routers for that traffic.\nAs the Internet and private internets grow in scale, a host of new demands march\nsteadily into view. Low-volume TELNET conversations are leapfrogged by high-\nvolume client/server applications.To this has been added more recently the tremen-\ndous volume of Web traffic, which is increasingly graphics intensive. Now real-time\nvoice and video applications add to the burden.\nTo cope with these demands, it is not enough to increase internet capacity. Sensi-\nble and effective methods for managing the traffic and controlling congestion are\nneeded. Historically, IP-based internets have been able to provide a simple best-effort\ndelivery service to all applications using an internet. But the needs of users have\n19.1 / MULTICASTING\nchanged.A company may have spent millions of dollars installing an IP-based internet\ndesigned to transport data among LANs but now finds that new real-time, multime-\ndia, and multicasting applications are not well supported by such a configuration.The\nonly networking scheme designed from day one to support both traditional TCP and\nUDP traffic and real-time traffic is ATM. However, reliance on ATM means either\nconstructing a second networking infrastructure for real-time traffic or replacing the\nexisting IP-based configuration with ATM, both of which are costly alternatives.\nThus, there is a strong need to be able to support a variety of traffic with a vari-\nety of quality-of-service (QoS) requirements, within the TCP/IP architecture. This\nchapter looks at the internetwork functions and services designed to meet this need.\nWe begin this chapter with a discussion of multicasting. Next we explore the\nissue of internetwork routing algorithms. Next, we look at the Integrated Services\nArchitecture (ISA), which provides a framework for current and future internet ser-\nvices. Then we examine differentiated services. Finally, we introduce the topics of\nservice level agreements and IP performance metrics.\nRefer to Figure 2.5 to see the position within the TCP/IP suite of the protocols\ndiscussed in this chapter.\n19.1 MULTICASTING\nTypically, an IP address refers to an individual host on a particular network. IP also\naccommodates addresses that refer to a group of hosts on one or more networks.\nSuch addresses are referred to as multicast addresses, and the act of sending a packet\nfrom a source to the members of a multicast group is referred to as multicasting.\nMulticasting has a number of practical applications. For example,\n• Multimedia: A number of users “tune in” to a video or audio transmission\nfrom a multimedia source station.\n• Teleconferencing: A group of workstations form a multicast group such that a\ntransmission from any member is received by all other group members.\n• Database: All copies of a replicated file or database are updated at the same time.\n• Distributed computation: Intermediate results are sent to all participants.\n• Real-time workgroup: Files, graphics, and messages are exchanged among\nactive group members in real time.\nMulticasting done within the scope of a single LAN segment is straightfor-\nward. IEEE 802 and other LAN protocols include provision for MAC-level multi-\ncast addresses.A packet with a multicast address is transmitted on a LAN segment.\nThose stations that are members of the corresponding multicast group recognize the\nmulticast address and accept the packet. In this case, only a single copy of the packet\nis ever transmitted.This technique works because of the broadcast nature of a LAN:\nA transmission from any one station is received by all other stations on the LAN.\nIn an internet environment, multicasting is a far more difficult undertaking. To\nsee this, consider the configuration of Figure 19.1; a number of LANs are intercon-\nnected by routers. Routers connect to each other either over high-speed links or across\na wide area network (network N4). A cost is associated with each link or network in\nCHAPTER 19 / INTERNETWORK OPERATION\neach direction,indicated by the value shown leaving the router for that link or network.\nSuppose that the multicast server on network N1 is transmitting packets to a multicast\naddress that represents the workstations indicated on networks N3, N5, N6. Suppose\nthat the server does not know the location of the members of the multicast group.Then\none way to assure that the packet is received by all members of the group is to\nbroadcast a copy of each packet to each network in the configuration, over the least-\ncost route for each network. For example, one packet would be addressed to N3 and\nwould traverse N1, link L3, and N3. Router B is responsible for translating the IP-level\nmulticast address to a MAC-level multicast address before transmitting the MAC\nframe onto N3.Table 19.1 summarizes the number of packets generated on the various\nlinks and networks in order to transmit one packet to a multicast group by this method.\nIn this table, the source is the multicast server on network N1 in Figure 19.1; the multi-\ncast address includes the group members on N3, N5, and N6. Each column in the table\nrefers to the path taken from the source host to a destination router attached to a par-\nticular destination network. Each row of the table refers to a network or link in the\nconfiguration of Figure 19.1. Each entry in the table gives the number of packets that\nFigure 19.1\nExample Configuration\nMulticast server\nGroup member\nGroup member\nGroup member\n19.1 / MULTICASTING\ntraverse a given network or link for a given path.A total of 13 copies of the packet are\nrequired for the broadcast technique.\nNow suppose the source system knows the location of each member of the\nmulticast group. That is, the source has a table that maps a multicast address into a\nlist of networks that contain members of that multicast group. In that case, the\nsource need only send packets to those networks that contain members of the\ngroup. We could refer to this as the multiple unicast strategy. Table 19.1 shows that\nin this case, 11 packets are required.\nBoth the broadcast and multiple unicast strategies are inefficient because they\ngenerate unnecessary copies of the source packet. In a true multicast strategy, the\nfollowing method is used:\n1. The least-cost path from the source to each network that includes members of\nthe multicast group is determined.This results in a spanning tree1 of the config-\nuration. Note that this is not a full spanning tree of the configuration. Rather, it\nis a spanning tree that includes only those networks containing group members.\n2. The source transmits a single packet along the spanning tree.\n3. The packet is replicated by routers only at branch points of the spanning tree.\nFigure 19.2a shows the spanning tree for transmissions from the source to the\nmulticast group, and Figure 19.2b shows this method in action.The source transmits a\nsingle packet over N1 to router D. D makes two copies of the packet, to transmit over\n(a) Broadcast\n(b) Multiple Unicast\n(c) Multicast\nTraffic Generated by Various Multicasting Strategies\n1The concept of spanning tree was introduced in our discussion of bridges in Chapter 15.A spanning tree\nof a graph consists of all the nodes of the graph plus a subset of the links (edges) of the graph that pro-\nvides connectivity (a path exists between any two nodes) with no closed loops (there is only one path\nbetween any two nodes).\nCHAPTER 19 / INTERNETWORK OPERATION\nlinks L3 and L4. B receives the packet from L3 and transmits it on N3, where it is read\nby members of the multicast group on the network. Meanwhile, C receives the packet\nsent on L4. It must now deliver that packet to both E and F. If network N4 were a\nbroadcast network (e.g., an IEEE 802 LAN), then C would only need to transmit one\ninstance of the packet for both routers to read. If N4 is a packet-switching WAN, then\nC must make two copies of the packet and address one to E and one to F. Each of\nthese routers, in turn, retransmits the received packet on N5 and N6, respectively. As\nTable 19.1 shows, the multicast technique requires only eight copies of the packet.\nRequirements for Multicasting\nIn ordinary unicast transmission over an internet,in which each datagram has a unique\ndestination network,the task of each router is to forward the datagram along the short-\nest path from that router to the destination network. With multicast transmission, the\nrouter may be required to forward two or more copies of an incoming datagram.In our\nexample,routers D and C both must forward two copies of a single incoming datagram.\nThus, we might expect that the overall functionality of multicast routing is\nmore complex than unicast routing.The following is a list of required functions:\n1. A convention is needed for identifying a multicast address. In IPv4, Class D\naddresses are reserved for this purpose. These are 32-bit addresses with 1110 as\ntheir high-order 4 bits, followed by a 28-bit group identifier. In IPv6, a 128-bit\nmulticast address consists of an 8-bit prefix of all ones, a 4-bit flags field, a 4-bit\nscope field, and a 112-bit group identifier.The flags field, currently, only indicates\nwhether this address is permanently assigned or not.The scope field indicates the\nscope of applicability of the address, ranging from a single network to global.\nFigure 19.2\nMulticast Transmission Example\n(a) Spanning tree from source to multicast group\n(b) Packets generated for multicast transmission\n19.1 / MULTICASTING\n2. Each node (router or source node participating in the routing algorithm) must\ntranslate between an IP multicast address and a list of networks that contain\nmembers of this group. This information allows the node to construct a short-\nest-path spanning tree to all of the networks containing group members.\n3. A router must translate between an IP multicast address and a network multi-\ncast address in order to deliver a multicast IP datagram on the destination net-\nwork. For example, in IEEE 802 networks, a MAC-level address is 48 bits long;\nif the highest-order bit is 1, then it is a multicast address. Thus, for multicast\ndelivery, a router attached to an IEEE 802 network must translate a 32-bit\nIPv4 or a 128-bit IPv6 multicast address into a 48-bit IEEE 802 MAC-level\nmulticast address.\n4. Although some multicast addresses may be assigned permanently, the more\nusual case is that multicast addresses are generated dynamically and that indi-\nvidual hosts may join and leave multicast groups dynamically. Thus, a mecha-\nnism is needed by which an individual host informs routers attached to the\nsame network as itself of its inclusion in and exclusion from a multicast group.\nIGMP, described subsequently, provides this mechanism.\n5. Routers must exchange two sorts of information. First, routers need to know\nwhich networks include members of a given multicast group. Second, routers\nneed sufficient information to calculate the shortest path to each network con-\ntaining group members. These requirements imply the need for a multicast\nrouting protocol. A discussion of such protocols is beyond the scope of this\n6. A routing algorithm is needed to calculate shortest paths to all group members.\n7. Each router must determine multicast routing paths on the basis of both\nsource and destination addresses.\nThe last point is a subtle consequence of the use of multicast addresses. To\nillustrate the point, consider again Figure 19.1. If the multicast server transmits a\nunicast packet addressed to a host on network N5, the packet is forwarded by router\nD to C, which then forwards the packet to E. Similarly, a packet addressed to a host\non network N3 is forwarded by D to B. But now suppose that the server transmits a\npacket with a multicast address that includes hosts on N3, N5, and N6. As we have\ndiscussed, D makes two copies of the packet and sends one to B and one to C.What\nwill C do when it receives a packet with such a multicast address? C knows that this\npacket is intended for networks N3, N5, and N6. A simple-minded approach would\nbe for C to calculate the shortest path to each of these three networks.This produces\nthe shortest-path spanning tree shown in Figure 19.3.As a result, C sends two copies\nof the packet out over N4, one intended for N5 and one intended for N6. But it also\nsends a copy of the packet to B for delivery on N3.Thus B will receive two copies of\nthe packet, one from D and one from C.This is clearly not what was intended by the\nhost on N1 when it launched the packet.\nTo avoid unnecessary duplication of packets, each router must route packets\non the basis of both source and multicast destination. When C receives a packet\nintended for the multicast group from a source on N1, it must calculate the spanning\nCHAPTER 19 / INTERNETWORK OPERATION\ntree with N1 as the root (shown in Figure 19.2a) and route on the basis of that\nspanning tree.\nInternet Group Management Protocol (IGMP)\nIGMP, defined in RFC 3376, is used by hosts and routers to exchange multicast\ngroup membership information over a LAN. IGMP takes advantage of the broad-\ncast nature of a LAN to provide an efficient technique for the exchange of informa-\ntion among multiple hosts and routers. In general, IGMP supports two principal\noperations:\n1. Hosts send messages to routers to subscribe to and unsubscribe from a multi-\ncast group defined by a given multicast address.\n2. Routers periodically check which multicast groups are of interest to which hosts.\nIGMP is currently at version 3. In IGMPv1, hosts could join a multicast group\nand routers used a timer to unsubscribe group members. IGMPv2 enabled a host to\nrequest to be unsubscribed from a group. The first two versions used essentially the\nfollowing operational model:\n• Receivers have to subscribe to multicast groups.\n• Sources do not have to subscribe to multicast groups.\n• Any host can send traffic to any multicast group.\nThis paradigm is very general, but it also has some weaknesses:\n1. Spamming of multicast groups is easy. Even if there are application level filters\nto drop unwanted packets, still these packets consume valuable resources in\nthe network and in the receiver that has to process them.\n2. Establishment of the multicast distribution trees is problematic. This is mainly\nbecause the location of sources is not known.\nFigure 19.3\nSpanning Tree from Router C to Multicast Group\n19.1 / MULTICASTING\n3. Finding globally unique multicast addresses is difficult. It is always possible\nthat another multicast group uses the same multicast address.\nIGMPv3 addresses these weaknesses by\n1. Allowing hosts to specify the list of hosts from which they want to receive traf-\nfic.Traffic from other hosts is blocked at routers.\n2. Allowing hosts to block packets that come from sources that send unwanted\nThe remainder of this section discusses IGMPv3.\nIGMP Message Format All IGMP messages are transmitted in IP datagrams.\nThe current version defines two message types: Membership Query and Member-\nship Report.\nA Membership Query message is sent by a multicast router. There are three\nsubtypes: a general query, used to learn which groups have members on an attached\nnetwork; a group-specific query, used to learn if a particular group has any members\non an attached network; and a group-and-source-specific query, used to learn if any\nattached device desires reception of packets sent to a specified multicast address,\nfrom any of a specified list of sources. Figure 19.4a shows the message format, which\nconsists of the following fields:\n• Type: Defines this message type.\n• Max Response Code: Indicates the maximum allowed time before sending a\nresponding report in units of 1/10 second.\n• Checksum: An error-detecting code, calculated as the 16-bit ones complement\naddition of all the 16-bit words in the message. For purposes of computation,\nthe Checksum field is itself initialized to a value of zero. This is the same\nchecksum algorithm used in IPv4.\n• Group Address: Zero for a general query message; a valid IP multicast group\naddress when sending a group-specific query or group-and-source-specific\n• S Flag: When set to one, indicates to any receiving multicast routers that they\nare to suppress the normal timer updates they perform upon hearing a query.\n• QRV (querier’s robustness variable): If nonzero, the QRV field contains the\nRV value used by the querier (i.e., the sender of the query). Routers adopt the\nRV value from the most recently received query as their own RV value, unless\nthat most recently received RV was zero, in which case the receivers use the\ndefault value or a statically configured value.The RV dictates how many times\na host will retransmit a report to assure that it is not missed by any attached\nmulticast routers.\n• QQIC (querier’s querier interval code): Specifies the QI value used by the\nquerier, which is a timer for sending multiple queries. Multicast routers that\nare not the current querier adopt the QI value from the most recently received\nquery as their own QI value, unless that most recently received QI was zero, in\nwhich case the receiving routers use the default QI value.\nCHAPTER 19 / INTERNETWORK OPERATION\nFigure 19.4\nIGMPv3 Message Formats\nNumber of sources (N)\nMax resp code\nSource address [1]\nSource address [2]\nSource address [N]\nGroup address (class D IPv4 address)\n(a) Membership query message\nNumber of group records (M)\n(b) Membership report message\nGroup record [1]\nGroup record [2]\nGroup record [M]\nRecord type\nAux data len\nNumber of sources (N)\n(c) Group record\nMulticast address\nSource address [1]\nSource address [2]\nSource address [N]\nAuxiliary data\n19.1 / MULTICASTING\n• Number of Sources: Specifies how many source addresses are present in this\nquery.This value is nonzero only for a group-and-source-specific query.\n• Source Addresses: If the number of sources is N, then there are N 32-bit uni-\ncast addresses appended to the message.\nA Membership Report message consists of the following fields:\n• Type: Defines this message type.\n• Checksum: An error-detecting code, calculated as the 16-bit ones complement\naddition of all the 16-bit words in the message.\n• Number of Group Records: Specifies how many group records are present in\nthis report.\n• Group Records: If the number of group records is M, then there are M 32-bit\nunicast group records appended to the message.\nA group record includes the following fields:\n• Record Type: Defines this record type, as described subsequently.\n• Aux Data Length: Length of the auxiliary data field, in 32-bit words.\n• Number of Sources: Specifies how many source addresses are present in this\n• Multicast Address: The IP multicast address to which this record pertains.\n• Source Addresses: If the number of sources is N, then there are N 32-bit uni-\ncast addresses appended to the message.\n• Auxiliary Data: Additional information pertaining to this record. Currently,\nno auxiliary data values are defined.\nIGMP Operation The objective of each host in using IGMP is to make itself\nknown as a member of a group with a given multicast address to other hosts on the\nLAN and to all routers on the LAN. IGMPv3 introduces the ability for hosts to sig-\nnal group membership with filtering capabilities with respect to sources. A host can\neither signal that it wants to receive traffic from all sources sending to a group\nexcept for some specific sources (called EXCLUDE mode) or that it wants to\nreceive traffic only from some specific sources sending to the group (called\nINCLUDE mode).To join a group, a host sends an IGMP membership report mes-\nsage, in which the group address field is the multicast address of the group.This mes-\nsage is sent in an IP datagram with the same multicast destination address. In other\nwords, the Group Address field of the IGMP message and the Destination Address\nfield of the encapsulating IP header are the same.All hosts that are currently mem-\nbers of this multicast group will receive the message and learn of the new group\nmember. Each router attached to the LAN must listen to all IP multicast addresses\nin order to hear all reports.\nTo maintain a valid current list of active group addresses, a multicast router\nperiodically issues an IGMP general query message, sent in an IP datagram with an\nall-hosts multicast address. Each host that still wishes to remain a member of one or\nmore multicast groups must read datagrams with the all-hosts address.When such a\nCHAPTER 19 / INTERNETWORK OPERATION\nhost receives the query, it must respond with a report message for each group to\nwhich it claims membership.\nNote that the multicast router does not need to know the identity of every host in\na group. Rather, it needs to know that there is at least one group member still active.\nTherefore, each host in a group that receives a query sets a timer with a random delay.\nAny host that hears another host claim membership in the group will cancel its own\nreport. If no other report is heard and the timer expires, a host sends a report.With this\nscheme,only one member of each group should provide a report to the multicast router.\nWhen a host leaves a group, it sends a leave group message to the all-routers\nstatic multicast address. This is accomplished by sending a membership report mes-\nsage with the INCLUDE option and a null list of source addresses; that is, no\nsources are to be included, effectively leaving the group. When a router receives\nsuch a message for a group that has group members on the reception interface, it\nneeds to determine if there are any remaining group members. For this purpose, the\nrouter uses the group-specific query message.\nGroup Membership with IPv6 IGMP was defined for operation with IPv4\nand makes use of 32-bit addresses. IPv6 internets need this same functionality.\nRather than to define a separate version of IGMP for IPv6, its functions have\nbeen incorporated into the new version of the Internet Control Message Protocol\n(ICMPv6). ICMPv6 includes all of the functionality of ICMPv4 and IGMP. For\nmulticast support, ICMPv6 includes both a group-membership query and a group-\nmembership report message, which are used in the same fashion as in IGMP.\n19.2 ROUTING PROTOCOLS\nThe routers in an internet are responsible for receiving and forwarding packets\nthrough the interconnected set of networks. Each router makes routing decision\nbased on knowledge of the topology and traffic/delay conditions of the internet. In a\nsimple internet, a fixed routing scheme is possible. In more complex internets, a\ndegree of dynamic cooperation is needed among the routers. In particular, the router\nmust avoid portions of the network that have failed and should avoid portions of the\nnetwork that are congested. To make such dynamic routing decisions, routers\nexchange routing information using a special routing protocol for that purpose.\nInformation is needed about the status of the internet, in terms of which networks\ncan be reached by which routes, and the delay characteristics of various routes.\nIn considering the routing function, it is important to distinguish two concepts:\n• Routing information: Information about the topology and delays of the internet\n• Routing algorithm: The algorithm used to make a routing decision for a par-\nticular datagram, based on current routing information\nAutonomous Systems\nTo proceed with our discussion of routing protocols, we need to introduce the con-\ncept of an autonomous system. An autonomous system (AS) exhibits the following\ncharacteristics:\n19.2 / ROUTING PROTOCOLS\n1. An AS is a set of routers and networks managed by a single organization.\n2. An AS consists of a group of routers exchanging information via a common rout-\ning protocol.\n3. Except in times of failure, an AS is connected (in a graph-theoretic sense); that\nis, there is a path between any pair of nodes.\nA shared routing protocol, which we shall refer to as an interior router pro-\ntocol (IRP), passes routing information between routers within an AS. The proto-\ncol used within the AS does not need to be implemented outside of the system.\nThis flexibility allows IRPs to be custom tailored to specific applications and\nrequirements.\nIt may happen, however, that an internet will be constructed of more than one\nAS. For example, all of the LANs at a site, such as an office complex or campus,\ncould be linked by routers to form an AS. This system might be linked through a\nwide area network to other ASs. The situation is illustrated in Figure 19.5. In this\ncase, the routing algorithms and information in routing tables used by routers in dif-\nferent ASs may differ. Nevertheless, the routers in one AS need at least a minimal\nlevel of information concerning networks outside the system that can be reached.\nAutonomous system 1 \nAutonomous system 2 \nInterior router protocol\nExterior router protocol\nFigure 19.5\nApplication of Exterior and Interior Routing Protocols\nCHAPTER 19 / INTERNETWORK OPERATION\nWe refer to the protocol used to pass routing information between routers in differ-\nent ASs as an exterior router protocol (ERP).2\nWe can expect that an ERP will need to pass less information than an IRP, for\nthe following reason. If a datagram is to be transferred from a host in one AS to a\nhost in another AS, a router in the first system need only determine the target AS\nand devise a route to get into that target system. Once the datagram enters the tar-\nget AS, the routers within that system can cooperate to deliver the datagram; the\nERP is not concerned with, and does not know about, the details of the route fol-\nlowed within the target AS.\nIn the remainder of this section, we look at what are perhaps the most impor-\ntant examples of these two types of routing protocols: BGP and OSPF. But first, it is\nuseful to look at a different way of characterizing routing protocols.\nApproaches to Routing\nInternet routing protocols employ one of three approaches to gathering and using\nrouting information: distance-vector routing, link-state routing, and path-vector\nDistance-vector routing requires that each node (router or host that imple-\nments the routing protocol) exchange information with its neighboring nodes. Two\nnodes are said to be neighbors if they are both directly connected to the same net-\nwork. This approach is that used in the first generation routing algorithm for\nARPANET, as described in Section 12.2. For this purpose, each node maintains a\nvector of link costs for each directly attached network and distance and next-hop\nvectors for each destination. The relatively simple Routing Information Protocol\n(RIP) uses this approach.\nDistance-vector routing requires the transmission of a considerable amount of\ninformation by each router. Each router must send a distance vector to all of its\nneighbors, and that vector contains the estimated path cost to all networks in the\nconfiguration. Furthermore, when there is a significant change in a link cost or when\na link is unavailable, it may take a considerable amount of time for this information\nto propagate through the internet.\nLink-state routing is designed to overcome the drawbacks of distance-vector\nrouting. When a router is initialized, it determines the link cost on each of its net-\nwork interfaces. The router then advertises this set of link costs to all other routers\nin the internet topology, not just neighboring routers. From then on, the router\nmonitors its link costs.Whenever there is a significant change (a link cost increases\nor decreases substantially, a new link is created, an existing link becomes unavail-\nable), the router again advertises its set of link costs to all other routers in the con-\nfiguration.\nBecause each router receives the link costs of all routers in the configuration,\neach router can construct the topology of the entire configuration and then calcu-\nlate the shortest path to each destination network. Having done this, the router can\nconstruct its routing table, listing the first hop to each destination. Because the\n2In the literature, the terms interior gateway protocol (IGP) and exterior gateway protocol (EGP) are\noften used for what are referred to here as IRP and ERP. However, because the terms IGP and EGP also\nrefer to specific protocols, we avoid their use to define the general concepts.\n19.2 / ROUTING PROTOCOLS\nrouter has a representation of the entire network, it does not use a distributed ver-\nsion of a routing algorithm, as is done in distance-vector routing. Rather, the router\ncan use any routing algorithm to determine the shortest paths. In practice, Dijkstra’s\nalgorithm is used.The Open Shortest Path First (OSPF) protocol is an example of a\nrouting protocol that uses link-state routing. The second-generation routing algo-\nrithm for ARPANET also uses this approach.\nBoth link-state and distance-vector approaches have been used for interior\nrouter protocols. Neither approach is effective for an exterior router protocol.\nIn a distance-vector routing protocol, each router advertises to its neighbors a\nvector listing each network it can reach, together with a distance metric associated\nwith the path to that network. Each router builds up a routing database on the basis\nof these neighbor updates but does not know the identity of intermediate routers\nand networks on any particular path.There are two problems with this approach for\nan exterior router protocol:\n1. This distance-vector protocol assumes that all routers share a common dis-\nCHAPTER 19 / INTERNETWORK OPERATION\nto become congested, and overall quality of operation.Another criterion that could\nbe used is minimizing the number of transit ASs.\nBorder Gateway Protocol\nThe Border Gateway Protocol (BGP) was developed for use in conjunction with\ninternets that employ the TCP/IP suite, although the concepts are applicable to any\ninternet. BGP has become the preferred exterior router protocol for the Internet.\nFunctions BGP was designed to allow routers, called gateways in the standard, in\ndifferent autonomous systems (ASs) to cooperate in the exchange of routing infor-\nmation. The protocol operates in terms of messages, which are sent over TCP con-\nnections. The repertoire of messages is summarized in Table 19.2. The current\nversion of BGP is known as BGP-4 (RFC 1771).\nThree functional procedures are involved in BGP:\n• Neighbor acquisition\n• Neighbor reachability\n• Network reachability\nTwo routers are considered to be neighbors if they are attached to the same\nnetwork. If the two routers are in different autonomous systems, they may wish to\nexchange routing information. For this purpose, it is necessary first to perform\nneighbor acquisition. In essence, neighbor acquisition occurs when two neighboring\nrouters in different autonomous systems agree to exchange routing information reg-\nularly.A formal acquisition procedure is needed because one of the routers may not\nwish to participate. For example, the router may be overburdened and does not want\nto be responsible for traffic coming in from outside the system.In the neighbor acqui-\nsition process, one router sends a request message to the other, which may either\naccept or refuse the offer.The protocol does not address the issue of how one router\nknows the address or even the existence of another router, nor how it decides that it\nneeds to exchange routing information with that particular router.These issues must\nbe dealt with at configuration time or by active intervention of a network manager.\nTo perform neighbor acquisition, two routers send Open messages to each\nother after a TCP connection is established. If each router accepts the request, it\nreturns a Keepalive message in response.\nOnce a neighbor relationship is established, the neighbor reachability proce-\ndure is used to maintain the relationship. Each partner needs to be assured that the\nother partner still exists and is still engaged in the neighbor relationship. For this\npurpose, the two routers periodically issue Keepalive messages to each other.\nBGP-4 Messages\nUsed to open a neighbor relationship with another router.\nUsed to (1) transmit information about a single route and/or (2) list \nmultiple routes to be withdrawn.\nUsed to (1) acknowledge an Open message and (2) periodically confirm \nthe neighbor relationship.\nNotification\nSend when an error condition is detected.\n19.2 / ROUTING PROTOCOLS\nThe final procedure specified by BGP is network reachability. Each router\nmaintains a database of the networks that it can reach and the preferred route for\nreaching each network.When a change is made to this database, the router issues an\nUpdate message that is broadcast to all other routers implementing BGP. Because\nthe Update message is broadcast, all BGP routers can build up and maintain their\nrouting information.\nBGP Messages Figure 19.6 illustrates the formats of all of the BGP messages.\nEach message begins with a 19-octet header containing three fields, as indicated by\nthe shaded portion of each message in the figure:\nFigure 19.6\nBGP Message Formats\n(c) Keepalive message\nUnfeasible routes\nWithdrawn routes\nattributes length\nPath attributes\nNetwork layer\nreachability\ninformation\n(b) Update message\nMy autonomous\nBGP identifier\nOpt parameter length\nOptional parameters\n(a) Open message\nError subcode\n(d) Notification message\nCHAPTER 19 / INTERNETWORK OPERATION\n• Marker: Reserved for authentication. The sender may insert a value in this\nfield that would be used as part of an authentication mechanism to enable the\nrecipient to verify the identity of the sender.\n• Length: Length of message in octets.\n• Type: Type of message: Open, Update, Notification, Keepalive.\nTo acquire a neighbor, a router first opens a TCP connection to the neighbor\nrouter of interest. It then sends an Open message.This message identifies the AS to\nwhich the sender belongs and provides the IP address of the router. It also includes\na Hold Time parameter, which indicates the number of seconds that the sender pro-\nposes for the value of the Hold Timer. If the recipient is prepared to open a neigh-\nbor relationship, it calculates a value of Hold Timer that is the minimum of its Hold\nTime and the Hold Time in the Open message.This calculated value is the maximum\nnumber of seconds that may elapse between the receipt of successive Keepalive\nand/or Update messages by the sender.\nThe Keepalive message consists simply of the header. Each router issues these\nmessages to each of its peers often enough to prevent the Hold Timer from expiring.\nThe Update message communicates two types of information:\n• Information about a single route through the internet. This information is\navailable to be added to the database of any recipient router.\n• A list of routes previously advertised by this router that are being withdrawn.\nAn Update message may contain one or both types of information. Informa-\ntion about a single route through the network involves three fields: the Network\nLayer Reachability Information (NLRI) field, the Total Path Attributes Length\nfield, and the Path Attributes field. The NLRI field consists of a list of identifiers of\nnetworks that can be reached by this route. Each network is identified by its IP\naddress, which is actually a portion of a full IP address. Recall that an IP address is a\n32-bit quantity of the form \nThe left-hand or prefix portion of this\nquantity identifies a particular network.\nThe Path Attributes field contains a list of attributes that apply to this particu-\nlar route.The following are the defined attributes:\n• Origin: Indicates whether this information was generated by an interior router\nprotocol (e.g., OSPF) or an exterior router protocol (in particular, BGP).\n• AS_Path: A list of the ASs that are traversed for this route.\n• Next_Hop: The IP address of the border router that should be used as the next\nhop to the destinations listed in the NLRI field.\n• Multi_Exit_Disc: Used to communicate some information about routes inter-\nnal to an AS.This is described later in this section.\n• Local_Pref: Used by a router to inform other routers within the same AS of its\ndegree of preference for a particular route. It has no significance to routers in\n• Atomic_Aggregate, Aggregator: These two fields implement the concept of\nroute aggregation. In essence, an internet and its corresponding address space\ncan be organized hierarchically (i.e., as a tree). In this case, network addresses\n5network, host6.\n19.2 / ROUTING PROTOCOLS\nare structured in two or more parts. All of the networks of a given subtree\nshare a common partial internet address. Using this common partial address,\nthe amount of information that must be communicated in NLRI can be signif-\nicantly reduced.\nThe AS_Path attribute actually serves two purposes.Because it lists the ASs that\na datagram must traverse if it follows this route, the AS_Path information enables a\nrouter to implement routing policies.That is, a router may decide to avoid a particular\npath to avoid transiting a particular AS. For example, information that is confidential\nmay be limited to certain kinds of ASs. Or a router may have information about the\nperformance or quality of the portion of the internet that is included in an AS that\nleads the router to avoid that AS. Examples of performance or quality metrics include\nlink speed, capacity, tendency to become congested, and overall quality of operation.\nAnother criterion that could be used is minimizing the number of transit ASs.\nThe reader may wonder about the purpose of the Next_Hop attribute. The\nrequesting router will necessarily want to know which networks are reachable via\nthe responding router, but why provide information about other routers? This is\nbest explained with reference to Figure 19.5. In this example, router R1 in\nautonomous system 1 and router R5 in autonomous system 2 implement BGP and\nacquire a neighbor relationship. R1 issues Update messages to R5, indicating which\nnetworks it can reach and the distances (network hops) involved. R1 also provides\nthe same information on behalf of R2.That is, R1 tells R5 what networks are reach-\nable via R2. In this example, R2 does not implement BGP. Typically, most of the\nrouters in an autonomous system will not implement BGP. Only a few routers will\nbe assigned responsibility for communicating with routers in other autonomous sys-\ntems. A final point: R1 is in possession of the necessary information about R2,\nbecause R1 and R2 share an interior router protocol (IRP).\nThe second type of update information is the withdrawal of one or more routes.\nIn this case, the route is identified by the IP address of the destination network.\nFinally, the Notification Message is sent when an error condition is detected.\nThe following errors may be reported:\n• Message header error: Includes authentication and syntax errors.\n• Open message error: Includes syntax errors and options not recognized in an\nOpen message.This message can also be used to indicate that a proposed Hold\nTime in an Open message is unacceptable.\n• Update message error: Includes syntax and validity errors in an Update message.\n• Hold timer expired: If the sending router has not received successive\nKeepalive and/or Update and/or Notification messages within the Hold Time\nperiod, then this error is communicated and the connection is closed.\n• Finite state machine error: Includes any procedural error.\n• Cease: Used by a router to close a connection with another router in the\nabsence of any other error.\nBGP Routing Information Exchange The essence of BGP is the exchange\nof routing information among participating routers in multiple ASs.This process can\nbe quite complex. In what follows, we provide a simplified overview.\nCHAPTER 19 / INTERNETWORK OPERATION\nLet us consider router R1 in autonomous system 1 (AS1), in Figure 19.5. To\nbegin, a router that implements BGP will also implement an internal routing proto-\ncol such as OSPF. Using OSPF, R1 can exchange routing information with other\nrouters within AS1 and build up a picture of the topology of the networks and\nrouters in AS1 and construct a routing table. Next, R1 can issue an Update message\nto R5 in AS2.The Update message could include the following:\n• AS_Path: The identity of AS1\n• Next_Hop: The IP address of R1\n• NLRI: A list of all of the networks in AS1\nThis message informs R5 that all of the networks listed in NLRI are reachable\nvia R1 and that the only autonomous system traversed is AS1.\nSuppose now that R5 also has a neighbor relationship with another router\nin another autonomous system, say R9 in AS3. R5 will forward the information\njust received from R1 to R9 in a new Update message. This message includes the\n• AS_Path: The list of identifiers \n• Next_Hop: The IP address of R5\n• NLRI: A list of all of the networks in AS1\nThis message informs R9 that all of the networks listed in NLRI are reachable\nvia R5 and that the autonomous systems traversed are AS2 and AS1. R9 must now\ndecide if this is its preferred route to the networks listed. It may have knowledge of\nan alternate route to some or all of these networks that it prefers for reasons of per-\nformance or some other policy metric. If R9 decides that the route provided in R5’s\nupdate message is preferable, then R9 incorporates that routing information into its\nrouting database and forwards this new routing information to other neighbors.This\nnew message will include an AS_Path field of \nIn this fashion, routing update information is propagated through the larger\ninternet, consisting of a number of interconnected autonomous systems. The\nAS_Path field is used to assure that such messages do not circulate indefinitely: If\nan Update message is received by a router in an AS that is included in the\nAS_Path field, that router will not forward the update information to other\nRouters within the same AS, called internal neighbors, may exchange BGP\ninformation. In this case, the sending router does not add the identifier of the com-\nmon AS to the AS_Path field. When a router has selected a preferred route to an\nexternal destination, it transmits this route to all of its internal neighbors. Each of\nthese routers then decides if the new route is preferred, in which case the new route\nis added to its database and a new Update message goes out.\nWhen there are multiple entry points into an AS that are available to a bor-\nder router in another AS, the Multi_Exit_Disc attribute may be used to choose\namong them. This attribute contains a number that reflects some internal metric\nfor reaching destinations within an AS. For example, suppose in Figure 19.5 that\nboth R1 and R2 implement BGP and both have a neighbor relationship with R5.\nEach provides an Update message to R5 for network 1.3 that includes a routing\n5AS3, AS2, AS16.\n19.2 / ROUTING PROTOCOLS\nmetric used internal to AS1, such as a routing metric associated with the OSPF\ninternal router protocol. R5 could then use these two metrics as the basis for choos-\ning between the two routes.\nOpen Shortest Path First (OSPF) Protocol\nThe OSPF protocol (RFC 2328) is now widely used as the interior router protocol in\nTCP/IP networks. OSPF computes a route through the internet that incurs the least\ncost based on a user-configurable metric of cost. The user can configure the cost to\nexpress a function of delay, data rate, dollar cost, or other factors. OSPF is able to\nequalize loads over multiple equal-cost paths.\nEach router maintains a database that reflects the known topology of the\nautonomous system of which it is a part. The topology is expressed as a directed\ngraph.The graph consists of the following:\n• Vertices, or nodes, of two types:\n2. network, which is in turn of two types\na. transit, if it can carry data that neither originate nor terminate on an end\nsystem attached to this network\nb. stub, if it is not a transit network\n• Edges of two types:\n1. graph edges that connect two router vertices when the corresponding\nrouters are connected to each other by a direct point-to-point link\n2. graph edges that connect a router vertex to a network vertex when the\nrouter is directly connected to the network\nFigure 19.7, based on one in RFC 2328, shows an example of an autonomous\nsystem, and Figure 19.8 is the resulting directed graph. The mapping is straight-\n• Two routers joined by a point-to-point link are represented in the graph as being\ndirectly connected by a pair of edges,one in each direction (e.g.,routers 6 and 10).\n• When multiple routers are attached to a network (such as a LAN or packet-\nswitching network), the directed graph shows all routers bidirectionally con-\nnected to the network vertex (e.g., routers 1, 2, 3, and 4 all connect to\nnetwork 3).\n• If a single router is attached to a network, the network will appear in the graph\nas a stub connection (e.g., network 7).\n• An end system, called a host, can be directly connected to a router, in which\ncase it is depicted in the corresponding graph (e.g., host 1).\n• If a router is connected to other autonomous systems, then the path cost to\neach network in the other system must be obtained by some exterior router\nprotocol (ERP). Each such network is represented on the graph by a stub\nand an edge to the router with the known path cost (e.g., networks 12\nthrough 15).\nCHAPTER 19 / INTERNETWORK OPERATION\nFigure 19.7\nA Sample Autonomous System\nA cost is associated with the output side of each router interface. This cost is\nconfigurable by the system administrator. Arcs on the graph are labeled with the\ncost of the corresponding router output interface. Arcs having no labeled cost have\na cost of 0. Note that arcs leading from networks to routers always have a cost of 0.\nA database corresponding to the directed graph is maintained by each router.\nIt is pieced together from link state messages from other routers in the internet.\nUsing Dijkstra’s algorithm (see Section 12.3), a router calculates the least-cost path\nto all destination networks.The result for router 6 of Figure 19.7 is shown as a tree in\nFigure 19.9, with R6 as the root of the tree.The tree gives the entire route to any des-\ntination network or host. However, only the next hop to the destination is used in the\nforwarding process. The resulting routing table for router 6 is shown in Table 19.3.\nThe table includes entries for routers advertising external routes (routers 5 and 7).\nFor external networks whose identity is known, entries are also provided.\n19.3 / INTEGRATED SERVICES ARCHITECTURE\nFigure 19.8\nDirected Graph of Autonomous System of Figure 19.7\n19.3 INTEGRATED SERVICES ARCHITECTURE\nTo meet the requirement for QoS-based service, the IETF is developing a suite of\nstandards under the general umbrella of the Integrated Services Architecture\n(ISA). ISA, intended to provide QoS transport over IP-based internets, is defined in\noverall terms in RFC 1633, while a number of other documents are being developed\nto fill in the details.Already, a number of vendors have implemented portions of the\nISA in routers and end-system software.\nThis section provides an overview of ISA.\nInternet Traffic\nTraffic on a network or internet can be divided into two broad categories: elastic\nand inelastic. A consideration of their differing requirements clarifies the need for\nan enhanced internet architecture.\nElastic Traffic Elastic traffic is that which can adjust, over wide ranges, to changes\nin delay and throughput across an internet and still meet the needs of its applica-\ntions.This is the traditional type of traffic supported on TCP/IP-based internets and\nCHAPTER 19 / INTERNETWORK OPERATION\nis the type of traffic for which internets were designed. Applications that generate\nsuch traffic typically use TCP or UDP as a transport protocol. In the case of UDP,\nthe application will use as much capacity as is available up to the rate that the appli-\ncation generates data. In the case of TCP, the application will use as much capacity\nas is available up to the maximum rate that the end-to-end receiver can accept data.\nAlso with TCP, traffic on individual connections adjusts to congestion by reducing\nthe rate at which data are presented to the network; this is described in Chapter 20.\nApplications that can be classified as elastic include the common applications\nthat operate over TCP or UDP, including file transfer (FTP), electronic mail\n(SMTP), remote login (TELNET), network management (SNMP), and Web access\n(HTTP). However, there are differences among the requirements of these applica-\ntions. For example,\n• E-mail is generally insensitive to changes in delay.\n• When file transfer is done interactively, as it frequently is, the user expects the\ndelay to be proportional to the file size and so is sensitive to changes in\nthroughput.\n• With network management, delay is generally not a serious concern. How-\never, if failures in an internet are the cause of congestion, then the need for\nFigure 19.9\nThe SPF Tree for Router R6\n19.3 / INTEGRATED SERVICES ARCHITECTURE\nSNMP messages to get through with minimum delay increases with increased\ncongestion.\n• Interactive applications, such as remote logon and Web access, are sensitive to\nIt is important to realize that it is not per-packet delay that is the quantity of\ninterest. As noted in [CLAR95], observation of real delays across the Internet sug-\ngest that wide variations in delay do not occur. Because of the congestion control\nmechanisms in TCP, when congestion develops, delays only increase modestly before\nthe arrival rate from the various TCP connections slow down. Instead, the QoS per-\nceived by the user relates to the total elapsed time to transfer an element of the cur-\nrent application. For an interactive TELNET-based application, the element may be\na single keystroke or single line. For a Web access, the element is a Web page, which\ncould be as little as a few kilobytes or could be substantially larger for an image-rich\npage. For a scientific application, the element could be many megabytes of data.\nFor very small elements, the total elapsed time is dominated by the delay time\nacross the internet. However, for larger elements, the total elapsed time is dictated by\nthe sliding-window performance of TCP and is therefore dominated by the throughput\nachieved over the TCP connection.Thus,for large transfers,the transfer time is propor-\ntional to the size of the file and the degree to which the source slows due to congestion.\nIt should be clear that even if we confine our attention to elastic traffic, a QoS-\nbased internet service could be of benefit.Without such a service, routers are dealing\nevenhandedly with arriving IP packets, with no concern for the type of application\nand whether a particular packet is part of a large transfer element or a small one.\nRouting Table for R6\nDestination\nCHAPTER 19 / INTERNETWORK OPERATION\nUnder such circumstances, and if congestion develops, it is unlikely that resources\nwill be allocated in such a way as to meet the needs of all applications fairly. When\ninelastic traffic is added to the mix, the results are even more unsatisfactory.\nInelastic Traffic Inelastic traffic does not easily adapt, if at all, to changes in delay\nand throughput across an internet. The prime example is real-time traffic. The\nrequirements for inelastic traffic may include the following:\n• Throughput: A minimum throughput value may be required. Unlike most elas-\ntic traffic, which can continue to deliver data with perhaps degraded service,\nmany inelastic applications absolutely require a given minimum throughput.\n• Delay: An example of a delay-sensitive application is stock trading; someone\nwho consistently receives later service will consistently act later, and with\ngreater disadvantage.\n• Jitter: The magnitude of delay variation, called jitter, is a critical factor in real-\ntime applications. Because of the variable delay imposed by the Internet, the\ninterarrival times between packets are not maintained at a fixed interval at the\ndestination.To compensate for this, the incoming packets are buffered, delayed\nsufficiently to compensate for the jitter, and then released at a constant rate to\nthe software that is expecting a steady real-time stream.The larger the allowable\ndelay variation, the longer the real delay in delivering the data and the greater\nthe size of the delay buffer required at receivers. Real-time interactive applica-\ntions, such as teleconferencing, may require a reasonable upper bound on jitter.\n• Packet loss: Real-time applications vary in the amount of packet loss, if any,\nthat they can sustain.\nThese requirements are difficult to meet in an environment with variable queu-\ning delays and congestion losses. Accordingly, inelastic traffic introduces two new\nrequirements into the internet architecture. First, some means is needed to give pref-\nerential treatment to applications with more demanding requirements. Applications\nneed to be able to state their requirements,either ahead of time in some sort of service\nrequest function, or on the fly, by means of fields in the IP packet header.The former\napproach provides more flexibility in stating requirements, and it enables the network\nto anticipate demands and deny new requests if the required resources are unavail-\nable.This approach implies the use of some sort of resource reservation protocol.\nA second requirement in supporting inelastic traffic in an internet architecture\nis that elastic traffic must still be supported. Inelastic applications typically do not\nback off and reduce demand in the face of congestion, in contrast to TCP-based\napplications. Therefore, in times of congestion, inelastic traffic will continue to sup-\nply a high load, and elastic traffic will be crowded off the internet. A reservation\nprotocol can help control this situation by denying service requests that would leave\ntoo few resources available to handle current elastic traffic.\nISA Approach\nThe purpose of ISA is to enable the provision of QoS support over IP-based inter-\nnets.The central design issue for ISA is how to share the available capacity in times\nof congestion.\n19.3 / INTEGRATED SERVICES ARCHITECTURE\nFor an IP-based internet that provides only a best-effort service, the tools for\ncontrolling congestion and providing service are limited. In essence, routers have\ntwo mechanisms to work with:\n• Routing algorithm: Most routing protocols in use in internets allow routes to\nbe selected to minimize delay. Routers exchange information to get a picture\nof the delays throughout the internet. Minimum-delay routing helps to bal-\nance loads, thus decreasing local congestion, and helps to reduce delays seen\nby individual TCP connections.\n• Packet discard: When a router’s buffer overflows, it discards packets.Typically,\nthe most recent packet is discarded. The effect of lost packets on a TCP con-\nnection is that the sending TCP entity backs off and reduces its load, thus help-\ning to alleviate internet congestion.\nThese tools have worked reasonably well. However, as the discussion in the\npreceding subsection shows, such techniques are inadequate for the variety of traffic\nnow coming to internets.\nISA is an overall architecture within which a number of enhancements to the tra-\nditional best-effort mechanisms are being developed. In ISA, each IP packet can be\nassociated with a flow.RFC 1633 defines a flow as a distinguishable stream of related IP\npackets that results from a single user activity and requires the same QoS.For example,\na flow might consist of one transport connection or one video stream distinguishable by\nthe ISA.A flow differs from a TCP connection in two respects:A flow is unidirectional,\nand there can be more than one recipient of a flow (multicast).Typically,an IP packet is\nidentified as a member of a flow on the basis of source and destination IP addresses and\nport numbers, and protocol type.The flow identifier in the IPv6 header is not necessar-\nily equivalent to an ISA flow,but in future the IPv6 flow identifier could be used in ISA.\nISA makes use of the following functions to manage congestion and provide\nQoS transport:\n• Admission control: For QoS transport (other than default best-effort transport),\nISA requires that a reservation be made for a new flow.If the routers collectively\ndetermine that there are insufficient resources to guarantee the requested QoS,\nthen the flow is not admitted. The protocol RSVP is used to make reservations.\n• Routing algorithm: The routing decision may be based on a variety of QoS\nparameters, not just minimum delay. For example, the routing protocol OSPF,\ndiscussed in Section 19.2, can select routes based on QoS.\n• Queuing discipline: A vital element of the ISA is an effective queuing policy\nthat takes into account the differing requirements of different flows.\n• Discard policy: A discard policy determines which packets to drop when a\nbuffer is full and new packets arrive.A discard policy can be an important ele-\nment in managing congestion and meeting QoS guarantees.\nISA Components\nFigure 19.10 is a general depiction of the implementation architecture for ISA\nwithin a router. Below the thick horizontal line are the forwarding functions of the\nrouter; these are executed for each packet and therefore must be highly optimized.\nCHAPTER 19 / INTERNETWORK OPERATION\nThe remaining functions, above the line, are background functions that create data\nstructures used by the forwarding functions.\nThe principal background functions are as follows:\n• Reservation protocol: This protocol is to reserve resources for a new flow at a\ngiven level of QoS. It is used among routers and between routers and end sys-\ntems. The reservation protocol is responsible for maintaining flow-specific\nstate information at the end systems and at the routers along the path of the\nflow. RSVP is used for this purpose.The reservation protocol updates the traf-\nfic control database used by the packet scheduler to determine the service\nprovided for packets of each flow.\n• Admission control: When a new flow is requested, the reservation protocol\ninvokes the admission control function. This function determines if sufficient\nresources are available for this flow at the requested QoS. This determination\nis based on the current level of commitment to other reservations and/or on\nthe current load on the network.\n• Management agent: A network management agent is able to modify the traf-\nfic control database and to direct the admission control module in order to set\nadmission control policies.\n• Routing protocol: The routing protocol is responsible for maintaining a rout-\ning database that gives the next hop to be taken for each destination address\nand each flow.\nThese background functions support the main task of the router, which is the\nforwarding of packets. The two principal functional areas that accomplish forward-\ning are the following:\n• Classifier and route selection: For the purposes of forwarding and traffic con-\ntrol, incoming packets must be mapped into classes.A class may correspond to\na single flow or to a set of flows with the same QoS requirements. For example,\nFigure 19.10\nIntegrated Services Architecture Implemented in Router\nprotocol(s)\nReservation\nClassifier &\nQoS queuing\nBest-effort queuing\n19.3 / INTEGRATED SERVICES ARCHITECTURE\nthe packets of all video flows or the packets of all flows attributable to a par-\nticular organization may be treated identically for purposes of resource alloca-\ntion and queuing discipline. The selection of class is based on fields in the IP\nheader. Based on the packet’s class and its destination IP address, this function\ndetermines the next-hop address for this packet.\n• Packet scheduler: This function manages one or more queues for each output\nport. It determines the order in which queued packets are transmitted and the\nselection of packets for discard, if necessary. Decisions are made based on a\nCHAPTER 19 / INTERNETWORK OPERATION\nFigure 19.11 illustrates this scheme and explains the use of the term bucket.\nThe bucket represents a counter that indicates the allowable number of octets of IP\ndata that can be sent at any time. The bucket fills with octet tokens at the rate of R\n(i.e., the counter is incremented R times per second), up to the bucket capacity (up\nto the maximum counter value). IP packets arrive and are queued for processing.\nAn IP packet may be processed if there are sufficient octet tokens to match the IP\ndata size. If so, the packet is processed and the bucket is drained of the correspond-\ning number of tokens. If a packet arrives and there are insufficient tokens available,\nthen the packet exceeds the TSpec for this flow. The treatment for such packets is\nnot specified in the ISA documents; common actions are relegating the packet to\nbest-effort service, discarding the packet, or marking the packet in such a way that it\nmay be discarded in future.\nOver the long run, the rate of IP data allowed by the token bucket is R. How-\never, if there is an idle or relatively slow period, the bucket capacity builds up, so\nthat at most an additional B octets above the stated rate can be accepted.Thus, B is\na measure of the degree of burstiness of the data flow that is allowed.\nGuaranteed Service The key elements of the guaranteed service are as follows:\n• The service provides assured capacity, or data rate.\nsize B bits\nToken replenishment\n1. Router puts tokens\ninto bucket at\npredetermined rate.\n2. Tokens can\naccumulate up\nto bucket size;\nexcess tokens\n3. Traffic seeks\nto network.\n4. Router's queue regulator\nrequests tokens equal to\nto size of the next packet.\n5. If tokens are available,\npacket is queued for\ntransmission.\n6. If tokens are not available,\npacket is either queued for\ntransmission but marked as\nexcess, buffered for later transmission,\nor discarded.\nFigure 19.11\nToken Bucket Scheme\n19.3 / INTEGRATED SERVICES ARCHITECTURE\n• There is a specified upper bound on the queuing delay through the network.\nThis must be added to the propagation delay, or latency, to arrive at the bound\non total delay through the network.\n• There are no queuing losses.That is, no packets are lost due to buffer overflow;\npackets may be lost due to failures in the network or changes in routing paths.\nWith this service, an application provides a characterization of its expected\ntraffic profile, and the service determines the end-to-end delay that it can guarantee.\nOne category of applications for this service is those that need an upper\nbound on delay so that a delay buffer can be used for real-time playback of\nincoming data, and that do not tolerate packet losses because of the degradation\nin the quality of the output. Another example is applications with hard real-time\nThe guaranteed service is the most demanding service provided by ISA.\nBecause the delay bound is firm, the delay has to be set at a large value to cover rare\ncases of long queuing delays.\nControlled Load The key elements of the controlled load service are as follows:\n• The service tightly approximates the behavior visible to applications receiving\nbest-effort service under unloaded conditions.\n• There is no specified upper bound on the queuing delay through the network.\nHowever, the service ensures that a very high percentage of the packets do not\nexperience delays that greatly exceed the minimum transit delay (i.e., the\ndelay due to propagation time plus router processing time with no queuing\n• A very high percentage of transmitted packets will be successfully delivered\n(i.e., almost no queuing loss).\nAs was mentioned, the risk in an internet that provides QoS for real-time\napplications is that best-effort traffic is crowded out. This is because best-effort\ntypes of applications employ TCP, which will back off in the face of congestion and\ndelays. The controlled load service guarantees that the network will set aside suffi-\ncient resources so that an application that receives this service will see a network\nthat responds as if these real-time applications were not present and competing for\nThe controlled service is useful for applications that have been referred to as\nadaptive real-time applications [CLAR92]. Such applications do not require an a\npriori upper bound on the delay through the network. Rather, the receiver measures\nthe jitter experienced by incoming packets and sets the playback point to the mini-\nmum delay that still produces a sufficiently low loss rate (e.g., video can be adaptive\nby dropping a frame or delaying the output stream slightly; voice can be adaptive by\nadjusting silent periods).\nQueuing Discipline\nAn important component of an ISA implementation is the queuing discipline used\nat the routers. Routers traditionally have used a first-in-first-out (FIFO) queuing\nCHAPTER 19 / INTERNETWORK OPERATION\ndiscipline at each output port. A single queue is maintained at each output port.\nWhen a new packet arrives and is routed to an output port, it is placed at the end of\nthe queue. As long as the queue is not empty, the router transmits packets from the\nqueue, taking the oldest remaining packet next.\nThere are several drawbacks to the FIFO queuing discipline:\n• No special treatment is given to packets from flows that are of higher priority\nor are more delay sensitive. If a number of packets from different flows are\nready to be forwarded, they are handled strictly in FIFO order.\n• If a number of smaller packets are queued behind a long packet, then FIFO\nqueuing results in a larger average delay per packet than if the shorter packets\nwere transmitted before the longer packet. In general, flows of larger packets\nget better service.\n• A greedy TCP connection can crowd out more altruistic connections. If con-\ngestion occurs and one TCP connection fails to back off, other connections\nalong the same path segment must back off more than they would otherwise\nhave to do.\nTo overcome the drawbacks of FIFO queuing, some sort of fair queuing scheme is\nused, in which a router maintains multiple queues at each output port (Figure\n19.12).With simple fair queuing, each incoming packet is placed in the queue for its\nflow. The queues are serviced in round-robin fashion, taking one packet from each\nnonempty queue in turn. Empty queues are skipped over.This scheme is fair in that\neach busy flow gets to send exactly one packet per cycle. Further, this is a form of\nload balancing among the various flows. There is no advantage in being greedy. A\ngreedy flow finds that its queues become long, increasing its delays, whereas other\nflows are unaffected by this behavior.\nA number of vendors have implemented a refinement of fair queuing known\nas weighted fair queuing (WFQ). In essence,WFQ takes into account the amount of\ntraffic through each queue and gives busier queues more capacity without com-\npletely shutting out less busy queues. In addition, WFQ can take into account the\namount of service requested by each traffic flow and adjust the queuing discipline\naccordingly.\n(b) Fair queuing\n(a) FIFO queuing\nMultiplexed\nMultiplexed\nFigure 19.12\nFIFO and Fair Queuing\n19.3 / INTEGRATED SERVICES ARCHITECTURE\nResource ReSerVation Protocol (RSVP)\nRFC 2205 defines RSVP, which provides supporting functionality for ISA.This sub-\nsection provides an overview.\nA key task, perhaps the key task, of an internetwork is to deliver data from a\nsource to one or more destinations with the desired quality of service (QoS), such as\nthroughput, delay, delay variance, and so on.This task becomes increasingly difficult\non any internetwork with increasing number of users, data rate of applications, and\nuse of multicasting. To meet these needs, it is not enough for an internet to react to\ncongestion. Instead a tool is needed to prevent congestion by allowing applications\nto reserve network resources at a given QoS.\nPreventive measures can be useful in both unicast and multicast transmission.\nFor unicast, two applications agree on a specific quality of service for a session and\nexpect the internetwork to support that quality of service. If the internetwork is\nheavily loaded, it may not provide the desired QoS and instead deliver packets at a\nreduced QoS. In that case, the applications may have preferred to wait before initiat-\ning the session or at least to have been alerted to the potential for reduced QoS. A\nway of dealing with this situation is to have the unicast applications reserve resources\nin order to meet a given quality of service.Routers along an intended path could then\npreallocate resources (queue space, outgoing capacity) to assure the desired QoS. If a\nrouter could not meet the resource reservation because of prior outstanding reserva-\ntions, then the applications could be informed. The applications may then decide to\ntry again at a reduced QoS reservation or may decide to try later.\nMulticast transmission presents a much more compelling case for implement-\ning resource reservation. A multicast transmission can generate a tremendous\namount of internetwork traffic if either the application is high-volume (e.g., video)\nor the group of multicast destinations is large and scattered, or both. What makes\nthe case for multicast resource reservation is that much of the potential load gener-\nated by a multicast source may easily be prevented.This is so for two reasons:\n1. Some members of an existing multicast group may not require delivery from a\nparticular source over some given period of time. For example, there may be\ntwo “channels” (two multicast sources) broadcasting to a particular multicast\ngroup at the same time. A multicast destination may wish to “tune in” to only\none channel at a time.\n2. Some members of a group may only be able to handle a portion of the source\ntransmission. For example, a video source may transmit a video stream that\nconsists of two components: a basic component that provides a reduced pic-\nture quality, and an enhanced component. Some receivers may not have the\nprocessing power to handle the enhanced component or may be connected to\nthe internetwork through a subnetwork or link that does not have the capacity\nfor the full signal.\nThus, the use of resource reservation can enable routers to decide ahead of\ntime if they can meet the requirement to deliver a multicast transmission to all des-\nignated multicast receivers and to reserve the appropriate resources if possible.\nInternet resource reservation differs from the type of resource reservation that\nmay be implemented in a connection-oriented network, such as ATM or frame relay.\nCHAPTER 19 / INTERNETWORK OPERATION\nAn internet resource reservation scheme must interact with a dynamic routing strat-\negy that allows the route followed by packets of a given transmission to change.\nWhen the route changes, the resource reservations must be changed.To deal with this\ndynamic situation, the concept of soft state is used.A soft state is simply a set of state\ninformation at a router that expires unless regularly refreshed from the entity that\nrequested the state. If a route for a given transmission changes, then some soft states\nwill expire and new resource reservations will invoke the appropriate soft states on\nthe new routers along the route. Thus, the end systems requesting resources must\nperiodically renew their requests during the course of an application transmission.\nBased on these considerations, the specification lists the following characteris-\ntics of RSVP:\n• Unicast and multicast: RSVP makes reservations for both unicast and multi-\ncast transmissions, adapting dynamically to changing group membership as\nwell as to changing routes, and reserving resources based on the individual\nrequirements of multicast members.\n• Simplex: RSVP makes reservations for unidirectional data flow.Data exchanges\nbetween two end systems require separate reservations in the two directions.\n• Receiver-initiated reservation: The receiver of a data flow initiates and main-\ntains the resource reservation for that flow.\n• Maintaining soft state in the internet: RSVP maintains a soft state at interme-\ndiate routers and leaves the responsibility for maintaining these reservation\nstates to end users.\n• Providing different reservation styles: These allow RSVP users to specify how\nreservations for the same multicast group should be aggregated at the interme-\ndiate switches. This feature enables a more efficient use of internet resources.\n• Transparent operation through non-RSVP routers: Because reservations and\nRSVP are independent of routing protocol, there is no fundamental conflict in\na mixed environment in which some routers do not employ RSVP. These\nrouters will simply use a best-effort delivery technique.\n• Support for IPv4 and IPv6: RSVP can exploit the Type-of-Service field in the\nIPv4 header and the Flow Label field in the IPv6 header.\n19.4 DIFFERENTIATED SERVICES\nThe Integrated Services Architecture (ISA) and RSVP are intended to support QoS\ncapability in the Internet and in private internets. Although ISA in general and\nRSVP in particular are useful tools in this regard, these features are relatively com-\nplex to deploy. Further, they may not scale well to handle large volumes of traffic\nbecause of the amount of control signaling required to coordinate integrated QoS\nofferings and because of the maintenance of state information required at routers.\nAs the burden on the Internet grows, and as the variety of applications grow,\nthere is an immediate need to provide differing levels of QoS to different traffic\nflows. The differentiated services (DS) architecture (RFC 2475) is designed to\n19.4 / DIFFERENTIATED SERVICES\nprovide a simple, easy-to-implement, low-overhead tool to support a range of net-\nwork services that are differentiated on the basis of performance.\nSeveral key characteristics of DS contribute to its efficiency and ease of\ndeployment:\n• IP packets are labeled for differing QoS treatment using the existing IPv4\n(Figure 18.6) or IPv6 (Figure 18.11) DS field.Thus, no change is required to IP.\n• A service level agreement (SLA) is established between the service provider\n(internet domain) and the customer prior to the use of DS. This avoids the\nneed to incorporate DS mechanisms in applications. Thus, existing applica-\ntions need not be modified to use DS.\n• DS provides a built-in aggregation mechanism. All traffic with the same DS\noctet is treated the same by the network service. For example, multiple voice\nconnections are not handled individually but in the aggregate. This provides\nfor good scaling to larger networks and traffic loads.\n• DS is implemented in individual routers by queuing and forwarding packets\nbased on the DS octet. Routers deal with each packet individually and do not\nhave to save state information on packet flows.\nToday, DS is the most widely accepted QoS mechanism in enterprise networks.\nAlthough DS is intended to provide a simple service based on relatively sim-\nple mechanisms, the set of RFCs related to DS is relatively complex. Table 19.4 sum-\nmarizes some of the key terms from these specifications.\nThe DS type of service is provided within a DS domain, which is defined as a con-\ntiguous portion of the Internet over which a consistent set of DS policies are admin-\nistered. Typically, a DS domain would be under the control of one administrative\nentity. The services provided across a DS domain are defined in an SLA, which is a\nservice contract between a customer and the service provider that specifies the for-\nwarding service that the customer should receive for various classes of packets. A\ncustomer may be a user organization or another DS domain. Once the SLA is estab-\nlished, the customer submits packets with the DS octet marked to indicate the packet\nclass.The service provider must assure that the customer gets at least the agreed QoS\nfor each packet class. To provide that QoS, the service provider must configure the\nappropriate forwarding policies at each router (based on DS octet value) and must\nmeasure the performance being provided for each class on an ongoing basis.\nIf a customer submits packets intended for destinations within the DS domain,\nthen the DS domain is expected to provide the agreed service. If the destination is\nbeyond the customer’s DS domain, then the DS domain will attempt to forward the\npackets through other domains, requesting the most appropriate service to match\nthe requested service.\nA draft DS framework document lists the following detailed performance\nparameters that might be included in an SLA:\n• Detailed service performance parameters such as expected throughput, drop\nprobability, latency\nCHAPTER 19 / INTERNETWORK OPERATION\n• Constraints on the ingress and egress points at which the service is provided,\nindicating the scope of the service\n• Traffic profiles that must be adhered to for the requested service to be provided,\nsuch as token bucket parameters\n• Disposition of traffic submitted in excess of the specified profile\nThe framework document also gives some examples of services that might be\n1. Traffic offered at service level A will be delivered with low latency.\n2. Traffic offered at service level B will be delivered with low loss.\n3. Ninety percent of in-profile traffic delivered at service level C will experience no\nmore than 50 ms latency.\n4. Ninety-five percent of in-profile traffic delivered at service level D will be delivered.\n5. Traffic offered at service level E will be allotted twice the bandwidth of traffic\ndelivered at service level F.\nTerminology for Differentiated Services\nBehavior Aggregate\nA set of packets with the same DS codepoint crossing a link in a particular\nSelects packets based on the DS field (BA classifier) or on multiple fields within\nthe packet header (MF classifier).\nDS Boundary Node\nA DS node that connects one DS domain to a node in another domain.\nDS Codepoint\nA specified value of the 6-bit DSCP portion of the 8-bit DS field in the IP header.\nA contiguous (connected) set of nodes, capable of implementing differentiated ser-\nvices, that operate with a common set of service provisioning policies and per-hop\nbehavior definitions.\nDS Interior Node\nA DS node that is not a DS boundary node.\nA node that supports differentiated services.Typically, a DS node is a router.A host\nsystem that provides differentiated services for applications in the host is also a DS\nThe process of discarding packets based on specified rules; also called policing.\nThe process of setting the DS codepoint in a packet. Packets may be marked on\ninitiation and may be re-marked by an en route DS node.\nThe process of measuring the temporal properties (e.g., rate) of a packet stream\nselected by a classifier.The instantaneous state of that process may affect marking,\nshaping, and dropping functions.\nPer-Hop Behavior \nThe externally observable forwarding behavior applied at a node to a behavior \nService Level \nA service contract between a customer and a service provider that specifies the \nAgreement (SLA)\nforwarding service a customer should receive.\nThe process of delaying packets within a packet stream to cause it to conform to\nsome defined traffic profile.\nTraffic Conditioning\nControl functions performed to enforce rules specified in a TCA, including\nmetering, marking, shaping, and dropping.\nTraffic Conditioning \nAn agreement specifying classifying rules and traffic conditioning rules that are to \nAgreement (TCA)\napply to packets selected by the classifier.\n19.4 / DIFFERENTIATED SERVICES\n6. Traffic with drop precedence X has a higher probability of delivery than traffic\nwith drop precedence Y.\nThe first two examples are qualitative and are valid only in comparison to\nother traffic, such as default traffic that gets a best-effort service. The next two\nexamples are quantitative and provide a specific guarantee that can be verified by\nmeasurement on the actual service without comparison to any other services\noffered at the same time. The final two examples are a mixture of quantitative and\nqualitative.\nPackets are labeled for service handling by means of the 6-bit DS field in the IPv4\nheader or the IPv6 header. The value of the DS field, referred to as the DS code-\npoint, is the label used to classify packets for differentiated services. Figure 19.13a\nshows the DS field.\nWith a 6-bit codepoint, there are in principle 64 different classes of traffic that\ncould be defined. These 64 codepoints are allocated across three pools of code-\npoints, as follows:\n• Codepoints of the form xxxxx0, where x is either 0 or 1, are reserved for\nassignment as standards.\n• Codepoints of the form xxxx11 are reserved for experimental or local use.\n• Codepoints of the form xxxx01 are also reserved for experimental or local use\nbut may be allocated for future standards action as needed.\nWithin the first pool, several assignments are made in RFC 2474. The code-\npoint 000000 is the default packet class. The default class is the best-effort forward-\ning behavior in existing routers. Such packets are forwarded in the order that they\nare received as soon as link capacity becomes available. If other higher-priority\nFigure 19.13\n(b) Codepoints for assured forwarding PHB\n(a) DS Field\nDrop precedence\nDS codepoint\nClass 4—best service\nDrop precedence\nLow—most important\nHigh—least important \nDifferentiated services codepoint\nClass selector\nExpedited forwarding (EF) behavior\nDefault behavior\nClass selector\nCHAPTER 19 / INTERNETWORK OPERATION\npackets in other DS classes are available for transmission, these are given prefer-\nence over best-effort default packets.\nCodepoints of the form xxx000 are reserved to provide backward compatibil-\nity with the IPv4 precedence service.To explain this requirement, we need to digress\nto an explanation of the IPv4 precedence service. The IPv4 type of service (TOS)\nfield includes two subfields: a 3-bit precedence subfield and a 4-bit TOS subfield.\nThese subfields serve complementary functions. The TOS subfield provides guid-\nance to the IP entity (in the source or router) on selecting the next hop for this data-\ngram, and the precedence subfield provides guidance about the relative allocation\nof router resources for this datagram.\nThe precedence field is set to indicate the degree of urgency or priority to be\nassociated with a datagram. If a router supports the precedence subfield, there are\nthree approaches to responding:\n• Route selection: A particular route may be selected if the router has a smaller\nqueue for that route or if the next hop on that route supports network prece-\ndence or priority (e.g., a token ring network supports priority).\n• Network service: If the network on the next hop supports precedence, then\nthat service is invoked.\n• Queuing discipline: A router may use precedence to affect how queues are\nhandled. For example, a router may give preferential treatment in queues to\ndatagrams with higher precedence.\nRFC 1812, Requirements for IP Version 4 Routers, provides recommenda-\ntions for queuing discipline that fall into two categories:\n• Queue service\n(a) Routers SHOULD implement precedence-ordered queue service. Prece-\ndence-ordered queue service means that when a packet is selected for out-\nput on a (logical) link, the packet of highest precedence that has been\nqueued for that link is sent.\n(b) Any router MAY implement other policy-based throughput management\nprocedures that result in other than strict precedence ordering, but it\nMUST be configurable to suppress them (i.e., use strict ordering).\n• Congestion control. When a router receives a packet beyond its storage capac-\nity, it must discard it or some other packet or packets.\n(a) A router MAY discard the packet it has just received; this is the simplest\nbut not the best policy.\n(b) Ideally, the router should select a packet from one of the sessions most\nheavily abusing the link, given that the applicable QoS policy permits this.\nA recommended policy in datagram environments using FIFO queues is\nto discard a packet randomly selected from the queue.An equivalent algo-\nrithm in routers using fair queues is to discard from the longest queue. A\nrouter MAY use these algorithms to determine which packet to discard.\n(c) If precedence-ordered queue service is implemented and enabled, the\nrouter MUST NOT discard a packet whose IP precedence is higher than\nthat of a packet that is not discarded.\n19.4 / DIFFERENTIATED SERVICES\n(d) A router MAY protect packets whose IP headers request the maximize reli-\nability TOS,except where doing so would be in violation of the previous rule.\n(e) A router MAY protect fragmented IP packets, on the theory that dropping\na fragment of a datagram may increase congestion by causing all frag-\nments of the datagram to be retransmitted by the source.\n(f) To help prevent routing perturbations or disruption of management func-\ntions, the router MAY protect packets used for routing control, link con-\ntrol, or network management from being discarded. Dedicated routers\n(i.e., routers that are not also general purpose hosts, terminal servers, etc.)\ncan achieve an approximation of this rule by protecting packets whose\nsource or destination is the router itself.\nThe DS codepoints of the form xxx000 should provide a service that at mini-\nmum is equivalent to that of the IPv4 precedence functionality.\nDS Configuration and Operation\nFigure 19.14 illustrates the type of configuration envisioned in the DS documents.A\nDS domain consists of a set of contiguous routers; that is, it is possible to get from\nany router in the domain to any other router in the domain by a path that does not\ninclude routers outside the domain.Within a domain, the interpretation of DS code-\npoints is uniform, so that a uniform, consistent service is provided.\nRouters in a DS domain are either boundary nodes or interior nodes. Typi-\ncally, the interior nodes implement simple mechanisms for handling packets based\nFigure 19.14\n\u0001 Border component\nShaper/dropper\n\u0001 Interior component\nQueue management\nCHAPTER 19 / INTERNETWORK OPERATION\non their DS codepoint values. This includes queuing discipline to give preferential\ntreatment depending on codepoint value, and packet-dropping rules to dictate\nwhich packets should be dropped first in the event of buffer saturation. The DS\nspecifications refer to the forwarding treatment provided at a router as per-hop\nbehavior (PHB).This PHB must be available at all routers, and typically PHB is the\nonly part of DS implemented in interior routers.\nThe boundary nodes include PHB mechanisms but more sophisticated traffic\nconditioning mechanisms are also required to provide the desired service. Thus,\ninterior routers have minimal functionality and minimal overhead in providing the\nDS service, while most of the complexity is in the boundary nodes. The boundary\nnode function can also be provided by a host system attached to the domain, on\nbehalf of the applications at that host system.\nThe traffic conditioning function consists of five elements:\n• Classifier: Separates submitted packets into different classes. This is the foun-\ndation of providing differentiated services. A classifier may separate traffic\nonly on the basis of the DS codepoint (behavior aggregate classifier) or based\non multiple fields within the packet header or even the packet payload (multi-\nfield classifier).\n• Meter: Measures submitted traffic for conformance to a profile. The meter\ndetermines whether a given packet stream class is within or exceeds the ser-\nvice level guaranteed for that class.\n• Marker: Re-marks packets with a different codepoint as needed. This may be\ndone for packets that exceed the profile; for example, if a given throughput is\nguaranteed for a particular service class, any packets in that class that exceed\nthe throughput in some defined time interval may be re-marked for best effort\nhandling. Also, re-marking may be required at the boundary between two DS\ndomains. For example, if a given traffic class is to receive the highest supported\npriority, and this is a value of 3 in one domain and 7 in the next domain, then\npackets with a priority 3 value traversing the first domain are remarked as pri-\nority 7 when entering the second domain.\n• Shaper: Delays packets as necessary so that the packet stream in a given class\ndoes not exceed the traffic rate specified in the profile for that class.\n• Dropper: Drops packets when the rate of packets of a given class exceeds that\nspecified in the profile for that class.\nFigure 19.15 illustrates the relationship between the elements of traffic condi-\ntioning. After a flow is classified, its resource consumption must be measured. The\nmetering function measures the volume of packets over a particular time interval to\ndetermine a flow’s compliance with the traffic agreement. If the host is bursty, a sim-\nple data rate or packet rate may not be sufficient to capture the desired traffic char-\nacteristics. A token bucket scheme, such as that illustrated in Figure 19.11, is an\nexample of a way to define a traffic profile to take into account both packet rate and\nburstiness.\nIf a traffic flow exceeds some profile, several approaches can be taken. Indi-\nvidual packets in excess of the profile may be re-marked for lower-quality handling\nand allowed to pass into the DS domain. A traffic shaper may absorb a burst of\n19.4 / DIFFERENTIATED SERVICES\npackets in a buffer and pace the packets over a longer period of time. A dropper\nmay drop packets if the buffer used for pacing becomes saturated.\nPer-Hop Behavior\nAs part of the DS standardization effort, specific types of PHB need to be defined,\nwhich can be associated with specific differentiated services. Currently, two stan-\ndards-track PHBs have been issued: expedited forwarding PHB (RFCs 3246 and\n3247) and assured forwarding PHB (RFC 2597).\nExpedited Forwarding PHB RFC 3246 defines the expedited forwarding\n(EF) PHB as a building block for low-loss, low-delay, and low-jitter end-to-end ser-\nvices through DS domains. In essence, such a service should appear to the endpoints\nas providing close to the performance of a point-to-point connection or leased line.\nIn an internet or packet-switching network, a low-loss, low-delay, and low-jitter\nservice is difficult to achieve. By its nature, an internet involves queues at each node, or\nrouter,where packets are buffered waiting to use a shared output link.It is the queuing\nbehavior at each node that results in loss, delays, and jitter.Thus, unless the internet is\ngrossly oversized to eliminate all queuing effects, care must be taken in handling traffic\nfor EF PHB to assure that queuing effects do not result in loss, delay, or jitter above a\ngiven threshold. RFC 3246 declares that the intent of the EF PHB is to provide a PHB\nin which suitably marked packets usually encounter short or empty queues. The rela-\ntive absence of queues minimizes delay and jitter. Furthermore, if queues remain short\nrelative to the buffer space available, packet loss is also kept to a minimum.\nThe EF PHB is designed to configuring nodes so that the traffic aggregate3 has\na well-defined minimum departure rate. (Well-defined means “independent of the\ndynamic state of the node.” In particular, independent of the intensity of other traf-\nfic at the node.) The general concept outlined in RFC 3246 is this:The border nodes\ncontrol the traffic aggregate to limit its characteristics (rate, burstiness) to some pre-\ndefined level. Interior nodes must treat the incoming traffic in such a way that queu-\ning effects do not appear. In general terms, the requirement on interior nodes is that\nthe aggregate’s maximum arrival rate must be less than the aggregate’s minimum\ndeparture rate.\n3The term traffic aggregate refers to the flow of packets associated with a particular service for a particu-\nFigure 19.15\nDS Traffic Conditioner\nCHAPTER 19 / INTERNETWORK OPERATION\nRFC 3246 does not mandate a specific queuing policy at the interior nodes to\nachieve the EF PHB.The RFC notes that a simple priority scheme could achieve the\ndesired effect, with the EF traffic given absolute priority over other traffic. So long\nas the EF traffic itself did not overwhelm an interior node, this scheme would result\nin acceptable queuing delays for the EF PHB. However, the risk of a simple priority\nscheme is that packet flows for other PHB traffic would be disrupted. Thus, some\nmore sophisticated queuing policy might be warranted.\nAssured Forwarding PHB The assured forwarding (AF) PHB is designed to\nprovide a service superior to best-effort but one that does not require the reserva-\ntion of resources within an internet and does not require the use of detailed dis-\ncrimination among flows from different users.The concept behind the AF PHB was\nfirst introduced in [CLAR98] and is referred to as explicit allocation. The AF PHB\nis more complex than explicit allocation, but it is useful to first highlight the key ele-\nments of the explicit allocation scheme:\n1. Users are offered the choice of a number of classes of service for their traffic.\nEach class describes a different traffic profile in terms of an aggregate data\nrate and burstiness.\n2. Traffic from a user within a given class is monitored at a boundary node. Each\npacket in a traffic flow is marked in or out based on whether it does or does not\nexceed the traffic profile.\n3. Inside the network, there is no separation of traffic from different users or even\ntraffic from different classes. Instead, all traffic is treated as a single pool of pack-\nets,with the only distinction being whether each packet has been marked in or out.\n4. When congestion occurs, the interior nodes implement a dropping scheme in\nwhich out packets are dropped before in packets.\n5. Different users will see different levels of service because they will have dif-\nferent quantities of in packets in the service queues.\nThe advantage of this approach is its simplicity.Very little work is required by\nthe internal nodes. Marking of the traffic at the boundary nodes based on traffic\nprofiles provides different levels of service to different classes.\nThe AF PHB defined in RFC 2597 expands on the preceding approach in the\nfollowing ways:\n1. Four AF classes are defined, allowing the definition of four distinct traffic pro-\nfiles.A user may select one or more of these classes to satisfy requirements.\n2. Within each class, packets are marked by the customer or by the service\nprovider with one of three drop precedence values. In case of congestion, the\ndrop precedence of a packet determines the relative importance of the packet\nwithin the AF class.A congested DS node tries to protect packets with a lower\ndrop precedence value from being lost by preferably discarding packets with a\nhigher drop precedence value.\nThis approach is still simpler to implement than any sort of resource reserva-\ntion scheme but provides considerable flexibility.Within an interior DS node, traffic\nfrom the four classes can be treated separately, with different amounts of resources\n19.5 / SERVICE LEVEL AGREEMENTS\n(buffer space, data rate) assigned to the four classes. Within each class, packets are\nhandled based on drop precedence. Thus, as RFC 2597 points out, the level of for-\nwarding assurance of an IP packet depends on\n• How much forwarding resources has been allocated to the AF class to which\nthe packet belongs\n• The current load of the AF class, and, in case of congestion within the class\n• The drop precedence of the packet\nRFC 2597 does not mandate any mechanisms at the interior nodes to manage\nthe AF traffic. It does reference the RED algorithm as a possible way of managing\ncongestion.\nFigure 19.13b shows the recommended codepoints for AF PHB in the DS field.\n19.5 SERVICE LEVEL AGREEMENTS\nA service level agreement (SLA) is a contract between a network provider and a\ncustomer that defines specific aspects of the service that is to be provided.The defi-\nnition is formal and typically defines quantitative thresholds that must be met. An\nSLA typically includes the following information:\n• A description of the nature of service to be provided: A basic service would be\nIP-based network connectivity of enterprise locations plus access to the Inter-\nnet.The service may include additional functions such as Web hosting, mainte-\nnance of domain name servers, and operation and maintenance tasks.\n• The expected performance level of the service: The SLA defines a number of\nmetrics, such as delay, reliability, and availability, with numerical thresholds.\n• The process for monitoring and reporting the service level: This describes how\nperformance levels are measured and reported.\nThe types of service parameters included in an SLA for an IP network are sim-\nilar to those provided for frame relay and ATM networks. A key difference is that,\nbecause of the unreliable datagram nature of an IP network, it is more difficult to\nrealize tightly defined constraints on performance, compared to the connection-ori-\nented frame relay and ATM networks.\nFigure 19.16 shows a typical configuration that lends itself to an SLA.In this case,\na network service provider maintains an IP-based network.A customer has a number\nof private networks (e.g., LANs) at various sites. Customer networks are connected to\nthe provider via access routers at the access points.The SLA dictates service and per-\nformance levels for traffic between access routers across the provider network. In\naddition, the provider network links to the Internet and thus provides Internet access\nfor the enterprise. For example, for the Internet Dedicated Service provided by MCI\n(http://global.mci.com/terms/us/products/dsl), the SLA includes the following items:\n• Availability: 100% availability.\n• Latency (delay): Average round-trip transmissions of \naccess routers in the contiguous United States. Average round-trip transmis-\nbetween an access router in the New York metropolitan\nCHAPTER 19 / INTERNETWORK OPERATION\narea and an access router in the London metropolitan area. Latency is calcu-\nlated by averaging sample measurements taken during a calendar month\nbetween routers.\n• Network packet delivery (reliability): Successful packet delivery rate of \n• Denial of service (DoS): Responds to DoS attacks reported by customer\nwithin 15 minutes of customer opening a complete trouble ticket. MCI defines\na DoS attack as more than 95% bandwidth utilization.\n• Network jitter: Jitter is defined as the variation or difference in the end-to-end\ndelay between received packets of an IP or packet stream. Jitter performance\nwill not exceed 1 ms between access routers.\nAn SLA can be defined for the overall network service. In addition, SLAs can\nbe defined for specific end-to-end services available across the carrier’s network,\nsuch as a virtual private network, or differentiated services.\n19.6 IP PERFORMANCE METRICS\nThe IPPM Performance Metrics Working Group (IPPM) is chartered by IETF to\ndevelop standard metrics that relate to the quality, performance, and reliability of\nInternet data delivery.Two trends dictate the need for such a standardized measure-\nment scheme:\nFigure 19.16\nTypical Framework for Service Level Agreement\nAccess routers\nCustomer networks\nISP network\n19.6 / IP PERFORMANCE METRICS\n1. The Internet has grown and continues to grow at a dramatic rate. Its topology\nis increasingly complex.As its capacity has grown, the load on the Internet has\ngrown at an even faster rate. Similarly, private internets, such as corporate\nintranets and extranets, have exhibited similar growth in complexity, capacity,\nand load. The sheer scale of these networks makes it difficult to determine\nquality, performance, and reliability characteristics.\n2. The Internet serves a large and growing number of commercial and personal\nusers across an expanding spectrum of applications. Similarly, private net-\nworks are growing in terms of user base and range of applications. Some of\nthese applications are sensitive to particular QoS parameters, leading users to\nrequire accurate and understandable performance metrics.\nA standardized and effective set of metrics enables users and service providers\nto have an accurate common understanding of the performance of the Internet and\nprivate internets. Measurement data is useful for a variety of purposes, including\n• Supporting capacity planning and troubleshooting of large complex internets\n• Encouraging competition by providing uniform comparison metrics across\nservice providers\n• Supporting Internet research in such areas as protocol design, congestion con-\ntrol, and quality of service\n• Verification of service level agreements\nTable 19.5 lists the metrics that have been defined in RFCs at the time of this\nwriting. Table 19.5a lists those metrics which result in a value estimated based on a\nsampling technique.The metrics are defined in three stages:\n• Singleton metric: The most elementary, or atomic, quantity that can be mea-\nsured for a given performance metric. For example, for a delay metric, a sin-\ngleton metric is the delay experienced by a single packet.\n• Sample metric: A collection of singleton measurements taken during a given\ntime period. For example, for a delay metric, a sample metric is the set of delay\nvalues for all of the measurements taken during a one-hour period.\n• Statistical metric: A value derived from a given sample metric by computing\nsome statistic of the values defined by the singleton metric on the sample. For\nexample, the mean of all the one-way delay values on a sample might be\ndefined as a statistical metric.\nThe measurement technique can be either active or passive. Active techniques\nrequire injecting packets into the network for the sole purpose of measurement.\nThere are several drawbacks to this approach.The load on the network is increased.\nThis, in turn, can affect the desired result. For example, on a heavily loaded network,\nthe injection of measurement packets can increase network delay, so that the mea-\nsured delay is greater than it would be without the measurement traffic. In addition,\nan active measurement policy can be abused for denial-of-service attacks disguised\nas legitimate measurement activity. Passive techniques observe and extract metrics\nCHAPTER 19 / INTERNETWORK OPERATION\nPercentile, median,\nminimum, inverse percentile\nPercentile, median,\nminimum, inverse percentile\nNumber or rate of loss dis-\ntances below a defined\nthreshold, number of loss\nperiods, pattern of period\nlengths, pattern of interloss\nperiod lengths.\nPercentile, inverse\npercentile, jitter, peak-to-\nOne-Way Delay\nRound-Trip Delay\nOne-Way Loss\nOne-Way Loss\nPacket Delay\nwhere Src transmits first bit of\npacket at T and Dst received last bit of\nwhere Src transmits first bit of\npacket at T and Src received last bit of\npacket immediately returned by Dst at\n(signifying successful\ntransmission and reception of packet);\n(signifying packet loss)\nLoss distance: Pattern showing the\ndistance between successive packet losses\nin terms of the sequence of packets\nLoss period: Pattern showing the number\nof bursty losses (losses involving\nconsecutive packets)\nPacket delay variation (pdv) for a pair of\npackets with a stream of packets \u0001 \ndifference between the one-way-delay of\nthe selected packets\nDelay = dT,\nDelay = dT,\nIP Performance Metrics\n(a) Sampled metrics\nMetric Name\nSingleton Definition\nStatistical Definitions\naddress of a host\naddress of a host\n(b) Other metrics\nMetric Name\nGeneral Definition\nConnectivity\nAbility to deliver a packet \nOne-way instantaneous connectivity, two-way\nover a transport connection.\ninstantaneous connectivity, one-way interval\nconnectivity, two-way interval connectivity,\ntwo-way temporal connectivity\nBulk Transfer \nLong-term average data \nrate (bps) over a single\ncongestion-aware transport\nconnection.\nBTC = 1data sent2/1elapsed time2\nFor the sample metrics, the simplest technique is to take measurements at\nfixed time intervals, known as periodic sampling. There are several problems with\nthis approach. First, if the traffic on the network exhibits periodic behavior, with a\nperiod that is an integer multiple of the sampling period (or vice versa), correlation\neffects may result in inaccurate values. Also, the act of measurement can perturb\nwhat is being measured (for example, injecting measurement traffic into a network\nalters the congestion level of the network), and repeated periodic perturbations can\ndrive a network into a state of synchronization (e.g., [FLOY94]), greatly magnifying\nwhat might individually be minor effects.Accordingly, RFC 2330 (Framework for IP\n19.7 / RECOMMENDED READING AND WEB SITES\nPerformance Metrics) recommends Poisson sampling. This method uses a Poisson\ndistribution to generate random time intervals with the desired mean value.\nMost of the statistical metrics listed in Table 19.5a are self-explanatory. The\npercentile metric is defined as follows: The xth percentile is a value y such that x%\nof measurements \nThe inverse percentile of x for a set of measurements is the\npercentage of all values \nFigure 19.17 illustrates the packet delay variation metric.This metric is used to\nmeasure jitter, or variability, in the delay of packets traversing the network.The sin-\ngleton metric is defined by selecting two packet measurements and measuring the\ndifference in the two delays. The statistical measures make use of the absolute val-\nues of the delays.\nTable 19.5b lists two metrics that are not defined statistically. Connectivity\ndeals with the issue of whether a transport-level connection is maintained by the\nnetwork. The current specification (RFC 2678) does not detail specific sample and\nstatistical metrics but provides a framework within which such metrics could be\ndefined. Connectivity is determined by the ability to deliver a packet across a con-\nnection within a specified time limit.The other metric, bulk transfer capacity, is sim-\nilarly specified (RFC 3148) without sample and statistical metrics but begins to\naddress the issue of measuring the transfer capacity of a network service with the\nimplementation of various congestion control mechanisms.\n19.7 RECOMMENDED READING AND WEB SITES\nA number of worthwhile books provide detailed coverage of various routing algorithms:\n[HUIT00], [BLAC00], and [PERL00]. [MOY98] provides a thorough treatment of OSPF.\nPerhaps the clearest and most comprehensive book-length treatment of Internet QoS is\n[ARMI00]. [XIAO99] provides an overview and overall framework for Internet QoS as well\nas integrated and differentiated services. [CLAR92] and [CLAR95] provide valuable surveys\nof the issues involved in internet service allocation for real-time and elastic applications,\nFigure 19.17\nModel for Defining Packet Delay Variation\nI1, I2 \u0001 times that mark that beginning and ending of the interval\nin which the packet stream from which the singleton\nmeasurement is taken occurs\nMP1, MP2 \u0001 source and destination measurement points\nP(i) \u0001 ith measured packet in a stream of packets\ndTi \u0001 one-way delay for P(i)\nCHAPTER 19 / INTERNETWORK OPERATION\nrespectively. [SHEN95] is a masterful analysis of the rationale for a QoS-based internet archi-\ntecture. [ZHAN95] is a broad survey of queuing disciplines that can be used in an ISA, includ-\ning an analysis of FQ and WFQ.\n[ZHAN93] is a good overview of the philosophy and functionality of RSVP, written by\nits developers. [WHIT97] is a broad survey of both ISA and RSVP.\n[CARP02] and [WEIS98] are instructive surveys of differentiated services, while\n[KUMA98] looks at differentiated services and supporting router mechanisms that go\nbeyond the current RFCs. For a thorough treatment of DS, see [KILK99].\nTwo papers that compare IS and DS in terms of services and performance are\n[BERN00] and [HARJ00].\n[VERM04] is an excellent surveys of service level agreements for IP networks.\n[BOUI02] covers the more general case of data networks. [MART02] examines limitations of\nIP network SLAs compared to data networks such as frame relay.\n[CHEN02] is a useful survey of Internet performance measurement issues. [PAXS96]\nprovides an overview of the framework of the IPPM effort.\nArmitage, G. Quality of Service in IP Networks. Indianapolis, IN: Macmillan\nTechnical Publishing, 2000.\nBernet,Y.“The Complementary Roles of RSVP and Differentiated Services in\nthe Full-Service QoS Network.” IEEE Communications Magazine, February 2000.\nBlack, U. IP Routing Protocols: RIP, OSPF, BGP, PNNI & Cisco Routing\nProtocols. Upper Saddle River, NJ: Prentice Hall, 2000.\nBouillet, E.; Mitra, D.; and Ramakrishnan, K.“The Structure and Management\nof Service Level Agreements in Networks.” IEEE Journal on Selected Areas in\nCommunications, May 2002.\nCarpenter, B., and Nichols, K. “Differentiated Services in the Internet.”\nProceedings of the IEEE, September 2002.\nChen, T. “Internet Performance Monitoring.” Proceedings of the IEEE,\nSeptember 2002.\nClark, D.; Shenker, S.; and Zhang, L. “Supporting Real-Time Applications in\nan Integrated Services Packet Network:Architecture and Mechanism” Proceedings,\nSIGCOMM ’92, August 1992.\nClark, D. Adding Service Discrimination to the Internet. MIT Laboratory for\nComputer Science Technical Report, September 1995. Available at http://\nana-www.lcs.mit.edu/anaWeb/papers.html\nHarju, J., and Kivimaki, P. “Cooperation and Comparison of DiffServ and\nIntServ: Performance Measurements.” Proceedings, 23rd Annual IEEE Conference\non Local Computer Networks, November 2000.\nHuitema,C.Routing in the Internet. Upper Saddle River,NJ:Prentice Hall,2000.\nKilkki, K. Differentiated Services for the Internet. Indianapolis, IN: Macmillan\nTechnical Publishing, 1999.\nKumar,V.; Lakshman,T.; and Stiliadis, D.“Beyond Best Effort: Router Archi-\ntectures for the Differentiated Services of Tomorrow’s Internet.” IEEE Communi-\ncations Magazine, May 1998.\nMartin, J., and Nilsson, A. “On Service Level Agreements for IP Networks.”\nProceeding IEEE INFOCOMM ’02, 2002.\nMoy, J. OSPF: Anatomy of an Internet Routing Protocol. Reading, MA:\nAddison-Wesley, 1998.\n19.8 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nautonomous system (AS)\nBorder Gateway Protocol\nbroadcast address\nDifferentiated Services (DS)\ndistance-vector routing\nelastic traffic\nexterior router protocol\ninelastic traffic\nIntegrated Services Architec-\ninterior router protocol\nInternet Group Management\nlink-state routing\nmulticast address\nmulticasting\nneighbor acquisition\nneighbor reachability\nPaxson, V. “Toward a Framework for Defining Internet Performance Metrics.”\nProceedings, INET ’96, 1996. http://www-nrg.ee.lbl.gov\nPerlman, R. Interconnections: Bridges, Routers, Switches, and Internetworking\nProtocols. Reading, MA:Addison-Wesley, 2000.\nShenker, S. “Fundamental Design Issues for the Future Internet.” IEEE Jour-\nnal on Selected Areas in Communications, September 1995.\nVerma, D. “Service Level Agreements on IP Networks.” Proceedings of the\nIEEE, September 2004.\nWeiss, W. “QoS with Differentiated Services.” Bell Labs Technical Journal,\nOctober–December 1998.\nWhite, P., and Crowcroft, J. “The Integrated Services in the Internet: State of\nthe Art.” Proceedings of the IEEE, December 1997.\nXiao, X., and Ni, L.“Internet QoS:A Big Picture.” IEEE Network, March/April\nZhang, L.; Deering, S.; Estrin, D.; Shenker, S.; and Zappala, D. “RSVP: A New\nResource ReSerVation Protocol.” IEEE Network, September 1993.\nZhang, H. “Service Disciplines for Guaranteed Performance Service in\nPacket-Switching Networks.” Proceedings of the IEEE, October 1995.\nRecommended Web sites:\n• Inter-Domain Routing working group: Chartered by IETF to revise BGP and\nrelated standards.The Web site includes all relevant RFCs and Internet drafts.\n• OSPF working group: Chartered by IETF to develop OSPF and related standards.\nThe Web site includes all relevant RFCs and Internet drafts.\n• RSVP Project: Home page for RSVP development.\n• IP Performance Metrics working group: Chartered by IETF to develop a set of\nstandard metrics that can be applied to the quality, performance, and reliability of Inter-\nnet data delivery services. The Web site includes all relevant RFCs and Internet drafts.\n19.8 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nCHAPTER 19 / INTERNETWORK OPERATION\nnetwork reachability\nOpen Shortest Path First\npath-vector routing\nper-hop behavior\nquality of service (QoS)\nqueuing discipline\nResource ReSerVation Proto-\nunicast address\nReview Questions\nList some practical applications of multicasting.\nSummarize the differences among unicast, multicast, and broadcast addresses.\nList and briefly explain the functions that are required for multicasting.\nWhat operations are performed by IGMP?\nWhat is an autonomous system?\nWhat is the difference between an interior router protocol and an exterior router pro-\nCompare the three main approaches to routing.\nList and briefly explain the three main functions of BGP.\nWhat is the Integrated Services Architecture?\nWhat is the difference between elastic and inelastic traffic?\nWhat are the major functions that are part of an ISA?\nList and briefly describe the three categories of service offered by ISA.\nWhat is the difference between FIFO queuing and WFQ queuing?\nWhat is the purpose of a DS codepoint?\nList and briefly explain the five main functions of DS traffic conditioning.\nWhat is meant by per-hop behavior?\nMost operating systems include a tool named “traceroute” (or “tracert”) that can be\nused to determine the path packets follow to reach a specified host from the system\nthe tool is being run on. A number of sites provide Web access to the “traceroute”\ntool, for example,\nhttp://www.supporttechnique.net/traceroute.ihtml\nhttp://www.t1shopper.com/tools/traceroute\nUse the “traceroute” tool to determine the path packets follow to reach the host\nwilliamstallings.com.\nA connected graph may have more than one spanning tree. Find all spanning trees of\nthis graph:\nIn the discussion of Figure 19.1, three alternatives for transmitting a packet to a mul-\nticast address were discussed: broadcast, multiple unicast, and true multicast. Yet\nanother alternative is flooding. The source transmits one packet to each neighboring\nrouter. Each router, when it receives a packet, retransmits the packet on all outgoing\ninterfaces except the one on which the packet is received. Each packet is labeled with\na unique identifier so that a router does not flood the same packet more than once.\nFill out a matrix similar to those of Table 19.1 and comment on the results.\n19.8 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nIn a manner similar to Figure 19.3, show the spanning tree from router B to the mul-\nticast group.\nIGMP specifies that query messages are sent in IP datagrams that have the Time to\nLive field set to 1.Why?\nIn IGMPv1 and IGMPv2, a host will cancel sending a pending membership report if it\nhears another host claiming membership in that group, in order to control the gener-\nation of IGMP traffic. However, IGMPv3 removes this suppression of host member-\nship reports.Analyze the reasons behind this design decision.\nIGMP Membership Queries include a “Max Resp Code” field that specifies the max-\nimum time allowed before sending a responding report. The actual time allowed,\ncalled the Max Resp Time, is represented in units of 1/10 second and is derived from\nthe Max Resp Code as follows:\nMaxRespTime is a floating-point value as follows:\nExplain the motivation for the smaller values and the larger values.\nMulticast applications call an API function on their sockets in order to ask the IP\nlayer to enable or disable reception of packets sent from some specific IP address(es)\nto a specific multicast address.\nFor each of these sockets, the system records the desired multicast reception state.\nIn addition to these per-socket multicast reception states, the system must maintain a\nmulticast reception state for each of its interfaces, which is derived from the per-\nsocket reception states.\nSuppose four multicast applications run on the same host, and participate in the\nsame multicast group, M1.The first application uses an \nter. The second one uses an \nfilter. The third one uses an\nfilter. And the fourth one uses an \nWhat’s the resulting multicast state (multicast address, filter mode, source list) for the\nnetwork interface?\nMulticast applications commonly use UDP or RTP (Real-Time Transport Protocol;\ndiscussed in Chapter 24) as their transport protocol. Multicast application do not use\nTCP as its transport protocol.What’s the problem with TCP?\nWith multicasting, packets are delivered to multiple destinations. Thus, in case of\nerrors (such as routing failures), one IP packet might trigger multiple ICMP error\npackets, leading to a packet storm. How is this potential problem avoided? Hint: Con-\nsult RFC 1122.\nBGP’s AS_PATH attribute identifies the autonomous systems through which routing\ninformation has passed. How can the AS_PATH attribute be used to detect routing\ninformation loops?\nBGP provides a list of autonomous systems on the path to the destination. However,\nthis information cannot be considered a distance metric.Why?\nRFC 2330 (Framework for IP Performance Metrics) defines percentile in the follow-\ning way. Given a collection of measurements, define the function F(x), which for any x\ngives the percentage of the total measurements that were \nIf x is less than the min-\nimum value observed, then \nIf it is greater or equal to the maximum value\nobserved, then \nThe yth percentile refer to the smallest value of x for\nConsider that we have the following measurements:\n7, 7, 4, 18,\nDetermine the following percentiles: 0, 25, 50, 100.\nF1x2 = 100%.\nINCLUDE5A36\nINCLUDE5A3, A46\nEXCLUDE5A1, A3, A46\nEXCLUDE5A1, A2, A36\n MaxRespTime = 1mant + 162 * 21exp+32\n MaxRespTime = 1mantƒ 0x102 V 1exp + 32 in C notation\nMaxRespCode Ú 128,\nIf MaxRespCode 6 128, MaxRespTime = Max Resp Code\nCHAPTER 19 / INTERNETWORK OPERATION\nFor the one-way and two-way delay metrics, if a packet fails to arrive within a reason-\nable period of time, the delay is taken to be undefined (informally, infinite). The\nthreshold of reasonable is a parameter of the methodology. Suppose we take a sample\nof one-way delays and get the following results: 100 ms, 110 ms, undefined, 90 ms,\n500 ms.What is the 50th percentile?\nRFC 2330 defines the median of a set of measurements to be equal to the 50th per-\ncentile if the number of measurements is odd. For an even number of measurements,\nsort the measurements in ascending order; the median is then the mean of the two\ncentral values. What is the median value for the measurements in the preceding two\nRFC 2679 defines the inverse percentile of x for a set of measurements to be the per-\ncentage of all values \nWhat is the inverse percentile of 103 ms for the measure-\nments in Problem 19.14?\nWhen multiple equal-cost routes to a destination exist, OSPF may distribute traffic\nequally among the routes. This is called load balancing. What effect does such load\nbalancing have on a transport layer protocol, such as TCP?\nIt is clear that if a router gives preferential treatment to one flow or one class of flows,\nthen that flow or class of flows will receive improved service. It is not as clear that the\noverall service provided by the internet is improved.This question is intended to illus-\ntrate an overall improvement. Consider a network with a single link modeled by an\nexponential server of rate \nand consider two classes of flows with Poisson\narrival rates of \nand that have utility functions \nrepresents the average queuing delay to class i. Thus, class 1\ntraffic is more sensitive to delay than class 2. Define the total utility of the network as\nAssume that the two classes are treated alike and that FIFO queuing is used.\nb. Now assume a strict priority service so that packets from class 1 are always trans-\nmitted before packets in class 2.What is V? Comment.\nProvide three examples (each) of elastic and inelastic Internet traffic. Justify each\nexample’s inclusion in their respective category.\nWhy does a Differentiated Services (DS) domain consist of a set of contiguous\nrouters? How are the boundary node routers different from the interior node routers\nin a DS domain?\nThe token bucket scheme places a limit on the length of time at which traffic can\ndepart at the maximum data rate. Let the token bucket be defined by a bucket size B\noctets and a token arrival rate of R octets/second, and let the maximum output data\nrate be M octets/s.\nDerive a formula for S, which is the length of the maximum-rate burst.That is, for\nhow long can a flow transmit at the maximum output rate when governed by a\ntoken bucket?\nb. What is the value of S for \nHint: The formula for S is not so simple as it might appear, because more tokens\narrive while the burst is being output.\nIn RSVP, because the UDP/TCP port numbers are used for packet classification, each\nrouter must be able to examine these fields. This requirement raises problems in the\nfollowing areas:\nIPv6 header processing\nb. IP-level security\nIndicate the nature of the problem in each area, and suggest a solution.\nM = 25 MB/s?\nB = 250 KB, R = 2 MB/s,\nV = U1 + U2.\nU2 = 4 - Tq2,\nU1 = 4 - 2Tq1\nl1 = l2 = 0.25\n20.1 Connection-Oriented Transport Protocol Mechanisms\n20.3 TCP Congestion Control\n20.5 Recommended Reading and Web Sites\n20.6 Key Terms, Review Questions, and Problems\nCHAPTER 20 / TRANSPORT PROTOCOLS\nThe transport protocol provides an end-to-end data transfer service\nthat shields upper-layer protocols from the details of the intervening\nnetwork or networks. A transport protocol can be either connection\noriented, such as TCP, or connectionless, such as UDP.\nIf the underlying network or internetwork service is unreliable, such\nas with the use of IP, then a reliable connection-oriented transport\nprotocol becomes quite complex.The basic cause of this complexity is\nthe need to deal with the relatively large and variable delays experi-\nenced between end systems. These large, variable delays complicate\nthe flow control and error control techniques.\nTCP uses a credit-based flow control technique that is somewhat dif-\nferent from the sliding-window flow control found in X.25 and\nCHAPTER 20 / TRANSPORT PROTOCOLS\n• Host address\n• Network number\nThe transport protocol must be able to derive the information listed above\nfrom the TS user address.Typically, the user address is specified as (Host, Port).The\nPort variable represents a particular TS user at the specified host. Generally, there\nwill be a single transport entity at each host, so a transport entity identification is not\nneeded. If more than one transport entity is present, there is usually only one of\neach type. In this latter case, the address should include a designation of the type of\ntransport protocol (e.g., TCP, UDP). In the case of a single network, Host identifies\nan attached network device. In the case of an internet, Host is a global internet\naddress. In TCP, the combination of port and host is referred to as a socket.\nBecause routing is not a concern of the transport layer, it simply passes the\nHost portion of the address down to the network service. Port is included in a trans-\nport header, to be used at the destination by the destination transport protocol entity.\nOne question remains to be addressed: How does the initiating TS user know\nthe address of the destination TS user? Two static and two dynamic strategies sug-\ngest themselves:\n1. The TS user knows the address it wishes to use ahead of time. This is basically\na system configuration function. For example, a process may be running that is\nonly of concern to a limited number of TS users, such as a process that collects\nstatistics on performance. From time to time, a central network management\nroutine connects to the process to obtain the statistics. These processes gener-\nally are not, and should not be, well known and accessible to all.\n2. Some commonly used services are assigned “well-known addresses.” Examples\ninclude the server side of FTP, SMTP, and some other standard protocols.\n3. A name server is provided. The TS user requests a service by some generic or\nglobal name. The request is sent to the name server, which does a directory\nlookup and returns an address.The transport entity then proceeds with the con-\nnection. This service is useful for commonly used applications that change loca-\ntion from time to time.For example,a data entry process may be moved from one\nhost to another on a local network to balance load.\n4. In some cases, the target user is to be a process that is spawned at request\ntime.The initiating user can send a process request to a well-known address.\nThe user at that address is a privileged system process that will spawn the\nnew process and return an address. For example, a programmer has devel-\noped a private application (e.g., a simulation program) that will execute on\na remote server but be invoked from a local workstation. A request can be\nissued to a remote job-management process that spawns the simulation\nMultiplexing Multiplexing was discussed in general terms in Section 18.1. With\nrespect to the interface between the transport protocol and higher-level protocols,\nthe transport protocol performs a multiplexing/demultiplexing function. That is,\nmultiple users employ the same transport protocol and are distinguished by port\nnumbers or service access points.\n20.1 / CONNECTION-ORIENTED TRANSPORT PROTOCOL MECHANISMS\n1Recall from Chapter 2 that the blocks of data (protocol data units) exchanged by TCP entities are\nreferred to as TCP segments.\nThe transport entity may also perform a multiplexing function with respect to\nthe network services that it uses. Recall that we defined upward multiplexing as the\nmultiplexing of multiple connections on a single lower-level connection, and down-\nward multiplexing as the splitting of a single connection among multiple lower-level\nconnections (Section 18.1).\nConsider, for example, a transport entity making use of an X.25 service. Why\nshould the transport entity employ upward multiplexing? There are, after all, 4095\nvirtual circuits available. In the typical case, this is more than enough to handle all\nactive TS users. However, most X.25 networks base part of their charge on virtual\ncircuit connect time, because each virtual circuit consumes some node buffer\nresources. Thus, if a single virtual circuit provides sufficient throughput for multiple\nTS users, upward multiplexing is indicated.\nOn the other hand, downward multiplexing or splitting might be used to\nimprove throughput. For example, each X.25 virtual circuit is restricted to a \n3-bit or 7-bit sequence number. A larger sequence space might be needed for\nhigh-speed, high-delay networks. Of course, throughput can only be increased\nso far. If there is a single host-node link over which all virtual circuits are mul-\ntiplexed, the throughput of a transport connection cannot exceed the data rate\nof that link.\nFlow Control Whereas flow control is a relatively simple mechanism at the\nlink layer, it is a rather complex mechanism at the transport layer, for two main\n• The transmission delay between transport entities is generally long compared\nto actual transmission time. This means that there is a considerable delay in\nthe communication of flow control information.\n• Because the transport layer operates over a network or internet, the amount\nof the transmission delay may be highly variable. This makes it difficult to\neffectively use a timeout mechanism for retransmission of lost data.\nIn general,there are two reasons why one transport entity would want to restrain\nthe rate of segment1 transmission over a connection from another transport entity:\n• The user of the receiving transport entity cannot keep up with the flow of data.\n• The receiving transport entity itself cannot keep up with the flow of segments.\nHow do such problems manifest themselves? Presumably a transport entity has\na certain amount of buffer space. Incoming segments are added to the buffer. Each\nbuffered segment is processed (i.e., the transport header is examined) and the data\nare sent to the TS user. Either of the two problems just mentioned will cause the\nbuffer to fill up.Thus, the transport entity needs to take steps to stop or slow the flow\nof segments to prevent buffer overflow.This requirement is difficult to fulfill because\nof the annoying time gap between sender and receiver.We return to this point subse-\nquently. First, we present four ways of coping with the flow control requirement.The\nreceiving transport entity can\nCHAPTER 20 / TRANSPORT PROTOCOLS\n1. Do nothing.\n2. Refuse to accept further segments from the network service.\n3. Use a fixed sliding-window protocol.\n4. Use a credit scheme.\nAlternative 1 means that the segments that overflow the buffer are discarded.\nlayer protocols in Chapter 7.The key ingredients, recall, are\n• The use of sequence numbers on data units\n• The use of a window of fixed size\nCHAPTER 20 / TRANSPORT PROTOCOLS\n(a) Send sequence space\n(b) Receive sequence space \nWindow of octets\nthat may be transmitted\nData octets already transmitted\nOctets not yet\nacknowledged\nData octets so far acknowledged\ntransmitted\nacknowledged\nInitial sequence\nnumber (ISN)\nWindow shrinks from\ntrailing edge as\nsegments are sent\nWindow expands\nfrom leading edge\nas credits are received\nWindow of octets\nthat may be accepted\nData octets already received\nOctets not yet\nacknowledged\nData octets so far acknowledged\nacknowledged\nInitial sequence\nnumber (ISN)\nWindow shrinks from\ntrailing edge as\nsegments are received\nWindow expands\nfrom leading edge\nas credits are sent\nFigure 20.2\nSending and Receiving Flow Control Perspectives\nthrough 2600 (5 segments). However, by the time that B’s message has arrived at A,\nA has already sent two segments, containing octets 1601 through 2000 (which was\npermissible under the initial allocation). Thus, A’s remaining credit upon receipt of\nB’s credit allocation is only 600 octets (3 segments). As the exchange proceeds, A\nadvances the trailing edge of its window each time that it transmits and advances the\nleading edge only when it is granted credit.\nFigure 20.2 shows the view of this mechanism from the sending and receiving\nsides (compare Figure 7.3). Typically, both sides take both views because data may\nbe exchanged in both directions. Note that the receiver is not required to immedi-\nately acknowledge incoming segments but may wait and issue a cumulative\nCHAPTER 20 / TRANSPORT PROTOCOLS\na request with a Passive Open command.A server program, such as time-sharing or a\nfile transfer application, might do this. The TS user may change its mind by sending a\nClose command. After the Passive Open command is issued, the transport entity cre-\nates a connection object of some sort (i.e., a table entry) that is in the LISTEN state.\nFrom the CLOSED state,a TS user may open a connection by issuing an Active\nOpen command, which instructs the transport entity to attempt connection establish-\nment with a designated remote TS user, which triggers the transport entity to send a\nSYN (for synchronize) segment. This segment is carried to the receiving transport\nentity and interpreted as a request for connection to a particular port. If the destina-\ntion transport entity is in the LISTEN state for that port, then a connection is estab-\nlished by the following actions by the receiving transport entity:\n• Signal the local TS user that a connection is open.\n• Send a SYN as confirmation to the remote transport entity.\n• Put the connection object in an ESTAB (established) state.\nWhen the responding SYN is received by the initiating transport entity, it too\ncan move the connection to an ESTAB state.The connection is prematurely aborted\nif either TS user issues a Close command.\nFigure 20.4 shows the robustness of this protocol. Either side can initiate a\nconnection. Further, if both sides initiate the connection at about the same time, it is\nestablished without confusion.This is because the SYN segment functions both as a\nCHAPTER 20 / TRANSPORT PROTOCOLS\nthat there are more difficulties with a reliable connection-oriented transport pro-\ntocol than any other sort of protocol.\nIn the remainder of this section, unless otherwise noted, the mechanisms dis-\ncussed are those used by TCP. Seven issues need to be addressed:\n• Ordered delivery\n• Retransmission strategy\n• Duplicate detection\n• Flow control\n• Connection establishment\n• Connection termination\n• Failure recovery\nOrdered Delivery With an unreliable network service, it is possible that seg-\nments, even if they are all delivered, may arrive out of order.The required solution\nto this problem is to number segments sequentially. We have seen that for data\nlink control protocols, such as HDLC, and for X.25, each data unit (frame, packet)\nis numbered sequentially with each successive sequence number being one more\nthan the previous sequence number. This scheme is used in some transport proto-\ncols, such as the ISO transport protocols. However,TCP uses a somewhat different\nscheme in which each data octet that is transmitted is implicitly numbered. Thus,\nthe first segment may have a sequence number of 1. If that segment has 200 octets\nof data, then the second segment would have the sequence number 201, and so on.\nFor simplicity in the discussions of this section, we will continue to assume that\neach successive segment’s sequence number is 200 more than that of the previous\nsegment; that is, each segment contains exactly 200 octets of data.\nRetransmission Strategy Two events necessitate the retransmission of a seg-\nment. First, a segment may be damaged in transit but nevertheless arrive at its des-\ntination. If a checksum is included with the segment, the receiving transport entity\ncan detect the error and discard the segment.The second contingency is that a seg-\nment fails to arrive. In either case, the sending transport entity does not know that\nthe segment transmission was unsuccessful. To cover this contingency, a positive\nCHAPTER 20 / TRANSPORT PROTOCOLS\nThe second case is discussed in the subsection on connection establishment.\nWe deal with the first case here.\nNotice that we say “a” duplicate rather than “the” duplicate. From the sender’s\npoint of view, the retransmitted segment is the duplicate. However, the retransmit-\nted segment may arrive before the original segment, in which case the receiver\nviews the original segment as the duplicate. In any case, two tactics are needed to\ncope with a duplicate received prior to the close of a connection:\nCHAPTER 20 / TRANSPORT PROTOCOLS\nConnection Establishment As with other protocol mechanisms,connection estab-\nlishment must take into account the unreliability of a network service.Recall that a con-\nnection establishment calls for the exchange of SYNs, a procedure sometimes referred\nto as a two-way handshake. Suppose that A issues a SYN to B. It expects to get a SYN\nback, confirming the connection. Two things can go wrong: A’s SYN can be lost or B’s\nanswering SYN can be lost.Both cases can be handled by use of a retransmit-SYN timer\n(Table 20.1).After A issues a SYN, it will reissue the SYN when the timer expires.\nThis gives rise, potentially, to duplicate SYNs. If A’s initial SYN was lost, there\nare no duplicates. If B’s response was lost, then B may receive two SYNs from A.\nFurther, if B’s response was not lost, but simply delayed,A may get two responding\nSYNs. All of this means that A and B must simply ignore duplicate SYNs once a\nconnection is established.\nThere are other problems to contend with.Just as a delayed SYN or lost response\nCHAPTER 20 / TRANSPORT PROTOCOLS\nsequence number, i. The value i is referred to as the initial sequence number (ISN)\nand is associated with the SYN; the first data octet to be transmitted will have\nsequence number \nThe responding SYN acknowledges the ISN with\nand includes its ISN. A acknowledges B’s SYN/ACK in its first data\nsegment, which begins with sequence number \nFigure 20.9b shows a situation\nin which an old SYN i arrives at B after the close of the relevant connection. B\nassumes that this is a fresh request and responds with SYN j,\nreceives this message, it realizes that it has not requested a connection and therefore\nsends an RST,\nNote that the \nportion of the RST message is essen-\ntial so that an old duplicate RST does not abort a legitimate connection establish-\nment. Figure 20.9c shows a case in which an old SYN/ACK arrives in the middle of\nAN = i + 1.\n1AN = i + 12\nSV     = state vector\nMSL = maximum segment lifetime\nActive open or\nactive open with data\nInitialize SV; send SYN\nUnspecified passive open or\nfully specified passive open\nInitialize SV\nReceive SYN\nSend SYN, ACK\nReceive SYN, ACK\nReceive FIN, ACK of SYN\nReceive FIN\nReceive FIN\nReceive FIN\nReceive FIN, ACK\nReceive SYN\nSend SYN, ACK\nFigure 20.8\nTCP Entity State Diagram\n20.1 / CONNECTION-ORIENTED TRANSPORT PROTOCOL MECHANISMS\na new connection establishment. Because of the use of sequence numbers in the\nCHAPTER 20 / TRANSPORT PROTOCOLS\ndefined in that diagram for connection termination is inadequate for an unreliable net-\nwork service. Misordering of segments could cause the following scenario.A transport\nentity in the CLOSE WAIT state sends its last data segment, followed by a FIN seg-\nment, but the FIN segment arrives at the other side before the last data segment. The\nreceiving transport entity will accept that FIN, close the connection, and lose the last\nsegment of data.To avoid this problem, a sequence number can be associated with the\nFIN, which can be assigned the next sequence number after the last octet of transmit-\nted data.With this refinement, the receiving transport entity, upon receiving a FIN, will\nwait if necessary for the late-arriving data before closing the connection.\nA more serious problem is the potential loss of segments and the potential pres-\nence of obsolete segments. Figure 20.8 shows that the termination procedure adopts a\nsimilar solution to that used for connection establishment. Each side must explicitly\nacknowledge the FIN of the other, using an ACK with the sequence number of the\nFIN to be acknowledged.For a graceful close,a transport entity requires the following:\n• It must send a FIN i and receive \n• It must receive a FIN j and send \n• It must wait an interval equal to twice the maximum expected segment lifetime.\nFailure Recovery When the system upon which a transport entity is running\nfails and subsequently restarts, the state information of all active connections is lost.\nThe affected connections become half open because the side that did not fail does\nnot yet realize the problem.\nThe still active side of a half-open connection can close the connection using a\nkeepalive timer. This timer measures the time the transport machine will continue to\nCHAPTER 20 / TRANSPORT PROTOCOLS\nTable 20.4 provides a brief definition of the parameters involved. The two passive \nopen commands signal the TCP user’s willingness to accept a connection request.The\nactive open with data allows the user to begin transmitting data with the opening of\nthe connection.\nTCP Header Format\nTCP uses only a single type of protocol data unit, called a TCP segment.The header\nis shown in Figure 20.10. Because one header must serve to perform all protocol\nmechanisms, it is rather large, with a minimum length of 20 octets. The fields are as\n• Source Port (16 bits): Source TCP user. Example values are Telnet \u0001 23;\nA complete list is maintained at http://www.iana.org/\nassignments/port-numbers.\n• Destination Port (16 bits): Destination TCP user.\n• Sequence Number (32 bits): Sequence number of the first data octet in this\nsegment except when the SYN flag is set. If SYN is set, this field contains the\ninitial sequence number (ISN) and the first data octet in this segment has\nsequence number ISN + 1.\nTFTP = 69; HTTP = 80.\nTCP Service Response Primitives\nDescription\nlocal-connection-name, source-port,\nInforms TCP user of connection name \ndestination-port, destination-address\nassigned to pending connection \nrequested in an Open primitive\nOpen Failure\nlocal-connection-name\nReports failure of an Active Open request\nOpen Success\nlocal-connection-name\nReports completion of pending Open request\nlocal-connection-name, data, data-length,\nReports arrival of data\nURGENT-flag\nlocal-connection-name\nReports that remote TCP user has issued a\nClose and that all data sent by remote user\nhas been delivered\nlocal-connection-name, description\nReports that the connection has been termi-\nnated; a description of the reason for \ntermination is provided\nlocal-connection-name, source-port,\nReports current status of connection\nsource-address, destination-port,\ndestination-address, connection-state,\nreceive-window, send-window, amount-\nawaiting-ACK, amount-awaiting-receipt,\nurgent-state, precedence, security, timeout\nlocal-connection-name, description\nReports service-request or internal error\nNot used for Unspecified Passive Open.\nCHAPTER 20 / TRANSPORT PROTOCOLS\nConnection Establishment Connection establishment in TCP always uses a\nthree-way handshake.When the SYN flag is set, the segment is essentially a request\nfor connection and functions as explained in Section 20.1. To initiate a connection,\nan entity sends a SYN,\nwhere X is the initial sequence number. The\nreceiver responds with SYN,\nby setting both the SYN and\nCHAPTER 20 / TRANSPORT PROTOCOLS\n• First-only: Maintain one retransmission timer for the entire queue. If an\nCHAPTER 20 / TRANSPORT PROTOCOLS\nfor recent segments, and then set the timer to a value somewhat greater than the\nestimated round-trip time.\nSimple Average A simple approach is to take the average of observed round-trip\ntimes over a number of segments. If the average accurately predicts future round-\ntrip times, then the resulting retransmission timer will yield good performance. The\nsimple averaging method can be expressed as\nwhere RTT(i) is the round-trip time observed for the ith transmitted segment, and\nARTT(K) is the average round-trip time for the first K segments.\nThis expression can be rewritten as\nWith this formulation, it is not necessary to recalculate the entire summation each\nExponential Average Note that each term in the summation is given equal\nweight; that is, each term is multiplied by the same constant \nTypically, we\nwould like to give greater weight to more recent instances because they are more\nlikely to reflect future behavior. A common technique for predicting the next value\non the basis of a time series of past values, and the one specified in RFC 793, is expo-\nnential averaging:\nwhere SRTT(K) is called the smoothed round-trip time estimate, and we define\nCompare this with Equation (20.2). By using a constant value of\nindependent of the number of past observations, we have a circum-\nstance in which all past values are considered, but the more distant ones have less\nweight.To see this more clearly, consider the following expansion of Equation (20.3):\nBecause both \nare less than one, each successive term in the preceding\nequation is smaller. For example, for \nthe expansion is\nThe older the observation, the less it is counted in the average.\nThe smaller the value of \nthe greater the weight given to the more recent\nobservations. For \nvirtually all of the weight is given to the four or five\nmost recent observations, whereas for \nthe averaging is effectively\nspread out over the ten or so most recent observations. The advantage of using a\n10.1282RTT1K - 12 + Á\nSRTT1K + 12 = 10.22RTT1K + 12 + 10.162RTT1K2 +\na211 - a2RTT1K - 12 + Á + aK11 - a2RTT112\nSRTT1K + 12 = 11 - a2RTT1K + 12 + a11 - a2RTT1K2+\na 10 6 a 6 12,\nSRTT102 = 0.\nSRTT1K + 12 = a * SRTT1K2 + 11 - a2 * RTT1K + 12\nARTT1K + 12 =\nK + 1 ARTT1K2 +\nK + 1 RTT1K + 12\nARTT1K + 12 =\n20.3 / TCP CONGESTION CONTROL\nsmall value of \nis that the average will quickly reflect a rapid change in \nthe observed quantity. The disadvantage is that if there is a brief surge in the\nvalue of the observed quantity and it then settles back to some relatively \nconstant value, the use of a small value of \nwill result in jerky changes in the\nFigure 20.11 compares simple averaging with exponential averaging (for two\ndifferent values of ). In part (a) of the figure, the observed value begins at 1, grows\ngradually to a value of 10, and then stays there. In part (b) of the figure, the\n(a) Increasing function\nSimple average\nObserved value\nObserved or average value\n(b) Decreasing function\nSimple average\nObserved value\nObserved or average value\nFigure 20.11\nUse of Exponential Averaging\nCHAPTER 20 / TRANSPORT PROTOCOLS\nobserved value begins at 20, declines gradually to 10, and then stays there. Note\nthat exponential averaging tracks changes in RTT faster than does simple averag-\ning and that the smaller value of \nresults in a more rapid reaction to the change in\nthe observed value.\nEquation (20.3) is used in RFC 793 to estimate the current round-trip time.As\nwas mentioned, the retransmission timer should be set at a value somewhat greater\nthan the estimated round-trip time. One possibility is to use a constant value:\nwhere RTO is the retransmission timer (also called the retransmission timeout)\nis a constant. The disadvantage of this is that \nis not proportional to\nSRTT. For large values of SRTT,\nis relatively small and fluctuations in the\nactual RTT will result in unnecessary retransmissions. For small values of SRTT,\nis relatively large and causes unnecessary delays in retransmitting lost \nsegments. Accordingly, RFC 793 specifies the use of a timer whose value is pro-\nportional to SRTT, within limits:\nwhere UBOUND and LBOUND are prechosen fixed upper and lower bounds on\nthe timer value and \nis a constant. RFC 793 does not recommend specific values\nbut does list as “example values” the following:\nbetween 0.8 and 0.9 and \nbetween 1.3 and 2.0.\nRTT Variance Estimation (Jacobson’s Algorithm) The technique speci-\nfied in the TCP standard, and described in Equations (20.3) and (20.4), enables a\nTCP entity to adapt to changes in round-trip time. However, it does not cope well\nwith a situation in which the round-trip time exhibits a relatively high variance.\n[ZHAN86] points out three sources of high variance:\n1. If the data rate on the TCP connection is relatively low, then the transmission\ndelay will be relatively large compared to propagation time and the variance\nin delay due to variance in IP datagram size will be significant.Thus, the SRTT\nestimator is heavily influenced by characteristics that are a property of the\ndata and not of the network.\n2. Internet traffic load and conditions may change abruptly due to traffic from\nother sources, causing abrupt changes in RTT.\n3. The peer TCP entity may not acknowledge each segment immediately because\nof its own processing delays and because it exercises its privilege to use cumu-\nCHAPTER 20 / TRANSPORT PROTOCOLS\n(a) Increasing function\n(b) Decreasing function\nObserved or average value\nObserved or average value\nFigure 20.12\nJacobson’s RTO Calculation\n1. What RTO value should be used on a retransmitted segment? The exponential\nRTO backoff algorithm is used for this purpose.\n2. Which round-trip samples should be used as input to Jacobson’s algorithm?\nKarn’s algorithm determines which samples to use.\nExponential RTO Backoff When a TCP sender times out on a segment, it\nmust retransmit that segment. RFC 793 assumes that the same RTO value will be\nused for this retransmitted segment. However, because the timeout is probably due\n20.3 / TCP CONGESTION CONTROL\nto network congestion, manifested as a dropped packet or a long delay in round-trip\ntime, maintaining the same RTO value is ill advised.\nConsider the following scenario. There are a number of active TCP connec-\ntions from various sources sending traffic into an internet. A region of congestion\ndevelops such that segments on many of these connections are lost or delayed past\nthe RTO time of the connections. Therefore, at roughly the same time, many seg-\nments will be retransmitted into the internet, maintaining or even increasing the\ncongestion. All of the sources then wait a local (to each connection) RTO time and\nretransmit yet again. This pattern of behavior could cause a sustained condition of\ncongestion.\nA more sensible policy dictates that a sending TCP entity increase its RTO\neach time a segment is retransmitted; this is referred to as a backoff process. In the\nscenario of the preceding paragraph, after the first retransmission of a segment on\neach affected connection, the sending TCP entities will all wait a longer time before\nperforming a second retransmission.This may give the internet time to clear the cur-\nrent congestion. If a second retransmission is required, each sending TCP entity will\nwait an even longer time before timing out for a third retransmission, giving the\ninternet an even longer period to recover.\nA simple technique for implementing RTO backoff is to multiply the RTO for\na segment by a constant value for each retransmission:\nEquation (20.6) causes RTO to grow exponentially with each retransmission. The\nmost commonly used value of q is 2. With this value, the technique is referred to as\nbinary exponential backoff. This is the same technique used in the Ethernet\nCSMA/CD protocol (Chapter 16).\nKarn’s Algorithm If no segments are retransmitted, the sampling process for\nJacobson’s algorithm is straightforward.The RTT for each segment can be included\nin the calculation. Suppose, however, that a segment times out and must be retrans-\nCHAPTER 20 / TRANSPORT PROTOCOLS\n3These algorithms were developed by Jacobson [JACO88] and are also described in RFC 2581. Jacobson\ndescribes things in units of TCP segments, whereas RFC 2581 relies primarily on units of TCP data octets,\nwith some reference to calculations in units of segments.We follow the development in [JACO88].\nKarn’s algorithm [KARN91] solves this problem with the following rules:\n1. Do not use the measured RTT for a retransmitted segment to update SRTT\nand SDEV [Equation (20.5)].\n2. Calculate the backoff RTO using Equation (20.6) when a retransmission occurs.\n4Kleinrock refers to this phenomenon as the long-tail effect during a rush-hour period. See Sections 2.7\nand 2.10 of [KLEI76] for a detailed discussion.\nWhen a new connection is opened, the TCP entity initializes \nis, TCP is only allowed to send 1 segment and then must wait for an acknowledg-\nCHAPTER 20 / TRANSPORT PROTOCOLS\nRound-trip times\nFigure 20.13\nIllustration of Slow Start and Congestion Avoidance\nalgorithm set the value of RTO at somewhat greater than the estimated round-trip\ntime SRTT. Several factors make this margin desirable:\n1. RTO is calculated on the basis of a prediction of the next RTT, estimated from\npast values of RTT. If delays in the network fluctuate, then the estimated RTT\nmay be smaller than the actual RTT.\n2. Similarly, if delays at the destination fluctuate, the estimated RTT becomes\nunreliable.\n3. The destination system may not ACK each segment but cumulatively ACK\nmultiple segments, while at the same time sending ACKs when it has any data\nto send.This behavior contributes to fluctuations in RTT.\nA consequence of these factors is that if a segment is lost,TCP may be slow to\nretransmit. If the destination TCP is using an in-order accept policy (see Section\n6.3), then many segments may be lost. Even in the more likely case that the destina-\ntion TCP is using an in-window accept policy, a slow retransmission can cause prob-\nlems.To see this, suppose that A transmits a sequence of segments, the first of which\nis lost. So long as its send window is not empty and RTO does not expire,A can con-\nmanagement, as described in Chapter 22.\nThe strengths of the connection-oriented approach are clear. It allows connec-\ntion-related features such as flow control, error control, and sequenced delivery.\nConnectionless service, however, is more appropriate in some contexts. At lower\ncwnd = ssthresh,\nCHAPTER 20 / TRANSPORT PROTOCOLS\nSource Port\nDestination Port\nFigure 20.14\nlayers (internet, network), a connectionless service is more robust (e.g., see discus-\nsion in Section 10.5). In addition, it represents a “least common denominator” of\nservice to be expected at higher layers. Further, even at transport and above there is\njustification for a connectionless service. There are instances in which the overhead\nof connection establishment and termination is unjustified or even counterproduc-\ntive. Examples include the following:\n• Inward data collection: Involves the periodic active or passive sampling of\ndata sources, such as sensors, and automatic self-test reports from security\nequipment or network components. In a real-time monitoring situation, the\nloss of an occasional data unit would not cause distress, because the next\nreport should arrive shortly.\n• Outward data dissemination: Includes broadcast messages to network users,\nthe announcement of a new node or the change of address of a service, and the\ndistribution of real-time clock values.\n• Request-response: Applications in which a transaction service is provided by a\ncommon server to a number of distributed TS users, and for which a single\nrequest-response sequence is typical. Use of the service is regulated at the appli-\ncation level,and lower-level connections are often unnecessary and cumbersome.\n• Real-time applications: Such as voice and telemetry, involving a degree of\nredundancy and/or a real-time transmission requirement.These must not have\nconnection-oriented functions such as retransmission.\nThus, there is a place at the transport level for both a connection-oriented and\na connectionless type of service.\nUDP sits on top of IP. Because it is connectionless, UDP has very little to do.\nEssentially,it adds a port addressing capability to IP.This is best seen by examining the\nUDP header, shown in Figure 20.14. The header includes a source port and destina-\ntion port. The Length field contains the length of the entire UDP segment, including\nheader and data.The checksum is the same algorithm used for TCP and IP. For UDP,\nthe checksum applies to the entire UDP segment plus a pseudoheader prefixed to the\nUDP header at the time of calculation and which is the same pseudoheader used for\nTCP. If an error is detected, the segment is discarded and no further action is taken.\nThe Checksum field in UDP is optional. If it is not used, it is set to zero. How-\never, it should be pointed out that the IP checksum applies only to the IP header\nand not to the data field, which in this case consists of the UDP header and the user\ndata. Thus, if no checksum calculation is performed by UDP, then no check is made\non the user data at either the transport or internet protocol layers.\n20.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\n20.5 RECOMMENDED READING AND WEB SITES\n[IREN99] is a comprehensive survey of transport protocol services and protocol mecha-\nnisms, with a brief discussion of a number of different transport protocols. Perhaps the\nbest coverage of the various TCP strategies for flow and congestion control is to be found\nin [STEV94]. An essential paper for understanding the issues involved is the classic\ndata stream push\nduplicate detection\nexponential average\nflow control\nKarn’s algorithm\nmultiplexing\nretransmission strategy\nsequence number\nTCP congestion control\nTCP implementation policy\nTransmission Control Protocol\ntransport protocol\nurgent data signaling\nUser Datagram Protocol (UDP)\nIren, S.; Amer, P.; and Conrad, P. “The Transport Layer: Tutorial and Survey.”\nACM Computing Surveys, December 1999.\nJacobson, V. “Congestion Avoidance and Control.” Proceedings, SIGCOMM\n’88, Computer Communication Review, August 1988; reprinted in Computer Com-\nmunication Review, January 1995; a slightly revised version is available at\nftp.ee.lbl.gov/papers/congavoid.ps.Z\nStevens, W. TCP/IP Illustrated, Volume 1: The Protocols. Reading, MA:\nAddison-Wesley, 1994.\nRecommended Web sites:\n• Center for Internet Research: One of the most active groups in the areas covered in\nthis chapter.The site contains many papers and useful pointers.\n• TCP Maintenance Working Group: Chartered by IETF to make minor revisions to\nTCP and to update congestion strategies and protocols. The Web site includes all rele-\nvant RFCs and Internet drafts.\n• TCP-Friendly Web site: Summarizes some of the recent work on adaptive conges-\ntion control algorithms for non-TCP-based applications, with a specific focus on\nschemes that share bandwidth fairly with TCP connections.\n20.6 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nCHAPTER 20 / TRANSPORT PROTOCOLS\nReview Questions\nWhat addressing elements are needed to specify a target transport service (TS) user?\nDescribe four strategies by which a sending TS user can learn the address of a receiv-\ning TS user.\nExplain the use of multiplexing in the context of a transport protocol.\nBriefly describe the credit scheme used by TCP for flow control.\nWhat is the key difference between the TCP credit scheme and the sliding-window\nflow control scheme used by many other protocols, such as HDLC?\nExplain the two-way and three-way handshake mechanisms.\nWhat is the benefit of the three-way handshake mechanism?\nDefine the urgent and push features of TCP.\nWhat is a TCP implementation policy option?\nHow can TCP be used to deal with network or internet congestion?\nWhat does UDP provide that is not provided by IP?\nIt is common practice in most transport protocols (indeed, most protocols at all\nlevels) for control and data to be multiplexed over the same logical channel on a\nper-user-connection basis. An alternative is to establish a single control transport\nconnection between each pair of communicating transport entities. This connection\nwould be used to carry control signals relating to all user transport connections\nbetween the two entities. Discuss the implications of this strategy.\nThe discussion of flow control with a reliable network service referred to a backpres-\nsure mechanism utilizing a lower-level flow control protocol. Discuss the disadvan-\ntages of this strategy.\nTwo transport entities communicate across a reliable network. Let the normalized\ntime to transmit a segment equal 1. Assume that the end-to-end propagation delay is\n3, and that it takes a time 2 to deliver data from a received segment to the transport\nuser. The sender is initially granted a credit of seven segments. The receiver uses a\nconservative flow control policy, and updates its credit allocation at every opportu-\nnity.What is the maximum achievable throughput?\nSomeone posting to comp.protocols.tcp-ip complained about a throughput of 120\nkbps on a 256-kbps link with a 128-ms round-trip delay between the United States\nand Japan, and a throughput of 33 kbps when the link was routed over a satellite.\nWhat is the utilization over the two links? Assume a 500-ms round-trip delay for\nthe satellite link.\nb. What does the window size appear to be for the two cases?\nHow big should the window size be for the satellite link?\nDraw diagrams similar to Figure 20.4 for the following (assume a reliable sequenced\nnetwork service):\nConnection termination: active/passive\nb. Connection termination: active/active\nConnection rejection\nd. Connection abortion: User issues an OPEN to a listening user, and then issues a\nCLOSE before any data are exchanged.\nWith a reliable sequencing network service, are segment sequence numbers strictly\nnecessary? What, if any, capability is lost without them?\nConsider a connection-oriented network service that suffers a reset. How could this\nbe dealt with by a transport protocol that assumes that the network service is reliable\nexcept for resets?\n20.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nThe discussion of retransmission strategy made reference to three problems associ-\nated with dynamic timer calculation.What modifications to the strategy would help to\nalleviate those problems?\nConsider a transport protocol that uses a connection-oriented network service. Sup-\npose that the transport protocol uses a credit allocation flow control scheme, and the\nnetwork protocol uses a sliding-window scheme. What relationship, if any, should\nthere be between the dynamic window of the transport protocol and the fixed win-\ndow of the network protocol?\nIn a network that has a maximum packet size of 128 bytes, a maximum packet lifetime\nof 30 s, and an 8-bit packet sequence number, what is the maximum data rate per con-\nIs a deadlock possible using only a two-way handshake instead of a three-way hand-\nshake? Give an example or prove otherwise.\nListed are four strategies that can be used to provide a transport user with the address\nof the destination transport user. For each one, describe an analogy with the Postal\nService user.\nKnow the address ahead of time.\nb. Make use of a “well-known address.”\nUse a name server.\nd. Addressee is spawned at request time.\nIn a credit flow control scheme such as that of TCP, what provision could be made for\ncredit allocations that are lost or misordered in transit?\nWhat happens in Figure 20.3 if a SYN comes in while the requested user is in\nCLOSED? Is there any way to get the attention of the user when it is not listening?\nIn discussing connection termination with reference to Figure 20.8, it was stated that\nin addition to receiving an acknowledgement of its FIN and sending an acknowledge-\nment of the incoming FIN, a TCP entity must wait an interval equal to twice the max-\nimum expected segment lifetime (the TIME WAIT state). Receiving an ACK to its\nFIN assures that all of the segments it sent have been received by the other side.\nSending an ACK to the other side’s FIN assures the other side that all its segments\nhave been received. Give a reason why it is still necessary to wait before closing the\nconnection.\nOrdinarily, the Window field in the TCP header gives a credit allocation in octets.\nWhen the Window Scale option is in use, the value in the Window field is multiplied\nwhere F is the value of the window scale option. The maximum value of F\nthat TCP accepts is 14.Why is the option limited to 14?\nSuppose the round-trip time (RTT) between two hosts is 100 ms, and that both hosts\nuse a TCP window of 32 Kbytes. What is the maximum throughput that can be\nachieved by means of TCP in this scenario?\nSuppose two hosts are connected with each other by a means of a 100 mbps link, and\nassume the round-trip time (RTT) between them is 1 ms. What is the minimum TCP\nwindow size that would let TCP achieve the maximum possible throughput between\nthese two hosts? (Note: Assume no overhead.)\nA host is receiving data from a remote peer by means of TCP segments with a pay-\nload of 1460 bytes. If TCP acknowledges every other segment, what is the minimum\nuplink bandwidth needed to achieve a data throughput of 1 MBytes per second,\nassuming there is no overhead below the network layer? (Note: Assume no options\nare used by TCP and IP.)\nAnalyze the advantages and disadvantages of performing congestion control at the\ntransport layer, rather than at the network layer.\nJacobson’s congestion control algorithm assumes most packet losses are caused by\nrouters dropping packets due to network congestion. However, packets may be also\ndropped if they are corrupted in their path to destination. Analyze the performance\nof TCP in a such lossy environment, due to Jacobson’s congestion control algorithm.\nCHAPTER 20 / TRANSPORT PROTOCOLS\nOne difficulty with the original TCP SRTT estimator is the choice of an initial value.\nIn the absence of any special knowledge of network conditions, the typical approach\nis to pick an arbitrary value, such as 3 seconds, and hope that this will converge quickly\nto an accurate value. If this estimate is too small, TCP will perform unnecessary\nretransmissions. If it is too large,TCP will wait a long time before retransmitting if the\nfirst segment is lost.Also, the convergence may be slow, as this problem indicates.\nand assume all measured RTT\nand no packet loss. What is SRTT(19)? Hint: Equation (20.3)\ncan be rewritten to simplify the calculation,using the expression \nand assume measured RTT \nand no packet loss.What is SRTT(19)?\nA poor implementation of TCP’s sliding-window scheme can lead to extremely poor\nperformance. There is a phenomenon known as the Silly Window Syndrome (SWS),\nwhich can easily cause degradation in performance by several factors of 10. As an\nexample of SWS, consider an application that is engaged in a lengthy file transfer, and\nthat TCP is transferring this file in 200-octet segments.The receiver initially provides a\ncredit of 1000.The sender uses up this window with 5 segments of 200 octets. Now sup-\nChapter 21 Network Security\nNetwork security has become increasingly important with the growth\nin the number and importance of networks. Chapter 21 provides a sur-\nvey of security techniques and services. The chapter begins with a look\nat encryption techniques for ensuring confidentiality, which include the\nuse of conventional and public-key encryption. Then the area of\nauthentication and digital signatures is explored. The two most impor-\ntant encryption algorithms, AES and RSA, are examined, as well as\nSHA-1, a one-way hash function important in a number of security\napplications. Chapter 21 also discusses SSL and the set of IP security\nChapter 22 Internet Applications—Electronic Mail \nand Network Management\nThe purpose of a communications architecture is to support distributed\napplications. Chapter 22 examines two of the most important of these\napplications; in each case, general principles are discussed, followed by a\nspecific example. The applications discussed are electronic mail and net-\nwork management. The corresponding examples are SMTP and MIME;\nChapter 23 Internet Applications—Internet Directory \nService and World Wide Web\nChapter 23 looks at several important modern application areas for the\nInternet. The Domain Name System (DNS) is a directory lookup service\nthat provides a mapping between the name of a host on the Internet and\nits numerical address; it is a required application as specified in RFC 1123.\nThe Hypertext Transfer Protocol (HTTP) supports the exchange of\nrequests and responses between Web browsers and Web servers.\nChapter 24 Internet Applications—Multimedia\nChapter 24 examines key topics related to multimedia.The chapter begins\nwith a discussion of audio and video compression. The Session Initiation\nProtocol (SIP) is an application-level control protocol for setting up, mod-\nifying, and terminating real-time sessions between participants over an IP\ndata network; these include telephony and multimedia sessions. Finally,\nthis chapter examines the Real-Time Transport Protocol (RTP).\nSecurity Requirements and Attacks\nConfidentiality with Symmetric Encryption\nMessage Authentication and Hash Functions\nPublic-Key Encryption and Digital Signatures\nSecure Socket Layer and Transport Layer Security\nIPv4 and IPv6 Security\nWI-FI Protected Access\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nCHAPTER 21 / NETWORK SECURITY\nTo guard against the baneful influence exerted by strangers is therefore an elementary\ndictate of savage prudence. Hence before strangers are allowed to enter a district, or at\nleast before they are permitted to mingle freely with the inhabitants, certain cere-\nmonies are often performed by the natives of the country for the purpose of disarming\nthe strangers of their magical powers, or of disinfecting, so to speak, the tainted atmos-\nphere by which they are supposed to be surrounded.\n—The Golden Bough, Sir James George Frazer\nNetwork security threats fall into two categories. Passive threats,\nsometimes referred to as eavesdropping, involve attempts by an\nattacker to obtain information relating to a communication. Active\nthreats involve some modification of the transmitted data or the cre-\nation of false transmissions.\nBy far the most important automated tool for network and communi-\ncations security is encryption. With symmetric encryption, two parties\nshare a single encryption/decryption key.The principal challenge with\nsymmetric encryption is the distribution and protection of the keys.A\npublic-key encryption scheme involves two keys, one for encryption\nand a paired key for decryption.The party that generated the key pair\nkeeps one of the keys private and makes the other key public.\nSymmetric encryption and public-key encryption are often combined\nin secure networking applications. Symmetric encryption is used to\nencrypt transmitted data, using a one-time or short-term session key.\nThe session key can be distributed by a trusted key distribution center\nor transmitted in encrypted form using public-key encryption. Public-\nkey encryption is also used to create digital signatures, which can\nauthenticate the source of transmitted messages.\nThe Secure Sockets Layer (SSL) and the follow-on Internet standard\nknown as Transport Layer Security (TLS) provide security services\nfor Web transactions.\nA security enhancement used with both IPv4 and IPv6, called IPSec,\nprovides both confidentiality and authentication mechanisms.\nThe requirements of information security within an organization have under-\ngone two major changes in the last several decades.Before the widespread use of\ndata processing equipment, the security of information felt to be valuable to an\norganization was provided primarily by physical and administrative means. An\n21.1 / SECURITY REQUIREMENTS AND ATTACKS\nexample of the former is the use of rugged filing cabinets with a combination lock\nfor storing sensitive documents. An example of the latter is personnel screening\nprocedures used during the hiring process.\nWith the introduction of the computer, the need for automated tools for\nprotecting files and other information stored on the computer became evident.\nThis is especially the case for a shared system, such as a time-sharing system, and\nthe need is even more acute for systems that can be accessed over a public tele-\nphone or data network.The generic name for the collection of tools designed to\nprotect data and to thwart hackers is computer security. Although this is an\nimportant topic, it is beyond the scope of this book.\nThe second major change that affected security is the introduction of dis-\ntributed systems and the use of networks and communications facilities for\ncarrying data between terminal user and computer and between computer and\ncomputer. Network security measures are needed to protect data during their\ntransmission and to guarantee that data transmissions are authentic.\nThe essential technology underlying virtually all automated network and\ncomputer security applications is encryption.Two fundamental approaches are in\nuse: symmetric encryption and public-key encryption, also known as asymmetric\nencryption.As we look at the various approaches to network security, these two\ntypes of encryption will be explored.\nThe chapter begins with an overview of the requirements for network\nsecurity. Next, we look at symmetric encryption and its use to provide confi-\ndentiality. This is followed by a discussion of message authentication. We then\nlook at the use of public-key encryption and digital signatures. The chapter\ncloses with an examination of security features in SSL, IPSec, and Wi-Fi Pro-\ntected Access.\n21.1 SECURITY REQUIREMENTS AND ATTACKS\nTo understand the types of threats to security that exist, we need to have a definition\nof security requirements. Computer and network security address four require-\n• Confidentiality: Requires that data only be accessible by authorized parties.\nThis type of access includes printing, displaying, and other forms of disclosure,\nincluding simply revealing the existence of an object.\n• Integrity: Requires that only authorized parties can modify data. Modification\nincludes writing, changing, changing status, deleting, and creating.\n• Availability: Requires that data are available to authorized parties.\n• Authenticity: Requires that a host or service be able to verify the identity of a user.\nA useful means of classifying security attacks (RFC 2828) is in terms of passive\nattacks and active attacks. A passive attack attempts to learn or make use of infor-\nmation from the system but does not affect system resources. An active attack\nattempts to alter system resources or affect their operation.\nCHAPTER 21 / NETWORK SECURITY\nPassive Attacks\nPassive attacks are in the nature of eavesdropping on, or monitoring of, transmis-\nsions. The goal of the opponent is to obtain information that is being transmitted.\nCHAPTER 21 / NETWORK SECURITY\n• Ciphertext: This is the scrambled message produced as output. It depends on\nthe plaintext and the secret key. For a given message, two different keys will\nproduce two different ciphertexts.\n• Decryption algorithm: This is essentially the encryption algorithm run in reverse.\nIt takes the ciphertext and the secret key and produces the original plaintext.\nThere are two requirements for secure use of symmetric encryption:\n1. We need a strong encryption algorithm. At a minimum, we would like the\nalgorithm to be such that an opponent who knows the algorithm and has\naccess to one or more ciphertexts would be unable to decipher the ciphertext\nor figure out the key.This requirement is usually stated in a stronger form:The\nopponent should be unable to decrypt ciphertext or discover the key even if he\nor she is in possession of a number of ciphertexts together with the plaintext\nthat produced each ciphertext.\n2. Sender and receiver must have obtained copies of the secret key in a secure\nfashion and must keep the key secure. If someone can discover the key and\nknows the algorithm, all communication using this key is readable.\nThere are two general approaches to attacking a symmetric encryption scheme.\nThe first attack is known as cryptanalysis. Cryptanalytic attacks rely on the nature of\nthe algorithm plus perhaps some knowledge of the general characteristics of the\nplaintext or even some sample plaintext-ciphertext pairs.This type of attack exploits\nthe characteristics of the algorithm to attempt to deduce a specific plaintext or to\ndeduce the key being used. If the attack succeeds in deducing the key, the effect is\ncatastrophic:All future and past messages encrypted with that key are compromised.\nThe second method, known as the brute-force attack, is to try every possible\nkey on a piece of ciphertext until an intelligible translation into plaintext is obtained.\nOn average, half of all possible keys must be tried to achieve success. Table 21.1\nshows how much time is involved for various key sizes. The table shows results for\neach key size, assuming that it takes \nto perform a single decryption, a reasonable\norder of magnitude for today’s computers.With the use of massively parallel organi-\nzations of microprocessors, it may be possible to achieve processing rates many\norders of magnitude greater.The final column of the table considers the results for a\nsystem that can process 1 million keys per microsecond. As one can see, at this per-\nformance level, a 56-bit key can no longer be considered computationally secure.\nAverage Time Required for Exhaustive Key Search\nTime Required at\nTime Required at \nKey Size (bits)\nAlternative Keys\n2.15 milliseconds\n10.01 hours\n26 characters \n(permutation)\n6.4 * 106 years\n2 * 1026 ms = 6.4 * 1012 years\n26! = 4 * 1026\n5.9 * 1030 years\n2167 ms = 5.9 * 1036 years\n2168 = 3.7 * 1050\n5.4 * 1018 years\n2127 ms = 5.4 * 1024 years\n2128 = 3.4 * 1038\n255 ms = 1142 years\n256 = 7.2 * 1016\n231 ms = 35.8 minutes\n232 = 4.3 * 109\nDecryptions/Ms\n1 Decryption/Ms\n21.2 / CONFIDENTIALITY WITH SYMMETRIC ENCRYPTION\nEncryption Algorithms\nThe most commonly used symmetric encryption algorithms are block ciphers. A\nblock cipher processes the plaintext input in fixed-size blocks and produces a block\nof ciphertext of equal size for each plaintext block. The two most important sym-\nmetric algorithms, both of which are block ciphers, are the Data Encryption Stan-\ndard (DES) and the Advanced Encryption Standard (AES).\nData Encryption Standard DES has been the dominant encryption algorithm\nsince its introduction in 1977. However, because DES uses only a 56-bit key, it was\nonly a matter of time before computer processing speed made DES obsolete. In\n1998, the Electronic Frontier Foundation (EFF) announced that it had broken a\nDES challenge using a special-purpose “DES cracker” machine that was built for\nless than $250,000. The attack took less than three days. The EFF has published a\ndetailed description of the machine, enabling others to build their own cracker\n[EFF98]. And, of course, hardware prices will continue to drop as speeds increase,\nmaking DES worthless.\nThe life of DES was extended by the use of triple DES (3DES), which involves\nrepeating the basic DES algorithm three times, using either two or three unique\nkeys, for a key size of 112 or 168 bits.\nThe principal drawback of 3DES is that the algorithm is relatively sluggish\nin software. A secondary drawback is that both DES and 3DES use a 64-bit \nblock size. For reasons of both efficiency and security, a larger block size is \nAdvanced Encryption Standard Because of these drawbacks, 3DES is not\na reasonable candidate for long-term use. As a replacement, the National Insti-\ntute of Standards and Technology (NIST) in 1997 issued a call for proposals for a\nnew Advanced Encryption Standard (AES), which should have a security\nstrength equal to or better than 3DES and significantly improved efficiency.\nIn addition to these general requirements, NIST specified that AES must be a\nsymmetric block cipher with a block length of 128 bits and support for key\nlengths of 128, 192, and 256 bits. Evaluation criteria include security, computa-\ntional efficiency, memory requirements, hardware and software suitability, and\nflexibility. In 2001, AES was issued as a federal information processing standard\n(FIPS 197).\nIn the description of this section, we assume a key length of 128 bits, which is\nlikely to be the one most commonly implemented.\nFigure 21.2 shows the overall structure of AES. The input to the encryption\nand decryption algorithms is a single 128-bit block. In FIPS 197, this block is\ndepicted as a square matrix of bytes. This block is copied into the State array,\nwhich is modified at each stage of encryption or decryption. After the final stage,\nState is copied to an output matrix. Similarly, the 128-bit key is depicted as a\nsquare matrix of bytes. This key is then expanded into an array of key schedule\nwords; each word is 4 bytes and the total key schedule is 44 words for the 128-bit\nkey. The ordering of bytes within a matrix is by column. So, for example, the first\nfour bytes of a 128-bit plaintext input to the encryption cipher occupy the first col-\numn of the in matrix, the second four bytes occupy the second column, and so on.\nCHAPTER 21 / NETWORK SECURITY\nAdd round key\nSubstitute bytes\nMix columns\nAdd round key\nSubstitute bytes\nMix columns\nAdd round key\nSubstitute bytes\nAdd round key\n(a) Encryption\nAdd round key\nInverse sub bytes\nInverse shift rows\nInverse mix cols\nAdd round key\nInverse sub bytes\nInverse shift rows\nInverse mix cols\nAdd round key\nInverse sub bytes\nInverse shift rows\nAdd round key\n(b) Decryption\nFigure 21.2\nAES Encryption and Decryption\nSimilarly, the first four bytes of the expanded key, which form a word, occupy the\nfirst column of the w matrix.\nThe following comments give some insight into AES:\n1. The key that is provided as input is expanded into an array of forty-four\n32-bit words, w[i]. Four distinct words (128 bits) serve as a round key for\neach round.\n2. Four different stages are used, one of permutation and three of substitution:\n21.2 / CONFIDENTIALITY WITH SYMMETRIC ENCRYPTION\n1The term S-box, or substitution box, is commonly used in the description of symmetric ciphers to refer\nto a table used for a table-lookup type of substitution mechanism.\n• Substitute bytes: Uses a table, referred to as an S-box,1 to perform a byte-\nby-byte substitution of the block\n• Shift rows: A simple permutation that is performed row by row\n• Mix columns: A substitution that alters each byte in a column as a function\nof all of the bytes in the column\n• Add round key: A simple bitwise XOR of the current block with a portion\nof the expanded key\n3. The structure is quite simple. For both encryption and decryption, the cipher\nbegins with an Add Round Key stage, followed by nine rounds that each\nincludes all four stages, followed by a tenth round of three stages. Figure 21.3\ndepicts the structure of a full encryption round.\n4. Only the Add Round Key stage makes use of the key. For this reason, the\ncipher begins and ends with an Add Round Key stage. Any other stage,\napplied at the beginning or end, is reversible without knowledge of the key\nand so would add no security.\nAddRoundKey\nFigure 21.3\nAES Encryption Round\nCHAPTER 21 / NETWORK SECURITY\n5. The Add Round Key stage by itself would not be formidable. The other three\nstages together scramble the bits, but by themselves would provide no security\nbecause they do not use the key.We can view the cipher as alternating operations\nof XOR encryption (Add Round Key) of a block, followed by scrambling of the\nblock (the other three stages), followed by XOR encryption, and so on. This\nscheme is both efficient and highly secure.\n6. Each stage is easily reversible. For the Substitute Byte, Shift Row, and Mix\nColumns stages, an inverse function is used in the decryption algorithm. For the\nAdd Round Key stage,the inverse is achieved by XORing the same round key to\nthe block, using the result that \n7. As with most block ciphers, the decryption algorithm makes use of the expanded\nkey in reverse order. However, the decryption algorithm is not identical to the\nencryption algorithm. This is a consequence of the particular structure of AES.\n8. Once it is established that all four stages are reversible, it is easy to verify that\ndecryption does recover the plaintext. Figure 21.2 lays out encryption and\ndecryption going in opposite vertical directions.At each horizontal point (e.g.,the\ndashed line in the figure), State is the same for both encryption and decryption.\n9. The final round of both encryption and decryption consists of only three\nstages. Again, this is a consequence of the particular structure of AES and is\nrequired to make the cipher reversible.\nLocation of Encryption Devices\nThe most powerful, and most common, approach to countering the threats to net-\nwork security is encryption. In using encryption, we need to decide what to\nencrypt and where the encryption gear should be located. As Figure 21.4\nA \u0001 A \u0001 B = B.\nPacket-switching\n\u0001 End-to-end encryption device\n\u0001 Link encryption device\nPSN \u0001 Packet-switching node\nFigure 21.4\nEncryption across a Packet-Switching Network\n21.2 / CONFIDENTIALITY WITH SYMMETRIC ENCRYPTION\nindicates, there are two fundamental alternatives: link encryption and end-to-end\nencryption.\nWith link encryption, each vulnerable communications link is equipped on\nboth ends with an encryption device. Thus, all traffic over all communications links\nis secured. Although this requires a lot of encryption devices in a large network, it\nprovides a high level of security. One disadvantage of this approach is that the mes-\nsage must be decrypted each time it enters a packet switch; this is necessary because\nthe switch must read the address (virtual circuit number) in the packet header to\nroute the packet. Thus, the message is vulnerable at each switch. If this is a public\npacket-switching network, the user has no control over the security of the nodes.\nWith end-to-end encryption, the encryption process is carried out at the two\nend systems. The source host or terminal encrypts the data. The data, in encrypted\nform, are then transmitted unaltered across the network to the destination terminal\nor host. The destination shares a key with the source and so is able to decrypt the\ndata. This approach would seem to secure the transmission against attacks on the\nnetwork links or switches.There is, however, still a weak spot.\nConsider the following situation.A host connects to a frame relay network, sets\nup a logical connection to another host, and is prepared to transfer data to that other\nhost using end-to-end encryption. Data are transmitted over such a network in the\nform of frames, or packets, consisting of a header and some user data. What part of\neach packet will the host encrypt? Suppose that the host encrypts the entire packet,\nincluding the header. This will not work because, remember, only the other host can\nperform the decryption. Each frame relay node will receive an encrypted packet and\nbe unable to read the header.Therefore, it will not be able to route the packet. It fol-\nlows that the host may only encrypt the user data portion of the packet and must\nleave the header in the clear, so that the network can read it.\nThus, with end-to-end encryption, the user data are secure. However, the\ntraffic pattern is not, because packet headers are transmitted in the clear. To\nachieve greater security, both link and end-to-end encryption are needed, as is\nshown in Figure 21.4.\nTo summarize, when both forms are employed, the host encrypts the user data\nportion of a packet using an end-to-end encryption key. The entire packet is then\nencrypted using a link encryption key. As the packet traverses the network, each\nswitch decrypts the packet using a link encryption key to read the header and then\nencrypts the entire packet again for sending it out on the next link. Now the entire\npacket is secure except for the time that the packet is actually in the memory of a\npacket switch, at which time the packet header is in the clear.\nKey Distribution\nFor symmetric encryption to work, the two parties to an exchange must share the\nsame key, and that key must be protected from access by others. Furthermore, fre-\nquent key changes are usually desirable to limit the amount of data compromised if\nan attacker learns the key.Therefore, the strength of any cryptographic system rests\nwith the key distribution technique, a term that refers to the means of delivering a\nkey to two parties that wish to exchange data without allowing others to see the key.\nKey distribution can be achieved in a number of ways. For two parties A and B,\nCHAPTER 21 / NETWORK SECURITY\n1. A key could be selected by A and physically delivered to B.\n2. A third party could select the key and physically deliver it to A and B.\n3. If A and B have previously and recently used a key, one party could transmit the\nnew key to the other, encrypted using the old key.\n4. If A and B each have an encrypted connection to a third party C, C could\ndeliver a key on the encrypted links to A and B.\nOptions 1 and 2 call for manual delivery of a key. For link encryption, this is a\nreasonable requirement, because each link encryption device is only going to be\nexchanging data with its partner on the other end of the link. However, for end-to-\nend encryption, manual delivery is awkward. In a distributed system, any given host\nor terminal may need to engage in exchanges with many other hosts and terminals\nover time. Thus, each device needs a number of keys, supplied dynamically. The\nproblem is especially difficult in a wide area distributed system.\nOption 3 is a possibility for either link encryption or end-to-end encryption, but\nif an attacker ever succeeds in gaining access to one key, then all subsequent keys are\nrevealed. Even if frequent changes are made to the link encryption keys, these should\nbe done manually. To provide keys for end-to-end encryption, option 4 is preferable.\nFigure 21.5 illustrates an implementation of option 4 for end-to-end encryp-\ntion. In the figure, link encryption is ignored.This can be added, or not, as required.\nFor this scheme, two kinds of keys are identified:\n• Session key: When two end systems (hosts, terminals, etc.) wish to communi-\ncate, they establish a logical connection (e.g., virtual circuit). For the duration\nof that logical connection, all user data are encrypted with a one-time session\nkey. At the conclusion of the session, or connection, the session key is\ndistribution\n1. Host sends packet requesting connection.\n2. Security service buffers packet; asks\n    KDC for session key.\n3. KDC distributes session key to both hosts.\n4. Buffered packet transmitted.\nApplication\nApplication\nFigure 21.5\nAutomatic Key Distribution for Connection-Oriented Protocol\n21.3 / MESSAGE AUTHENTICATION AND HASH FUNCTIONS\n• Permanent key: A permanent key is a key used between entities for the pur-\npose of distributing session keys.\nThe configuration consists of the following elements:\n• Key distribution center: The key distribution center determines which systems\nare allowed to communicate with each other. When permission is granted for\ntwo systems to establish a connection, the key distribution center provides a\none-time session key for that connection.\n• Security service module (SSM): This module, which may consist of functional-\nity at one protocol layer, performs end-to-end encryption and obtains session\nkeys on behalf of users.\nThe steps involved in establishing a connection are shown in the figure. When\none host wishes to set up a connection to another host, it transmits a connection-\nrequest packet (step 1).The SSM saves that packet and applies to the KDC for permis-\nsion to establish the connection (step 2).The communication between the SSM and the\nKDC is encrypted using a master key shared only by this SSM and the KDC. If the\nKDC approves the connection request, it generates the session key and delivers it to\nthe two appropriate SSMs, using a unique permanent key for each SSM (step 3). The\nrequesting SSM can now release the connection request packet,and a connection is set\nup between the two end systems (step 4).All user data exchanged between the two end\nsystems are encrypted by their respective SSMs using the one-time session key.\nThe automated key distribution approach provides the flexibility and dynamic\ncharacteristics needed to allow a number of terminal users to access a number of\nhosts and for the hosts to exchange data with each other.\nAnother approach to key distribution uses public-key encryption, which is dis-\ncussed in Section 21.4.\nTraffic Padding\nWe mentioned that, in some cases, users are concerned about security from traffic\nanalysis.With the use of link encryption, packet headers are encrypted, reducing the\nopportunity for traffic analysis. However, it is still possible in those circumstances\nfor an attacker to assess the amount of traffic on a network and to observe the\namount of traffic entering and leaving each end system. An effective countermea-\nsure to this attack is traffic padding.\nTraffic padding is a function that produces ciphertext output continuously,\neven in the absence of plaintext. A continuous random data stream is generated.\nWhen plaintext is available, it is encrypted and transmitted. When input plaintext is\nnot present, the random data are encrypted and transmitted. This makes it impossi-\nble for an attacker to distinguish between true data flow and noise and therefore\nimpossible to deduce the amount of traffic.\n21.3 MESSAGE AUTHENTICATION AND HASH FUNCTIONS\nEncryption protects against passive attack (eavesdropping). A different require-\nment is to protect against active attack (falsification of data and transactions). Pro-\ntection against such attacks is known as message authentication.\nCHAPTER 21 / NETWORK SECURITY\nApproaches to Message Authentication\nA message, file, document, or other collection of data is said to be authentic when it\nis genuine and came from its alleged source. Message authentication is a procedure\nthat allows communicating parties to verify that received messages are authentic.\nCHAPTER 21 / NETWORK SECURITY\nuse of DES. DES is used to generate an encrypted version of the message, and the\nlast number of bits of ciphertext are used as the code.A 16- or 32-bit code is typical.\nThe process just described is similar to encryption. One difference is that the\nauthentication algorithm need not be reversible, as it must for decryption. It turns\nout that because of the mathematical properties of the authentication function, it is\nless vulnerable to being broken than encryption.\nOne-Way Hash Function A variation on the message authentication code that\nhas received much attention recently is the one-way hash function.As with the mes-\nsage authentication code, a hash function accepts a variable-size message M as input\nand produces a fixed-size message digest H(M) as output. Unlike the MAC, a hash\nfunction does not also take a secret key as input.To authenticate a message, the mes-\nsage digest is sent with the message in such a way that the message digest is authentic.\nFigure 21.7 illustrates three ways in which the message can be authenticated.\nThe message digest can be encrypted using symmetric encryption (part a); if it is\nassumed that only the sender and receiver share the encryption key, then authentic-\nity is assured.The message digest can also be encrypted using public-key encryption\n(part b); this is explained in Section 21.4. The public-key approach has two advan-\ntages: it provides a digital signature as well as message authentication, and it does\nnot require the distribution of keys to communicating parties.\nThese two approaches have an advantage over approaches that encrypt the\nentire message in that less computation is required. Nevertheless, there has been\ninterest in developing a technique that avoids encryption altogether. Several rea-\nsons for this interest are pointed out in [TSUD92]:\n• Encryption software is quite slow. Even though the amount of data to be\nencrypted per message is small, there may be a steady stream of messages into\nand out of a system.\n• Encryption hardware costs are nonnegligible. Low-cost chip implementations\nof DES are available, but the cost adds up if all nodes in a network must have\nthis capability.\n• Encryption hardware is optimized toward large data sizes. For small blocks of\ndata,a high proportion of the time is spent in initialization/invocation overhead.\n• Encryption algorithms may be covered by patents. Some encryption algo-\nrithms, such as the RSA public-key algorithm, are patented and must be\nlicensed, adding a cost.\n• Encryption algorithms may be subject to export control.\nFigure 21.7c shows a technique that uses a hash function but no encryption for\nmessage authentication. This technique assumes that two communicating parties,\nsay A and B, share a common secret value \nWhen A has a message to send to B,\nit calculates the hash function over the concatenation of the secret value and the\n2 It then sends \nto B. Because B possesses\nit can recompute \nBecause the secret value itself is\nnot sent, it is not possible for an attacker to modify an intercepted message.As long\nMDM = H1SAB7M2.\ndenotes concatenation.\n21.3 / MESSAGE AUTHENTICATION AND HASH FUNCTIONS\n(b) Using public-key encryption\n(c) Using secret value\n(a) Using conventional encryption\nDestination B \nFigure 21.7\nMessage Authentication Using a One-Way Hash Function\nas the secret value remains secret, it is also not possible for an attacker to generate a\nfalse message.\nThis third technique, using a shared secret value, is the one adopted for IP\nsecurity; it has also been specified for SNMPv3, discussed in Chapter 22.\nSecure Hash Functions\nThe one-way hash function, or secure hash function, is important not only in\nmessage authentication but in digital signatures. In this section, we begin with \nCHAPTER 21 / NETWORK SECURITY\na discussion of requirements for a secure hash function. Then we look at one of\nthe most important hash functions, SHA.\nHash Function Requirements The purpose of a hash function is to produce a\n“fingerprint” of a file, message, or other block of data. To be useful for message\nauthentication, a hash function H must have the following properties:\n1. H can be applied to a block of data of any size.\n2. H produces a fixed-length output.\n3. H(x) is relatively easy to compute for any given x, making both hardware and\nsoftware implementations practical.\n4. For any given code h, it is computationally infeasible to find x such that\n5. For any given block x, it is computationally infeasible to find \n6. It is computationally infeasible to find any pair (x, y) such that \nThe first three properties are requirements for the practical application of a\nhash function to message authentication.\nThe fourth property is the one-way property: It is easy to generate a code\ngiven a message, but virtually impossible to generate a message given a code.\nThis property is important if the authentication technique involves the use of a\nsecret value (Figure 21.7c). The secret value itself is not sent; however, if the\nhash function is not one way, an attacker can easily discover the secret value: If\nthe attacker can observe or intercept a transmission, the attacker obtains the\nmessage M and the hash code \nThe attacker then inverts the\nhash function to obtain \nBecause the attacker now has\nit is a trivial matter to recover \nThe fifth property guarantees that it is impossible to find an alternative mes-\nsage with the same hash value as a given message. This prevents forgery when an\nencrypted hash code is used (Figures 21.7a and b). If this property were not true, an\nattacker would be capable of the following sequence: First, observe or intercept a\nmessage plus its encrypted hash code; second, generate an unencrypted hash code\nfrom the message; third, generate an alternate message with the same hash code.\nA hash function that satisfies the first five properties in the preceding list is\nreferred to as a weak hash function. If the sixth property is also satisfied, then it is\nreferred to as a strong hash function. The sixth property protects against a sophisti-\ncated class of attack known as the birthday attack.3\nIn addition to providing authentication, a message digest also provides data\nintegrity. It performs the same function as a frame check sequence: If any bits in the\nmessage are accidentally altered in transit, the message digest will be in error.\nThe SHA Secure Hash Function\nThe Secure Hash Algorithm (SHA) was developed by NIST and published as a fed-\neral information processing standard (FIPS 180) in 1993; a revised version was\nissued as FIPS 180-1 in 1995 and is generally referred to as SHA-1.\nSAB7M = H-11MDM2.\nMDM = H1SAB7M2.\nH1x2 = H1y2.\nH1y2 = H1x2.\n3See [STAL06] for a discussion of birthday attacks.\n21.3 / MESSAGE AUTHENTICATION AND HASH FUNCTIONS\nSHA-1 produces a hash value of 160 bits. In 2002, NIST produced a new ver-\nsion of the standard, FIPS 180-2, that defined three new versions of SHA, with hash\nvalue lengths of 256, 384, and 512 bits, known as SHA-256, SHA-384, and SHA-512.\nThese new versions have the same underlying structure and use the same types of\nmodular arithmetic and logical binary operations as SHA-1. In 2005, NIST\nannounced the intention to phase out approval of SHA-1 and move to a reliance on\nthe other SHA versions by 2010. Shortly thereafter, a research team described an\nattack in which two separate messages could be found that deliver the same SHA-1\noperations, far fewer than the \noperations previously thought\nneeded to find a collision with an SHA-1 hash [WANG05].This result should hasten\nthe transition to the other versions of SHA.\nIn this section, we provide a description of SHA-512. The other versions are\nquite similar.The algorithm takes as input a message with a maximum length of less\nbits and produces as output a 512-bit message digest. The input is\nprocessed in 1024-bit blocks. Figure 21.8 depicts the overall processing of a message\nto produce a digest.The processing consists of the following steps:\nStep 1: Append padding bits. The message is padded so that its length is congruent\nto 896 modulo 1024 [length mod \n). Padding is always added, even if\nthe message is already of the desired length. Thus, the number of padding bits is in\nthe range of 1 to 1024. The padding consists of a single 1-bit followed by the neces-\nsary number of 0-bits.\nStep 2: Append length. A block of 128 bits is appended to the message.This block is\ntreated as an unsigned 128-bit integer (most significant byte first) and contains the\nlength of the original message (before the padding) The inclusion of a length value\nmakes more difficult a kind of attack known as a padding attack [TSUD92].\nThe outcome of the first two steps yields a message that is an integer multiple\nof 1024 bits in length. In Figure 21.8, the expanded message is represented as the\nN \u0006 1024 bits\n\u0002 \u0001 word-by-word addition mod 264\nFigure 21.8\nMessage Digest Generation Using SHA-512\nCHAPTER 21 / NETWORK SECURITY\nsequence of 1024-bit blocks \nso that the total length of the\nexpanded message is \nStep 3: Initialize MD buffer. A 512-bit buffer is used to hold intermediate and final\nresults of the hash function.\nStep 4: Process message in 512-bit (16-word) blocks. The heart of the algorithm is a\nmodule that consists of 80 rounds of processing.The 80 rounds have the same struc-\nture but vary some constants and logical functions.\nStep 5: Output. After all N 1024-bit blocks have been processed, the output from\nthe Nth stage is the 512-bit message digest.\nThe SHA-512 algorithm has the property that every bit of the hash code is a\nfunction of every bit of the input. The complex repetition of the basic function F\nproduces results that are well mixed; that is, it is unlikely that two messages chosen\nat random, even if they exhibit similar regularities, will have the same hash code.\nUnless there is some hidden weakness in SHA-512, which has not so far been pub-\nlished, the difficulty of coming up with two messages having the same message\ndigest is on the order of \noperations, while the difficulty of finding a message\nwith a given digest is on the order of \noperations.\n21.4 PUBLIC-KEY ENCRYPTION AND DIGITAL SIGNATURES\nOf equal importance to symmetric encryption is public-key encryption, which finds\nuse in message authentication and key distribution. This section looks first at the\nbasic concept of public-key encryption, followed by a discussion of digital signa-\ntures. Then we discuss the most widely used public-key algorithm: RSA. We then\nlook at the problem of key distribution.\nPublic-Key Encryption\nPublic-key encryption, first publicly proposed by Diffie and Hellman in 1976\n[DIFF76], is the first truly revolutionary advance in encryption in literally thousands\nof years. For one thing, public-key algorithms are based on mathematical functions\nrather than on simple operations on bit patterns. More important, public-key cryp-\ntography is asymmetric, involving the use of two separate keys, in contrast to sym-\nmetric encryption, which uses only one key. The use of two keys has profound\nconsequences in the areas of confidentiality, key distribution, and authentication.\nBefore proceeding, we should first mention several common misconceptions\nconcerning public-key encryption. One is that public-key encryption is more secure\nfrom cryptanalysis than symmetric encryption. In fact, the security of any encryp-\ntion scheme depends on (1) the length of the key and (2) the computational work\ninvolved in breaking a cipher. There is nothing in principle about either symmetric\nor public-key encryption that makes one superior to another from the viewpoint of\nresisting cryptanalysis. A second misconception is that public-key encryption is a\ngeneral-purpose technique that has made symmetric encryption obsolete. On the\ncontrary, because of the computational overhead of current public-key encryption\nschemes, there seems no foreseeable likelihood that symmetric encryption will be\nM1, M2, Á , MN,\n21.4 / PUBLIC-KEY ENCRYPTION AND DIGITAL SIGNATURES\nabandoned. Finally, there is a feeling that key distribution is trivial when using pub-\nlic-key encryption, compared to the rather cumbersome handshaking involved with\nkey distribution centers for symmetric encryption. In fact, some form of protocol is\nneeded, often involving a central agent, and the procedures involved are no simpler\nor any more efficient than those required for symmetric encryption.\nA public-key encryption scheme has six ingredients (Figure 21.9):\n(a) Encryption\n(b) Authentication\nTransmitted\nEncryption algorithm\n(e.g., RSA)\nDecryption algorithm\n(reverse of encryption\nBob's private\nBob's public\nY \u0001 E[PRb, X]\nTransmitted\nEncryption algorithm\n(e.g., RSA)\nDecryption algorithm\n(reverse of encryption\nAlice's public\nAlice's private\nY \u0001 E[PUa, X]\nFigure 21.9\nPublic-Key Cryptography\nCHAPTER 21 / NETWORK SECURITY\n• Plaintext: This is the readable message or data that is fed into the algorithm as\n• Encryption algorithm: The encryption algorithm performs various transfor-\nmations on the plaintext.\n• Public and private key: This is a pair of keys that have been selected so that if\none is used for encryption the other is used for decryption.The exact transfor-\nmations performed by the encryption algorithm depend on the public or pri-\nvate key that is provided as input.\n• Ciphertext: This is the scrambled message produced as output. It depends on\nthe plaintext and the key. For a given message, two different keys will produce\ntwo different ciphertexts.\n• Decryption algorithm: This algorithm accepts the ciphertext and the matching\nkey and produces the original plaintext.\nAs the names suggest, the public key of the pair is made public for others to use,\nwhile the private key is known only to its owner.A general-purpose public-key crypto-\ngraphic algorithm relies on one key for encryption and a different but related key for\ndecryption.Furthermore,these algorithms have the following important characteristics:\n• It is computationally infeasible to determine the decryption key given only\nknowledge of the cryptographic algorithm and the encryption key.\n• For most public-key schemes, either of the two related keys can be used for\nencryption, with the other used for decryption.\nThe essential steps are the following:\n1. Each user generates a pair of keys to be used for the encryption and decryp-\ntion of messages.\n2. Each user places one of the two keys in a public register or other accessible file.\nThis is the public key.The companion key is kept private.As Figure 21.9 suggests,\neach user maintains a collection of public keys obtained from others.\n3. If Bob wishes to send a private message to Alice,Bob encrypts the message using\nAlice’s public key.\n4. When Alice receives the message, she decrypts it using her private key. No\nother recipient can decrypt the message because only Alice knows Alice’s \nprivate key.\nWith this approach, all participants have access to public keys, and private\nkeys are generated locally by each participant and therefore need never be distrib-\nuted. As long as a user protects his or her private key, incoming communication is\nsecure. At any time, a user can change the private key and publish the companion\npublic key to replace the old public key.\nDigital Signature\nPublic-key encryption can be used in another way,as illustrated in Figure 21.9b.Suppose\nthat Bob wants to send a message to Alice and,although it is not important that the mes-\nsage be kept secret,he wants Alice to be certain that the message is indeed from him.In\n21.4 / PUBLIC-KEY ENCRYPTION AND DIGITAL SIGNATURES\nthis case Bob uses his own private key to encrypt the message.When Alice receives the\nciphertext, she finds that she can decrypt it with Bob’s public key, thus proving that the\nmessage must have been encrypted by Bob. No one else has Bob’s private key and\ntherefore no one else could have created a ciphertext that could be decrypted with\nBob’s public key.Therefore, the entire encrypted message serves as a digital signature.\nIn addition, it is impossible to alter the message without access to Bob’s private key, so\nthe message is authenticated both in terms of source and in terms of data integrity.\nIn the preceding scheme,the entire message is encrypted,which,although validat-\nCHAPTER 21 / NETWORK SECURITY\n4It can be shown that when n is a product of two primes, pq, then f1n2 = 1p - 121q - 12.\nSelect p, q\np and q both prime, p \u000e q\nCalculate n \u0001 p \u0007 q\nCalculate d\nPrivate key\nCiphertext:\nCalculate f(n) \u0001 (p \u0003 1)(q \u0003 1)\nSelect integer e\ngcd(f(n), e) \u0001 1; 1 \b e \b f(n)\nde mod f(n) \u0001 1\nPU \u0001 {e, n}\nPR \u0001 {d, n}\nC \u0001 Me (mod n)\nKey Generation\nCiphertext:\nM \u0001 Cd (mod n)\nFigure 21.10\nThe RSA Algorithm\ndecryption.Next,we need the quantity \n, referred to as the Euler totient of n, which\nis the number of positive integers less than n and relatively prime to n.4 Then select an\ninteger e that is relatively prime to \n[i.e., the greatest common divisor of e and\nis 1]. Finally, calculate d such that de mod \nIt can be shown that d and e\nhave the desired properties.\nSuppose that user A has published its public key and that user B wishes to\nsend the message M to A. Then B calculates \n(mod n) and transmits C. On\nreceipt of this ciphertext, user A decrypts by calculating \nAn example, from [SING99], is shown in Figure 21.11. For this example, the\nkeys were generated as follows:\n1. Select two prime numbers,\n2. Calculate\n3. Calculate\n4. Select e such that e is relatively prime to \nand less than \nchoose e = 7.\nf1n2 = 1p - 121q - 12 = 16 * 10 = 160.\nn = pq = 17 * 11 = 187.\n21.4 / PUBLIC-KEY ENCRYPTION AND DIGITAL SIGNATURES\n7 mod 187 \u0001 11\nKU \u0001 7, 187\nKR \u0001 23, 187\n23 mod 187 \u0001 88\nFigure 21.11\nExample of RSA Algorithm\n5. Determine d such that de mod \nThe correct value is\nThe resulting keys are public key \nand private key\nThe example shows the use of these keys for a plaintext input of\nFor encryption, we need to calculate \nmod 187. Exploiting the\nproperties of modular arithmetic, we can do this as follows:\nFor decryption, we calculate \nThere are two possible approaches to defeating the RSA algorithm.The first is\nthe brute-force approach: Try all possible private keys. Thus, the larger the number\nof bits in e and d, the more secure the algorithm. However, because the calculations\ninvolved, both in key generation and in encryption/decryption, are complex, the\nlarger the size of the key, the slower the system will run.\nMost discussions of the cryptanalysis of RSA have focused on the task of fac-\ntoring n into its two prime factors. For a large n with large prime factors, factoring is a\nhard problem, but not as hard as it used to be.A striking illustration of this is the fol-\nlowing. In 1977, the three inventors of RSA dared Scientific American readers to\ndecode a cipher they printed in Martin Gardner’s “Mathematical Games” column.\n 1123 mod 187 = 111 * 121 * 55 * 33 * 332 mod 187 = 79,720,245 mod 187 = 88\n118 mod 187 = 214,358,881 mod 187 = 33\n114 mod 187 = 14,641 mod 187 = 55\n112 mod 187 = 121\n111 mod 187 = 11\n1118 mod 1872 * 1118 mod 1872] mod 187\n 1123 mod 187 = [1111 mod 1872 * 1112 mod 1872 * 1114 mod 1872 *\n 887 mod 187 = 188 * 77 * 1322 mod 187 = 894,432 mod 187 = 11\n 884 mod 187 = 59,969,536 mod 187 = 132\n 882 mod 187 = 7744 mod 187 = 77\n 881 mod 187 = 88\n 887 mod 187 = [1884 mod 1872 * 1882 mod 1872 * 1881 mod 1872] mod 187\nPR = 523, 1876.\nPU = 57, 1876\n23 * 7 = 161 = 10 * 160 + 1.\nCHAPTER 21 / NETWORK SECURITY\nThey offered a $100 reward for the return of a plaintext sentence, an event they pre-\ndicted might not occur for some 40 quadrillion years. In April of 1994, a group work-\ning over the Internet and using over 1600 computers claimed the prize after only\neight months of work [LEUT94]. This challenge used a public-key size (length of n)\nof 129 decimal digits, or around 428 bits. This result does not invalidate the use of\nRSA;it simply means that larger key sizes must be used.Currently,a 1024-bit key size\n(about 300 decimal digits) is considered strong enough for virtually all applications.\nKey Management\nWith symmetric encryption, a fundamental requirement for two parties to communi-\ncate securely is that they share a secret key. Suppose Bob wants to create a messaging\napplication that will enable him to exchange e-mail securely with anyone who has\naccess to the Internet or to some other network that the two of them share. Suppose\nBob wants to do this using only symmetric encryption. With symmetric encryption,\nBob and his correspondent, say, Alice, must come up with a way to share a unique\nsecret key that no one else knows. How are they going to do that? If Alice is in the\nnext room from Bob, Bob could generate a key and write it down on a piece of paper\nor store it on a diskette and hand it to Alice. But if Alice is on the other side of the\ncontinent or the world, what can Bob do? Well, he could encrypt this key using sym-\nmetric encryption and e-mail it to Alice, but this means that Bob and Alice must\nshare a secret key to encrypt this new secret key. Furthermore, Bob and everyone\nelse who uses this new e-mail package faces the same problem with every potential\ncorrespondent: Each pair of correspondents must share a unique secret key.\nHow to distribute secret keys securely is the most difficult problem for sym-\nmetric encryption. This problem is wiped away with public-key encryption by the\nsimple fact that the private key is never distributed. If Bob wants to correspond with\nAlice and other people, he generates a single pair of keys, one private and one pub-\nlic. He keeps the private key secure and broadcasts the public key to all and sundry.\nIf Alice does the same, then Bob has Alice’s public key, Alice has Bob’s public key,\nand they can now communicate securely. When Bob wishes to communicate with\nAlice, Bob can do the following:\n1. Prepare a message.\n2. Encrypt that message using symmetric encryption with a one-time symmetric\nsession key.\n3. Encrypt the session key using public-key encryption with Alice’s public key.\n4. Attach the encrypted session key to the message and send it to Alice.\nOnly Alice is capable of decrypting the session key and therefore of recover-\ning the original message.\nIt is only fair to point out, however, that we have replaced one problem with\nanother. Alice’s private key is secure because she need never reveal it; however,\nBob must be sure that the public key with Alice’s name written all over it is in\nfact Alice’s public key. Someone else could have broadcast a public key and said\nit was Alice’s.\nThe solution to this problem is the public-key certificate. In essence, a cer-\ntificate consists of a public key plus a User ID of the key owner, with the whole\n21.5 / SECURE SOCKET LAYER AND TRANSPORT LAYER SECURITY\nblock signed by a trusted third party. Typically, the third party is a certificate\nauthority (CA) that is trusted by the user community, such as a government\nagency or a financial institution. A user can present his or her public key to the\nauthority in a secure manner and obtain a certificate. The user can then publish\nthe certificate. Anyone needing this user’s public key can obtain the certificate\nand verify that it is valid by way of the attached trusted signature. Figure 21.12\nillustrates the process.\n21.5 SECURE SOCKET LAYER AND TRANSPORT \nOne of the most widely used security services is the Secure Sockets Layer (SSL)\nand the follow-on Internet standard known as Transport Layer Security (TLS),\nthe latter defined in RFC 2246. SSL is a general-purpose service implemented as\na set of protocols that rely on TCP. At this level, there are two implementation\nchoices. For full generality, SSL (or TLS) could be provided as part of the under-\nlying protocol suite and therefore be transparent to applications. Alternatively,\nSSL can be embedded in specific packages. For example, Netscape and Microsoft\nExplorer browsers come equipped with SSL, and most Web servers have imple-\nmented the protocol.\nThis section discusses SSLv3. Only minor changes are found in TLS.\nUnsigned certificate:\ncontains user ID,\nuser's public key\nSigned certificate:\nRecipient can verify\nsignature using CA's\npublic key.\nGenerate hash\ncode of unsigned\ncertificate\nEncrypt hash code\nwith CA's private key\nto form signature\nFigure 21.12\nPublic-Key Certificate Use\nCHAPTER 21 / NETWORK SECURITY\nSSL Architecture\nSSL is designed to make use of TCP to provide a reliable end-to-end secure service.SSL\nis not a single protocol but rather two layers of protocols, as illustrated in Figure 21.13.\nThe SSL Record Protocol provides basic security services to various higher-\nlayer protocols. In particular, the Hypertext Transfer Protocol (HTTP), which pro-\nvides the transfer service for Web client/server interaction, can operate on top of SSL.\nThree higher-layer protocols are defined as part of SSL: the Handshake Protocol, the\nChange Cipher Spec Protocol, and the Alert Protocol. These SSL-specific protocols\nare used in the management of SSL exchanges and are examined later in this section.\nTwo important SSL concepts are the SSL session and the SSL connection,\nwhich are defined in the specification as follows:\n• Connection: A connection is a transport (in the OSI layering model defini-\ntion) that provides a suitable type of service. For SSL, such connections are\npeer-to-peer relationships. The connections are transient. Every connection is\nassociated with one session.\n• Session: An SSL session is an association between a client and a server. Ses-\nsions are created by the Handshake Protocol. Sessions define a set of crypto-\ngraphic security parameters, which can be shared among multiple connections.\nSessions are used to avoid the expensive negotiation of new security parame-\nters for each connection.\nBetween any pair of parties (applications such as HTTP on client and server),\nthere may be multiple secure connections. In theory, there may also be multiple\nsimultaneous sessions between parties, but this feature is not used in practice.\nSSL Record Protocol\nThe SSL Record Protocol provides two services for SSL connections:\n• Confidentiality: The Handshake Protocol defines a shared secret key that is\nused for symmetric encryption of SSL payloads.\n• Message integrity: The Handshake Protocol also defines a shared secret key\nthat is used to form a message authentication code (MAC).\nSSL Record Protocol\nCipher Spec\nFigure 21.13\nSSL Protocol Stack\n21.5 / SECURE SOCKET LAYER AND TRANSPORT LAYER SECURITY\nFigure 21.14 indicates the overall operation of the SSL Record Protocol. The\nfirst step is fragmentation. Each upper-layer message is fragmented into blocks of\nbytes (16,384 bytes) or less. Next, compression is optionally applied. The next\nstep in processing is to compute a message authentication code over the compressed\ndata. Next, the compressed message plus the MAC are encrypted using symmetric\nencryption.\nThe final step of SSL Record Protocol processing is to prepend a header, con-\nsisting of the following fields:\n• Content Type (8 bits): The higher-layer protocol used to process the enclosed\n• Major Version (8 bits): Indicates major version of SSL in use. For SSLv3, the\nvalue is 3.\n• Minor Version (8 bits): Indicates minor version in use. For SSLv3, the value \n• Compressed Length (16 bits): The length in bytes of the plaintext fragment (or\ncompressed fragment if compression is used). The maximum value is\nThe content types that have been defined are change_cipher_spec, alert, hand-\nshake, and application_data.The first three are the SSL-specific protocols, discussed\nnext. Note that no distinction is made among the various applications (e.g., HTTP)\nthat might use SSL; the content of the data created by such applications is opaque \nThe Record Protocol then transmits the resulting unit in a TCP segment.\nReceived data are decrypted, verified, decompressed, and reassembled and then\ndelivered to higher-level users.\n214 + 2048.\nApplication data\nrecord header\nFigure 21.14\nSSL Record Protocol Operation\nCHAPTER 21 / NETWORK SECURITY\nChange Cipher Spec Protocol\nThe Change Cipher Spec Protocol is one of the three SSL-specific protocols that use\nthe SSL Record Protocol, and it is the simplest. This protocol consists of a single\nmessage, which consists of a single byte with the value 1. The sole purpose of this\nmessage is to cause the pending state to be copied into the current state, which\nupdates the cipher suite to be used on this connection.\nAlert Protocol\nThe Alert Protocol is used to convey SSL-related alerts to the peer entity. As with\nother applications that use SSL, alert messages are compressed and encrypted, as\nspecified by the current state.\nEach message in this protocol consists of two bytes. The first byte takes the\nvalue warning(1) or fatal(2) to convey the severity of the message. If the level is\nfatal, SSL immediately terminates the connection. Other connections on the same\nsession may continue, but no new connections on this session may be established.\nThe second byte contains a code that indicates the specific alert. An example of a\nfatal alert is an incorrect MAC.An example of a nonfatal alert is a close_notify mes-\nsage, which notifies the recipient that the sender will not send any more messages on\nthis connection.\nHandshake Protocol\nThe most complex part of SSL is the Handshake Protocol. This protocol allows\nthe server and client to authenticate each other and to negotiate an encryption\nand MAC algorithm and cryptographic keys to be used to protect data sent in an\nSSL record. The Handshake Protocol is used before any application data is\ntransmitted.\nThe Handshake Protocol consists of a series of messages exchanged by client\nand server. Figure 21.15 shows the initial exchange needed to establish a logical\nconnection between client and server. The exchange can be viewed as having four\nPhase 1 is used to initiate a logical connection and to establish the security\ncapabilities that will be associated with it. The exchange is initiated by the client,\nwhich sends a client_hello message with the following parameters:\n• Version: The highest SSL version understood by the client.\n• Random: A client-generated random structure, consisting of a 32-bit time-\nstamp and 28 bytes generated by a secure random number generator. These\nvalues are used during key exchange to prevent replay attacks.\n• Session ID: A variable-length session identifier. A nonzero value indicates\nthat the client wishes to update the parameters of an existing connection or\ncreate a new connection on this session. A zero value indicates that the client\nwishes to establish a new connection on a new session.\n• CipherSuite: This is a list that contains the combinations of cryptographic\nalgorithms supported by the client, in decreasing order of preference. Each\nelement of the list (each cipher suite) defines both a key exchange algorithm\nand a CipherSpec.\n21.5 / SECURE SOCKET LAYER AND TRANSPORT LAYER SECURITY\n• Compression Method: This is a list of the compression methods the client\nAfter sending the client_hello message, the client waits for the server_hello\nmessage, which contains the same parameters as the client_hello message.\nThe details of phase 2 depend on the underlying public-key encryption scheme\nthat is used. In some cases, the server passes a certificate to the client, possibly addi-\ntional key information, and a request for a certificate from the client.\nEstablish security capabilities, including\nprotocol version, session ID, cipher suite,\ncompression method, and initial random\nServer may send certificate, key exchange,\nand request certificate. Server signals end\nof hello message phase.\nClient sends certificate if requested. Client\nsends key exchange. Client may send\ncertificate verification.\nChange cipher suite and finish\nhandshake protocol.\nNote: Shaded transfers are\noptional or situation-dependent\nmessages that are not always sent.\nclient_hello\nserver_hello\ncertificate\nserver_key_exchange\ncertificate_request\nserver_hello_done\nchange_cipher_spec\ncertificate\nclient_key_exchange\ncertificate_verify\nchange_cipher_spec\nFigure 21.15\nHandshake Protocol Action\nCHAPTER 21 / NETWORK SECURITY\nThe final message in phase 2,and one that is always required,is the server_done\nmessage, which is sent by the server to indicate the end of the server hello and associ-\nated messages. After sending this message, the server will wait for a client response.\nIn phase 3, upon receipt of the server_done message, the client should verify\nthat the server provided a valid certificate if required and check that the\nserver_hello parameters are acceptable. If all is satisfactory, the client sends one or\nmore messages back to the server, depending on the underlying public-key scheme.\nPhase 4 completes the setting up of a secure connection. The client sends a\nchange_cipher_spec message and copies the pending CipherSpec into the current\nCipherSpec. Note that this message is not considered part of the Handshake Proto-\ncol but is sent using the Change Cipher Spec Protocol.The client then immediately\nsends the finished message under the new algorithms, keys, and secrets. The fin-\nished message verifies that the key exchange and authentication processes were\nsuccessful.\nchange_cipher_spec message, transfers the pending to the current CipherSpec, and\nsends its finished message. At this point the handshake is complete and the client\nand server may begin to exchange application layer data.\n21.6 IPV4 AND IPV6 SECURITY\nIn 1994, the Internet Architecture Board (IAB) issued a report entitled Security in\nthe Internet Architecture (RFC 1636). The report stated the general consensus that\nthe Internet needs more and better security, and it identified key areas for security\nmechanisms.Among these were the need to secure the network infrastructure from\nunauthorized monitoring and control of network traffic and the need to secure end-\nuser-to-end-user traffic using authentication and encryption mechanisms.\nThese concerns are fully justified. The Computer Emergency Response Team\n(CERT) Coordination Center (CERT/CC) reports an ever-increasing number of\nInternet-related vulnerabilities (http://www.cert.org). These include security weak-\nnesses in the operating systems of attached computers (e.g.,Windows, Linux) as well\nas vulnerabilities in Internet routers and other network devices. Similarly CERT/CC\ndocuments a growing number of security-related incidents. These include denial of\nservice attacks; IP spoofing, in which intruders create packets with false IP\naddresses and exploit applications that use authentication based on IP; and various\nforms of eavesdropping and packet sniffing, in which attackers read transmitted\nCHAPTER 21 / NETWORK SECURITY\n• IP destination address: Currently, only unicast addresses are allowed; this is\nthe address of the destination endpoint of the SA, which may be an end-user\nsystem or a network system such as a firewall or router.\n• Security protocol identifier: This indicates whether the association is an AH or\nESP security association.\nHence, in any IP packet, the security association is uniquely identified by the\nDestination Address in the IPv4 or IPv6 header and the SPI in the enclosed exten-\nsion header (AH or ESP).\nAn IPSec implementation includes a security association database that defines\nthe parameters associated with each SA. A security association is defined by the \nfollowing parameters:\n• Sequence number counter: A 32-bit value used to generate the sequence \nnumber field in AH or ESP headers.\n• Sequence counter overflow: A flag indicating whether overflow of the\nsequence number counter should generate an auditable event and prevent \nfurther transmission of packets on this SA.\n• Antireplay window: Used to determine whether an inbound AH or ESP\npacket is a replay, by defining a sliding window within which the sequence\nnumber must fall.\n• AH information: Authentication algorithm, keys, key lifetimes, and related\nparameters being used with AH.\n• ESP information: Encryption and authentication algorithm, keys, initialization\nvalues, key lifetimes, and related parameters being used with ESP.\n• Lifetime of this security association: A time interval or byte count after which\nan SA must be replaced with a new SA (and new SPI) or terminated, plus an\nindication of which of these actions should occur.\n• IPSec protocol mode: Tunnel, transport, or wildcard (required for all imple-\nmentations).These modes are discussed later in this section.\n• Path MTU: Any observed path maximum transmission unit (maximum size of\na packet that can be transmitted without fragmentation) and aging variables\n(required for all implementations).\nThe key management mechanism that is used to distribute keys is coupled to\nthe authentication and privacy mechanisms only by way of the security parameters\nCHAPTER 21 / NETWORK SECURITY\nFor IPv6, examples in the base header are Version (immutable), Destination\nAddress (mutable but predictable), and Flow Label (mutable and zeroed for \ncalculation).\nEncapsulating Security Payload\nThe encapsulating security payload provides confidentiality services, including con-\nCHAPTER 21 / NETWORK SECURITY\n• Access control: This function enforces the use of the authentication function,\nroutes the messages properly, and facilitates key exchange. It can work with a\nvariety of authentication protocols.\n• Privacy with message integrity: MAC-level data (e.g., an LLC PDU) are\nencrypted, along with a message integrity code that ensures that the data have\nnot been altered.\nAuthentication operates at a level above the LLC and MAC protocols and is\nconsidered beyond the scope of 802.11. There are a number of popular authentica-\ntion protocols in use, including the Extensible Authentication Protocol (EAP) and\nthe Remote Authentication Dial-In User Service (RADIUS).These are not covered\nin this book.The remainder of this section examines access control and privacy with\nmessage integrity.\nAccess Control5\nIEEE 802.11i makes use of another standard that was designed to provide access\ncontrol functions for LANs. The standard is IEEE 802.1X, Port-Based Network\nAccess Control. IEEE 802.1X uses the terms supplicant, authenticator, and\nauthentication server (AS). In the context of an 802.11 WLAN, the first two terms\ncorrespond to the wireless station and the AP. The AS is typically a separate device\non the wired side of the network (i.e., accessible over the DS) but could also reside\ndirectly on the authenticator.\nBefore a supplicant is authenticated by the AS, using an authentication proto-\ncol, the authenticator only passes control or authentication messages between the\nsupplicant and the AS; the 802.1X control channel is unblocked but the 802.11 data\nchannel is blocked. Once a supplicant is authenticated and keys are provided, the\nauthenticator can forward data from the supplicant, subject to predefined access\ncontrol limitations for the supplicant to the network. Under these circumstances, the\ndata channel is unblocked.\nAs indicated in Figure 21.19, 802.1X uses the concepts of controlled and\nuncontrolled ports. Ports are logical entities defined within the authenticator and\nrefer to physical network connections. For a WLAN, the authenticator (the AP) may\nhave only two physical ports, one connecting to the DS and one for wireless com-\nmunication within its BSS. Each logical port is mapped to one of these two physical\nports. An uncontrolled port allows the exchange of PDUs between the supplicant\nand other the AS regardless of the authentication state of the supplicant. A con-\ntrolled port allows the exchange of PDUs between a supplicant and other systems\non the LAN only if the current state of the supplicant authorizes such an exchange.\nThe 802.1X framework, with an upper-layer authentication protocol, fits nicely\nwith a BSS architecture that includes a number of wireless stations and an AP. How-\never, for an IBSS, there is no AP. For an IBSS, 802.11i provides a more complex solu-\ntion that, in essence, involves pairwise authentication between stations on the IBSS.\n5In this subsection, we are discussing access control as a security function.This is a different function than\nmedium access control (MAC) as described in Chapter 15. Unfortunately, the literature and the stan-\ndards use the term access control in both contexts.\n21.8 / RECOMMENDED READING AND WEB SITES\nAuthentication server\nUncontrolled\nAccess point\nwireless stations\non this BSS\nFigure 21.19\n802.11i Access Control\nPrivacy with Message Integrity\nIEEE 802.11i defines two schemes for protecting data transmitted in 802.11 MAC\nPDUs. The first scheme is known as the Temporal Key Integrity Protocol (TKIP) or\nWPA-1.TKIP is designed to require only software changes to devices that are imple-\nmented with an older wireless LAN security approach called Wired Equivalent Pri-\nvacy (WEP); it uses the same RC4 stream encryption algorithm as WEP.The second\nscheme is known as Counter Mode-CBC MAC Protocol (CCMP) or WPA-2. CCMP\nmakes use of the Advanced Encryption Standard (AES) encryption protocol.6\nBoth TKIP and WPA-2 add a message integrity code (MIC) to the 802.11\nMAC frame after the data field. The MIC is generated by an algorithm, called\nMichael, that computes a 64-bit value calculated using the source and destination\nMAC address values and the Data field. This value is then encrypted using a sepa-\nrate key from that used for encrypting the Data fields.Thus, both the data and MIC\nfields are encrypted. The use of a more complex algorithm, a separate encryption\nkey, and a 64-bit length, all make the MIC and substantially stronger message\nauthentication feature than the ICV. The MIC serves the purpose of message\nauthentication, a function described earlier in this chapter.\n21.8 RECOMMENDED READING AND WEB SITES\nThe topics in this chapter are covered in greater detail in [STAL06]. For coverage of crypto-\ngraphic algorithms, [SCHN96] is a valuable reference work; it contains descriptions of virtually\nevery cryptographic algorithm and protocol in use up to the time of the book’s publication.\n6The AES algorithm is described in detail in [STAL06] and [STAL02].\nCHAPTER 21 / NETWORK SECURITY\nReview Questions\nWhat is the difference between passive and active security threats?\nList and briefly define categories of passive and active security threats.\nWhat are DES and triple DES?\nHow is the AES expected to be an improvement over triple DES?\nExplain traffic padding.\nList and briefly define various approaches to message authentication.\nWhat is a secure hash function?\nSchneier, B. Applied Cryptography. New York:Wiley, 1996.\nStallings, W. Cryptography and Network Security: Principles and Practice,\nFourth Edition. Upper Saddle River, NJ: Prentice Hall, 2003.\nRecommended Web sites:\n• Coast: Comprehensive set of links related to cryptography and network security\n• IETF Security Area: Provides up-to-date information on Internet security standard-\nization efforts\n• IEEE Technical Committee on Security and Privacy: Provides copies of IEEE’s\nnewsletter and information on IEEE-related activities\n21.9 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nactive attack\nauthenticity\navailability\nAdvanced Encryption\nStandard (AES)\nbrute-force attack\nconfidentiality\ncryptanalysis\nData Encryption Standard\ndecryption algorithm\ndenial of service\ndigital signature\nencryption algorithm\nhash function\nIP Security (IPSec)\nkey distribution\nkey distribution center\nkey management\nmessage authentication\nmessage authentication code\none-way hash function\npassive attack\nprivate key\npublic-key certificate\npublic-key encryption\nsecure hash function\nSecure Socket Layer \nsession key\nsymmetric encryption\ntraffic analysis\ntraffic padding\nTransport Layer Security\n21.9 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nExplain the difference between symmetric encryption and public-key encryption.\nWhat are the distinctions among the terms public key, private key, secret key?\nWhat is a digital signature?\nWhat is a public-key certificate?\nWhat protocols comprise SSL?\nWhat is the difference between and SSL connection and an SSL session?\nWhat services are provided by the SSL Record Protocol?\nWhat services are provided by IPSec?\nGive some examples where traffic analysis could jeopardize security. Describe situa-\ntions where end-to-end encryption combined with link encryption would still allow\nenough traffic analysis to be dangerous.\nKey distribution schemes using an access control center and/or a key distribution cen-\nter have central points vulnerable to attack. Discuss the security implications of such\ncentralization.\nSuppose that someone suggests the following way to confirm that the two of you are\nboth in possession of the same secret key.You create a random bit string the length of\nthe key, XOR it with the key, and send the result over the channel. Your partner\nXORs the incoming block with the key (which should be the same as your key) and\nsends it back. You check and if what you receive is your original random string, you\nhave verified that your partner has the same secret key, yet neither of you has ever\ntransmitted the key. Is there a flaw in this scheme?\nPrior to the discovery of any specific public-key schemes, such as RSA, an existence\nproof was developed whose purpose was to demonstrate that public-key encryption is\npossible in theory. Consider the functions \nwhere all values are integers with \ncan be represented\nby a vector M1 of length N, in which the kth entry is the value of \ncan be represented by \nmatrices M2 and M3. The intent is to represent the\nencryption/decryption process by table lookups for tables with very large values of N.\nSuch tables would be impractically huge but could, in principle, be constructed. The\nscheme works as follows: Construct M1 with a random permutation of all integers\nbetween 1 and N; that is, each integer appears exactly once in M1. Construct M2 so\nthat each row contains a random permutation of the first N integers. Finally, fill in M3\nto satisfy the following condition:\nM1 takes an input k and produces an output x.\nM2 takes inputs x and p giving output z.\nM3 takes inputs z and k and produces p.\nThe three tables, once constructed, are made public.\nIt should be clear that it is possible to construct M3 to satisfy the preceding condi-\ntion.As an example, fill in M3 for the following simple case:\nf31f21f11k2, p2, k2 = p for all k, p with 1 … k, p … N\n1 … xi, yi, zi … N.\nf11x12 = z1; f21x2, y22 = z2; f31x3, y32 = z3,\nCHAPTER 21 / NETWORK SECURITY\nConvention: The ith element of M1 corresponds to \nThe ith row of\nM2 corresponds to\nthe jth column of M2 corresponds to \nThe ith row\nof M3 corresponds to \nthe jth column of M3 corresponds to \nb. Describe the use of this set of tables to perform encryption and decryption\nbetween two users.\nArgue that this is a secure scheme.\nPerform encryption and decryption using the RSA algorithm, as in Figure 21.11, for\nthe following:\nHint: Decryption is not as hard as you think; use\nsome finesse.\nIn a public-key system using RSA, you intercept the ciphertext \nsent to a user\nwhose public key is \nWhat is the plaintext M?\nIn an RSA system, the public key of a given user is \nWhat is the pri-\nvate key of this user?\nSuppose we have a set of blocks encoded with the RSA algorithm and we don’t have\nthe private key.Assume \ne is the public key. Suppose also someone tells us he\nor she knows one of the plaintext blocks has a common factor with n. Does this help\nus in any way?\nShow how RSA can be represented by matrices M1, M2, and M3 of Problem 21.4.\nConsider the following scheme:\n1. Pick an odd number, E.\n2. Pick two prime numbers, P and Q, where \nis evenly divisible\n3. Multiply P and Q to get N.\n4. Calculate\nIs this scheme equivalent to RSA? Show why or why not.\nConsider using RSA with a known key to construct a one-way hash function. Then\nprocess a message consisting of a sequence of blocks as follows: Encrypt the first\nblock, XOR the result with the second block and encrypt again, and so on. Show that\nthis scheme is not secure by solving the following problem. Given a two-block mes-\nsage B1, B2, and its hash\nGiven an arbitrary block C1, choose C2 so that \nIn SSL and TLS, why is there a separate Change Cipher Spec Protocol rather than\nincluding a change_cipher_spec message in the Handshake Protocol?\nIn discussing AH processing, it was mentioned that not all of the fields in an IP header\nare included in MAC calculation.\nFor each of the fields in the IPv4 header, indicate whether the field is immutable,\nmutable but predictable, or mutable (zeroed prior to ICV calculation).\nb. Do the same for the IPv6 header.\nDo the same for the IPv6 extension headers.\nIn each case, justify your decision for each field.\nRSAH1C1, C22 = RSAH1B1, B22.\nRSAH1B1, B22 = RSA1RSA1B12 \u0001 B22\n1P - 121Q - 121E - 12 + 1\n1P - 121Q - 12 - 1\ne = 31, n = 3599.\ne = 5, n = 35.\np = 17; q = 31, e = 7; M = 2.\np = 11; q = 13, e = 11; M = 7\np = 7; q = 11, e = 17; M = 8\np = 5; q = 11, e = 3; M = 9\np = 3; q = 11, d = 7; M = 5\nINTERNET APPLICATIONS—\nElectronic Mail—SMTP and MIME\nNetwork Management—SNMP\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nCHAPTER 22 / INTERNET APPLICATIONS\nOne of the most exciting aspects of birds’ lives is how they interact with others during\nsuch social activities as defending territories, courting mates, nesting, raising young,\nand flocking. Birds’ level of sociability changes with the seasons; they may be\ngregarious at certain times of year yet highly territorial at others. Some of the most\nfascinating behavior occurs in spring and summer when birds are engaged in\nbreeding. During a social interaction, an individual is coordinating its activities with\nthose of another.This inevitably requires communication.\n—Secret Lives of Common Birds, Marie Read\nThe most widely used protocol for the transmission of electronic mail\nis SMTP. SMTP assumes that the content of the message is a simple\ntext block. The recent MIME standard expands SMTP to support\ntransmission of multimedia information.\nThe most important standardized scheme for supporting network\nmanagement applications is the Simple Network Management Proto-\ncol (SNMP). The original version of SNMP is available on a wide\narray of products and is widely used. SNMPv2 contains a number of\nfunctional enhancements to SNMP and is supplanting it. SNMPv3\nprovides security features that are added on to SNMPv2.\nAll of the protocols and functions described in Part Five are geared\ntoward one objective: the support of distributed applications that involve\nthe interaction of multiple independent systems. In the OSI model, such\napplications occupy the application layer and are directly supported by the\npresentation layer. In the TCP/IP suite, such applications typically rely on\nTCP or UDP for support.\nIn this chapter, we examine two applications that give the reader a\nfeel for the range and diversity of applications supported by a communica-\ntions architecture.The chapter begins with electronic mail, with the SMTP\nand MIME standards as examples; SMTP provides a basic e-mail service,\nwhile MIME adds multimedia capability to SMTP. The chapter then dis-\ncusses network management, a support-type application, designed to\nassure the effective monitoring and control of a distributed system. The\nspecific protocol that is examined is the Simple Network Management Pro-\ntocol (SNMP), which is designed to operate in both the TCP/IP and OSI\nenvironments.\nRefer to Figure 2.5 to see the position within the TCP/IP suite of the\nprotocols discussed in this chapter.\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\n22.1 ELECTRONIC MAIL—SMTP AND MIME\nThe most heavily used application in virtually any distributed system is electronic\nmail. The Simple Mail Transfer Protocol (SMTP) has always been the workhorse of\nthe TCP/IP suite. However, SMTP has traditionally been limited to the delivery of\nsimple text messages. In recent years, there has been a demand for the capability to\ndeliver mail containing various types of data, including voice, images, and video\nclips. To satisfy this requirement, a new electronic mail standard, which builds on\nSMTP, has been defined: the Multi-Purpose Internet Mail Extension (MIME). In\nthis section, we first examine SMTP, and then look at MIME.\nSimple Mail Transfer Protocol (SMTP)\nSMTP is the standard protocol for transferring mail between hosts in the TCP/IP\nsuite; it is defined in RFC 821.\nAlthough messages transferred by SMTP usually follow the format defined in\nRFC 822, described later, SMTP is not concerned with the format or content of mes-\nsages themselves, with two exceptions.This concept is often expressed by saying that\nSMTP uses information written on the envelope of the mail (message header), but\nCHAPTER 22 / INTERNET APPLICATIONS\nTCP to port 25\nSMTP receiver\n(a) Outgoing mail\nUser mailboxes\nTCP from foreign\nSMTP sender\nto local port 25\n(b) Incoming mail\nFigure 22.1\nSMTP Mail Flow\nmailbox names. If any blind carbon copies (BCCs) are indicated, the user agent\nneeds to prepare messages that conform to this requirement. The basic idea is that\nthe multiple formats and styles preferred by humans in the user interface are\nreplaced by a standardized list suitable for the SMTP send program.\nThe SMTP sender takes messages from the outgoing mail queue and transmits\nthem to the proper destination host via SMTP transactions over one or more TCP\nconnections to port 25 on the target hosts.A host may have multiple SMTP senders\nactive simultaneously if it has a large volume of outgoing mail, and should also have\nthe capability of creating SMTP receivers on demand so that mail from one host\ncannot delay mail from another.\nWhenever the SMTP sender completes delivery of a particular message to\none or more users on a specific host, it deletes the corresponding destinations\nfrom that message’s destination list. When all destinations for a particular\nmessage are processed, the message is deleted from the queue. In processing a\nqueue, the SMTP sender can perform a variety of optimizations. If a particular\nmessage is sent to multiple users on a single host, the message text need be sent\nonly once. If multiple messages are ready to send to the same host, the SMTP\nsender can open a TCP connection, transfer the multiple messages, and then close\nthe connection rather than opening and closing a connection for each message.\nThe SMTP sender must deal with a variety of errors.The destination host may\nbe unreachable, out of operation, or the TCP connection may fail while mail is being\ntransferred. The sender can requeue the mail for later delivery but give up after\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\nsome period rather than keep the message in the queue indefinitely. A common\nerror is a faulty destination address, which can occur due to user input error or\nbecause the intended destination user has a new address on a different host. The\nSMTP sender must either redirect the message if possible or return an error notifi-\ncation to the message’s originator.\nThe SMTP protocol is used to transfer a message from the SMTP sender to\nthe SMTP receiver over a TCP connection. SMTP attempts to provide reliable oper-\nation but does not guarantee to recover from lost messages. SMTP does not return\nCHAPTER 22 / INTERNET APPLICATIONS\nSMTP Commands\nCommand Form\nDescription\nHELO <SP> <domain> <CRLF>\nSend identification\nMAIL <SP> FROM:<reverse-path> <CRLF>\nIdentifies originator of mail\nRCPT <SP> TO:<forward-path> <CRLF>\nIdentifies recipient of mail\nDATA <CRLF>\nTransfer message text\nRSET <CRLF>\nAbort current mail transaction\nNOOP <CRLF>\nNo operation\nQUIT <CRLF>\nClose TCP connection\nSEND <SP> FROM:<reverse-path> <CRLF>\nSend mail to terminal\nSOML <SP> FROM:<reverse-path> <CRLF>\nSend mail to terminal if possible; otherwise\nSAML <SP> FROM:<reverse-path> <CRLF>\nSend mail to terminal and mailbox\nVRFY <SP> <string> <CRLF>\nConfirm user name\nEXPN <SP> <string> <CRLF>\nReturn membership of mailing list\nHELP [<SP> <string>] <CRLF>\nSend system-specific documentation\nTURN <CRLF>\nReverse role of sender and receiver\ncarriage return, line feed\nSquare brackets denote optional elements.\nShaded commands are optional in a conformant SMTP implementation.\nTable 22.1 lists the SMTP commands. Each command consists of a single\nline of text, beginning with a four-letter command code followed in some cases by\nan argument field. Most replies are a single-line, although multiple-line replies\nare possible. The table indicates those commands that all receivers must be able\nto recognize. The other commands are optional and may be ignored by the\nSMTP replies are listed in Table 22.2. Each reply begins with a three-digit\ncode and may be followed by additional information.The leading digit indicates the\ncategory of the reply:\n• Positive Completion reply: The requested action has been successfully com-\npleted.A new request may be initiated.\n• Positive Intermediate reply: The command has been accepted,but the requested\naction is being held in abeyance, pending receipt of further information. The\nsender-SMTP should send another command specifying this information. This\nreply is used in command sequence groups.\n• Transient Negative Completion reply: The command was not accepted and the\nrequested action did not occur. However, the error condition is temporary and\nthe action may be requested again.\n• Permanent Negative Completion reply: The command was not accepted and\nthe requested action did not occur.\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\nSMTP Replies\nDescription\nPositive Completion Reply\nSystem status, or system help reply\nHelp message (Information on how to use the receiver or the meaning of a particular non-standard\ncommand; this reply is useful only to the human user)\n<domain> Service ready\n<domain> Service closing transmission channel\nRequested mail action okay, completed\nUser not local; will forward to <forward-path>\nPositive Intermediate Reply\nStart mail input; end with <CRLF>.<CRLF>\nTransient Negative Completion Reply\n<domain> Service not available, losing transmission channel (This may be a reply to any command\nif the service knows it must shut down)\nRequested mail action not taken: mailbox unavailable (e.g., mailbox busy)\nRequested action aborted: local error in processing\nRequested action not taken: insufficient system storage\nPermanent Negative Completion Reply\nSyntax error, command unrecognized (This may include errors such as command line too long)\nSyntax error in parameters or arguments\nCommand not implemented\nBad sequence of commands\nCommand parameter not implemented\nRequested action not taken: mailbox unavailable (e.g., mailbox not found, no access)\nUser not local; please try <forward-path>\nRequested mail action aborted: exceeded storage allocation\nRequested action not taken: mailbox name not allowed (e.g., mailbox syntax incorrect)\nTransaction failed\nBasic SMTP operation occurs in three phases: connection setup, exchange of\none or more command-response pairs, and connection termination. We examine\neach phase in turn.\nConnection Setup An SMTP sender will attempt to set up a TCP connection\nwith a target host when it has one or more mail messages to deliver to that host.The\nsequence is quite simple:\n1. The sender opens a TCP connection with the receiver.\n2. Once the connection is established, the receiver identifies itself with “220 Service\nCHAPTER 22 / INTERNET APPLICATIONS\n3. The sender identifies itself with the HELO command.\n4. The receiver accepts the sender’s identification with “250 OK.”\nIf the mail service on the destination is unavailable, the destination \nhost returns a “421 Service Not Available” reply in step 2 and the process is ter-\nMail Transfer Once a connection has been established, the SMTP sender may\nsend one or more messages to the SMTP receiver. There are three logical phases to\nthe transfer of a message:\n1. A MAIL command identifies the originator of the message.\n2. One or more RCPT commands identify the recipients for this message.\n3. A DATA command transfers the message text.\nThe MAIL command gives the reverse path, which can be used to report\nerrors. If the receiver is prepared to accept messages from this originator, it\nreturns a “250 OK” reply. Otherwise the receiver returns a reply indicating failure\nto execute the command (codes 451, 452, 552) or an error in the command (codes\n421, 500, 501).\nThe RCPT command identifies an individual recipient of the mail \ndata; multiple recipients are specified by multiple use of this command. A\nseparate reply is returned for each RCPT command, with one of the following\npossibilities:\n1. The receiver accepts the destination with a 250 reply; this indicates that the\ndesignated mailbox is on the receiver’s system.\n2. The destination will require forwarding and the receiver will forward (251).\n3. The destination requires forwarding but the receiver will not forward; the sender\nmust resend to the forwarding address (551).\n4. A mailbox does not exist for this recipient at this host (550).\n5. The destination is rejected due to some other failure to execute (codes 450,\n451, 452, 552, 553) or an error in the command (codes 421, 500, 501, 503).\nThe advantage of using a separate RCPT phase is that the sender will not send\nthe message until it is assured that the receiver is prepared to receive the message for\nat least one recipient, thereby avoiding the overhead of sending an entire message\nonly to learn that the destination is unknown. Once the SMTP receiver has agreed to\nreceive the mail message for at least one recipient, the SMTP sender uses the DATA\ncommand to initiate the transfer of the message. If the SMTP receiver is still pre-\npared to receive the message, it returns a 354 message; otherwise the receiver returns\na reply indicating failure to execute the command (codes 451, 554) or an error in the\ncommand (codes 421, 500, 501, 503). If the 354 reply is returned, the SMTP sender\nproceeds to send the message over the TCP connection as a sequence of ASCII lines.\nThe end of the message is indicated by a line containing only a period. The SMTP\nreceiver responds with a 250 OK reply if the message is accepted or with the appro-\npriate error code (451, 452, 552, 554).\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\nS: MAIL FROM:<Smith@Alpha.ARPA>\nS: RCPT TO:<Jones@Beta.ARPA>\nS: RCPT TO:<Green@Beta.ARPA>\nR: 550 No such user here\nS: RCPT TO:<Brown@Beta.ARPA>\nR: 354 Start mail input; end with <CRLF>.<CRLF>\nS: Blah blah blah...\nS: ...etc. etc. etc.\nS: <CRLF>.<CRLF>\nThe SMTP sender is transmitting mail that originates with the user\nSmith@Alpha.ARPA.The message is addressed to three users on machine Beta.ARPA,\nnamely, Jones, Green, and Brown. The SMTP receiver indicates that it has mailboxes\nfor Jones and Brown but does not have information on Green. Because at least one of\nthe intended recipients has been verified,the sender proceeds to send the text message.\nConnection Closing The SMTP sender closes the connection in two steps. First,\nthe sender sends a QUIT command and waits for a reply.The second step is to initi-\nate a TCP close operation for the TCP connection. The receiver initiates its TCP\nclose after sending its reply to the QUIT command.\nRFC 822 RFC 822 defines a format for text messages that are sent using elec-\ntronic mail.The SMTP standard adopts RFC 822 as the format for use in construct-\ning messages for transmission via SMTP. In the RFC 822 context, messages are\nCHAPTER 22 / INTERNET APPLICATIONS\nAnother field that is commonly found in RFC 822 headers is Message-ID.This\nfield contains a unique identifier associated with this message.\nMultipurpose Internet Mail Extensions (MIME)\nMIME is an extension to the RFC 822 framework that is intended to address some\nof the problems and limitations of the use of SMTP and RFC 822 for electronic\nmail. [RODR02] lists the following limitations of the SMTP/822 scheme:\n1. SMTP cannot transmit executable files or other binary objects. A number of\nschemes are in use for converting binary files into a text form that can be used\nby SMTP mail systems, including the popular UNIX UUencode/UUdecode\nscheme. However, none of these is a standard or even a de facto standard.\n2. SMTP cannot transmit text data that includes national language characters\nbecause these are represented by 8-bit codes with values of 128 decimal or\nhigher, and SMTP is limited to 7-bit ASCII.\n3. SMTP servers may reject mail messages over a certain size.\n4. SMTP gateways that translate between the character codes ASCII and EBCDIC\ndo not use a consistent set of mappings, resulting in translation problems.\n5. SMTP gateways to X.400 electronic mail networks cannot handle nontextual\ndata included in X.400 messages.\n6. Some SMTP implementations do not adhere completely to the SMTP standards\ndefined in RFC 821. Common problems include\n• Deletion, addition, or reordering of carriage return and linefeed\n• Truncating or wrapping lines longer than 76 characters\n• Removal of trailing white space (tab and space characters)\n• Padding of lines in a message to the same length\n• Conversion of tab characters into multiple space characters\nMIME is intended to resolve these problems in a manner that is compatible\nwith existing RFC 822 implementations.The specification is provided in RFCs 2045\nthrough 2049.\nDate:Tue, 16 Jan 1996 10:37:17 (EST)\nFrom:“William Stallings” <ws@host.com>\nSubject:The Syntax in RFC 822\nTo: Smith@ Other -Host.com\nCc: Jones@ Yet-Another-Host.com\nHello. This section begins the actual message body, which is delimited \nfrom the message heading by a blank line.\nlines.The most frequently used keywords are From,To, Subject, and Date. Here is an\nexample message:\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\nOverview The MIME specification includes the following elements:\n1. Five new message header fields are defined, which may be included in an RFC\n822 header. These fields provide information about the body of the message.\n2. A number of content formats are defined, thus standardizing representations\nthat support multimedia electronic mail.\n3. Transfer encodings are defined that enable the conversion of any content for-\nmat into a form that is protected from alteration by the mail system.\nIn this subsection, we introduce the five message header fields. The next two\nsubsections deal with content formats and transfer encodings.\nThe five header fields defined in MIME are as follows:\n• MIME-Version: Must have the parameter value 1.0. This field indicates that\nthe message conforms to the RFCs.\n• Content-Type: Describes the data contained in the body with sufficient detail that\nthe receiving user agent can pick an appropriate agent or mechanism to present\nthe data to the user or otherwise deal with the data in an appropriate manner.\n• Content-Transfer-Encoding: Indicates the type of transformation that has\nbeen used to represent the body of the message in a way that is acceptable for\nmail transport.\n• Content-ID: Used to uniquely identify MIME entities in multiple contexts.\n• Content-Description: A plaintext description of the object with the body; this\nis useful when the object is not displayable (e.g., audio data).\nAny or all of these fields may appear in a normal RFC 822 header. A\ncompliant implementation must support the MIME-Version, Content-Type, and\nContent-Transfer-Encoding fields; the Content-ID and Content-Description fields\nare optional and may be ignored by the recipient implementation.\nMIME Content Types The bulk of the MIME specification is concerned with\nthe definition of a variety of content types. This reflects the need to provide stan-\ndardized ways of dealing with a wide variety of information representations in a\nmultimedia environment.\nTable 22.3 lists the MIME content types.There are seven different major types\nof content and a total of 14 subtypes. In general, a content type declares the general\ntype of data, and the subtype specifies a particular format for that type of data.\nFor the text type of body,no special software is required to get the full meaning of\nthe text, aside from support of the indicated character set.The only defined subtype is\nplaintext, which is simply a string of ASCII characters or ISO 8859 characters.An ear-\nlier version of the MIME specification included a richtext subtype,which allows greater\nformatting flexibility. It is expected that this subtype will reappear in a later RFC.\nThe multipart type indicates that the body contains multiple, independent\nparts. The Content-Type header field includes a parameter, called boundary, that\ndefines the delimiter between body parts. This boundary should not appear in any\nparts of the message. Each boundary starts on a new line and consists of two\nhyphens followed by the boundary value. The final boundary, which indicates the\nend of the last part, also has a suffix of two hyphens.Within each part, there may be\nan optional ordinary MIME header.\nCHAPTER 22 / INTERNET APPLICATIONS\nMIME Content Types\nDescription\nUnformatted text; may be ASCII or ISO 8859.\nThe different parts are independent but are to be transmitted\ntogether.They should be presented to the receiver in the order that\nthey appear in the mail message.\nDiffers from Mixed only in that no order is defined for delivering the\nparts to the receiver.\nAlternative\nThe different parts are alternative versions of the same information.\nThey are ordered in increasing faithfulness to the original and the\nrecipient’s mail system should display the “best” version to the user.\nSimilar to Mixed, but the default type/subtype of each part is\nmessage/rfc822.\nThe body is itself an encapsulated message that conforms to RFC 822.\nUsed to allow fragmentation of large mail items, in a way that is\ntransparent to the recipient.\nExternal-body\nContains a pointer to an object that exists elsewhere.\nThe image is in JPEG format, JFIF encoding.\nThe image is in GIF format.\nMPEG format.\nSingle-channel 8-bit ISDN \nencoding at a sample rate of 8 kHz.\nApplication\nAdobe Postscript\noctet-stream\nGeneral binary data consisting of 8-bit bytes.\nFrom: John Smith <js@company.com>\nTo: Ned Jones <ned@soft.com>\nSubject: Sample message\nMIME-Version: 1.0\nContent-type: multipart/mixed; boundary=”simple boundary”\nThis is the preamble. It is to be ignored, though it is a handy place for mail\ncomposers to include an explanatory note to non-MIME conformant \n—simple boundary\nThis is implicitly typed plain ASCII text. It does NOT end with a linebreak.\n—simple boundary\nContent-type: text/plain; charset=us-ascii\nThis is explicitly typed plain ASCII text. It DOES end with a linebreak.\n—simple boundary—\nThis is the epilogue. It is also to be ignored.\nHere is a simple example of a multipart message, containing two parts both\nconsisting of simple text:\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\nFrom: John Smith <js@company.com>\nTo: Ned Jones <ned@soft.com>\nSubject: Formatted text mail\nMIME-Version: 1.0\nContent-Type: multipart/alternative; boundary=”boundary42”\n—boundary42\nContent-Type: text/plain; charset=us-ascii\n...plaintext version of message goes here....\n—boundary42\nContent-Type: text/richtext\n.... RFC 1341 richtext version of same message goes here ...\n—boundary42—\nThere are four subtypes of the multipart type, all of which have the same over-\nall syntax. The multipart/mixed subtype is used when there are multiple independent\nbody parts that need to be bundled in a particular order. For the multipart/parallel\nsubtype, the order of the parts is not significant. If the recipient’s system is appropriate,\nthe multiple parts can be presented in parallel. For example, a picture or text part \ncould be accompanied by a voice commentary that is played while the picture or text \nis displayed.\nFor the multipart/alternative subtype, the various parts are different represen-\ntations of the same information.The following is an example:\nIn this subtype, the body parts are ordered in terms of increasing pref-\nerence. For this example, if the recipient system is capable of displaying the\nmessage in the richtext format, this is done; otherwise, the plaintext format\nThe multipart/digest subtype is used when each of the body parts is\ninterpreted as an RFC 822 message with headers. This subtype enables the\nconstruction of a message whose parts are individual messages. For exam-\nple, the moderator of a group might collect e-mail messages from partici-\npants, bundle these messages, and send them out in one encapsulating\nMIME message.\nThe message type provides a number of important capabilities in\nMIME. The message/rfc822 subtype indicates that the body is an entire\nmessage, including header and body. Despite the name of this subtype, the\nencapsulated message may be not only a simple RFC 822 message, but any\nMIME message.\nThe message/partial subtype enables fragmentation of a large mes-\nsage into a number of parts, which must be reassembled at the destination.\nCHAPTER 22 / INTERNET APPLICATIONS\nFor this subtype, three parameters are specified in the Content-Type: Message/Par-\ntial field:\n• id: A value that is common to each fragment of the same message, so that the\nfragments can be identified at the recipient for reassembly, but unique across\ndifferent messages.\n• number: A sequence number that indicates the position of this fragment in the\noriginal message.The first fragment is numbered 1, the second 2, and so on.\n• total: The total number of parts. The last fragment is identified by having the\nsame value for the number and total parameters.\nThe message/external-body subtype indicates that the actual data to be con-\nveyed in this message are not contained in the body. Instead, the body contains the\ninformation needed to access the data. As with the other message types, the mes-\nsage/external-body subtype has an outer header and an encapsulated message with\nits own header. The only necessary field in the outer header is the Content-Type\nfield, which identifies this as a message/external-body subtype. The inner header is\nthe message header for the encapsulated message.\nThe Content-Type field in the outer header must include an access-type pa-\nrameter, which has one of the following values:\n• FTP: The message body is accessible as a file using the file transfer protocol\n(FTP). For this access type, the following additional parameters are manda-\ntory: name, the name of the file; and site, the domain name of the host where\nthe file resides. Optional parameters are directory, the directory in which the\nfile is located; and mode, which indicates how FTP should retrieve the file\n(e.g., ASCII, image). Before the file transfer can take place, the user will need\nto provide a user id and password.These are not transmitted with the message\nfor security reasons.\n• TFTP: The message body is accessible as a file using the trivial file transfer\nprotocol (TFTP). The same parameters as for FTP are used, and the user id\nand password must also be supplied.\n• Anon-FTP: Identical to FTP, except that the user is not asked to supply a user\nid and password.The parameter name supplies the name of the file.\n• local-file: The message body is accessible as a file on the recipient’s machine.\n• AFS: The message body is accessible as a file via the global AFS (Andrew File\nSystem).The parameter name supplies the name of the file.\n• mail-server: The message body is accessible by sending an e-mail message\nto a mail server. A server parameter must be included that gives the e-mail\naddress of the server. The body of the original message, known as the\nphantom body, should contain the exact command to be sent to the mail\nThe image type indicates that the body contains a displayable image.The sub-\ntype, jpeg or gif, specifies the image format. In the future, more subtypes will be\nadded to this list.\nThe video type indicates that the body contains a time-varying picture image,\npossibly with color and coordinated sound.The only subtype so far specified is mpeg.\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\nMIME Transfer Encodings\nThe data are all represented by short lines of ASCII characters.\nThe lines are short, but there may be non-ASCII characters (octets with the high-order\nNot only may non-ASCII characters be present but the lines are not necessarily short\nenough for SMTP transport.\nquoted-printable\nEncodes the data in such a way that if the data being encoded are mostly ASCII text, the\nencoded form of the data remains largely recognizable by humans.\nEncodes data by mapping 6-bit blocks of input to 8-bit blocks of output, all of which are\nprintable ASCII characters.\nA named nonstandard encoding.\nThe audio type indicates that the body contains audio data. The only subtype,\nbasic, conforms to an ISDN service known as “64-kbps, 8-kHz Structured, Usable\nfor Speech Information,” with a digitized speech algorithm referred to as \nPCM (pulse code modulation). This general type is the typical way of transmitting\nspeech signals over a digital network.The term \nrefers to the specific encoding\ntechnique; it is the standard technique used in North America and Japan.A compet-\ning system, known as A-law, is standard in Europe.\nThe application type refers to other kinds of data, typically either uninter-\npreted binary data or information to be processed by a mail-based application. The\napplication/octet-stream subtype indicates general binary data in a sequence of\noctets. RFC 2045 recommends that the receiving implementation should offer to put\nthe data in a file or use the data as input to a program.\nThe application/Postscript subtype indicates the use of Adobe Postscript.\nMIME Transfer Encodings The other major component of the MIME specifi-\ncation, in addition to content type specification, is a definition of transfer encodings\nfor message bodies. The objective is to provide reliable delivery across the largest\nrange of environments.\nThe MIME standard defines two methods of encoding data. The Content-\nTransfer-Encoding field can actually take on six values, as listed in Table 22.4.\nHowever, three of these values (7bit, 8bit, and binary) indicate that no encoding\nhas been done but provide some information about the nature of the data. For\nSMTP transfer, it is safe to use the 7bit form. The 8bit and binary forms may be\nusable in other mail transport contexts. Another Content-Transfer-Encoding\nvalue is x-token, which indicates that some other encoding scheme is used, for\nwhich a name is to be supplied. This could be a vendor-specific or application-\nspecific scheme. The two actual encoding schemes defined are quoted-printable\nand base64. Two schemes are defined to provide a choice between a transfer tech-\nnique that is essentially human readable, and one that is safe for all types of data\nin a way that is reasonably compact.\nThe quoted-printable transfer encoding is useful when the data consist largely\nof octets that correspond to printable ASCII characters. In essence, it represents\nnonsafe characters by the hexadecimal representation of their code and introduces\nCHAPTER 22 / INTERNET APPLICATIONS\nreversible (soft) line breaks to limit message lines to 76 characters. The encoding\nrules are as follows:\n1. General 8-bit representation: This rule is to be used when none of the other\nrules apply. Any character is represented by an equal sign followed by a two-\ndigit hexadecimal representation of the octet’s value. For example, the ASCII\nform feed, which has an 8-bit value of decimal 12, is represented by \n2. Literal representation: Any character in the range decimal 33 (“!”) through deci-\nexcept decimal 61 \nis represented as that ASCII character.\n3. White space: Octets with the values 9 and 32 may be represented as ASCII tab\nand space characters, respectively, except at the end of a line. Any white space\n(tab or blank) at the end of a line must be represented by rule 1. On decoding,\nany trailing white space on a line is deleted. This eliminates any white space\nadded by intermediate transport agents.\n4. Line breaks:Any line break,regardless of its initial representation,is represented\nby the RFC 822 line break, which is a carriage-return/line-feed combination.\n5. Soft line breaks: If an encoded line would be longer than 76 characters (exclud-\ning <CRLF>), a soft line break must be inserted at or before character position\n75. A soft line break consists of the hexadecimal sequence 3D0D0A, which is\nthe ASCII code for an equal sign followed by carriage return, line feed.\nThe base64 transfer encoding,also known as radix-64 encoding,is a common one\nfor encoding arbitrary binary data in such a way as to be invulnerable to the processing\nby mail transport programs.This technique maps arbitrary binary input into printable\ncharacter output.The form of encoding has the following relevant characteristics:\n1. The range of the function is a character set that is universally representable at\nall sites, not a specific binary encoding of that character set. Thus, the charac-\nters themselves can be encoded into whatever form is needed by a specific sys-\ntem. For example, the character “E” is represented in an ASCII-based system\nas hexadecimal 45 and in an EBCDIC-based system as hexadecimal C5.\n2. The character set consists of 65 printable characters, one of which is used for\npadding.With \navailable characters, each character can be used to repre-\nsent 6 bits of input.\n3. No control characters are included in the set.Thus, a message encoded in radix 64\ncan traverse mail handling systems that scan the data stream for control characters.\n4. The hyphen character (“-”) is not used. This character has significance in the\nRFC 822 format and should therefore be avoided.\nTable 22.5 shows the mapping of 6-bit input values to characters.The character\nset consists of the alphanumeric characters plus \nand “/”. The \ncharacter is\nused as the padding character.\nFigure 22.2 illustrates the simple mapping scheme. Binary input is processed in\nblocks of 3 octets, or 24 bits. Each set of 6 bits in the 24-bit block is mapped into a\ncharacter. In the figure, the characters are shown encoded as 8-bit quantities. In this\ntypical case, each 24-bit input is expanded to 32 bits of output.\n22.1 / ELECTRONIC MAIL—SMTP AND MIME\nRadix-64 Encoding\nFor example, consider the 24-bit raw text sequence 00100011 01011100\n10010001, which can be expressed in hexadecimal as 235C91. We arrange this input\nin blocks of 6 bits:\n001000 110101 110010 010001\nThe extracted 6-bit decimal values are 8, 53, 50, and 17. Looking these up in Table\n22.5 yields the radix-64 encoding as the following characters: I1yR. If these charac-\nters are stored in 8-bit ASCII format with parity bit set to zero, we have\n01001001 00110001 01111001 01010010\nIn hexadecimal, this is 49317952.To summarize,\nBinary representation\n00100011 01011100 10010001\nHexadecimal representation\nRadix-64 Encoding of Input Data\nCharacter representation\nASCII code (8 bit, zero parity)\n01001001 00110001 01111001 01010010\nHexadecimal representation\nCHAPTER 22 / INTERNET APPLICATIONS\n22.2 NETWORK MANAGEMENT—SNMP\nNetworks and distributed processing systems are of critical and growing impor-\ntance in business, government, and other organizations. Within a given organiza-\ntion, the trend is toward larger, more complex networks supporting more\napplications and more users. As these networks grow in scale, two facts become\npainfully evident:\n• The network and its associated resources and distributed applications become\nindispensable to the organization.\n• More things can go wrong, disabling the network or a portion of the network\nor degrading performance to an unacceptable level.\nA large, reliable network cannot be put together and managed by human effort\nalone. The complexity of such a system dictates the use of automated network man-\nagement tools. The urgency of the need for such tools is increased, and the difficulty\nof supplying such tools is also increased, if the network includes equipment from\nmultiple vendors. In response, standards that deal with network management have\nbeen developed, covering services, protocols, and management information base.\nThis section begins with an introduction to the overall concepts of standard-\nized network management. The remainder of the section is devoted to a discussion\nof SNMP, the most widely used network management standard.\nNetwork Management Systems\nA network management system is a collection of tools for network monitoring and\ncontrol that is integrated in the following senses:\n• A single operator interface with a powerful but user-friendly set of commands\nfor performing most or all network management tasks.\n4 characters \u0001 32 bits\nFigure 22.2\nPrintable Encoding of Binary Data into Radix-64 Format\n22.2 / NETWORK MANAGEMENT—SNMP\n• A minimal amount of additional equipment.That is, most of the hardware and\nsoftware required for network management is incorporated into the existing\nuser equipment.\nA network management system consists of incremental hardware and soft-\nware additions implemented among existing network components. The software\nused in accomplishing the network management tasks resides in the host computers\nand communications processors (e.g., networks switches, routers). A network man-\nagement system is designed to view the entire network as a unified architecture,\nwith addresses and labels assigned to each point and the specific attributes of each\nelement and link known to the system. The active elements of the network provide\nregular feedback of status information to the network control center.\nSimple Network Management Protocol Version 1 (SNMPv1)\nSNMP was developed for use as a network management tool for networks and\ninternetworks operating TCP/IP. It has since been expanded for use in all types \nof networking environments. The term simple network management protocol\n(SNMP) is actually used to refer to a collection of specifications for network \nmanagement that include the protocol itself, the definition of a database, and associ-\nated concepts.\nBasic Concepts The model of network management that is used for SNMP\nincludes the following key elements:\n• Management station, or manager\n• Management information base\n• Network management protocol\nThe management station is typically a standalone device, but may be a capa-\nbility implemented on a shared system. In either case, the management station\nserves as the interface for the human network manager into the network manage-\nment system.The management station will have, at minimum,\n• A set of management applications for data analysis, fault recovery, and so on\n• An interface by which the network manager may monitor and control the\n• The capability of translating the network manager’s requirements into the\nactual monitoring and control of remote elements in the network\n• A database of network management information extracted from the data-\nbases of all the managed entities in the network\nOnly the last two elements are the subject of SNMP standardization.\nThe other active element in the network management system is the\nmanagement agent. Key platforms, such as hosts, bridges, routers, and hubs, may \nbe equipped with agent software so that they may be managed from a manage-\nment station. The agent responds to requests for information from a management\nstation, responds to requests for actions from the management station, and may\nCHAPTER 22 / INTERNET APPLICATIONS\nasynchronously provide the management station with important but unsolicited\ninformation.\nTo manage resources in the network, each resource is represented as an\nobject. An object is, essentially, a data variable that represents one aspect of the\nmanaged agent. The collection of objects is referred to as a management informa-\ntion base (MIB). The MIB functions as a collection of access points at the agent\nfor the management station. These objects are standardized across systems of a\nparticular class (e.g., bridges all support the same management objects). A man-\nagement station performs the monitoring function by retrieving the value of MIB\nobjects.A management station can cause an action to take place at an agent or can\nchange the configuration settings of an agent by modifying the value of specific\nThe management station and agents are linked by a network management\nprotocol. The protocol used for the management of TCP/IP networks is the Simple\nNetwork Management Protocol (SNMP).An enhanced version of SNMP, known as\nSNMPv2, is intended for both TCP/IP- and OSI-based networks. Each of these\nprotocols includes the following key capabilities:\n• Get: Enables the management station to retrieve the value of objects at the\n• Set: Enables the management station to set the value of objects at the agent\n• Notify: Enables an agent to send unsolicited notifications to the management\nstation of significant events\nIn a traditional centralized network management scheme, one host in the\nconfiguration has the role of a network management station; there may be one or\ntwo other management stations in a backup role. The remainder of the devices on\nthe network contain agent software and a MIB, to allow monitoring and control\nfrom the management station. As networks grow in size and traffic load, such a\ncentralized system is unworkable. Too much burden is placed on the management\nstation, and there is too much traffic, with reports from every single agent having to\nwend their way across the entire network to headquarters. In such circumstances, a\ndecentralized, distributed approach works best (e.g., Figure 22.3). In a decentralized\nnetwork management scheme, there may be multiple top-level management\nstations, which might be referred to as management servers. Each such server might\ndirectly manage a portion of the total pool of agents. However, for many of the\nagents, the management server delegates responsibility to an intermediate manager.\nThe intermediate manager plays the role of manager to monitor and control the\nagents under its responsibility. It also plays an agent role to provide information and\naccept control from a higher-level management server. This type of architecture\nspreads the processing burden and reduces total network traffic.\nNetwork Management Protocol Architecture SNMP is an application-\nlevel protocol that is part of the TCP/IP protocol suite. It is intended to operate over\nthe user datagram protocol (UDP). Figure 22.4 suggests the typical configuration of\nprotocols for SNMPv1. For a standalone management station, a manager process\ncontrols access to a central MIB at the management station and provides an inter-\nface to the network manager. The manager process achieves network management\n22.2 / NETWORK MANAGEMENT—SNMP\nby using SNMP, which is implemented on top of UDP, IP, and the relevant network-\ndependent protocols (e.g., Ethernet,ATM, frame relay).\nEach agent must also implement SNMP, UDP, and IP. In addition, there is an\nagent process that interprets the SNMP messages and controls the agent’s MIB. For\nan agent device that supports other applications, such as FTP,TCP as well as UDP is\nrequired. In Figure 22.4, the shaded portions depict the operational environment:\nthat which is to be managed.The unshaded portions provide support to the network\nmanagement function.\nFigure 22.5 provides a somewhat closer look at the protocol context of SNMP.\nFrom a management station, three types of SNMP messages are issued on behalf of a\nmanagement applications: GetRequest, GetNextRequest, and SetRequest. The first\ntwo are two variations of the get function. All three messages are acknowledged by\nthe agent in the form of a GetResponse message, which is passed up to the\nmanagement application. In addition, an agent may issue a trap message in response\nto an event that affects the MIB and the underlying managed resources. Management\nrequests are sent to UDP port 161, while the agent sends traps to UDP port 162.\nBecause SNMP relies on UDP, which is a connectionless protocol, SNMP is\nitself connectionless. No ongoing connections are maintained between a manage-\nment station and its agents. Instead, each exchange is a separate transaction\nbetween a management station and an agent.\nCentral site\nManagement server\nIntermediate manager\n(manager/agent)\nFigure 22.3\nExample Distributed Network Management Configuration\nCHAPTER 22 / INTERNET APPLICATIONS\nSimple Network Management Protocol Version 2 (SNMPv2)\nIn August of 1988, the specification for SNMP was issued and rapidly became the\ndominant network management standard. A number of vendors offer standalone\nnetwork management workstations based on SNMP, and most vendors of bridges,\nrouters, workstations, and PCs offer SNMP agent packages that allow their products\nto be managed by an SNMP management station.\nAs the name suggests, SNMP is a simple tool for network management.\nIt defines a limited, easily implemented MIB of scalar variables and two-\ndimensional tables, and it defines a streamlined protocol to enable a manager to\nget and set MIB variables and to enable an agent to issue unsolicited notifica-\ntions, called traps. This simplicity is the strength of SNMP. SNMP is easily imple-\nmented and consumes modest processor and network resources. Also, the\nstructure of the protocol and the MIB are sufficiently straightforward that it is\nnot difficult to achieve interoperability among management stations and agent\nsoftware from a mix of vendors.\nWith its widespread use, the deficiencies of SNMP became increasingly appar-\nent; these include both functional deficiencies and a lack of a security facility. As a\nresult, an enhanced version, known as SNMPv2, was issued (RFCs 1901, 1905 through\n1909, and 2578 through 2580). SNMPv2 has quickly gained support, and a number of\nvendors announced products within months of the issuance of the standard.\nNetwork-dependent protocols\nNetwork-dependent protocols\nManagement station\nNetwork-dependent\nNetwork-dependent\nFigure 22.4\nSNMPv1 Configuration\n22.2 / NETWORK MANAGEMENT—SNMP\n1There is a slight fuzziness about the term MIB. In its singular form, the term MIB can be used to refer to\nthe entire database of management information at a manager or an agent. It can also be used in singular\nor plural form to refer to a specific defined collection of management information that is part of an over-\nall MIB. Thus, the SNMPv2 standard includes the definition of several MIBs and incorporates, by refer-\nence, MIBs defined in SNMPv1.\nThe Elements of SNMPv2 As with SNMPv1, SNMPv2 provides a framework\non which network management applications can be built.Those applications, such as\nfault management, performance monitoring, accounting, and so on, are outside the\nscope of the standard.\nSNMPv2 provides the infrastructure for network management. Figure 22.6 is\nan example of a configuration that illustrates that infrastructure.\nThe essence of SNMPv2 is a protocol that is used to exchange management\ninformation. Each “player” in the network management system maintains a local\ndatabase of information relevant to network management, known as the MIB. The\nSNMPv2 standard defines the structure of this information and the allowable data\ntypes;this definition is known as the structure of management information (SMI).We\ncan think of this as the language for defining management information.The standard\nalso supplies a number of MIBs that are generally useful for network management.1\nIn addition, new MIBs may be defined by vendors and user groups.\nAt least one system in the configuration must be responsible for network man-\nagement. It is here that any network management applications are hosted. There\nmay be more than one of these management stations, to provide redundancy or\nsimply to split up the duties in a large network. Most other systems act in the role of\nagent. An agent collects information locally and stores it for later access by a\nGetNextRequest\nGetResponse\nSNMP management station\nSNMP manager\nNetwork-dependent protocols\nGetNextRequest\nGetResponse\nNetwork-dependent protocols\nSNMP messages\nApplication\nmanages objects\nManagement application\nManaged resources\nSNMP-managed objects\nFigure 22.5\nThe Role of SNMPv1\nCHAPTER 22 / INTERNET APPLICATIONS\nmanager/agent\nManager server\nManagement applications\nSNMPv2 manager\nmanager/agent\nElement manager\nSNMPv2 agent\nSNMPv2 agent\nSNMPv2 agent\nFigure 22.6\nSNMPv2-Managed Configuration\nmanager. The information includes data about the system itself and may also\ninclude traffic information for the network or networks to which the agent attaches.\nSNMPv2 supports either a highly centralized network management strategy or a\ndistributed one. In the latter case, some systems operate both in the role of manager\nand of agent. In its agent role, such a system will accept commands from a superior\nmanagement system. Some of those commands relate to the local MIB at the agent.\nOther commands require the agent to act as a proxy for remote devices.In this case,the\nproxy agent assumes the role of manager to access information at a remote agent, and\nthen assumes the role of an agent to pass that information on to a superior manager.\nAll of these exchanges take place using the SNMPv2 protocol, which is a sim-\nple request/response type of protocol. Typically, SNMPv2 is implemented on top of\nthe user datagram protocol (UDP), which is part of the TCP/IP suite. Because\n22.2 / NETWORK MANAGEMENT—SNMP\nAllowable Data Types in SNMPv2\nDescription\nIntegers in the range of \nIntegers in the range of 0 to \nA nonnegative integer that may be incremented modulo \nA nonnegative integer that may be incremented modulo \nA nonnegative integer that may increase or decrease, but shall not exceed a\nmaximum value.The maximum value can not be greater than \nA nonnegative integer that represents the time, modulo \nin hundredths of a\nOctet strings for arbitrary binary or textual data; may be limited to 255 octets.\nA 32-bit internet address.\nAn arbitrary bit field.\nAn enumeration of named bits.\nAdministratively assigned name to object or other standardized element.\nValue is a sequence of up to 128 nonnegative integers.\nSNMPv2 exchanges are in the nature of discrete request-response pairs, an ongoing\nreliable connection is not required.\nStructure of Management Information The SMI defines the general\nframework within which a MIB can be defined and constructed.The SMI identifies\nthe data types that can be used in the MIB, and how resources within the MIB are\nrepresented and named.The philosophy behind SMI is to encourage simplicity and\nextensibility within the MIB. Thus, the MIB can store only simple data types:\nscalars and two-dimensional arrays of scalars, called tables. The SMI does not sup-\nport the creation or retrieval of complex data structures. This philosophy is in con-\ntrast to that used with OSI systems management, which provides for complex data\nstructures and retrieval modes to support greater functionality. SMI avoids com-\nplex data types and structures to simplify the task of implementation and to\nenhance interoperability. MIBs will inevitably contain vendor-created data types\nand, unless tight restrictions are placed on the definition of such data types, inter-\noperability will suffer.\nThere are three key elements in the SMI specification.At the lowest level, the\nSMI specifies the data types that may be stored. Then the SMI specifies a formal\ntechnique for defining objects and tables of objects. Finally, the SMI provides a\nscheme for associating a unique identifier with each actual object in a system, so\nthat data at an agent can be referenced by a manager.\nTable 22.6 shows the data types that are allowed by the SMI. This is a fairly\nrestricted set of types. For example, real numbers are not supported. However, it is\nrich enough to support most network management requirements.\nProtocol Operation The heart of the SNMPv2 framework is the protocol itself.\nThe protocol provides a straightforward, basic mechanism for the exchange of man-\nagement information between manager and agent.\nCHAPTER 22 / INTERNET APPLICATIONS\n(d) variable-bindings\nnon-repeaters max-repetitions\nvariable-bindings\n(c) GetBulkRequest-PDU\nerror-status\nCHAPTER 22 / INTERNET APPLICATIONS\nas a principal, which may be an individual or an application or a group of individu-\nals or applications.\nThe authentication mechanism in USM assures that a received message was\ntransmitted by the principal whose identifier appears as the source in the message\nheader. This mechanism also assures that the message has not been altered in\ntransit and has not been artificially delayed or replayed. The sending principal\nprovides authentication by including a message authentication code with the\nCHAPTER 22 / INTERNET APPLICATIONS\ncopies and these are sent out independently.An alternative approach is to determine\nthe route for each destination first. Then a single message is sent out on a common\nportion of the route and copies are only made when the routes diverge; this process is\nreferred to as mail-bagging. Discuss the relative advantages and disadvantages of the\ntwo methods.\nExcluding the connection establishment and termination, what is the minimum num-\nber of network round trips to send a small email message using SMTP?\nExplain the differences between the intended use for the quoted-printable and\nBase64 encodings\nSuppose you need to send one message to three different users: user1@example.com,\nuser2@example.com, and user3@example.com. Is there any difference between send-\ning one separate message per user and sending only one message with multiple\n(three) recipients? Explain.\nWe’ve seen that the character sequence “<CR><LF>.<CR><LF>” indicates the end\nof mail data to a SMTP-server.What happens if the mail data itself contains that char-\nacter sequence?\nUsers are free to define and use additional header fields other than the ones defined\nin RFC 822. Such header fields must begin with the string “X-”.Why?\nSuppose you find some technical problems with the mail account user@example.com.\nWho should you try to contact in order to solve them?\nAlthough TCP is a full-duplex protocol, SMTP uses TCP in a half-duplex fashion.The\nclient sends a command and then stops and waits for the reply. How can this half-\nduplex operation fool the TCP slow start mechanism when the network is running\nnear capacity?\nThe original (version 1) specification of SNMP has the following definition of a new\nGauge ::= [APPLICATION 2] IMPLICIT INTEGER (0..4294967295)\nThe standard includes the following explanation of the semantics of this type:\nThis application-wide type represents a non-negative integer, which may\nincrease or decrease, but which latches at a maximum value.This standard\nspecifies a maximum value of \n(4294967295 decimal) for gauges.\nUnfortunately, the word latch is not defined, and this has resulted in two different\ninterpretations. The SNMPv2 standard cleared up the ambiguity with the following\ndefinition:\nThe value of a Gauge has its maximum value whenever the information being\nmodeled is greater than or equal to that maximum value; if the information\nbeing modeled subsequently decreases below the maximum value, the Gauge\nalso decreases.\nWhat is the alternative interpretation?\nb. Discuss the pros and cons of the two interpretations.\nBecause SNMP uses two different port numbers (UDP ports 161 and 162), a single\nsystem can easily run both a manager and an agent. What would happen if the same\nport number were used for both?\nINTERNET APPLICATIONS—INTERNET\nInternet Directory Service: DNS\nWeb Access—HTTP\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nCHAPTER 23 / INTERNET APPLICATIONS\nLife in the modern world is coming to depend more and more upon technical means\nof communication. Without such technical aids the modern city-state could not exist,\nfor it is only by means of them that trade and business can proceed; that goods and\nservices can be distributed where needed; that railways can run on schedule; that law\nand order are maintained; that education is possible. Communication renders true\nsocial life practicable, for communication means organization.\n—On Human Communication, Colin Cherry\nThe rapid growth in the use of the Web is due to the standardization\nof all the elements that support Web applications. A key element is\nHTTP, which is the protocol for the exchange of Web-based informa-\ntion between Web browsers and Web servers.\nThree types of intermediate devices can be used in an HTTP net-\nworks: proxies, gateways, and tunnels.\nHTTP uses a request/response style of communication.\nThe Domain Name System (DNS) is a directory lookup service that\nprovides a mapping between the name of a host on the Internet and\nits numerical address.\nDNS makes use of a distributed, hierarchical database to maintain a\nmapping from names to addresses and to provide related information\nabout hosts on the Internet.\nThis chapter looks at two of the most widely used and more advanced Internet\napplication areas.This chapter and the next should give the reader a feel for the\nrange and diversity of applications supported by a communications architecture.\nThe chapter begins with DNS, which is an essential name/address directory\nlookup service for the Internet.Then we look at HTTP, which is the support pro-\ntocol on which the World Wide Web (WWW) operates.\n23.1 INTERNET DIRECTORY SERVICE: DNS\nThe Domain Name System (DNS) is a directory lookup service that provides a map-\nping between the name of a host on the Internet and its numerical address. DNS is\nessential to the functioning of the Internet. It is defined in RFCs 1034 and 1035.\n23.1 / INTERNET DIRECTORY SERVICE: DNS\nFour elements comprise the DNS:\n• Domain name space: DNS uses a tree-structured name space to identify\nresources on the Internet.\n• DNS database: Conceptually, each node and leaf in the name space tree struc-\nture names a set of information (e.g., IP address, type of resource) that is con-\ntained in a resource record (RR).The collection of all RRs is organized into a\ndistributed database.\n• Name servers: These are server programs that hold information about a por-\ntion of the domain name tree structure and the associated RRs.\n• Resolvers: These are programs that extract information from name servers in\nresponse to client requests. A typical client request is for an IP address corre-\nsponding to a given domain name.\nIn the next two sections, we examine domain names and the DNS database,\nrespectively.We then describe the operation of DNS, which includes a discussion of\nname servers and resolvers.\nDomain Names\nThe 32-bit IP address provides a way of uniquely identifying devices attached to\nthe Internet. This address is interpreted as having two components: a network\nnumber, which identifies a network on the Internet, and a host address, which\nidentifies a unique host on that network. The practical use of IP addresses\npresents two problems:\n1. Routers devise a path through the Internet on the basis of the network num-\nber. If each router needed to keep a master table that listed every network and\nthe preferred path to that network, the management of the tables would be\ncumbersome and time consuming. It would be better to group the networks in\nsuch a way as to simplify the routing function.\n2. The 32-bit address is usually written as four decimal numbers, corresponding\nto the four octets of the address.This number scheme is effective for computer\nprocessing but is not convenient for users, who can more easily remember\nnames than numerical addresses.\nThese problems are addressed by the concept of domain. In general terms, a\ndomain refers to a group of hosts that are under the administrative control of a sin-\ngle entity, such as a company or government agency. Domains are organized hier-\narchically, so that a given domain may consist of a number of subordinate domains.\nNames are assigned to domains and reflect this hierarchical organization.\nFigure 23.1 shows a portion of the domain naming tree. At the very top level\nare a small number of domains that encompass the entire Internet. Table 23.1 lists\ncurrently defined top-level domains. Each subordinate level is named by prefixing a\nsubordinate name to the name at the next highest level. For example,\n• edu is the domain of college-level U.S. educational institutions.\n• mit.edu is the domain for M.I.T. (the Massachusetts Institute of Technology).\n• lcs.mit.edu is the domain for the Laboratory for Computer Science at M.I.T.\nCHAPTER 23 / INTERNET APPLICATIONS\nFigure 23.1\nPortion of Internet Domain Tree\nTop-Level Internet Domains\nCHAPTER 23 / INTERNET APPLICATIONS\nResource Record Types\nDescription\nA host address.This RR type maps the name of a system to its IP address. Some\nsystems (e.g., routers) have multiple addresses, and there is a separate RR for\nCanonical name. Specifies an alias name for a host and maps this to the canonical\n(true) name.\nHost information. Designates the processor and operating system used by the\nMailbox or mail list information. Maps a mailbox or mail list name to a host\nMail exchange. Identifies the systems that relay mail into the organization.\nAuthoritative name server for this domain.\nDomain name pointer. Points to another part of the domain name space.\nStart of a zone of authority (which part of naming hierarchy is implemented).\nIncludes  parameters related to this zone.\nFor a given service provides name of server or servers in domain that provide that \nArbitrary text. Provides a way to add text comments to the database.\nWell-known services. May list the application services available at this host.\n2The SRV RR type is defined in RFC 2782.\ndescribed in general terms. In essence, the domain name in a RR must\ncorrespond to the human readable form, which consists of a series of labels of\nalphanumeric characters or hyphens, with each pair of labels separated by a\n• Type: Identifies the type of resource in this RR.The various types are listed in\nTable 23.2.2\n• Class: Identifies the protocol family. The only commonly used value is IN, for\nthe Internet.\n• Time to Live: Typically, when a RR is retrieved from a name server, the\nretriever will cache the RR so that it need not query the name server repeat-\nedly. This field specifies the time interval that the resource record may be\ncached before the source of the information should again be consulted.A zero\nvalue is interpreted to mean that the RR can only be used for the transaction\nin progress and should not be cached.\n• Rdata Field Length: Length of the Rdata field in octets.\n• Rdata: A variable-length string of octets that describes the resource. The for-\nmat of this information varies according to the type of the RR. For example,\nfor the A type, the Rdata is a 32-bit IP address, and for the CNAME type, the\nRdata is a domain name.\n23.1 / INTERNET DIRECTORY SERVICE: DNS\nFigure 23.3\nDNS Name Resolution\nDNS Operation\nDNS operation typically includes the following steps (Figure 23.3):\n1. A user program requests an IP address for a domain name.\n2. A resolver module in the local host or local ISP formulates a query for a local\nname server in the same domain as the resolver.\n3. The local name server checks to see if the name is in its local database or cache,\nand, if so, returns the IP address to the requestor. Otherwise, the name server\nqueries other available name servers, starting down from the root of the DNS\ntree or as high up the tree as possible.\n4. When a response is received at the local name server, it stores the name/address\nmapping in its local cache and may maintain this entry for the amount of time\nspecified in the time to live field of the retrieved RR.\n5. The user program is given the IP address or an error message.\nThe results of these behind-the-scenes activities are seen by the user in a way\nillustrated in Figure 23.4. Here, a user issues a Telnet connection request to\nlocis.loc.gov.This is resolved by DNS to the IP address of 140.147.254.3.\nThe distributed DNS database that supports the DNS functionality must be\nupdated frequently because of the rapid and continued growth of the Internet.\nAccordingly, dynamic updating functions for DNS have been defined. In essence,\nDNS name servers automatically send out updates to other relevant name servers as\nconditions warrant.\nThe Server Hierarchy The DNS database is distributed hierarchically, residing\nin DNS name servers scattered throughout the Internet. Name servers can be oper-\nated by any organization that owns a domain; that is, any organization that has\nresponsibility for a subtree of the hierarchical domain name space. Each name\nserver is configured with a subset of the domain name space, known as a zone, which\nCHAPTER 23 / INTERNET APPLICATIONS\ntelnet locis.loc.gov\nTrying 140.147.254.3...\nConnected to locis.loc.gov.\nEscape character is '^]'.\nTo make a choice: type a number, then press ENTER\nCHAPTER 23 / INTERNET APPLICATIONS\nTypically, single queries are carried over UDP. Queries for a group of names\nare carried over TCP.\nName Resolution\nAs Figure 23.3 indicates, each query begins at a name\nresolver located in the user host system (e.g., gethostbyname in UNIX). Each\nresolver is configured to know the name and address of a local DNS name server. If\nthe resolver does not have the requested name in its cache, it sends a DNS query to\nthe local DNS server, which either returns an address immediately or does so after\nquerying one or more other servers.Again, resolvers use UDP for single queries and\nTCP for group queries.\nThere are two methods by which queries are forwarded and results returned.\nSuppose a name server (A) forwards a DNS request to another name server (B). If\nB has the name/address in its local cache or local database, it can return the IP\naddress to A. If not, then B can do either of the following:\n1. Query another name server for the desired result and then send the result\nback to A.This is known as a recursive technique.\n2. Return to A the address of the next server (C) to whom the request should be\nsent. A then sends out a new DNS request to C. This is known as the iterative\nIn exchanges between name servers, either the iterative or recursive technique\nmay be used. For requests sent by a name resolver, the recursive technique is used.\nDNS Messages DNS messages use a single format, shown in Figure 23.5. There\nare five possible sections to a DNS message: header, question, answer, authority, and\nadditional records.\nThe header section is always present and consists of the following fields:\n• Identifier: Assigned by the program that generates any kind of query. The\nsame identifier is used in any response, enabling the sender to match queries\nand responses.\n• Query Response: Indicates whether this message is a query or response.\n• Opcode: Indicates whether this is a standard query, an inverse query (address\nto name), or a server status request. This value is set by the originator and\ncopied into the response.\n• Authoritative Answer: Valid in a response, and indicates whether the respond-\ning name server is an authority for the domain name in question.\n• Truncated: Indicates whether the response message was truncated due to\nlength greater then permitted on the transmission channel. If so, the requestor\nwill use a TCP connection to resend the query.\n• Recursion Desired: If set, directs the server to pursue the query recursively.\n• Recursion Available: Set or cleared in a response to denote whether recursive\nquery support is available in the name server.\n• Response Code: Possible values are: no error, format error (server unable to\ninterpret query), server failure, name error (domain name does not exist),\nnot implemented (this kind of query not supported), and refused (for policy\nIdentitifier\n. . . label n\nlabel 1 . . .\nQuery Class\nAnswer Section\nAuthority Section\nAdditional Records Section\nQR \u0001 query/response bit\nAA \u0001 authoritative answer\nTC \u0001 truncated\nRD \u0001 recursion desired\nRA \u0001 recursion available\nRCODE \u0001 response code\nQDcount \u0001 number of entries in question section\nANcount \u0001 number of resource records in answer section\nNScount \u0001 number of name server resource records in authority section\nARcount \u0001 number of resource records in additional records section\nFigure 23.5\nDNS Message Format\nCHAPTER 23 / INTERNET APPLICATIONS\n• QDcount: Number of entries in question section (zero or more).\n• ANcount: Number of RRs in answer section (zero or more).\n• NScount: Number of RRs in authority section (zero or more).\n• ARcount: Number of RRs in additional records section (zero or more).\nThe question section contains the queries for the name server. If present, it\ntypically contains only one entry. Each entry contains the following:\n• Domain Name: A domain name represented as a sequence of labels, where\neach label consists of a length octet followed by that number of octets. The\ndomain name terminates with the zero length octet for the null label of \n• Query Type: Indicates type of query.The values for this field include all values\nvalid for the Type field in the RR format (Figure 23.2), together with some\nmore general codes that match more than one type of RR.\n• Query Class: Specifies the class of query, typically Internet.\nThe answer section contains RRs that answer the question; the authority section\ncontains RRs that point toward an authoritative name server; the additional records\nsection contains RRs that relate to the query but are not strictly answers for the\n23.2 WEB ACCESS—HTTP\nThe Hypertext Transfer Protocol (HTTP) is the foundation protocol of the World\nWide Web (WWW) and can be used in any client/server application involving hyper-\ntext. The name is somewhat misleading in that HTTP is not a protocol for transfer-\nring hypertext; rather it is a protocol for transmitting information with the efficiency\nnecessary for making hypertext jumps. The data transferred by the protocol can be\nplaintext, hypertext, audio, images, or any Internet-accessible information.\nWe begin with an overview of HTTP concepts and operation and then look at\nsome of the details, basing our discussion on the most recent version to be put on the\nInternet standards track, HTTP 1.1 (RFC 2616). A number of important terms\ndefined in the HTTP specification are summarized in Table 23.4; these will be intro-\nduced as the discussion proceeds.\nHTTP Overview\nHTTP is a transaction-oriented client/server protocol. The most typical use of\nHTTP is between a Web browser and a Web server. To provide reliability, HTTP\nmakes use of TCP. Nevertheless, HTTP is a “stateless” protocol: Each transaction is\ntreated independently.Accordingly, a typical implementation will create a new TCP\nconnection between client and server for each transaction and then terminate the\nconnection as soon as the transaction completes, although the specification does not\ndictate this one-to-one relationship between transaction and connection lifetimes.\nThe stateless nature of HTTP is well suited to its typical application.A normal\nsession of a user with a Web browser involves retrieving a sequence of Web pages\n23.2 / WEB ACCESS—HTTP\nKey Terms Related to HTTP\nOrigin Server\nA program’s local store of response messages and \nThe server on which a given resource resides or\nthe subsystem that controls its message storage,\nis to be created.\nretrieval, and deletion.A cache stores cacheable \nresponses in order to reduce the response time\nand network bandwidth consumption on future,\nAn intermediary program that acts\nequivalent requests.Any client or server may  \nas both a server and a client for the purpose of\ninclude a cache, though a cache cannot be used \nmaking requests on behalf of other clients.\nby a server while it is acting as a tunnel.\nRequests are serviced internally or by passing\nthem, with possible translation, on to other\nservers.A proxy must interpret and, if \nAn application program that establishes  \nnecessary, rewrite a request message before \nconnections for the purpose of sending requests.\nforwarding it. Proxies are often used as \nclient-side portals through network firewalls\nand as helper applications for handling requests\nA transport layer virtual circuit established\nvia protocols not implemented by the user agent.\nbetween two application programs for the\npurposes of communication.\nA network data object or service which can be\nidentified by a URI.\nA particular representation or rendition of\na data resource, or reply from a service\nresource, that may be enclosed within a request\nAn application program that accepts \nor response message.An entity consists of entity\nconnections in order to service requests\nheaders and an entity body.\nby sending back responses.\nA server that acts as an intermediary for some\nAn intermediary program that is acting as a blind\nother server. Unlike a proxy, a gateway receives\nrelay between two connections. Once active, a \nrequests as if it were the original server for the\ntunnel is not considered a party to the HTTP\nrequested resource; the requesting client may\ncommunication, though the tunnel may have \nnot be aware that it is communicating with a\nbeen initiated by an HTTP request.A tunnel \ngateway. Gateways are often used as server-side\nceases to exist when both ends of the relayed\nportals through network firewalls and as\nconnections are closed.Tunnels are used when a\nprotocol translators for access to resources\nportal is necessary and the intermediary cannot, or\nstored on non-HTTP systems.\nshould not, interpret the relayed communication.\nThe basic unit of HTTP communication,\nThe client that initiates a request.These are\nconsisting of a structured sequence of octets \noften browsers, editors, spiders, or other\ntransmitted via the connection.\nend-user tools.\nand documents. The sequence is, ideally, performed rapidly, and the locations of the\nvarious pages and documents may be a number of widely distributed servers.\nAnother important feature of HTTP is that it is flexible in the formats that it\ncan handle.When a client issues a request to a server, it may include a prioritized list\nof formats that it can handle, and the server replies with the appropriate format. For\nexample, a Lynx browser cannot handle images, so a Web server need not transmit\nany images on Web pages. This arrangement prevents the transmission of unneces-\nsary information and provides the basis for extending the set of formats with new\nstandardized and proprietary specifications.\nCHAPTER 23 / INTERNET APPLICATIONS\nOrigin server\nOrigin server\nRequest chain\nResponse chain\nOrigin server\nRequest chain\nResponse chain\nRequest chain\nResponse chain\nTCP connection\nFigure 23.6\nExamples of HTTP Operation\nCHAPTER 23 / INTERNET APPLICATIONS\nindicating at least I and at most J elements, each separated by a comma and optional linear white space.\n• A semicolon at the right of a rule starts a comment that continues to the end of the line.\n6I7#6J7element\n6I7*6J7element\nname = definition\nCHAPTER 23 / INTERNET APPLICATIONS\nHTTP-Message \u0001 Simple-Request | Simple-Response | Full-Request | \nFull-Response\nFull-Request \u0001\nRequest-Line\n*( General-Header |  Request-Header |  Entity-Header )\n[ Entity-Body ]\nFull-Response \u0001 Status-Line\n*( General-Header |  Response-Header |  Entity-Header )\n[ Entity-Body ]\nSimple-Request \u0001 “GET” SP Request-URL CRLF\nSimple-Response \u0001 [ Entity-Body ]\nThe Simple-Request and Simple-Response messages were defined in HTTP/0.9.\nThe request is a simple GET command with the requested URL; the response is sim-\nply a block containing the information identified in the URL. In HTTP/1.1, the use of\nthese simple forms is discouraged because it prevents the client from using content\nnegotiation and the server from identifying the media type of the returned entity.\nWith full requests and responses, the following fields are used:\n• Request-Line: Identifies the message type and the requested resource\n• Status-Line: Provides status information about this response\n• General-Header: Contains fields that are applicable to both request and\nresponse messages but that do not apply to the entity being transferred\n• Request-Header: Contains information about the request and the client\n• Response-Header: Contains information about the response\n• Entity-Header: Contains information about the resource identified by the\nrequest and information about the entity body\n• Entity-Body: The body of the message\nAll of the HTTP headers consist of a sequence of fields, following the same\ngeneric format as RFC 822 (described in Chapter 22). Each field begins on a new\nline and consists of the field name followed by a colon and the field value.\nAlthough the basic transaction mechanism is simple, there is a large number of\nfields and parameters defined in HTTP. In the remainder of this section, we look at\nthe general header fields. Following sections describe request headers, response\nheaders, and entities.\nGeneral Header Fields General header fields can be used in both request and\nresponse messages. These fields are applicable in both types of messages and con-\ntain information that does not directly apply to the entity being transferred. The\nfields are as follows:\n23.2 / WEB ACCESS—HTTP\n• Cache-Control: Specifies directives that must be obeyed by any caching mech-\nanisms along the request/response chain. The purpose is to prevent a cache\nfrom adversely interfering with this particular request or response.\n• Connection: Contains a list of keywords and header field names that only apply\nto this TCP connection between the sender and the nearest nontunnel recipient.\n• Date: Date and time at which the message originated.\n• Forwarded: Used by gateways and proxies to indicate intermediate steps along\na request or response chain. Each gateway or proxy that handles a message\nmay attach a Forwarded field that gives its URL.\n• Keep-Alive: May be present if the keep-alive keyword is present in an\nincoming Connection field, to provide information to the requester of \nthe persistent connection. This field may indicate a maximum time that the\nsender will keep the connection open waiting for the next request or the\nmaximum number of additional requests that will be allowed on the current\npersistent connection.\n• MIME-Version: Indicates that the message complies with the indicated ver-\nsion of MIME.\n• Pragma: Contains implementation-specific directives that may apply to any\nrecipient along the request/response chain.\n• Upgrade: Used in a request to specify what additional protocols the client sup-\nports and would like to use; used in a response to indicate which protocol will\nRequest Messages\nA full request message consists of a status line followed by one or more general,\nrequest, and entity headers, followed by an optional entity body.\nRequest Methods A full request message always begins with a Request-Line,\nwhich has the following format:\nThe Method parameter indicates the actual request command, called a\nmethod in HTTP. Request-URL is the URL of the requested resource, and HTTP-\nVersion is the version number of HTTP used by the sender.\nThe following request methods are defined in HTTP/1.1:\n• OPTIONS: A request for information about the options available for the\nrequest/response chain identified by this URL.\n• GET: A request to retrieve the information identified in the URL and return\nit in a entity body. A GET is conditional if the If-Modified-Since header field\nis included and is partial if a Range header field is included.\n• HEAD: This request is identical to a GET, except that the server’s response\nmust not include an entity body; all of the header fields in the response are the\nsame as if the entity body were present. This enables a client to get informa-\ntion about a resource without transferring the entity body.\nRequest-Line = Method SP Request-URL SP HTTP-Version CRLF\nCHAPTER 23 / INTERNET APPLICATIONS\n• POST: A request to accept the attached entity as a new subordinate to the iden-\ntified URL.The posted entity is subordinate to that URL in the same way that a\nfile is subordinate to a directory containing it, a news article is subordinate to a\nnewsgroup to which it is posted, or a record is subordinate to a database.\n• PUT: A request to accept the attached entity and store it under the supplied\nURL. This may be a new resource with a new URL or a replacement of the\nCHAPTER 23 / INTERNET APPLICATIONS\ndescribed in Chapter 21.\n• Content-Range: For future study. The intent is that this will indicate a portion\nof the identified resource that is included in this response.\n• Content-Type: Indicates the media type of the entity body.\n• Content-Version: A version tag associated with an evolving entity.\n• Derived-From: Indicates the version tag of the resource from which this entity was\nderived before modifications were made by the sender.This field and the Content-\nVersion field can be used to manage multiple updates by a group of users.\n• Expires: Date/time after which the entity should be considered stale.\n• Last-Modified: Date/time that the sender believes the resource was last \n• Link: Defines links to other resources.\n• Title: A textual title for the entity.\n• Transfer-Encoding: Indicates what type of transformation has been applied to\nthe message body to transfer it safely between the sender and the recipient.\nThe only encoding defined in the standard is chunked. The chunked option\ndefines a procedure for breaking an entity body into labeled chunks that are\ntransmitted separately.\n• URL-Header: Informs the recipient of other URLs by which the resource can\nbe identified.\n• Extension-Header: Allows additional fields to be defined without changing the\nprotocol, but these fields cannot be assumed to be recognizable by the recipient.\nEntity Body An entity body consists of an arbitrary sequence of octets. HTTP is\ndesigned to be able to transfer any type of content, including text, binary data,\naudio, images, and video.When an entity body is present in a message, the interpre-\ntation of the octets in the body is determined by the entity header fields Content-\nEncoding, Content-Type, and Transfer-Encoding.These define a three-layer, ordered\nencoding model:\nThe data are the content of a resource identified by a URL.The Content-Type\nfield determines the way in which the data are interpreted.A Content-Encoding may\nbe applied to the data and stored at the URL instead of the data. Finally, on transfer,\na Transfer-Encoding may be applied to form the entity body of the message.\n23.3 RECOMMENDED READING AND WEB SITES\n[MOGU02] discusses the design strengths and weaknesses of HTTP. [GOUR02] provides\ncomprehensive coverage of HTTP. Another good treatment is [KRIS01]. [MOCK88] is an\noverview of DNS.\n1Content-Encoding1Content-Type1data222\nentity-body :=Transfer-Encoding\nCHAPTER 23 / INTERNET APPLICATIONS\nGourley, D., et al. HTTP:The Definitive Guide. Sebastopol, CA: O’Reilly, 2002.\nKrishnamurthy, B., and Rexford, J. Web Protocols and Practice: HTTP/1.1. Net-\nworking Protocols, Caching, and Traffic Measurement. Upper Saddle River, NJ:\nPrentice Hall, 2001.\nMockapetris, P., and Dunlap, K.“Development of the Domain Name System.”\nACM Computer Communications Review, August 1988.\nMogul, J. “Clarifying the Fundamentals of HTTP.” Proceedings of the\nEleventh International Conference on World Wide Web, 2002.\nRecommended Web Sites:\n• WWW Consortium: Contains up-to-date information on HTTP and related topics.\n• DNS Extensions Working Group: Chartered by IETF to develop standards related\nto DNS.The Web site includes all relevant RFCs and Internet drafts.\n23.4 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nReview Questions\nWhat is DNS?\nWhat is the difference between a name server and a resolver in DNS?\nWhat is a DNS resource record?\nGive a brief description of DNS operation.\nWhat is the difference between a domain and a zone?\nExplain the difference between the recursive technique and the iterative technique in\nWhat is meant by saying that HTTP is a stateless protocol?\nExplain the differences among HTTP proxy, gateway, and tunnel.\nWhat is the function of the cache in HTTP?\nBackus-Naur Form (BNF)\ndomain name\nDomain Name Service (DNS)\nHTTP gateway\nHTTP method\nHTTP tunnel\nHypertext Transfer Protocol\niterative technique\nname server\norigin server\nrecursive technique\nresource record (RR)\nroot name server\nUniform Resource Locator\n23.4 /KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nNote: For some of the problems in this chapter, you will need to consult the relevant \nClassify a DNS resolver and a DNS name server as either client, server, or both.\nA DNS resolver typically issues a query using UDP but may also use TCP. Is there a\nproblem using TCP for this purpose? If so, what do you suggest is the solution?\nHint: Consider the TCP and UDP headers.\nWhat’s the main difference between a primary and a secondary name server?\nName servers can be accessed on UDP port 53 as well as on TCP port 53. When is\neach protocol used, and why?\nWe query an authoritative name server for the ‘example.com’ zone, in order to get the\nIP address of www.example.com, the Web site of a large company. We get eight A\nrecords in response to our query.We repeat this query several times, and note that we\ncontinue getting the same eight A records, but in a different order each time. Suggest\na reason why.\nThe dig tool provides easy interactive access to the DNS. The dig tool is available for\nUNIX and Windows operating systems. It can also be used from the Web. Here are\nthree sites that, at the time of this writing, provided free access to dig:\nhttp://www.gont.com.ar/tools/dig\nhttp://www.webmaster-toolkit.com/dig.shtml\nhttp://www.webhostselect.com/whs/dig-tool.jsp\nUse the dig tool to get the list of root servers.\nDiscuss the advantages of using several stub resolvers along with a caching-only name\nserver, instead of several full resolvers.\nChoose a root server, and use the dig tool to send it a query for the IP address \nof www.example.com, with the RD (Recursion Desired) bit set. Does it support recur-\nsive lookups? Why or why not?\nType dig www.example.com A in order to get the IP address of www.example.com.\nWhat’s the TTL of the A record returned in the response? Wait a while, and repeat the\nquery.Why has the TTL changed?\nWith the widespread use of x-DSL and cable-modem technologies, many home users\nnow host Web sites on their own desktop computers.As their IP addresses are dynam-\nically assigned by their Internet Service Providers (ISPs), users must update their\nDNS records every time their IP addresses change (it’s usually done by some com-\nputer software on the user machine that automatically contacts the name server to\nupdate the corresponding data whenever the assigned IP address changes). This ser-\nvice is usually called Dynamic DNS. However, in order for these updates to work as\nexpected, there’s one field of each resource record that must be set to a quite differ-\nent value from the typical ones.Which one, and why?\nSecondary name servers periodically query the primary to check whether the zone\ndata has been updated. Regardless of how many resource records the zone data con-\ntains, the secondary name servers need to query the primary only one resource record\nto detect any changes on the zone data.Which resource record will they query? How\nwill they use the requested information to detect changes?\nA user on the host 170.210.17.145 is ‘using a Web browser to visit www.example.com.\nIn order to resolve the ‘www.example.com’ domain to an IP address, a query is \nsent to an authoritative name server for the ‘example.com’ domain. In response, the\nname server returns a list of four IP addresses, in the following order\nEven though it is the last IP\naddress in the list returned by the name server, the Web browser creates a connection\nto 170.210.17.130.Why?\n5192.168.0.1, 128.0.0.1, 200.47.57.1, 170.210.10.1306.\nCHAPTER 23 / INTERNET APPLICATIONS\nBefore the deployment of the Domain Name System, a simple text file (HOSTS.TXT)\ncentrally maintained at the SRI Network Information Center was used to enable map-\nping between host names and addresses. Each host connected to the Internet had to\nhave an updated local copy of it to be able to use host names instead of having to cope\ndirectly with their IP addresses. Discuss the main advantages of the DNS over the old\ncentralized HOSTS.TXT system.\nPrior to persistent connections, one separate TCP connection was used to fetch each\nURL.Analyze the advantages of persistent connections over the old HTTP paradigm\nof one connection per data transfer.\nINTERNET APPLICATIONS—\nAudio and Video Compression\nReal-Time Traffic\nVoice Over IP and Multimedia Support—SIP\nReal-Time Transport Protocol (RTP)\nRecommended Reading and Web Sites\nKey Terms, Review Questions, and Problems\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nThe Session Initiation Protocol (SIP) is an application-level control\nprotocol for setting up, modifying, and terminating real-time sessions\nbetween participants over an IP data network.\nSIP uses the Session Description Protocol (SDP) to describe the\nmedia content to be used during a session.\nThe Real-Time Transport Protocol (RTP) is a transport-level alterna-\ntive to TCP or UDP for supporting real-time traffic.\nPrior to the recent explosion of sophisticated research, scientists believed that birds\nrequired no special awareness or intelligence to perform their migrations and their\nnavigational and homing feats. Accumulated research shows that in addition to\nperforming the difficult tasks of correcting for displacement (by storms, winds,\nmountains, and other hindrances), birds integrate an astonishing variety of celestial,\natmospheric, and geological information to travel between their winter and summer\nhomes. In brief, avian navigation is characterized by the ability to gather a variety of\ninformational cues and to interpret and coordinate them so as to move closer\ntoward a goal.\n—The Human Nature of Birds, Theodore Barber\nWith the increasing availability of broadband access to the Internet has come an\nincreased interest in Web-based and Internet-based multimedia applications.The\nterm multimedia refers to the use of multiple forms of information,including text,\nstill images, audio, and video.The reader may find it useful to review Section 2.6\nbefore proceeding.\nAn in-depth discussion of multimedia applications is well beyond the scope\nof this book. In this chapter, we focus on a few key topics. First, we look at audio\nand video compression, which is quite common in multimedia applications.Then\nwe examine some of the key characteristics of real-time traffic. Next we look at\nSIP and its use to support voice over IP. Finally, we examine the real-time trans-\nport protocol.\n24.1 AUDIO AND VIDEO COMPRESSION\nIn Chapter 3, we looked at some of the fundamental characteristics of both audio\nand video transmission. Then Chapter 5 introduced techniques such as pulse code\nmodulation (PCM) for digitizing audio and video data for digital transmission. For\n24.1 / AUDIO AND VIDEO COMPRESSION\nmultimedia applications, it is important to make the most efficient use of transmis-\nsion capacity as possible.Accordingly, much attention has been paid to the develop-\nment of compression algorithms for both audio and video transmissions. This\nsection provides an overview.\nThe techniques discussed in this section were standardized by the Moving Pic-\nture Experts Group (MPEG). MPEG, under the auspices of the International Orga-\nnization for Standardization (ISO), has developed standards for video and\nassociated audio in digital form, where the digital form may be stored on a variety of\ndevices, such as CD-ROM, tapes, and writable optical disks, and transmitted on\ncommunications channels such as ISDN and LANs. The MPEG effort covers not\nonly video compression, but also audio compression and associated system issues\nand formats. The premise of the MPEG effort is that a video signal can be com-\npressed to a bit rate of about 1.5 Mbps with acceptable quality and that correspond-\ning efficiencies are achievable for audio transmission.\nBefore proceeding, we introduce two terms. Data compression falls into two\nbroad categories: lossless and lossy.With lossless compression, no information is lost\nand the decompressed data are identical to the original uncompressed data. The\nefficiency of lossless compression is limited to the entropy, or redundancy, of the\ndata source. In other words, compression is limited to the process of eliminating\nsome or all of the redundancy in a bit stream, so that no information is lost. With\nlossy compression, the decompressed data may be an acceptable approximation\n(according to some fidelity criterion) to the original uncompressed data. For\nexample, for image or video compression, the criterion may be that the\ndecompressed image is indistinguishable from the original to the human eye. In\nwhat follows, we will see that lossy compression is used for both audio and video.\nHowever, in the case of audio, the fidelity of the output is so high that, for all\npractical purposes, the compression is lossless.\nAudio Compression\nThe first step in the development of an audio compression algorithm is to digitize the\naudio signal, using a technique such as PCM. It is important to note that PCM or a\nsimilar technique does in fact provide a measure of compression. Recall from\nChapter 5 that the sampling theorem states that if a signal f(t) is sampled at regular\nintervals of time and at a rate higher than twice the highest signal frequency, then the\nsamples contain all the information of the original signal. The function f(t) may be\nreconstructed from these samples by the use of a lowpass filter. For this technique to\nreproduce the original signal, the samples must have analog values that have infinite\nprecision; this is known as pulse amplitude modulation (PAM).To create a digital sig-\nnal, each sample is quantized to a value that can be represented by a fixed number of\nbits, producing pulse code modulation (PCM).A PCM-encoded signal produces only\nan approximation of the original signal. If unlimited fidelity were required, then an\nunlimited number of bits would be needed for each sample. The fact that a fixed,\nfinite number of bits is used per sample results, in effect, in compression.\nTaking a simple-minded approach, further compression could be achieved by\nreducing the frequency of sampling or reducing the number of bits per sample.\nHowever, there is another approach that can produce significant compression and\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nat the same time retain a fidelity that is equivalent to lossless compression. Such an\napproach is taken in the MPEG standard.\nThe MPEG standard for audio compression is quite complex and beyond our\nscope. In fact, the standard provides for three layers of compression. Layer 3, popu-\nlarly known as MP3, provides a compression ratio of 10 : 1. In what follows, we look\nat the basic approach used in all of the MPEG audio compression algorithms.\nEffective audio compression takes into account the physiology of human hear-\ning. The compression algorithm exploits a phenomenon known as simultaneous\nauditory masking.This masking is an effect produced by the way the human nervous\nsystem perceives sound. In essence, if two signals are sufficiently near to one\nanother and one tone is stronger, the weaker signal is simply not perceived at all;\nthe human hearing apparatus masks it.Thus, what the percipient hears is exactly the\nsame whether the weaker tone is there or not.\nFigure 24.1a shows, in general terms, how masking is used to encode audio\nsignals. The input is partitioned into time frames ranging in duration from 2 to \n50 ms. A time-frequency analysis module decomposes each frame. At its simplest,\nthis module determines the amplitude in each of a sequence of narrow frequency\nsubbands; more complex analysis algorithms are typical. In any case, the output of\nthis module is a set of parameters that define the acoustic signal in that particular\ntime frame and that can be quantized. In parallel, a psychoacoustic module ana-\nlyzes the time frame for masking effects and other properties that can be\nexploited to achieve compression. Based on this analysis, a bit allocation module\ndecides how to apportion the total number of code bits available for the quantiza-\ntion of the subband signals. The resulting quantized signal is then fed into a loss-\nless coding module that eliminates any redundancies in the digital signal to\nachieve maximum compression.\nFigure 24.1b shows the inverse operation performed at a destination system to\nreproduce the original audio signal. The unpacking module recovers the quantized\nsignal by inverting the lossless compression.The resulting signal is then processed to\nproduce the audio output.\nTime/frequency\nPsychoacoustic\nQuantization\nFrequency-to-time\nreconstruction\n(a) MPEG audio encoder\n(b) MPEG audio decoder\nFigure 24.1\nMPEG Audio Compression and Decompression\n24.1 / AUDIO AND VIDEO COMPRESSION\nIntraframe mode\nDCT: Discrete cosine transform\nQ: quantizer\nVLC: variable-length coder\nFM: frame memory\nME: motion estimator\nFigure 24.2\nMPEG Block Diagram\nVideo Compression\nA moving picture is simply a succession of still pictures. Accordingly, one can\nachieve a degree of compression by independently compressing each still picture\nin the sequence that makes up the moving picture. But much more can be done.\nEven in a moving picture with a lot of action, the differences between adjacent\nstill pictures are generally small compared to the amount of information in a sin-\ngle still picture. This suggests that an encoding of the differences between adja-\ncent still pictures is a fruitful approach to compression; this is a tool used in\nOverview of the Video Compression Algorithm Figure 24.2 illustrates\nthe MPEG video compression scheme. The input to the MPEG compression mod-\nule is a sequence of video frames. Each frame is processed separately, being treated\nas a single still image. While operating on a single frame, the MPEG coder is in\nintraframe mode. In this mode, the algorithm performs the following steps:\n1. Preliminary scaling and color conversion. Each frame is converted into a stan-\ndardized representation known as Source Input Format (SIF), and color infor-\nmation is translated into a scheme known as YUV.\n2. Color subsampling. Brightness is the dominant component of color seen by the\nhuman eye, while hue is less important. Accordingly, the algorithm is able to\nreduce the hue information by about 75% with little effect on subjective fidelity.\n3. Discrete cosine transformation (DCT). This process maps each \npoints (pixels) into a set of numbers similar to a Fourier transform of the block.\nIn essence, the DCT provides a frequency domain representation of the image.\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nThis transformation does not result in any compression but provides suitable\ninput for later stages.\n4. Quantization. The DCT values are quantized into a finite number of possible\nvalues (similar to pulse-code modulation quantization). The more quantization\nlevels that are used, the greater the picture fidelity, but the less the amount of\ncompression.\n5. Run-length encoding. The quantized DCT values are represented using a run-\nlength encoding technique.\n6. Huffman coding. The data stream from the preceding step is compressed using\nHuffman coding, a lossless compression technique that assigns the most\ncommon bit sequences from the preceding step to symbols that are as short as\nAlthough significant compression can be achieved by simply processing a\nvideo signal as a sequence of still images, as just described, this approach fails to\nexploit the considerable redundancy present in all video sequences. Typically, many\nof the pixels will change very little or not at all from one frame to the next, or the\nchange simply involves the movement of a pattern of pixels from one location on a\nframe to a nearby location on the next frame. The MPEG studies indicate an addi-\ntional compression on the order of a factor of 3 [GALL91] by exploiting these\nredundancies in an interframe mode.\nFor interframe mode, similar blocks of pixels common to two or more succes-\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nHaving determined the matching block from the preceding frame, the MPEG\nalgorithm records the motion vector and the prediction error, which is \nmatrix of differences between the current macroblock, in frame c, and the reference\nmacroblock, in frame r:\nis the prediction error,\nis the value of the pixel located \nat position (x, y) in frame i, and \nis the motion vector for frame j relative to\nThus, the current frame is mapped into a matrix of prediction error values, one\nfor each pixel position, and of motion vector values, one for each macroblock. The\nprediction error matrix will have many zero values.This matrix is encoded using the\nDCT-quantization technique and should yield a higher compression ratio than sim-\nply encoding the original pixel matrix.\nThe MPEG standard does not dictate how the matching process is to be done.\nTypically, the motion vector for a macroblock is obtained by minimizing a cost func-\ntion that measures the difference between a macroblock and each predictor candi-\ndate.The calculation can be expressed as\nmacroblock in the current frame \ndisplacement vector with respect to the reference frame \nsearch range in the reference frame\nThe value of m that minimizes the preceding expression is used as the motion\nfor this block. The search range could encompass only small displace-\nments or could range up to the entire frame size.\nInterpolation Although prediction results in higher compression ratios than a\nsimple frame-by-frame compression, more can be done. In particular, MPEG allows\nsome video frames to be encoded using two reference frames, one in the past and\none in the future. This approach, called bidirectional interpolation, results in higher\ncompression ratios than prediction based on one reference frame.\nTo see why bidirectional interpolation can improve results, consider a scene\nthat is moving with respect to the picture frame at a rate of one half pixel per frame.\nIf we attempt to predict a macroblock in the current frame based on the immedi-\nately preceding frame, no exact matching block will be found. Similarly, no exact\nmatch to the macroblock will be found in the immediately following frame. How-\never, an average of the best match from the preceding and following frames pro-\nvides an exact prediction, so that the error matrix is all zeroes.\nFigure 24.3 illustrates the technique used in bidirectional interpolation. The\ncurrent frame, referred to as a B frame, is processed against two reference frames,\none before and one after this frame in time. Each macroblock can be encoded using\na block from the preceding frame (forward prediction), the following frame \nC[Ic1x, y2 - Ir11x, y2 + m2] d\nEc1x, y2 = Ic1x, y2 - Ir[1x, y2 + Mrc]\n24.1 / AUDIO AND VIDEO COMPRESSION\nPrediction Modes for Macroblock in B Picture\nForward predicted\nBackward predicted\nvector (x, y).\nIN01z + M012 + IN21z + M212\nIN11z2 = IN21z + M212\nIN11z2 = IN01z + M012\n(backward prediction), or one block from each reference frame (averaging),\nwhichever gives the minimum error matrix. Table 24.1 summarizes the calculations\nfor each option, with frame 1 being the current frame, frame 0 the preceding refer-\nence frame, and frame 2 the following reference frame.\nIn the case of bidirectional interpolation, more information must be encoded.\nAs with predicted frames, a matrix of differences is produced and then encoded\nusing DCT. In addition, each macroblock is encoded with an indication of the pre-\ndiction mode (forward, backward, average) and one or two motion vectors.\nFrame Ordering Three types of frames are defined in MPEG:\n• Intraframe (I): Encoded in JPEG style as an independent still image\n• Predicted (P): Encoded with reference to the preceding anchor frame\n• Bidirectional interpolated (B): Encoded with reference to the preceding and\nthe following anchor frames\nThe relative frequency of these types of frames within a video stream is a con-\nfigurable parameter and must satisfy several tradeoffs. First, there is the need to\nsatisfy the requirements for random access and fast forward/reverse searches,\ndescribed earlier. These requirements place a lower bound on the fraction of I\nframes in the encoded stream. Second, there is a tradeoff between computational\ncomplexity and the number of B frames: More B frames means more computation.\nFinally, B frames can only be processed with respect to I and P frames; that is, one\nB frame cannot serve as a reference frame for another B frame. Therefore, the\nhigher the fraction of B frames, the greater the average distance between a B frame\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nForward prediction\nBidirectional prediction\nTransmission order\nGroup of pictures\n1 4 2 3 6 5 8 7 ...\nI P B B P B I B ...\nFigure 24.4\nExample of Temporal Picture Structure\nframe for B frames 2 and 3. Frames 5 and 6 are interchanged for the same reason. B\nframe 7 is recorded as part of the next group because it is encoded after I frame 8.\n24.2 REAL-TIME TRAFFIC\nThe widespread deployment of high-speed LANs and WANs and the increase in the\nline capacity on the Internet and other internets has opened up the possibility of\nusing IP-based networks for the transport of real-time traffic. However, it is impor-\ntant to recognize that the requirements of real-time traffic differ from those of high-\nspeed but non-real-time traffic.\nWith traditional internet applications, such as file transfer, electronic mail, and\nclient/server applications including the Web, the performance metrics of interest are\ngenerally throughput and delay. There is also a concern with reliability, and mecha-\nnisms are used to make sure that no data are lost, corrupted, or misordered during\ntransit. By contrast, real-time applications are more concerned with timing issues. In\nmost cases, there is a requirement that data be delivered at a constant rate equal to\nthe sending rate. In other cases, a deadline is associated with each block of data, such\nthat the data are not usable after the deadline has expired.\n24.2 / REAL-TIME TRAFFIC\nmultimedia server\nConstant flow of packets\n(160 data octets every 20 ms)\nPackets delivered with original spacing\n(some may be missing)\nPackets arrive unevenly spaced\nDestination\nmultimedia PC\nFigure 24.5\nReal-Time Traffic\nReal-Time Traffic Characteristics\nFigure 24.5 illustrates a typical real-time environment. Here, a server is generating\naudio to be transmitted at 64 kbps.The digitized audio is transmitted in packets con-\ntaining 160 octets of data, so that one packet is issued every 20 ms.These packets are\npassed through an internet and delivered to a multimedia PC, which plays the audio\nin real time as it arrives. However, because of the variable delay imposed by the\nInternet, the interarrival times between packets are not maintained at a fixed 20 ms\nat the destination. To compensate for this, the incoming packets are buffered,\ndelayed slightly, and then released at a constant rate to the software that generates\nThe compensation provided by the delay buffer is limited. To understand this,\nwe need to define the concept of delay jitter, which is the maximum variation in\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\ndelay experienced by packets in a single session. For example, if the minimum end-\nto-end delay seen by any packet is 1 ms and the maximum is 6 ms, then the delay jit-\nter is 5 ms.As long as the time delay buffer delays incoming packets by at least 5 ms,\nthen the output of the buffer will include all incoming packets. However, if the\nbuffer delayed packets only by 4 ms, then any incoming packets that had experi-\nenced a relative delay of more than 4 ms (an absolute delay of more than 5 ms)\nwould have to be discarded so as not to be played back out of order.\nThe description of real-time traffic so far implies a series of equal-size packets\ngenerated at a constant rate. This is not always the profile of the traffic. Figure 24.6\nillustrates some of the common possibilities:\n• Continuous data source: Fixed-size packets are generated at fixed intervals.\nThis characterizes applications that are constantly generating data, have few\nredundancies, and that are too important to compress in a lossy way. Examples\nare air traffic control radar and real-time simulations.\n• On/off source: The source alternates between periods when fixed-size packets\nare generated at fixed intervals and periods of inactivity. A voice source, such\nas in telephony or audio conferencing, fits this profile.\n• Variable packet size: The source generates variable-length packets at uniform\nintervals. An example is digitized video in which different frames may experi-\nence different compression ratios for the same output quality level.\nRequirements for Real-Time Communication\n[ARAS94] lists the following as desirable properties for real-time communication:\n• Low jitter\n• Low latency\n• Ability to easily integrate non-real-time and real-time services\n• Adaptable to dynamically changing network and traffic conditions\n• Good performance for large networks and large numbers of connections\n• Modest buffer requirements within the network\n• High effective capacity utilization\n• Low overhead in header bits per packet\n• Low processing overhead per packet within the network and at the end system\n(a) Continuous data source\n(c) Compressed video source\n(b) Voice source\nwith silent intervals\nFigure 24.6\nReal-Time Packet Transmission (based on [ARAS94])\n24.3 / VOICE OVER IP AND MULTIMEDIA SUPPORT—SIP\nThese requirements are difficult to meet in a wide area IP-based network or inter-\nnet. Neither TCP nor UDP by itself is appropriate. We will see that RTP provides a\nreasonable foundation for addressing these issues.\nHard versus Soft Real-Time Applications\nA distinction needs to be made between hard and soft real-time communication\napplications. Soft real-time applications can tolerate the loss of some portion of the\ncommunicated data, while hard real-time applications have zero loss tolerance. In\ngeneral,soft real-time applications impose fewer requirements on the network,and it\nis therefore permissible to focus on maximizing network utilization, even at the cost\nof some lost or misordered packets. In hard real-time applications, a deterministic\nupper bound on jitter and high reliability take precedence over network utilization\nconsiderations.\n24.3 VOICE OVER IP AND MULTIMEDIA SUPPORT—SIP\nThe Session Initiation Protocol (SIP), defined in RFC 3261, is an application-\nlevel control protocol for setting up, modifying, and terminating real-time ses-\nsions between participants over an IP data network. The key driving force\nbehind SIP is to enable Internet telephony, also referred to as voice over IP\n(VoIP). SIP can support any type of single media or multimedia session, including\nteleconferencing.\nSIP supports five facets of establishing and terminating multimedia communi-\n• User location: Users can move to other locations and access their telephony or\nother application features from remote locations.\n• User availability: Determination of the willingness of the called party to\nengage in communications.\n• User capabilities: Determination of the media and media parameters to be used.\n• Session setup: Setup up point-to-point and multiparty calls, with agreed ses-\nsion parameters.\n• Session management: Including transfer and termination of sessions, modify-\ning session parameters, and invoking services.\nSIP employs design elements developed for earlier protocols. SIP is based\non an HTTP-like request/response transaction model. Each transaction consists\nof a client request that invokes a particular method, or function, on the server\nand at least one response. SIP uses most of the header fields, encoding rules, and\nstatus codes of HTTP. This provides a readable text-based format for displaying\ninformation. SIP also uses concepts similar to the recursive and iterative\nsearches of DNS. SIP incorporates the use of a Session Description Protocol\n(SDP), which defines session content using a set of types similar to those used in\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nSIP Components and Protocols\nAn SIP network can be viewed of consisting of components defined on two dimen-\nsions: client/server and individual network elements. RFC 3261 defines client and\nserver as follows:\n• Client: A client is any network element that sends SIP requests and receives\nSIP responses. Clients may or may not interact directly with a human user.\nUser agent clients and proxies are clients.\n• Server: A server is a network element that receives requests in order to service\nthem and sends back responses to those requests. Examples of servers are\nproxies, user agent servers, redirect servers, and registrars.\nThe individual elements of a standard SIP network are as follows:\n• User Agent: Resides in every SIP end station. It acts in two roles:\n—User agent client (UAC): Issues SIP requests\n—User agent server (UAS): Receives SIP requests and generates a response\nthat accepts, rejects, or redirects the request\n• Redirect Server: Used during session initiation to determine the address of the\ncalled device.The redirect server returns this information to the calling device,\ndirecting the UAC to contact an alternate URI. This is analogous to iterative\nsearches in DNS.\n• Proxy Server: An intermediary entity that acts as both a server and a client for\nthe purpose of making requests on behalf of other clients. A proxy server pri-\nmarily plays the role of routing, which means its job is to ensure that a request\nis sent to another entity closer to the targeted user. Proxies are also useful for\nenforcing policy (for example, making sure a user is allowed to make a call).A\nproxy interprets, and, if necessary, rewrites specific parts of a request message\nbefore forwarding it.This is analogous to recursive searches in DNS.\n• Registrar: A server that accepts REGISTER requests and places the informa-\ntion it receives (the SIP address and associated IP address of the registering\ndevice) in those requests into the location service for the domain it handles.\n• Location Service: A location service is used by a SIP redirect or proxy server\nto obtain information about a callee’s possible location(s). For this purpose,\nthe location service maintains a database of SIP-address/IP-address mappings.\nThe various servers are defined in RFC 3261 as logical devices. They may be\nimplemented as separate servers configured on the Internet or they may be com-\nbined into a single application that resides in a physical server.\nFigure 24.7 shows how some of the SIP components relate to one another and\nthe protocols that are employed. A user agent acting as a client (in this case UAC\nalice) uses SIP to set up a session with a user agent that will act as a server (in this\ncase UAS bob). The session initiation dialogue uses SIP and involves one or more\nproxy servers to forward requests and responses between the two user agents. The\nuser agents also make use of the Session Description Protocol (SDP), which is used\nto describe the media session.\n24.3 / VOICE OVER IP AND MULTIMEDIA SUPPORT—SIP\nMedia (RTP)\nUser agent Alice\nUser agent Bob\nFigure 24.7\nSIP Components and Protocols\nThe proxy servers may also act as redirect servers as needed. If redirection is\ndone, a proxy server will need to consult the location service database, which may be\ncollocated with a proxy server or not.The communication between the proxy server\nand the location service is beyond the scope of the SIP standard. DNS is also an\nimportant part of SIP operation. Typically, a UAC will make a request using the\ndomain name of the UAS, rather than an IP address. A proxy server will need to\nconsult a DNS server to find a proxy server for the target domain.\nSIP typically runs on top of UDP for performance reasons, and provides its\nown reliability mechanisms, but may also use TCP. If a secure, encrypted transport\nmechanism is desired, SIP messages may alternatively be carried over the Transport\nLayer Security (TLS) protocol, described in Chapter 21.\nAssociated with SIP is the Session Description Protocol (SDP), defined in\nRFC 2327. SIP is used to invite one or more participants to a session, while the SDP-\nencoded body of the SIP message contains information about what media encod-\nings (e.g., voice, video) the parties can and will use. Once this information is\nexchanged and acknowledged, all participants are aware of the participants’ IP\naddresses, available transmission capacity, and media type. Then data transmission\nbegins, using an appropriate transport protocol. Typically, the Real-Time Transport\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\n1Figures 24.8 through 24.11 are adapted from ones developed by Professor H. Charles Baker of Southern\nMethodist University.\nProtocol (RTP), described subsequently, is used. Throughout the session,\nparticipants can make changes to session parameters, such as new media types or\nnew parties to the session, using SIP messages.\nSIP Uniform Resource Identifier\nA resource within a SIP network is identified by a Uniform Resource Identifier\n(URI). Examples of communications resources include the following:\n• A user of an online service\n• An appearance on a multiline phone\n• A mailbox on a messaging system\n• A telephone number at a gateway service\n• A group (such as “sales” or “helpdesk”) in an organization\nSIP URIs have a format based on email address formats, namely user@domain.\nThere are two common schemes.An ordinary SIP URI is of the form\nsip:bob@biloxi.com\nThe URI may also include a password, port number, and related parameters. If\nsecure transmission is required, “sip:” is replaced by “sips:”. In the latter case, SIP\nmessages are transported over TLS.\nExamples of Operation\nThe SIP specification is quite complex; the main document, RFC 3261, is 269 pages\nlong.To give some feel for its operation, we present a few examples.\nFigure 24.8 shows an unsuccessful attempt by user Alice to establish a session\nwith user Bob, whose URI is bob@biloxi.com.1 Alice’s UAC is configured to communi-\ncate with a proxy server (the outbound server) in its domain and begins by sending an\nINVITE message to the proxy server that indicates its desire to invite Bob’s UAS into\na session (1); the server acknowledges the request (2). Although Bob’s UAS is identi-\nfied by its URI, the outbound proxy server needs to take into account the possibility\nthat Bob is not currently available or that Bob has moved.Accordingly, the outbound\nproxy server should forward the INVITE request to the proxy server that is responsi-\nble for the domain biloxi.com.The outbound proxy thus consults a local DNS server to\nobtain the IP address of the biloxi.com proxy server (3),by asking for the SRV resource\nrecord (Table 23.2) that contains information on the proxy server for biloxi.com.\nThe DNS server responds (4) with the IP address of the biloxi.com proxy server\n(the inbound server).Alice’s proxy server can now forward the INVITE message to\nthe inbound proxy server (5), which acknowledges the message (6). The inbound\nproxy server now consults a location server to determine the location of Bob (7),\nand the location server responds that Bob is not signed in, and therefore not avail-\nable for SIP messages (8). This information is communicated back to the outbound\nproxy server (9, 10) and then to Alice (11, 12).\n24.3 / VOICE OVER IP AND MULTIMEDIA SUPPORT—SIP\n(192.0.2.4)\n(15.16.17.18)\n3. DNS Query:\n4. Response:\nUser agent Alice\n(12.26.17.91)\nUser agent Bob\n(not signed in)\n7. LS Query: sip:\nbob@biloxi.com?\n8. Response:\nnot signed in\n6. 100 Trying\n9. 480 temporarily\nunavailable\nTo: sip:bob@biloxi.com\n2. 100 Trying\n11. 480 temporarily\nunavailable\nTo: sip:bob@biloxi.com\nFigure 24.8\nSIP Call Setup Attempt Scenario\n2PSTN is the public switched telephone network.\nThe next example (Figure 24.9) makes use of two message types that are not yet\npart of the SIP standard but that are documented in RFC 2848 and are likely to be\nincorporated in a later revision of SIP.These message types support telephony applica-\ntions.At the end of the preceding example,Alice was informed that Bob was not avail-\nable.Alice’s UAC then issues a SUBSCRIBE message (1),indicating that it wants to be\ninformed when Bob is available.This request is forwarded through the two proxies in\nour example to a PINT (PSTN-Internet Networking)2server (2, 3).A PINT server acts\nas a gateway between an IP network from which comes a request to place a telephone\ncall and a telephone network that executes the call by connecting to the destination\ntelephone. In this example, we assume that the PINT server logic is collocated with the\nlocation service. It could also be the case that Bob is attached to the Internet rather\nthan a PSTN, in which case the equivalent of PINT logic is needed to handle SUB-\nSCRIBE requests. In this example, we assume that latter and assume that the PINT\nfunctionality is implemented in the location service. In any case, the location service\nauthorizes subscription by returning an OK message (4), which is passed back to Alice\n(5,6).The location service then immediately sends a NOTIFY message with Bob’s cur-\nrent status of not signed in (7, 8, 9), which Alice’s UAC acknowledges (10, 11, 12).\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nLocation service\nPINT server\nUser agent Alice\nUser agent Bob\n(not signed in)\n2. SUBSCRIBE\nTo: sip:bob@biloxi.com\n1. SUBSCRIBE\n<Not Signed In>\nTo: sip:bob@biloxi.com\n<Not Signed In>\n<Not Signed In>\n3. SUBSCRIBE\n    To: sip:bob@biloxi.com\nFigure 24.9\nSIP Presence Example\nFigure 24.10 continues the example of Figure 24.9. Bob signs on by sending a\nREGISTER message to the proxy in its domain (1).The proxy updates the database\nat the location service to reflect registration (2).The update is confirmed to the proxy\n(3),which confirms the registration to Bob (4).The PINT functionality learns of Bob’s\nnew status from the location server (here we assume that they are collocated) and\nsends a NOTIFY message containing the new status of Bob (5), which is forwarded to\nAlice (6, 7).Alice’s UAC acknowledges receipt of the notification (8, 9, 10).\nNow that Bob is registered,Alice can try again to establish a session, as shown\nin Figure 24.11. This figure shows the same flow as Figure 24.8, with a few differ-\nences. We assume that Alice’s proxy server has cached the IP address of the proxy\nserver for domain biloxi.com, and therefore need not consult the DNS server.A ring-\ning response is sent from Bob back to Alice (8, 9, 10) while the UAS at Bob is alert-\ning the local media application (e.g., telephony). When the media application\naccepts the call, Bob’s UAS sends back an OK response to Alice (11, 12, 13).\nFinally, Alice’s UAC sends an acknowledgement message to Bob’s UAS to\nconfirm the reception of the final response (14). In this example, the ACK is sent\ndirectly from Alice to Bob, bypassing the two proxies. This occurs because the end-\npoints have learned each other’s address from the INVITE/200 (OK) exchange,\nwhich was not known when the initial INVITE was sent.The media session has now\nbegun, and Alice and Bob can exchange data over an RTP connection.\n24.3 / VOICE OVER IP AND MULTIMEDIA SUPPORT—SIP\nUser agent Alice\nUser agent Bob\n<Signed In>\n<Signed In>\n<Signed In>\n1. REGISTER\nContact: bob@1.2.3.4)\n2. Updata Database:\nB \u0001 bob@1.2.3.4\nFigure 24.10\nSIP Registration and Notification Example\nSIP Messages\nAs was mentioned, SIP is a text-based protocol with a syntax similar to that of\nHTTP. There are two different types of SIP messages, requests and responses. The\nformat difference between the two types of messages is seen in the first line. The\nfirst line of a request has a method, defining the nature of the request and a\nRequest-URI, indicating where the request should be sent. The first line of a\nresponse has a response code.All messages include a header, consisting of a number\nof lines, each line beginning with a header label.A message can also contain a body,\nsuch as an SDP media description.\nSIP Requests RFC 3261 defines the following methods:\n• REGISTER: Used by a user agent to notify a SIP network of its current IP\naddress and the URLs for which it would like to receive calls\n• INVITE: Used to establish a media session between user agents\n• ACK: Confirms reliable message exchanges\n• CANCEL: Terminates a pending request, but does not undo a completed call\n• BYE: Terminates a session between two users in a conference\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\n(192.0.2.4)\n(15.16.17.18)\nUser agent Alice\n(12.26.17.91)\nUser agent Bob\n5. LS Query: sip:\nbob@biloxi.com?\n6. Response:\nsip:bob@1.2.3.4\n4. 100 Trying\nMedia (RTP)\n9. 180 Ringing\n8. 180 Ringing\nTo: sip:bob@biloxi.com\nTo: sip:bob@biloxi.com\n2. 100 Trying\n10. 180 Ringing\nTo: sip:bob@biloxi.com\nFigure 24.11\nSIP Successful Call Setup\n• OPTIONS: Solicits information about the capabilities of the callee, but does\nnot set up a call\nFor example, the header of message (1) in Figure 24.11 might look like this:\nINVITE sip:bob@biloxi.com SIP/2.0\nVia: SIP/2.0/UDP 12.26.17.91:5060\nMax-Forwards: 70\nTo: Bob <sip:bob@biloxi.com>\nFrom: Alice <sip:alice@atlanta.com>;tag=1928301774\nCall-ID: a84b4c76e66710@12.26.17.91\nCSeq: 314159 INVITE\nContact: <sip:alice@atlanta.com>\nContent-Type: application/sdp\nContent-Length: 142\nThe boldface type used for header labels is not typical but is used here for clar-\nity. The first line contains the method name (INVITE), a SIP URI, and the version\n24.3 / VOICE OVER IP AND MULTIMEDIA SUPPORT—SIP\nnumber of SIP that is used. The lines that follow are a list of header fields. This\nexample contains the minimum required set.\nThe Via headers show the path the request has taken in the SIP network (source\nand intervening proxies) and are used to route responses back along the same path. In\nmessage (1), there is only one Via header, inserted by Alice.The Via line contains the\nIP address (12.26.17.91),port number (5060),and transport protocol (UDP) that Alice\nwants Bob to use in his response. Subsequent proxies add additional Via headers.\nMax-Forwards serves to limit the number of hops a request can make on the way\nto its destination. It consists of an integer that is decremented by one by each proxy\nthat forwards the request. If the Max-Forwards value reaches 0 before the request\nreaches its destination, it will be rejected with a 483 (Too Many Hops) error response.\nTo contains a display name (Bob) and a SIP or SIPS URI (sip:bob@biloxi.com)\ntoward which the request was originally directed. From also contains a display name\n(Alice) and a SIP or SIPS URI (sip:alice@atlanta.com) that indicate the originator\nof the request.This header field also has a tag parameter containing a random string\n(1928301774) that was added to the URI by the UAC. It is used to identify the session.\nCall-ID contains a globally unique identifier for this call, generated by the\ncombination of a random string and the host name or IP address. The combination\nof the To tag, From tag, and Call-ID completely defines a peer-to-peer SIP relation-\nship between Alice and Bob and is referred to as a dialog.\nCSeq or Command Sequence contains an integer and a method name. The\nCSeq number is initialized at the start of a call (314159 in this example), incre-\nmented for each new request within a dialog, and is a traditional sequence number.\nThe CSeq is used to distinguish a retransmission from a new request.\nThe Contact header contains a SIP URI for direct communication between\nUAs. While the Via header field tells other elements where to send the response, the\nContact header field tells other elements where to send future requests for this dialog.\nThe Content-Type indicates the type of the message body. Content-Length\ngives the length in octets of the message body.\nSIP Responses The response types defined in RFC 3261 are in the following \ncategories:\n• Provisional (1xx): Request received and being processed.\n• Success (2xx): The action was successfully received, understood, and accepted.\n• Redirection (3xx): Further action needs to be taken in order to complete the\n• Client Error (4xx): The request contains bad syntax or cannot be fulfilled at\nthis server.\n• Server Error (5xx): The server failed to fulfill an apparently valid request.\n• Global Failure (6xx): The request cannot be fulfilled at any server.\nFor example, the header of message (11) in Figure 24.11 might look like this:\nSIP/2.0 200 OK\nVia: SIP/2.0/UDP server10.biloxi.com\nVia: SIP/2.0/UDP bigbox3.site3.atlanta.com\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nVia: SIP/2.0/UDP 12.26.17.91:5060\nTo: Bob <sip:bob@biloxi.com>;tag=a6c85cf\nFrom: Alice <sip:alice@atlanta.com>;tag=1928301774\nCall-ID: a84b4c76e66710@12.26.17.91\nCSeq: 314159 INVITE\nContact: <sip:bob@biloxi.com>\nContent-Type: application/sdp\nContent-Length: 131\nThe first line contains the version number of SIP that is used and the response\ncode and name.The lines that follow are a list of header fields.The Via,To, From, Call-\nID, and CSeq header fields are copied from the INVITE request. (There are three Via\nheader field values—one added by Alice’s SIP UAC, one added by the atlanta.com\nproxy,and one added by the biloxi.com proxy.) Bob’s SIP phone has added a tag para-\nmeter to the To header field.This tag will be incorporated by both endpoints into the\ndialog and will be included in all future requests and responses in this call.\nSession Description Protocol\nThe Session Description Protocol (SDP), defined in RFC 2327, describes the con-\ntent of sessions, including telephony, Internet radio, and multimedia applications.\nSDP includes information about the following [SCHU99]:\n• Media streams: A session can include multiple streams of differing content.\nSDP currently defines audio, video, data, control, and application as stream\ntypes, similar to the MIME types used for Internet mail (Table 22.3).\n• Addresses: Indicates the destination addresses, which may be a multicast\naddress, for a media stream.\n• Ports: For each stream, the UDP port numbers for sending and receiving are\n• Payload types: For each media stream type in use (e.g., telephony), the pay-\nload type indicates the media formats that can be used during the session.\n• Start and stop times: These apply to broadcast sessions, like a television or\nradio program. The start, stop, and repeat times of the session are indicated.\n• Originator: For broadcast sessions, the originator is specified, with contact\ninformation. This may be useful if a receiver encounters technical difficulties.\n24.4 REAL-TIME TRANSPORT PROTOCOL (RTP)\nThe most widely used transport-level protocol is TCP. Although TCP has proven\nits value in supporting a wide range of distributed applications, it is not suited for\nuse with real-time distributed applications. By a real-time distributed applica-\ntion, we mean one in which a source is generating a stream of data at a constant\nrate, and one or more destinations must deliver that data to an application at the\nsame constant rate. Examples of such applications include audio and video\n24.4 / REAL-TIME TRANSPORT PROTOCOL (RTP)\nconferencing, live video distribution (not for storage but for immediate play),\nshared workspaces, remote medical diagnosis, telephony, command and control\nsystems, distributed interactive simulations, games, and real-time monitoring. A\nnumber of features of TCP disqualify it for use as the transport protocol for such\napplications:\n1. TCP is a point-to-point protocol that sets up a connection between two end-\npoints.Therefore, it is not suitable for multicast distribution.\n2. TCP includes mechanisms for retransmission of lost segments, which then arrive\nout of order. Such segments are not usable in most real-time applications.\n3. TCP contains no convenient mechanism for associating timing information\nwith segments, which is another real-time requirement.\nThe other widely used transport protocol, UDP, does not exhibit the first two\ncharacteristics listed but, like TCP, does not provide timing information. By itself,\nUDP does not provide any general-purpose tools useful for real-time applications.\nAlthough each real-time application could include its own mechanisms for\nsupporting real-time transport, there are a number of common features that warrant\nthe definition of a common protocol. A protocol designed for this purpose is the\nReal-Time Transport Protocol (RTP), defined in RFC 1889. RTP is best suited to\nsoft real-time communication. It lacks the necessary mechanisms to support hard\nreal-time traffic.\nThis section provides an overview of RTP. We begin with a discussion of real-\ntime transport requirements. Next, we examine the philosophical approach of RTP.\nThe remainder of the section is devoted to the two protocols that make up RTP: the\nfirst is simply called RTP and is a data transfer protocol; the other is a control pro-\ntocol known as RTCP (RTP Control Protocol).\nRTP Protocol Architecture\nIn RTP, there is close coupling between the RTP functionality and the application-\nlayer functionality. Indeed, RTP is best viewed as a framework that applications can\nuse directly to implement a single protocol. Without the application-specific infor-\nmation, RTP is not a full protocol. On the other hand, RTP imposes a structure and\ndefines common functions so that individual real-time applications are relieved of\npart of their burden.\nRTP follows the principles of protocol architecture design outlined in a paper\nby Clark and Tennenhouse [CLAR90]. The two key concepts presented in that\npaper are application-level framing and integrated layer processing.\nApplication-Level Framing In a traditional transport protocol, such as TCP,\nthe responsibility for recovering from lost portions of data is performed transpar-\nently at the transport layer. [CLAR90] lists two scenarios in which it might be more\nappropriate for recovery from lost data to be performed by the application:\n1. The application,within limits,may accept less than perfect delivery and continue\nunchecked.This is the case for real-time audio and video. For such applications,\nit may be necessary to inform the source in more general terms about the qual-\nity of the delivery rather than to ask for retransmission. If too much data are\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nbeing lost, the source might perhaps move to a lower-quality transmission that\nplaces lower demands on the network, increasing the probability of delivery.\n2. It may be preferable to have the application rather than the transport protocol\nprovide data for retransmission.This is useful in the following contexts:\n(a) The sending application may recompute lost data values rather than storing\n(b) The sending application can provide revised values rather than simply\nretransmitting lost values,or send new data that “fix”the consequences of the\noriginal loss.\nTo enable the application to have control over the retransmission function,\nClark and Tennenhouse propose that lower layers, such as presentation and trans-\nport, deal with data in units that the application specifies. The application should\nbreak the flow of data into application-level data units (ADUs), and the lower lay-\ners must preserve these ADU boundaries as they process the data.The application-\nlevel frame is the unit of error recovery. Thus, if a portion of an ADU is lost in\ntransmission, the application will typically be unable to make use of the remaining\nportions. In such a case, the application layer will discard all arriving portions and\narrange for retransmission of the entire ADU, if necessary.\nIntegrated Layer Processing In a typical layered protocol architecture, such as\nTCP/IP or OSI, each layer of the architecture contains a subset of the functions to be\nperformed for communications, and each layer must logically be structured as a sepa-\nrate module in end systems.Thus, on transmission, a block of data flows down through\nand is sequentially processed by each layer of the architecture.This structure restricts\nthe implementer from invoking certain functions in parallel or out of the layered order\nto achieve greater efficiency. Integrated layer processing, as proposed in [CLAR90],\ncaptures the idea that adjacent layers may be tightly coupled and that the implementer\nshould be free to implement the functions in those layers in a tightly coupled manner.\nThe idea that a strict protocol layering may lead to inefficiencies has been pro-\npounded by a number of researchers. For example, [CROW92] examined the ineffi-\nciencies of running a remote procedure call (RPC) on top of TCP and suggested a\ntighter coupling of the two layers. The researchers argued that the integrated layer\nprocessing approach is preferable for efficient data transfer.\nFigure 24.12 illustrates the manner in which RTP realizes the principle of inte-\ngrated layer processing. RTP is designed to run on top of a connectionless transport\nprotocol such as UDP. UDP provides the basic port addressing functionality of the\ntransport layer. RTP contains further transport-level functions, such as sequencing.\nHowever, RTP by itself is not complete. It is completed by modifications and/or\nadditions to the RTP headers to include application-layer functionality. The figure\nindicates that several different standards for encoding video data can be used in\nconjunction with RTP for video transmission.\nRTP Data Transfer Protocol\nWe first look at the basic concepts of the RTP data transfer protocol and then exam-\nine the protocol header format. Throughout this section, the term RTP will refer to\nthe RTP data transfer protocol.\n24.4 / REAL-TIME TRANSPORT PROTOCOL (RTP)\nNetwork access\nFigure 24.12\nRTP Protocol Architecture [THOM96]\nRTP Concepts RTP supports the transfer of real-time data among a number of\nparticipants in a session.A session is simply a logical association among two or more\nRTP entities that is maintained for the duration of the data transfer. A session is\n• RTP port number: The destination port address is used by all participants for\nRTP transfers. If UDP is the lower layer, this port number appears in the Des-\ntination Port field (see Figure 2.3) of the UDP header.\n• RTCP port number: The destination port address is used by all participants\nfor RTCP transfers.\n• Participant IP addresses: This can either be a multicast IP address, so that the\nmulticast group defines the participants, or a set of unicast IP addresses.\nThe process of setting up a session is beyond the scope of RTP and RTCP.\nAlthough RTP can be used for unicast real-time transmission, its strength lies\nin its ability to support multicast transmission. For this purpose, each RTP data unit\nincludes a source identifier that identifies which member of the group generated \nthe data. It also includes a timestamp so that the proper timing can be re-created on\nthe receiving end using a delay buffer. RTP also identifies the payload format of the\ndata being transmitted.\nRTP allows the use of two kinds of RTP relays: translators and mixers. First we\nneed to define the concept of relay.A relay operating at a given protocol layer is an\nintermediate system that acts as both a destination and a source in a data transfer.\nFor example, suppose that system A wishes to send data to system B but cannot do\nso directly. Possible reasons are that B may be behind a firewall or B may not be\nable to use the format transmitted by A. In such a case, A may be able to send the\ndata to an intermediate relay R. R accepts the data unit, makes any necessary\nchanges or performs any necessary processing, and then transmits the data to B.\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nA mixer is an RTP relay that receives streams of RTP packets from one or\nmore sources, combines these streams, and forwards a new RTP packet stream to\none or more destinations.The mixer may change the data format or simply perform\nthe mixing function. Because the timing among the multiple inputs is not typically\nsynchronized, the mixer provides the timing information in the combined packet\nstream and identifies itself as the source of synchronization.\nAn example of the use of a mixer is to combine of a number of on/off sources\nsuch as audio. Suppose that a number of systems are members of an audio session\nand each generates its own RTP stream. Most of the time only one source is active,\nalthough occasionally more than one source will be “speaking” at the same time.\nA new system may wish to join the session, but its link to the network may not be\nof sufficient capacity to carry all of the RTP streams. Instead, a mixer could\nreceive all of the RTP streams, combine them into a single stream, and retransmit\nthat stream to the new session member. If more than one incoming stream is\nactive at one time, the mixer would simply sum their PCM values.The RTP header\ngenerated by the mixer includes the identifier(s) of the source(s) that contributed\nto the data in each packet.\nThe translator is a simpler device that produces one or more outgoing RTP\npackets for each incoming RTP packet.The translator may change the format of the\ndata in the packet or use a different lower-level protocol suite to transfer from one\ndomain to another. Examples of translator use are as follows:\n• A potential recipient may not be able to handle a high-speed video signal used\nby the other participants. The translator converts the video to a lower-quality\nformat requiring a lower data rate.\n• An application-level firewall may prevent the forwarding of RTP packets.Two\ntranslators are used, one on each side of the firewall, with the outside one tun-\nneling all multicast packets received through a secure connection to the trans-\nlator inside the firewall.The inside translator then sends out RTP packets to a\nmulticast group protected by the firewall.\n• A translator can replicate an incoming multicast RTP packet and send it to a\nnumber of unicast destinations.\nRTP Fixed Header Each RTP packet includes a fixed header and may also\ninclude additional application-specific header fields. Figure 24.13 shows the fixed\nheader.The first 12 octets (shaded portion) are always present and consist of the fol-\nlowing fields:\n• Version (2 bits): Current version is 2.\n• Padding (1 bit): Indicates whether padding octets appear at the end of the pay-\nload. If so, the last octet of the payload contains a count of the number of\npadding octets. Padding is used if the application requires that the payload be\nan integer multiple of some length, such as 32 bits.\n• Extension (1 bit): If set, the fixed header is followed by exactly one extension\nheader, which is used for experimental extensions to RTP.\n• CSRC Count (4 bits): The number of CSRC (contributing source) identifiers\nthat follow the fixed header.\n24.4 / REAL-TIME TRANSPORT PROTOCOL (RTP)\nSynchronization Source (SSRC) Identifier\nContributing Source (CSRC) Identifier\nContributing Source (CSRC) Identifier\nPayload Type\nSequence Number\nV \u0001 Version\nX \u0001 Extension\nCC \u0001 CSRC count\nFigure 24.13\n• Marker (1 bit): The interpretation of the marker bit depends on the payload type;\nit is typically used to indicate a boundary in the data stream. For video, it is set to\nmark the end of a frame. For audio, it is set to mark the beginning of a talk spurt.\n• Payload Type (7 bits): Identifies the format of the RTP payload, which follows\nthe RTP header.\n• Sequence Number (16 bits): Each source starts with a random sequence num-\nber, which is incremented by one for each RTP data packet sent. This allows\nfor loss detection and packet sequencing within a series of packets with the\nsame timestamp. A number of consecutive packets may have the same time-\nstamp if they are logically generated at the same time; an example is several\npackets belonging to the same video frame.\n• Timestamp (32 bits): Corresponds to the generation instant of the first octet of\ndata in the payload. The time units of this field depend on the payload type.\nThe values must be generated from a local clock at the source.\n• Synchronization Source Identifier: A randomly generated value that uniquely\nidentifies the source within a session.\nFollowing the fixed header, there may be one or more of the following field:\n• Contributing Source Identifier: Identifies a contributing source for the pay-\nload.These identifiers are supplied by a mixer.\nThe Payload Type field identifies the media type of the payload and the format\nof the data, including the use of compression or encryption. In a steady state, a\nsource should only use one payload type during a session but may change the pay-\nload type in response to changing conditions, as discovered by RTCP. Table 24.2\nsummarizes the payload types defined in RFC 1890.\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nPayload Types for Standard Audio and Video Encodings (RFC 1890)\nunassigned audio\nDV14 audio (8 kHz)\nDV14 audio (16 kHz)\nL16 audio (stereo)\nL16 audio (mono)\nunassigned audio\nunassigned audio\nunassigned video\nunassigned video\nRTP Control Protocol (RTCP)\nThe RTP data transfer protocol is used only for the transmission of user data, typi-\ncally in multicast fashion among all participants in a session.A separate control pro-\ntocol (RTCP) also operates in a multicast fashion to provide feedback to RTP data\nsources as well as all session participants. RTCP uses the same underlying transport\nservice as RTP (usually UDP) and a separate port number. Each participant period-\nically issues an RTCP packet to all other session members. RFC 1889 outlines four\nfunctions performed by RTCP:\n• Quality of service (QoS) and congestion control: RTCP provides feedback on\nthe quality of data distribution. Because RTCP packets are multicast, all ses-\nsion members can assess how well other members are performing and receiv-\ning. Sender reports enable receivers to estimate data rates and the quality of\nthe transmission. Receiver reports indicate any problems encountered by\nreceivers, including missing packets and excessive jitter. For example, an audio-\nvideo application might decide to reduce the rate of transmission over low-\nspeed links if the traffic quality over the links is not high enough to support the\ncurrent rate.The feedback from receivers is also important in diagnosing distri-\nbution faults. By monitoring reports from all session recipients, a network man-\nager can tell whether a problem is specific to a single user or more widespread.\n• Identification: RTCP packets carry a persistent textual description of the\nRTCP source. This provides more information about the source of data pack-\nets than the random SSRC identifier and enables a user to associate multiple\nstreams from different sessions. For example, separate sessions for audio and\nvideo may be in progress.\n• Session size estimation and scaling: To perform the first two functions, all par-\nticipants send periodic RTCP packets.The rate of transmission of such packets\nmust be scaled down as the number of participants increases. In a session with\n24.4 / REAL-TIME TRANSPORT PROTOCOL (RTP)\nfew participants, RTCP packets are sent at the maximum rate of one every five\nseconds. RFC 1889 includes a relatively complex algorithm by which each par-\nticipant limits its RTCP rate on the basis of the total session population. The\nobjective is to limit RTCP traffic to less than 5% of total session traffic.\n• Session control: RTCP optionally provides minimal session control information.\nAn example is a participant identification to be displayed in the user interface.\nAn RTCP transmission consists of a number of separate RTCP packets bun-\ndled in a single UDP datagram (or other lower-level data unit). The following\npacket types are defined in RFC 1889:\n• Sender Report (SR)\n• Receiver Report (RR)\n• Source Description (SDES)\n• Goodbye (BYE)\n• Application Specific\nFigure 24.14 depicts the formats of these packet types. Each type begins with a\n32-bit word containing the following fields:\n• Version (2 bits): Current version is 2.\n• Padding (1 bit): If set, indicates that this packet contains padding octets at the\nend of the control information. If so, the last octet of the padding contains a\ncount of the number of padding octets.\n• Count (5 bits): The number of reception report blocks contained in an SR or\nRR packet (RC), or the number of source items contained in an SDES or\nBYE packet.\n• Packet Type (8 bits): Identifies RTCP packet type.\n• Length (16 bits): Length of this packet in 32 bit words, minus one.\nIn addition, the Sender Report and Receiver Report packets contain the fol-\nlowing field:\n• Synchronization Source Identifier: Identifies the source of this RTCP packet\nWe now turn to a description of each packet type.\nSender Report (SR) RTCP receivers provide reception quality feedback using\na Sender Report or a Receiver Report, depending on whether the receiver is also a\nsender during this session. Figure 24.14a shows the format of a Sender Report. The\nSender Report consists of a header, already described; a sender information block;\nand zero or more reception report blocks. The sender information block includes\nthe following fields:\n• NTP Timestamp (64 bits): The absolute wall clock time when this report was\nsent; this is an unsigned fixed-point number with the integer part in the first 32\nbits and the fractional part in the last 32 bits.This may be used by the sender in\ncombination with timestamps returned in receiver reports to measure round-\ntrip time to those receivers.\nSSRC of sender\nRTP timestamp\nSender's packet count\nSender's octet count\nInterarrival jitter\nTime of last sender report\nDelay since last sender report\nCumulative number of packets lost\nSSRC_1 (SSRC of first source)\nSSRC_n (SSRC of nth source)\nNTP timestamp (most sig. word)\nNTP timestamp (least sig. word)\nExtended highest sequence number received\nInterarrival jitter\nTime of last sender report\nDelay since last sender report\nCumulative number of packets lost\nExtended highest sequence number received\nreport block 1\nreport block n\ninformation\n(a) RTCP sender report\nSSRC of sender\nInterarrival jitter\nTime of last sender report\nDelay since last sender report\nCumulative number of packets lost\nSSRC_1 (SSRC of first source)\nExtended highest sequence number received\nInterarrival jitter\nTime of last sender report\nDelay since last sender report\nCumulative number of packets lost\nSSRC_n (SSRC of nth source)\nExtended highest sequence number received\nname (ASCII)\nApplication-dependent data\nreport block 1\nreport block n\n(b) RTCP receiver report\n(c) RTCP application-defined packet\n(d) RTCP source description\nSSRC/CSRC_1\nSSRC/CSRC_n\n(e) RTCP BYE\nSSRC/CSRC_1\nSSRC/CSRC_n\nReason for leaving\nFigure 24.14\nRTCP Formats\n24.4 / REAL-TIME TRANSPORT PROTOCOL (RTP)\n• RTP Timestamp (32 bits): This is a the relative time used to create timestamps\nin RTP data packets. This lets recipients place this report in the appropriate\ntime sequence with RTP data packets from this source.\n• Sender’s Packet Count (32 bits): Total number of RTP data packets transmit-\nted by this sender so far in this session.\n• Sender’s Octet Count (32 bits): Total number of RTP payload octets transmit-\nted by this sender so far in this session.\nFollowing the sender information block are zero or more reception report\nblocks. One reception block is included for each source from which this participant\nhas received data during this session. Each block includes the following fields:\n• SSRC_n (32 bits): Identifies the source referred to by this report block.\n• Fraction Lost (8 bits): The fraction of RTP data packets from SSRC_n lost\nsince the previous SR or RR packet was sent.\n• Cumulative Number of Packets Lost (24 bits): Total number of RTP data\npackets from SSRC_n lost during this session.\n• Extended Highest Sequence Number Received (32 bits): The least significant\n16 bits record the highest RTP data sequence number received from SSRC_n.\nThe most significant 16 bits record the number of times the sequence number\nhas wrapped back to zero.\n• Interarrival Jitter (32 bits): An estimate of the jitter experienced on RTP data\npackets from SSRC_n, explained later.\n• Last SR Timestamp (32 bits): The middle 32 bits of the NTP timestamp in the\nlast SR packet received from SSRC_n. This captures the least significant half\nof the integer and the most significant half of the fractional part of the time-\nstamp and should be adequate.\n• Delay Since Last SR (32 bits): The delay, expressed in units of \nbetween receipt of the last SR packet from SSRC_n and the transmission of\nthis report block. These last two fields can be used by a source to estimate\nround-trip time to a particular receiver.\nRecall that delay jitter was defined as the maximum variation in delay experi-\nenced by packets in a single session.There is no simple way to measure this quantity\nat the receiver, but it is possible to estimate the average jitter in the following way.\nAt a particular receiver, define the following parameters for a given source:\nS(I) \u0001 Timestamp from RTP data packet I.\nR(I) \u0001 Time of arrival for RTP data packet I. expressed in RTP time-\nstamp units.The receiver must use the same clock frequency\n(increment interval) as the source but need not synchronize\ntime values with the source.\nD(I) \u0001 The difference between the interarrival time at the receiver and\nthe spacing between adjacent RTP data packets leaving the source.\nJ(I) \u0001 Estimated average interarrival jitter up to the receipt of RTP\ndata packet I.\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\n3For comparison, see Equation (20.3).\nThe value of D(I) is calculated as\nThus, D(I) measures how much the spacing between arriving packets differs from\nthe spacing between transmitted packets. In the absence of jitter, the spacings will\nbe the same and D(I) will have a value of 0.The interarrival jitter is calculated con-\ntinuously as each data packet I is received, according to the formula\nIn this equation, J(I) is calculated as an exponential average3 of observed values of\nD(I). Only a small weight is given to the most recent observation, so that temporary\nfluctuations do not invalidate the estimate.\nThe values in the Sender Report enable senders, receivers, and network man-\nagers to monitor conditions on the network as they relate to a particular session. For\nexample, packet loss values give an indication of persistent congestion, while the jit-\nter measures transient congestion. The jitter measure may provide a warning of\nincreasing congestion before it leads to packet loss.\nReceiver Report (RR) The format for the Receiver Report (Figure 24.14b) is\nthe same as that for a Sender Report, except that the Packet Type field has a differ-\nent value and there is no sender information block.\nSource Description (SDES) The Source Description packet (Figure 24.14d) is\nused by a source to provide more information about itself. The packet consists of a\n32-bit header followed by zero or more chunks, each of which contains information\ndescribing this source. Each chunk begins with an identifier for this source or for a\ncontributing source.This is followed by a list of descriptive items. Table 24.3 lists the\ntypes of descriptive items defined in RFC 1889.\n16 J1I - 12 + 1\n16 ƒ D1I2 ƒ\nD1I2 = 1R1I2 - R1I - 122 - 1S1I2 - S1I - 122\nSDES Types (RFC 1889)\nDescription\nEnd of SDES list\nCanonical name: unique among all participants within one RTP session\nReal user name of the source\nE-mail address\nTelephone number\nGeographic location\nName of application generating the stream\nTransient message describing the current state of the source\nPrivate experimental or application-specific extensions\n24.5 / RECOMMENDED READING AND WEB SITES\nGoodbye (BYE) The BYE packet indicates that one or more sources are no\nlonger active. This confirms to receivers that a prolonged silence is due to\ndeparture rather than network failure. If a BYE packet is received by a mixer,\nit is forwarded with the list of sources unchanged. The format of the BYE \npacket consists of a 32-bit header followed by one or more source identifiers.\nOptionally, the packet may include a textual description of the reason for \nApplication-Defined Packet This packet is intended for experimental use for\nfunctions and features that are application specific. Ultimately, an experimental\npacket type that proves generally useful may be assigned a packet type number and\nbecome part of the standardized RTCP.\n24.5 RECOMMENDED READING AND WEB SITES\n[GALL91] is a good overview of MPEG. [CHIA98] is a brief survey of all the MPEG\nstandards. [KOEN99] provides an overview of MPEG-4; [BATT99] and [BATT00] are a more\ndetailed treatment. [NACK99a] and [NACK99b] cover MPEG-7 in detail.\nA good technical treatment of the algorithms in this chapter is [SAYO06]. [GOOD02]\nand [SCHU99] discuss SIP in the context of VoIP. [DIAN02] looks at SIP in the context of the\nsupport of multimedia services over the Internet.\nBattista, S.; Casalio, F.; and Lande, C. “MPEG-4: A Multimedia Standard for the\nThird Millennium, Part 1.” IEEE Multimedia, October–December 1999.\nBattista, S.; Casalio, F.; and Lande, C.“MPEG-4:A Multimedia Standard for the\nThird Millennium, Part 2.” IEEE Multimedia, January–March 2000.\nChiariglione, L. “The Impact of MPEG Standards on Multimedia Industry.”\nProceedings of the IEEE, June 1998.\nDianda, J.; Gurbani, V.; and Jones, M. “Session Initiation Protocol Services \nArchitecture.” Bell Labs Technical Journal, Volume 7, Number 1, 2002.\nGall, D. “MPEG: A Video Compression Standard for Multimedia Applica-\ntions.” Communications of the ACM, April 1991.\nGoode, B. “Voice Over Internet Protocol (VoIP).” Proceedings of the IEEE,\nSeptember 2002.\nKoenen, R. “MPEG-4: Multimedia for Our Time.” IEEE Spectrum, February\nNack, F., and Lindsay, A. “Everything You Wanted to Know about MPEG-7,\nPart 1.”IEEE Multimedia, July–September 1999.\nNack, F., and Lindsay, A. “Everything You Wanted to Know about MPEG-7,\nPart 2.”IEEE Multimedia, October–December 1999.\nSayood, K. Introduction to Data Compression. New York: Elsevier, 2006.\nSchulzrinne, H., and Rosenberg, J.“The IETF Internet Telephony Architecture\nand Protocols.” IEEE Network, May/June 1999.\nCHAPTER 24 / INTERNET APPLICATIONS—MULTIMEDIA\nRecommended Web Sites:\n• MPEG Pointers and Resources: An exhaustive list of links to MPEG-related sites,\nincluding products, software, video files, announcements, FAQs, and technical informa-\n• SIP Forum: Nonprofit organization to promote SIP. Site contains product informa-\ntion, white papers, and other useful information and links.\n• SIP Working Group: Chartered by IETF to develop standards related to SIP. The\nWeb site includes all relevant RFCs and Internet drafts.\n• Audio/Video Transport Working Group: Chartered by IETF to develop standards\nrelated to RTP.The Web site includes all relevant RFCs and Internet drafts.\n• About RTP: Web site devoted to RTP developments, including technical and industry\ndevelopments.\n24.6 KEY TERMS, REVIEW QUESTIONS,AND PROBLEMS\nlossless compression\nlossy compression\nReal-Time Transport Protocol\nRTP Control Protocol\nSession Description Protocol\nSession Initiation Protocol\nSIP location service\nSIP proxy server\nSIP redirect server\nSIP registrar\nvoice over IP \nReview Questions\nWhat is the distinction between lossy and lossless compression?\nWhat are the five key services provided by SIP?\nList and briefly define the major components in an SIP network.\nWhat is the Session Description Protocol?\nWhat are some desirable properties for real-time communications?\nWhat is the difference between hard and soft real-time applications?\nWhat is the purpose of RTP?\nWhat is the difference between RTP and RTCP?\nIn the MPEG block diagram shown in Figure 24.2, interframe processing involves\ncomparing the current frame to a processed copy of preceding frames\nWhy not do the comparison between input frames?\n1DCT1Q1Q-11DCT-11F22222.\n24.6 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS\nA single video source transmits 30 frames per second, each containing 2 Mbits of\ndata.The data experiences a delay jitter of 1 s.What size of delay buffer is required at\nthe destination to eliminate the jitter?\nArgue the effectiveness, or lack thereof, of using RTP as a means of alleviating net-\nwork congestion for multicast traffic.\nIn RTP, senders periodically transmit a sender report message that provides an\nabsolute timestamp (the NTP Timestamp). The use of this absolute timestamp is\nessential to synchronize multiple streams, such as a video and an audio channel. Why\ncan’t RTP’s Timestamp field be used for that purpose?\nIllustrate how the last two fields in an RTCP SR or RR receiver report block can be\nused to calculate round-trip propagation time.\nThis page intentionally left blank \nlation is preferred; it makes for simpler expressions, and is it intuitively more satisfying to have frequency\nexpressed in Hz rather than radians per second.\nA.2 / FOURIER TRANSFORM REPRESENTATION OF APERIODIC SIGNALS\nThis relates to the earlier representation as follows:\nExamples of the Fourier series for periodic signals are shown in Figure A.1.\nA.2 FOURIER TRANSFORM REPRESENTATION \nFor a periodic signal, we have seen that its spectrum consists of discrete frequency\ncomponents, at the fundamental frequency and its harmonics. For an aperiodic sig-\nnal, the spectrum consists of a continuum of frequencies. This spectrum can be\ndefined by the Fourier transform. For a signal x(t) with a spectrum X(f), the follow-\ning relationships hold:\nThe presence of an imaginary number in the equations is a\nmatter of convenience. The imaginary component has a physical interpretation\nhaving to do with the phase of a waveform, and a discussion of this topic is\nbeyond the scope of this book.\nFigure A.2 presents some examples of Fourier transform pairs.\nPower Spectral Density and Bandwidth\nThe absolute bandwidth of any time-limited signal is infinite. In practical terms,\nhowever, most of the power in a signal is concentrated in some finite band, and the\neffective bandwidth consists of that portion of the spectrum that contains most of\nthe power.To make this concept precise, we need to define the power spectral den-\nsity (PSD). In essence, the PSD describes the power content of a signal as a func-\ntion of frequency, so that it shows how much power is present over various\nfrequency bands.\nFirst, we observe the power in the time domain. A function x(t) usually speci-\nfies a signal in terms of either voltage or current. In either case, the instantaneous\npower in the signal is proportional to \nWe define the average power of a\ntime-limited signal as\nƒ x1t2ƒ 2 dt\n-qx1t2e- j2pft dt\n-qX1f2ej2pft df\nun = tan-1a -Bn",
    "unit": "Unit 1",
    "source_type": "textbook",
    "book_priority": 2,
    "source_file": "Data and Computer Communications by William Stallings",
    "chunk_id": "Data and Computer Communications by William Stallings_chunk_0"
  },
  {
    "text": "UNIT II PHYSICAL AND DATA LINK LAYERS 9 \nWired and wireless media – Functions of physical layer – Modems – Transmission errors – \nError detection and correction – Framing - Flow control – Sublayers of DLL – Broadcast \nnetworks – Collision Domain - Ethernet – CSMA/CD – Token Ring – VLAN – LAN Analyzer – \nIEEE 802.11 - WLAN – CSMA/CA – Bluetooth – Ad hoc networks. \nUnit II: Physical and Data Link Layers with a focus on Wired and Wireless Media, \nPhysical Layer, and Data Link Layer (DLL): \nWired and Wireless Media: \n• Wired Media: \no Physical cables (e.g., twisted pair, coaxial cable, fiber optics) used for data \ntransmission. \no Advantages: Higher speed, lower interference. \no Disadvantages: Limited mobility, installation cost. \n• Wireless Media: \no Uses electromagnetic waves (e.g., radio waves, microwaves) for \ncommunication. \no Advantages: Mobility, easy to install. \no Disadvantages: More prone to interference and security issues, lower \nFunctions of the Physical Layer: \n• Responsible for the transmission of raw bits over a communication medium. \n• Converts digital data into signals (electrical, optical, or radio) for transmission. \n• Ensures that signals are correctly encoded, modulated, and transmitted. \n• Handles hardware elements like cables, connectors, and signal synchronization. \n• Modulator-Demodulator: Converts digital data from a computer into analog \nsignals for transmission over analog communication lines (like phone lines) and \nvice versa. \n• Role: Enables computers to communicate over long distances using telephone \ninfrastructure. \nTransmission Errors: \n• Transmission errors occur when data is corrupted during transmission due to \nfactors like noise, attenuation, or interference. \n• Bit errors may occur when a ‘1’ is mistaken for a ‘0’ or vice versa. \nError Detection and Correction: \n• Error Detection: Methods to detect errors in transmitted data, ensuring data \no Techniques: Parity Check, Checksum, Cyclic Redundancy Check (CRC). \n• Error Correction: Methods to not only detect but also correct errors in \ntransmission. \no Techniques: Hamming Code, Reed-Solomon Codes. \nFraming (Data Link Layer): \n• Framing refers to dividing the stream of bits into smaller, manageable units called \n• Allows for synchronization between sender and receiver, and error detection can be \napplied on individual frames. \nFlow Control: \n• Mechanism to ensure that a sender doesn’t overwhelm a receiver by sending data \no Examples: Stop-and-Wait Protocol, Sliding Window Protocol. \nSublayers of Data Link Layer (DLL): \n1. Logical Link Control (LLC): Provides error checking, flow control, and manages \ndata exchange between devices. \n2. Media Access Control (MAC): Controls how devices on a network gain access to \nthe medium and permission to transmit data. It uses physical addressing (MAC \naddresses). \nBroadcast Networks: \n• Networks where data sent by one device can be received by all other devices in the \nnetwork (e.g., Ethernet LAN). \n• Broadcast networks rely on protocols like CSMA/CD to manage multiple devices \ntransmitting on the same network medium. \nCollision Domain: \n• A network segment where data packets can collide when multiple devices \ntransmit simultaneously. \n• Hubs increase the size of a collision domain, while switches segment it, reducing \ncollisions and improving efficiency. \n• A widely used wired LAN technology that uses twisted pair cables or fiber optics. \n• Operates primarily at the data link layer and the physical layer. \nCSMA/CD (Carrier Sense Multiple Access with Collision Detection): \n• A protocol used in Ethernet networks to manage access to the shared medium. \n• CSMA: Devices sense the carrier (the medium) to see if it’s free before transmitting. \n• CD: If two devices transmit simultaneously and a collision occurs, they stop, wait a \nrandom amount of time, and retransmit. \nEthernet and Related Concepts: \n• Ethernet: A widely used wired LAN technology based on the CSMA/CD protocol. \n• CSMA/CD (Carrier Sense Multiple Access with Collision Detection): A protocol \nfor managing data collisions in wired networks. Devices listen to the network before \ntransmitting and detect collisions to retransmit the data. \n• Token Ring: A LAN protocol where a token circulates in the network, and only the \ndevice holding the token can transmit data, preventing collisions. \n• VLAN (Virtual Local Area Network): Logical segmentation of a LAN into multiple, \nisolated broadcast domains, improving security and reducing broadcast traffic. \n• LAN Analyzer: A tool used to monitor, analyze, and troubleshoot LAN traffic. \nWireless Networking Concepts: \n• IEEE 802.11 WLAN: The standard for wireless LANs, commonly known as Wi-Fi. It \nprovides wireless connectivity between devices using access points. \n• CSMA/CA (Carrier Sense Multiple Access with Collision Avoidance): A protocol \nfor wireless networks where devices avoid collisions by waiting for a clear channel \nbefore transmitting. \n• Bluetooth: A short-range wireless technology used for connecting devices like \nphones, laptops, and peripherals. \n• Ad Hoc Networks: Decentralized wireless networks where devices communicate \ndirectly without relying on a fixed infrastructure (e.g., Wi-Fi Direct, Bluetooth). \n***************************************Wired Media \nWired media refers to physical cables that transmit data using electrical or optical signals. \nThey are typically used for local area networks (LANs), data centers, and various \ntelecommunications applications. \nTypes of Wired Media: \n1. Twisted Pair Cable: \na. Description: Consists of pairs of insulated copper wires twisted together to \nreduce electromagnetic interference. \nb. Categories: \ni. Cat 5e: Supports speeds up to 1 Gbps for short distances. \nii. Cat 6/6a: Supports speeds up to 10 Gbps for distances up to 55 \nmeters (Cat 6) and up to 100 meters (Cat 6a). \nc. Applications: Commonly used in Ethernet networks. \n2. Coaxial Cable: \na. Description: Composed of a central conductor, insulating layer, metallic \nshield, and outer cover. \nb. Bandwidth: Higher bandwidth capacity than twisted pair cables. \nc. Applications: Used in cable television, internet connections, and some \n3. Fiber Optic Cable: \na. Description: Composed of thin strands of glass or plastic (optical fibers) \nthat transmit data as light signals. \ni. Single-mode Fiber: Suitable for long-distance communication (up to \n40 km or more) with higher bandwidth. \nii. Multi-mode Fiber: Suitable for shorter distances (up to 2 km) and \nlower bandwidth applications. \nc. Applications: High-speed internet connections, data centers, backbone \nAdvantages of Wired Media: \n• Higher Speeds: Generally provide faster data transfer rates compared to wireless \n• Lower Latency: Less prone to delays in transmission. \n• Security: More secure than wireless as physical access to cables is required to \nintercept data. \n• Reliability: Less susceptible to interference from external factors like \nelectromagnetic interference (EMI). \nDisadvantages of Wired Media: \n• Mobility: Limited mobility; devices must be physically connected to the network. \n• Installation Costs: Can be more expensive to install due to the need for cabling \nand infrastructure. \n• Physical Limitations: Limited by distance; signal quality can degrade over long \nWireless Media \nWireless media refers to data transmission methods that use electromagnetic waves to \ntransmit data without physical cables. Wireless networks are widely used for mobile \ncommunication, internet access, and local networking. \nTypes of Wireless Media: \n1. Radio Waves: \na. Description: Use radio frequency (RF) signals to transmit data over the air. \nb. Applications: Wi-Fi networks, Bluetooth devices, and cellular networks. \n2. Microwaves: \na. Description: Higher frequency than radio waves; used for point-to-point \ncommunication. \nb. Applications: Satellite communications, microwave relay links. \n3. Infrared: \na. Description: Uses infrared light for short-range communication. \nb. Applications: Remote controls, some wireless peripherals (e.g., wireless \na. Description: Uses visible light for data transmission. \nb. Applications: Emerging technology for high-speed wireless communication \nin environments where RF communication is not feasible. \nAdvantages of Wireless Media: \n• Mobility: Allows devices to connect to the network without being physically \n• Easy Installation: Generally easier and quicker to deploy since no cabling is \n• Flexibility: Supports a range of devices, including mobile phones, tablets, and \nDisadvantages of Wireless Media: \n• Speed Limitations: Generally slower than wired media due to factors like signal \ndegradation and interference. \n• Interference: Susceptible to interference from other devices, obstacles, and \nenvironmental factors. \n• Security Risks: More vulnerable to eavesdropping and unauthorized access if not \nproperly secured. \nWired Media \nWireless Media \nTwisted pair, coaxial, fiber \nRadio waves, microwaves, infrared, Li-\nGenerally higher speeds \nGenerally lower speeds \nLower latency \nHigher latency \nLimited mobility \nHigh mobility \nInstallation More complex, higher cost \nEasier, lower cost \nReliability \nMore reliable, less \ninterference \nLess reliable, more prone to \ninterference \nMore secure \nMore vulnerable to security threats \nBoth wired and wireless media play crucial roles in modern networking. The choice \nbetween them depends on specific use cases, performance requirements, and \nenvironmental considerations. \n************************************************************************8 \nTransmission Errors in Data Communication \nTransmission errors occur when data is corrupted during the transmission process from a \nsender to a receiver. These errors can result from various factors, including: \n• Noise: Interference from electrical devices, environmental conditions, or physical \nobstructions. \n• Signal Degradation: Loss of signal strength over long distances, especially in wired \ncommunications. \n• Interference: External electromagnetic interference from other devices or \n• Packet Loss: Loss of data packets due to network congestion or faulty hardware. \nTo ensure the integrity of transmitted data, error detection and correction mechanisms are \nError Detection \n1. Parity Check \no Let's say we have a 4-bit data sequence: 1011. \no Even Parity: The number of 1's in 1011 is 3 (odd). To make it even, we add a \nparity bit of 1. The transmitted data becomes 10111. \no Receiver's Check: The receiver gets 10111, counts the number of 1's (which \nis now 4), and determines that the parity is even, so it assumes there is no \no If the received sequence is 10011 (where one bit has changed), the number \nof 1's is 3 (odd). The receiver detects a discrepancy in parity, indicating an \n2. Checksum \no Consider a data packet consisting of two 8-bit bytes: 11010101 (213 in \ndecimal) and 01100010 (98 in decimal). \no Checksum Calculation: \n▪ Add the two bytes: \n1 00110111 (Carry bit is ignored, resulting in 01101111) \n- The checksum is the one's complement of the sum:  \n- One’s complement of `01101111` is `10010000` (144 in decimal). \no Transmitted Data: The packet sent would be 11010101, 01100010, and \no Receiver's Check: \n▪ The receiver adds the two data bytes and the received checksum: \n1 00110111 (Again, ignoring the carry bit) \n- If the result is `11111111` (all bits 1), the data is considered \nvalid. If it’s anything else, an error is detected. \n3. Cyclic Redundancy Check (CRC) \no Suppose we want to send the data 1101 using a divisor polynomial 1011. \no Data: 1101 \no Append Zeros: Append n-1 zeros (where n is the degree of the divisor) to the \ndata. Thus, 1101000. \no Division: \n▪ Perform binary division of 1101000 by 1011. \n▪ The remainder is calculated as 0010. \no Transmitted Data: The transmitted message is 11010010 (original data + \no Receiver's Check: \n▪ The receiver performs the same division with the received data. If the \nremainder is 0, the data is assumed to be correct; if not, an error is \nError Correction \n1. Hamming Code \no Consider a 4-bit data sequence: 1011. \no Step 1: Determine the number of redundant bits. For 4 data bits, you need 3 \nredundant bits (r bits satisfy 2r≥m+r+12^r \\geq m + r + 12r≥m+r+1 where \nm is data bits).",
    "unit": "Unit 2",
    "source_type": "notes",
    "book_priority": 0,
    "source_file": "CN unit 2",
    "chunk_id": "CN unit 2_chunk_0"
  },
  {
    "text": "UNIT I DATA COMMUNICATION AND NETWORKING 9 \nData communication systems – Components and their functions - Building networks – \nHosts and - Networking devices – Switched Networks and Broadcast Networks – \nTransmission medium - Networking Devices: Hubs, Bridges, Switches, Routers, and \nGateways - Edge, Access and Core - networks – Role of software and hardware in \nnetworking – Layered Architecture – OSI and TCP/IP - Reference Models. \nHere’s an in-depth exploration of the key concepts in data communication systems and \nnetworking: \n1. Components of Data Communication Systems \n• Message: This is the information being transmitted between devices. It could be in \nthe form of text, audio, video, or other data types. Effective communication \ndepends on the accuracy and speed at which messages are sent and received. \n• Sender: The originator of the message, such as a computer or mobile device, that \nuses a transmission medium to send data. \n• Receiver: The destination of the message. It interprets the incoming data and \nprovides it to the end user or system application. \n• Transmission Medium: The channel through which the message is sent from \nsender to receiver. This can include: \no Wired Media: Like twisted-pair cables, coaxial cables, and fiber optic \ncables, providing high-speed, reliable connections. \no Wireless Media: Such as radio waves, microwaves, and infrared, enabling \nflexible and mobile communication. \n• Protocol: A set of standardized rules that govern the communication between \ndevices, ensuring the data is formatted, transmitted, and received correctly. \nExamples include TCP/IP, HTTP, and FTP. \n2. Building Networks \n• Networks are systems that interconnect devices to allow for the exchange of data. \nNetworks can be as small as a few devices in a room or as large as the global \n• Hosts: These are devices connected to a network, including computers, \nsmartphones, servers, and IoT devices. Hosts typically generate, receive, or \nprocess data on a network. \n• Networking Devices: Devices like routers, switches, and hubs enable data \nmovement across networks by directing traffic efficiently and maintaining \nconnectivity among hosts. \n3. Switched Networks and Broadcast Networks \n• Switched Networks: Utilize switches to route data to its intended recipient using \nMAC addresses. In switched networks, only the target device receives the data \npacket, leading to efficient use of bandwidth and enhanced privacy. \n• Broadcast Networks: Send data packets to all devices on the network segment. \nEach device checks the packet and processes it if it’s addressed to them. While \nsimple to implement, broadcast networks can be inefficient on larger scales due to \nunnecessary data processing. \n4. Transmission Medium \n• Transmission mediums serve as the conduits for data communication and can vary \nwidely depending on the network’s requirements. \n• Wired Transmission: \no Twisted Pair Cables: Common in local area networks (LANs), they consist of \npairs of wires twisted together to reduce electromagnetic interference. \no Coaxial Cables: Used for cable TV and internet, coaxial cables provide a \nshielded medium that reduces interference and supports high-frequency \no Fiber Optic Cables: Provide very high bandwidth and long-distance \ncommunication using light signals, widely used for backbone networks and \nlong-distance telecommunications. \n• Wireless Transmission: \no Radio Waves: Enable wireless communication for Wi-Fi, cellular networks, \nand Bluetooth, offering flexibility and mobility. \no Microwaves: Used for satellite communication and point-to-point links, \noffering higher bandwidth but requiring line-of-sight communication. \n5. Networking Devices \n• Hubs: Basic devices that connect multiple devices in a network but broadcast all \nincoming data to every device, creating potential congestion. \n• Bridges: Devices that connect different segments of a network, filtering traffic \nbased on MAC addresses to reduce congestion and improve network performance. \n• Switches: Advanced versions of hubs, switches use MAC addresses to direct data \npackets only to the intended device, enhancing efficiency. \n• Routers: Operate at the network layer, forwarding data between different networks \nbased on IP addresses and determining the best path for data. \n• Gateways: Connect networks using different protocols, translating data from one \nprotocol to another to enable communication across different network \narchitectures. \n6. Network Types \n• Edge Networks: These provide connectivity for end devices, often serving as the \npoint of entry to larger networks, such as internet service providers. \n• Access Networks: Typically provide local connectivity, such as a Wi-Fi network, \nallowing users to connect to other network resources and services. \n• Core Networks: Provide high-speed, reliable connections within the larger network, \noften functioning as the backbone for other network types and enabling long-\ndistance data transfer. \n7. Role of Software and Hardware in Networking \n• Hardware: The physical infrastructure required for networking, including devices \nlike routers, switches, and cables, that support data movement. \n• Software: Includes operating systems, network protocols, and applications that \ncontrol how data is formatted, transmitted, and processed, ensuring effective \ncommunication between devices. \n8. Layered Architecture – OSI and TCP/IP Models \n• OSI Model: \no A seven-layer model designed to standardize network functions. The layers \ninclude Physical, Data Link, Network, Transport, Session, Presentation, and \nApplication. \no Each layer performs specific functions, with data moving through each layer, \nundergoing transformations and encapsulations to ensure successful \n• TCP/IP Model: \no A simplified four-layer model used primarily for internet-based \ncommunications. The layers include Network Access, Internet, Transport, \nand Application. \no The TCP/IP model focuses on practical implementation, with the Application \nlayer encompassing several OSI layers. \nIn summary, data communication systems involve a complex interplay of components, \ntechnologies, and protocols that ensure data is efficiently, reliably, and securely \nexchanged across various network infrastructures. By understanding each element’s role, \nwe can build and maintain robust networks capable of supporting modern communication \n************************************************************************************* \nTransmission mediums are essential components in any data communication system, as \nthey provide the pathway for data to travel between devices. These mediums can be \nbroadly classified into two categories: wired (guided) and wireless (unguided). Each \ncategory has various types, each with unique characteristics, advantages, and limitations. \nHere’s an in-depth look at different transmission mediums, \n1. Wired Transmission Mediums (Guided Media) \nWired transmission mediums involve physical cables that guide the data signals from the \nsender to the receiver. These include: \n• Twisted Pair Cable: \no Description: This cable consists of pairs of insulated copper wires twisted \ntogether to reduce electromagnetic interference. It is widely used in local \narea networks (LANs), telephone systems, and DSL connections. \n▪ Unshielded Twisted Pair (UTP): Common in Ethernet networks; \ninexpensive and easy to install. \n▪ Shielded Twisted Pair (STP): Contains additional shielding to reduce \ninterference, offering better performance but at a higher cost. \no Advantages: Inexpensive, easy to install, and widely available. \no Limitations: Susceptible to interference and signal attenuation over long \n• Coaxial Cable: \no Description: Coaxial cables have a central conductor surrounded by \ninsulation, a metallic shield, and an outer cover. They are commonly used for \ncable television, broadband internet, and other high-frequency signal \ntransmission. \no Advantages: Higher bandwidth than twisted pair, less interference, and \nbetter signal retention over long distances. \no Limitations: Bulkier than twisted pairs, more expensive, and less flexible to \n• Fiber Optic Cable: \no Description: Fiber optic cables transmit data as pulses of light through \nstrands of glass or plastic. They are used for high-speed data transmission, \nparticularly over long distances and in backbone networks. \n▪ Single-Mode Fiber (SMF): Transmits one light signal at a time, \nsuitable for long distances and high-bandwidth applications. \n▪ Multi-Mode Fiber (MMF): Allows multiple light signals to travel \nthrough the fiber, suitable for shorter distances and lower-cost \napplications. \no Advantages: Extremely high bandwidth, low signal loss, and immune to \nelectromagnetic interference. \no Limitations: Expensive to install and maintain, requires specialized \nequipment and expertise. \n2. Wireless Transmission Mediums (Unguided Media) \nWireless transmission mediums use electromagnetic waves to transmit data without \nphysical cables, offering greater flexibility and mobility. These include: \n• Radio Waves: \no Description: Radio waves can travel long distances and penetrate through \nwalls and obstacles, making them ideal for broadcast communications (e.g., \nAM/FM radio, TV) and mobile communications (e.g., Wi-Fi, cellular \no Advantages: Wide coverage area, good penetration, and suitable for mobile \ncommunication. \no Limitations: Susceptible to interference and eavesdropping; bandwidth is \ngenerally limited compared to wired options. \n• Microwaves: \no Description: Microwaves require a direct line of sight between the \ntransmitter and receiver and are commonly used for satellite \ncommunications and point-to-point links. \n▪ Terrestrial Microwaves: Used for short-range communication \nbetween ground stations. \n▪ Satellite Microwaves: Used for long-range communication, \nbouncing signals off satellites. \no Advantages: High bandwidth, suitable for long-distance communication \nwith minimal delays. \no Limitations: Requires line of sight, affected by weather conditions, and more \nexpensive to deploy. \n• Infrared (IR): \no Description: Infrared waves are used for short-range communication, such \nas remote controls and some wireless devices. They require line of sight and \nare primarily used for indoor applications. \no Advantages: Secure over short distances, inexpensive, and not affected by \nradio interference. \no Limitations: Limited range, requires direct line of sight, and affected by \nobstructions and ambient light. \n3. Comparing Wired vs. Wireless Transmission Mediums \n• Wired Media: \no Offers higher reliability, bandwidth, and security. \no Best suited for fixed installations where high data rates are required. \n• Wireless Media: \no Provides greater flexibility and mobility. \no Suited for areas where physical cabling is impractical or costly. \no Ideal for mobile devices and applications where users need to remain \nconnected on the move. \nIn summary, choosing the right transmission medium depends on various factors such as \nthe required bandwidth, distance, cost, security, and the specific application or \nenvironment. Wired mediums are generally more reliable and suitable for high-speed, fixed \ninstallations, while wireless mediums provide flexibility and ease of access, particularly for \nmobile and temporary setups. Each medium plays a vital role in enabling efficient and \neffective data communication across different types of networks. \n****************************************************************************** \nNetworking devices are essential components in a network that facilitate communication \nbetween different devices. Key networking devices include: \na. A hub is a basic networking device that connects multiple computers in a \nb. It broadcasts data to all devices connected, regardless of the destination, \nmaking it less efficient. \nc. Works at the physical layer (Layer 1) of the OSI model. \na. A bridge connects two or more network segments and filters traffic between \nthem based on MAC addresses. \nb. It helps reduce traffic by only forwarding data to the correct destination \nc. Operates at the data link layer (Layer 2). \na. A switch is more advanced than a hub. It connects devices in a network and \ndirects data to the correct device using MAC addresses. \nb. Unlike a hub, a switch only sends data to the specific device it’s intended for, \nimproving efficiency. \nc. Also operates at the data link layer (Layer 2). \na. A router connects different networks and routes data between them. It uses \nIP addresses to determine the best path for data to travel. \nb. Operates at the network layer (Layer 3). \n5. Gateway: \na. A gateway is a device that connects two different networks, often with \ndifferent protocols. It translates data from one format to another. \nb. Works at multiple layers, typically including the application layer (Layer 7), \nbut it can involve other layers as well depending on its function. \nThese devices play distinct roles in managing data flow in a network, enhancing efficiency \nand security. \n*********************************************************************************** \nIn networking, Edge, Access, and Core refer to different layers or segments of a network \ninfrastructure, each serving specific functions: \n1. Edge Network: \na. The edge network is the outermost part of a network, where it interacts with \nexternal networks, such as the internet or other organizations' networks. \nb. Devices at the edge often include routers, firewalls, and edge switches. \nc. This layer handles external traffic and secures the internal network from \noutside threats. \n2. Access Network: \na. The access network connects end-user devices, like computers, phones, \nand IoT devices, to the network. \nb. It includes switches, wireless access points, and sometimes hubs that allow \nusers to connect to resources within the network. \nc. This is where most users interact with the network. \n3. Core Network: \na. The core network is the backbone of the entire network, providing high-speed \nand reliable data transmission between different parts of the network. \nb. It connects multiple access networks and ensures that data flows smoothly \nwithin and across the network. \nc. Core routers and switches are used in this layer for fast, efficient data \nforwarding and routing. \nThese layers work together to create a scalable, efficient, and secure network \ninfrastructure. \n**********************************************************************************88 \nLayered Architecture – OSI and TCP/IP Models \nThe OSI (Open Systems Interconnection) and TCP/IP (Transmission Control \nProtocol/Internet Protocol) models are frameworks used to understand and design \nnetwork communication. \nOSI Model (7 Layers): \nThe OSI model is a theoretical framework that standardizes networking functions into \nseven distinct layers: \n1. Physical Layer: Transmits raw bitstreams over a physical medium (e.g., cables, \nradio signals). \n2. Data Link Layer: Ensures reliable data transfer between two directly connected \nnodes; manages MAC addresses and error detection. \n3. Network Layer: Handles packet routing and forwarding across different networks \nusing IP addresses. \n4. Transport Layer: Ensures complete data transfer with error detection and \ncorrection, flow control, and retransmission (e.g., TCP, UDP). \n5. Session Layer: Manages sessions or connections between applications. \n6. Presentation Layer: Translates data formats (e.g., encryption, compression) to \nensure proper data interpretation. \n7. Application Layer: Provides network services directly to user applications (e.g., \nHTTP, FTP, SMTP). \nTCP/IP Model (4 Layers): \nThe TCP/IP model, a more practical framework, forms the basis of the internet. It has four \n1. Network Interface (Link) Layer: Combines the physical and data link layers from \nthe OSI model. It handles hardware addressing and the physical transmission of \n2. Internet Layer: Corresponds to the OSI's network layer and is responsible for \nrouting and forwarding data across networks using IP. \n3. Transport Layer: Similar to OSI's transport layer, it manages end-to-end \ncommunication and data integrity (e.g., TCP, UDP). \n4. Application Layer: Combines the OSI's session, presentation, and application \nlayers, providing application-level services like web browsing and email. \nKey Differences: \n• The OSI model is more detailed (7 layers), while the TCP/IP model is more simplified \n(4 layers). \n• The OSI model is a reference tool, while the TCP/IP model is based on actual \nprotocols used on the internet. \n*******************************************************************************88 \nBasic Understanding: \n1. What is the OSI model and why is it important in networking? \na. The OSI model is a 7-layer framework that standardizes the functions of a \nnetwork system into layers. It helps understand and troubleshoot network \noperations by breaking them into clear, functional segments. \n2. Explain the key functions of each layer in the OSI model. \na. Physical: Transmits raw bits over physical mediums. \nb. Data Link: Ensures node-to-node data transfer and error detection. \nc. Network: Handles packet routing and forwarding using IP addresses. \nd. Transport: Ensures reliable data transfer with flow control, error detection \n(e.g., TCP/UDP). \ne. Session: Manages and controls communication sessions. \nf. Presentation: Formats data, handles encryption and compression. \ng. Application: Provides network services to applications (e.g., HTTP, FTP). \n3. What is the TCP/IP model? How does it differ from the OSI model? \na. The TCP/IP model is a 4-layer framework used to describe internet \ncommunication. It combines OSI layers into broader categories and is based \non actual protocols (e.g., TCP, IP), unlike the theoretical OSI model. \n4. List the layers of the OSI and TCP/IP models and their key protocols. \na. OSI: Physical, Data Link, Network, Transport, Session, Presentation, \nApplication. \nb. TCP/IP: Link (Ethernet), Internet (IP), Transport (TCP/UDP), Application \n(HTTP, FTP). \n5. Why was the OSI model developed, and what advantages does it offer? \na. The OSI model was developed to standardize networking protocols and \nfacilitate multi-vendor equipment interoperability. It allows for easier \ntroubleshooting and modular development. \n6. Which layer of the OSI model handles routing? \na. The Network layer handles routing by determining the best path for data \npackets using IP addresses. \nComparison and Differences: \n1. Compare and contrast the OSI and TCP/IP models. \na. The OSI model has 7 layers, focusing on a clear separation of functions, \nwhile the TCP/IP model has 4 layers and is based on practical protocols used \nin internet communication. TCP/IP combines the OSI’s Presentation, \nSession, and Application layers into one Application layer. \n2. Why does the TCP/IP model have fewer layers than the OSI model? \na. The TCP/IP model is more simplified, grouping several OSI layers into \nbroader categories, reflecting practical implementations rather than \ntheoretical distinctions. \n3. What is the role of the transport layer in both the OSI and TCP/IP models? \na. In both models, the transport layer ensures end-to-end communication, \ndata integrity, error correction, and flow control using protocols like TCP and \n4. Which OSI layers are combined into a single layer in the TCP/IP model? \na. The Application, Presentation, and Session layers of the OSI model are \ncombined into the Application layer in the TCP/IP model. \n5. In which layer of the OSI model does encryption occur, and how is it handled in \nthe TCP/IP model? \na. Encryption typically occurs at the Presentation layer in the OSI model. In \nthe TCP/IP model, encryption is often implemented at the Application layer \n(e.g., HTTPS). \nFunctions and Protocols: \n1. What protocols operate at the transport layer of the TCP/IP model? \na. TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) \noperate at the transport layer, providing reliable and unreliable \ncommunication, respectively. \n2. Explain the function of the data link layer and its role in error detection. \na. The data link layer ensures reliable data transfer between two directly \nconnected devices, using MAC addresses to identify devices. It also \nperforms error detection using checksums and error-correcting codes. \n3. How does the network layer in the OSI model handle packet routing? \na. The network layer uses IP addresses to determine the best route for packets \nbetween different networks. Routers handle this task, ensuring packets \nreach their destination across interconnected networks. \n4. What are the functions of the application layer in the TCP/IP model? \na. The application layer provides network services to applications. It manages \nprotocols like HTTP (for web browsing), FTP (for file transfers), and SMTP (for \nScenario-Based: \n1. In which layer would you troubleshoot network addressing issues, and why? \na. Network addressing issues are typically troubleshooted at the Network \nlayer, as it handles logical addressing using IP addresses. \n2. If two computers are unable to communicate due to differences in data \nrepresentation, which OSI layer is responsible for resolving this? \na. The Presentation layer is responsible for resolving differences in data \nformats, encryption, and compression between systems. \n3. How does the transport layer handle reliability and flow control in data \ntransmission? \na. The transport layer (specifically TCP) ensures reliability through error",
    "unit": "Unit 1",
    "source_type": "notes",
    "book_priority": 0,
    "source_file": "CNS unit 1",
    "chunk_id": "CNS unit 1_chunk_0"
  },
  {
    "text": "Topics covered\n• Data Link Layer Control Protocols\nFlow control\nError control\nFlow Control\n• ensure sending entity does not overwhelm receiving entity\n• by preventing buffer overflow\n• influenced by:\n• transmission time\n• time taken to emit all bits into medium\n• propagation time\n• time for a bit to traverse the link\n• assume here no errors but varying delays\nModel of Frame Transmission\nStop and Wait Flow Control\n• source transmits frame\n• destination receives frame and replies with \nacknowledgement (ACK)\n• source waits for ACK before sending next\n• destination can stop flow by not send ACK\n• works well for a few large frames\n• Stop and wait becomes inadequate if large block of data \nis split into small frames \nStop and Wait Link Utilization\nSliding Windows Flow Control\n• allows multiple numbered frames to be in transit\n• receiver has buffer W long\n• transmitter sends up to W frames without ACK\n• ACK includes number of next frame expected\n• sequence number is bounded by size of field (k)\n• frames are numbered modulo 2k\n• giving max window size of up to 2k - 1\n• receiver can ack frames without permitting further transmission \n(Receive Not Ready)\n• must send a normal acknowledge to resume\n• if have full-duplex link, can piggyback ACks\nSliding Window Diagram\nSliding Window Example\n2. Error Control - detection and correction of errors \n• 2 types of errors are:\n• lost frames : fails to arrive at rxr\n• damaged frames : recognizable frame arrives..but bits damaged\n• common techniques for error control are based on :\n• error detection",
    "unit": "Unit 2",
    "source_type": "notes",
    "book_priority": 0,
    "source_file": "DATA LINK CONTROL PROTOCOLS",
    "chunk_id": "DATA LINK CONTROL PROTOCOLS_chunk_0"
  },
  {
    "text": "5: DataLink Layer\nData Link Layer\nComputer Networking: \nA Top Down Approach \nFeaturing the Internet, \n2nd edition. \nJim Kurose, Keith Ross\nAddison-Wesley, July \nA note on the use of these ppt slides:\nWe’re making these slides freely available to all (faculty, students, readers). \nThey’re in PowerPoint form so you can add, modify, and delete slides  \n(including this one) and slide content to suit your needs. They obviously \nrepresent a lot of work on our part. In return for use, we only ask the \n\u0001 If you use these slides (e.g., in a class) in substantially unaltered form, \nthat you mention their source (after all, we’d like people to use our book!)\n\u0001 If you post any slides in substantially unaltered form on a www site, that \nyou note that they are adapted from (or perhaps identical to) our slides, and \nChapter 5: The Data Link Layer\n❒understand principles behind data link layer \n❍error detection, correction\n❍sharing a broadcast channel: multiple access\n❍link layer addressing\n❍reliable data transfer, flow control: done!\n❒instantiation and implementation of various link \nlayer technologies\n5: DataLink Layer\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction \n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer\nLink Layer: Introduction\nSome terminology:\n❒hosts and routers are nodes\n(bridges and switches too)\n❒communication channels that \nconnect adjacent nodes along \ncommunication path are links\n❍wired links\n❍wireless links\n❒2-PDU is a frame,\nencapsulates datagram\ndata-link layer has responsibility of \ntransferring datagram from one node \nto adjacent node over a link\n5: DataLink Layer\nLink layer: context\n❒Datagram transferred by \ndifferent link protocols \nover different links:\n❍e.g., Ethernet on first link, \nframe relay on \nintermediate links, 802.11 \non last link\n❒Each  link protocol \nprovides different \n❍e.g., may or may not \nprovide rdt over link\ntransportation analogy\n❒trip from Princeton to \n❍limo: Princeton to JFK\n❍plane: JFK to Geneva\n❍train: Geneva to Lausanne\n❒tourist = datagram\n❒transport segment = \ncommunication link\n❒transportation mode = \nlink layer protocol\n❒travel agent = routing \n5: DataLink Layer\nLink Layer Services\n❒Framing, link access:\n❍encapsulate datagram into frame, adding header, trailer\n❍channel access if shared medium\n❍‘physical addresses’ used in frame headers to identify \nsource, dest  \n• different from IP address!\n❒Reliable delivery between adjacent nodes\n❍we learned how to do this already (chapter 3)!\n❍seldom used on low bit error link (fiber, some twisted \n❍wireless links: high error rates\n• Q: why both link-level and end-end reliability?\n5: DataLink Layer\nLink Layer Services (more)\n❒Flow Control:\n❍pacing between adjacent sending and receiving nodes\n❒Error Detection:\n❍errors caused by signal attenuation, noise. \n❍receiver detects presence of errors: \n• signals sender for retransmission or drops frame \n❒Error Correction:\n❍receiver identifies and corrects bit error(s) without \nresorting to retransmission\n❒Half-duplex and full-duplex\n❍with half duplex, nodes at both ends of link can transmit, \nbut not at same time\n5: DataLink Layer\nAdaptors Communicating\n❒link layer implemented in \n“adaptor” (aka NIC)\n❍Ethernet card, PCMCI \ncard, 802.11 card\n❒sending side:\n❍encapsulates datagram in \n❍adds error checking bits, \nrdt, flow control, etc.\n❒receiving side\n❍looks for errors, rdt, flow \ncontrol, etc\n❍extracts datagram, passes \nto rcving node\n❒adapter is semi-\n❒link & physical layers\nlink layer protocol\n5: DataLink Layer\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction\n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer 5a-10\nError Detection\nEDC= Error Detection and Correction bits (redundancy)\nD    = Data protected by error checking, may include header fields \n• Error detection not 100% reliable!\n• protocol may miss some errors, but rarely\n• larger EDC field yields better detection and correction\n5: DataLink Layer 5a-11\nParity Checking\nSingle Bit Parity:\nDetect single bit errors\nTwo Dimensional Bit Parity:\nDetect and correct single bit errors\n5: DataLink Layer 5a-12\nInternet checksum\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction \n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer 5a-16\nMultiple Access Links and Protocols\nTwo types of “links”:\n❒point-to-point\n❍PPP for dial-up access\n❍point-to-point link between Ethernet switch and host\n❒broadcast (shared wire or medium)\n❍traditional Ethernet\n❍upstream HFC\n❍802.11 wireless LAN\n5: DataLink Layer 5a-17\nMultiple Access protocols\n❒single shared broadcast channel \n❒two or more simultaneous transmissions by nodes: \ninterference \n❍only one node can send successfully at a time \nmultiple access protocol\n❒distributed algorithm that determines how nodes \nshare channel, i.e., determine when node can transmit\n❒communication about channel sharing must use channel \n❒what to look for in multiple access protocols: \n5: DataLink Layer 5a-18\nIdeal Mulitple Access Protocol\nBroadcast channel of rate R bps\n1. When one node wants to transmit, it can send at \n2. When M nodes want to transmit, each can send at \naverage rate R/M\n3. Fully decentralized:\n❍no special node to coordinate transmissions\n❍no synchronization of clocks, slots\n5: DataLink Layer 5a-19\nMAC Protocols: a taxonomy\nThree broad classes:\n❒Channel Partitioning\n❍divide channel into smaller “pieces” (time slots, \nfrequency, code)\n❍allocate piece to node for exclusive use\n❒Random Access\n❍channel not divided, allow collisions\n❍“recover” from collisions\n❒“Taking turns”\n❍tightly coordinate shared access to avoid collisions\n5: DataLink Layer 5a-20\nChannel Partitioning MAC protocols: TDMA\nTDMA: time division multiple access\n❒access to channel in \"rounds\" \n❒each station gets fixed length slot (length = pkt \ntrans time) in each round \n❒unused slots go idle \n❒example: 6-station LAN, 1,3,4 have pkt, slots 2,5,6 \n5: DataLink Layer 5a-21\nChannel Partitioning MAC protocols: FDMA\nFDMA: frequency division multiple access\n❒channel spectrum divided into frequency bands\n❒each station assigned fixed frequency band\n❒unused transmission time in frequency bands go idle \n❒example: 6-station LAN, 1,3,4 have pkt, frequency \nbands 2,5,6 idle \nfrequency bands\n5: DataLink Layer 5a-22\nChannel Partitioning (CDMA)\nCDMA (Code Division Multiple Access)\n❒unique “code” assigned to each user; i.e., code set partitioning\n❒used mostly in wireless broadcast channels (cellular, satellite,\n❒all users share same frequency, but each user has own \n“chipping” sequence (i.e., code) to encode data\n❒encoded signal = (original data) X (chipping sequence)\n❒decoding: inner-product of encoded signal and chipping \n❒allows multiple users to “coexist” and transmit simultaneously \nwith minimal interference (if codes are “orthogonal”)\n5: DataLink Layer 5a-23\nCDMA Encode/Decode\n5: DataLink Layer 5a-24\nCDMA: two-sender interference\n5: DataLink Layer 5a-25\nRandom Access Protocols\n❒When node has packet to send\n❍transmit at full channel data rate R.\n❍no a priori coordination among nodes\n❒two or more transmitting nodes -> “collision”,\n❒random access MAC protocol specifies: \n❍how to detect collisions\n❍how to recover from collisions (e.g., via delayed \nretransmissions)\n❒Examples of random access MAC protocols:\n❍slotted ALOHA\n❍CSMA, CSMA/CD, CSMA/CA\n5: DataLink Layer 5a-26\nSlotted ALOHA\nAssumptions\n❒all frames same size\n❒time is divided into \nequal size slots, time to \ntransmit 1 frame\n❒nodes start to transmit \nframes only at \nbeginning of slots\n❒nodes are synchronized\n❒if 2 or more nodes \ntransmit in slot, all \nnodes detect collision\n❒when node obtains fresh \nframe, it transmits in next \n❒no collision, node can send \nnew frame in next slot\n❒if collision, node \nretransmits frame in each \nsubsequent slot with prob. \np until success\n5: DataLink Layer 5a-27\nSlotted ALOHA\n❒single active node can \ncontinuously transmit \nat full rate of channel\n❒highly decentralized: \nonly slots in nodes \nneed to be in sync\n❒collisions, wasting slots\n❒idle slots\n❒nodes may be able to \ndetect collision in less \nthan time to transmit \n5: DataLink Layer 5a-28\nSlotted Aloha efficiency\n❒Suppose N nodes with \nmany frames to send, \neach transmits in slot \nwith probability p\n❒prob that 1st node has \nsuccess in a slot\n= p(1-p)N-1\n❒prob that any node has \na success = Np(1-p)N-1\n❒For max efficiency \nwith N nodes, find p* \nthat maximizes \n❒For many nodes, take \nlimit of Np*(1-p*)N-1 \nas N goes to infinity, \ngives 1/e = .37\nEfficiency is the long-run \nfraction of successful slots \nwhen there’s many nodes, each \nwith many frames to send\nAt best: channel\nused for useful \ntransmissions 37%\n5: DataLink Layer 5a-29\nPure (unslotted) ALOHA\n❒unslotted Aloha: simpler, no synchronization\n❒when frame first arrives\n❍transmit immediately \n❒collision probability increases:\n❍frame sent at t0 collides with other frames sent in [t0-1,t0+1]\n5: DataLink Layer 5a-30\nPure Aloha efficiency\nP(success by given node) = P(node transmits) .\nP(no other node transmits in [p0-1,p0] .\nP(no other node transmits in [p0-1,p0] \n= p . (1-p)N-1 . (1-p)N-1\n= p . (1-p)2(N-1)\n… choosing optimum p and then letting n -> infty ...\n= 1/(2e) = .18 \nEven worse !\n5: DataLink Layer 5a-31\nCSMA (Carrier Sense Multiple Access)\nCSMA: listen before transmit:\n❒If channel sensed idle: transmit entire frame\n❒If channel sensed busy, defer transmission \n❒Human analogy: don’t interrupt others!\n5: DataLink Layer 5a-32\nCSMA collisions\ncollisions can still occur:\npropagation delay means \ntwo nodes may not hear\neach other’s transmission\nentire packet transmission \ntime wasted\nspatial layout of nodes \nrole of distance & propagation \ndelay in determining collision \nprobability\n5: DataLink Layer 5a-33\nCSMA/CD (Collision Detection)\nCSMA/CD: carrier sensing, deferral as in CSMA\n❍collisions detected within short time\n❍colliding transmissions aborted, reducing channel \n❒collision detection:\n❍easy in wired LANs: measure signal strengths, \ncompare transmitted, received signals\n❍difficult in wireless LANs: receiver shut off while \ntransmitting\n❒human analogy: the polite conversationalist \n5: DataLink Layer 5a-34\nCSMA/CD collision detection\n5: DataLink Layer 5a-35\n“Taking Turns” MAC protocols\nchannel partitioning MAC protocols:\n❍share channel efficiently and fairly at high load\n❍inefficient at low load: delay in channel access, \n1/N bandwidth allocated even if only 1 active \nRandom access MAC protocols\n❍efficient at low load: single node can fully \nutilize channel\n❍high load: collision overhead\n“taking turns” protocols\nlook for best of both worlds!\n5: DataLink Layer 5a-36\n“Taking Turns” MAC protocols\n❒master node \n“invites” slave nodes \nto transmit in turn\n❍polling overhead \n❍single point of \nfailure (master)\nToken passing:\n❒control token passed from \none node to next \nsequentially.\n❒token message\n❍token overhead \n❍single point of failure (token)\n5: DataLink Layer 5a-37\nSummary of MAC protocols\n❒What do you do with a shared media?\n❍Channel Partitioning, by time, frequency or code\n• Time Division,Code Division, Frequency Division\n❍Random partitioning (dynamic), \n• ALOHA, S-ALOHA, CSMA, CSMA/CD\n• carrier sensing: easy in some technologies (wire), hard \nin others (wireless)\n• CSMA/CD used in Ethernet\n❍Taking Turns\n• polling from a central site, token passing\n5: DataLink Layer 5a-38\nLAN technologies\nData link layer so far:\n❍services, error detection/correction, multiple \nNext: LAN technologies\n❍addressing\n❍hubs, bridges, switches\n5: DataLink Layer 5a-39\nLAN Addresses and ARP\n32-bit IP address:\n❒network-layer address\n❒used to get datagram to destination IP network \n(recall IP network definition)\nLAN (or MAC or physical or Ethernet) address: \n❒used to get datagram from one interface to another \nphysically-connected interface (same network)\n❒48 bit MAC address (for most LANs) \nburned in the adapter ROM\n5: DataLink Layer 5a-40\nLAN Addresses and ARP\nEach adapter on LAN has unique LAN address\n5: DataLink Layer 5a-41\nLAN Address (more)\n❒MAC address allocation administered by IEEE\n❒manufacturer buys portion of MAC address space \n(to assure uniqueness)\n(a) MAC address: like Social Security Number\n(b) IP address: like postal address\n❒MAC flat address  => portability \n❍can move LAN card from one LAN to another\n❒IP hierarchical address NOT portable\n❍depends on IP network to which node is attached\n5: DataLink Layer 5a-42\nRecall earlier routing discussion\nStarting at A, given IP \ndatagram addressed to B:\n❒look up net. address of B, find B \non same net. as A\n❒link layer send datagram to B \ninside link-layer frame\nframe source,\ndest address\ndatagram source,\ndest address\n5: DataLink Layer 5a-43\nARP: Address Resolution Protocol\n❒Each IP node (Host, \nRouter) on LAN has  \n❒ARP Table: IP/MAC \naddress mappings for \nsome LAN nodes\n< IP address; MAC address; TTL>\nTTL (Time To Live): time \nafter which address \nmapping will be forgotten \n(typically 20 min)\nQuestion: how to determine\nMAC address of B\nknowing B’s IP address?\n5: DataLink Layer 5a-44\nARP protocol\n❒A wants to send datagram \nto B, and A knows B’s IP \n❒Suppose B’s MAC address \nis not in A’s ARP table.\n❒A broadcasts ARP query \npacket, containing B's IP \n❍all machines on LAN \nreceive ARP query\n❒B receives ARP packet, \nreplies to A with its (B's) \nMAC address\n❍frame sent to A’s MAC \naddress (unicast)\n❒A caches (saves) IP-to-\nMAC address pair in its \nARP table until information \nbecomes old (times out) \n❍soft state: information \nthat times out (goes \naway) unless refreshed\n❒ARP is “plug-and-play”:\n❍nodes create their ARP \ntables without \nintervention from net \nadministrator\n5: DataLink Layer 5a-45\nRouting to another LAN\nwalkthrough: send datagram from A to B via R\nassume  A knows B IP address\n❒Two ARP tables in  router R, one for each IP \nnetwork (LAN)\n5: DataLink Layer 5a-46\n❒A creates datagram with source A, destination B \n❒A uses ARP to get R’s MAC address for 111.111.111.110\n❒A creates link-layer frame with R's MAC address as dest, \nframe contains A-to-B IP datagram\n❒A’s data link layer sends frame \n❒R’s data link layer receives frame \n❒R removes IP datagram from Ethernet frame, sees its \ndestined to B\n❒R uses ARP to get B’s physical layer address \n❒R creates frame containing A-to-B IP datagram sends to B\n5: DataLink Layer 5a-47\n“dominant” LAN technology: \n❒cheap $20 for 100Mbs!\n❒first widely used LAN technology\n❒Simpler, cheaper than token LANs and ATM\n❒Kept up with speed race: 10, 100, 1000 Mbps \nMetcalfe’s Ethernet\n5: DataLink Layer 5a-48\nEthernet Frame Structure\nSending adapter encapsulates IP datagram (or other \nnetwork layer protocol packet) in Ethernet frame\n❒7 bytes with pattern 10101010 followed by one \nbyte with pattern 10101011\n❒used to synchronize receiver, sender clock rates\n5: DataLink Layer 5a-49\nEthernet Frame Structure \n❒Addresses: 6 bytes\n❍if adapter receives frame with matching destination \naddress, or with broadcast address (eg ARP packet), it \npasses data in frame to net-layer protocol\n❍otherwise, adapter discards frame\n❒Type: indicates the higher layer protocol, mostly \nIP but others may be supported such as Novell \nIPX and AppleTalk)\n❒CRC: checked at receiver, if error is detected, the \nframe is simply dropped\n5: DataLink Layer 5a-50\nUnreliable, connectionless service\n❒Connectionless: No handshaking between sending \nand receiving adapter. \n❒Unreliable: receiving adapter doesn’t send acks or \nnacks to sending adapter\n❍stream of datagrams passed to network layer can have \n❍gaps will be filled if app is using TCP\n❍otherwise, app will see the gaps\n5: DataLink Layer 5a-51\nEthernet uses CSMA/CD\n❒adapter doesn’t transmit \nif it senses that some \nother adapter is \ntransmitting, that is, \ncarrier sense\n❒transmitting adapter \naborts when it senses \nthat another adapter is \ntransmitting, that is, \ncollision detection\n❒Before attempting a \nretransmission, \nadapter waits a \nrandom time, that is, \nrandom access\n5: DataLink Layer 5a-52\nEthernet CSMA/CD algorithm\n1. Adaptor gets datagram \nfrom and creates frame\n2. If adapter senses channel \nidle, it starts to transmit \nframe. If it senses \nchannel busy, waits until \nchannel idle and then \n3. If adapter transmits \nentire frame without \ndetecting another \ntransmission, the adapter \nis done with frame !\n4. If adapter detects \nanother transmission while \ntransmitting,  aborts and \nsends jam signal\n5. After aborting, adapter \nenters exponential \nbackoff: after the mth\ncollision, adapter chooses \na K at random from \n{0,1,2,…,2m-1}. Adapter \nwaits K*512 bit times and \nreturns to Step 2\n5: DataLink Layer 5a-53\nEthernet’s CSMA/CD (more)\nJam Signal: make sure all \nother transmitters are \naware of collision; 48 bits;\nBit time: .1 microsec for 10 \nMbps Ethernet ;\nfor K=1023, wait time is \nabout 50 msec\nExponential Backoff:\n❒Goal: adapt retransmission \nattempts to estimated \ncurrent load\n❍heavy load: random wait \nwill be longer\n❒first collision: choose K \nfrom {0,1}; delay is K x 512 \nbit transmission times\n❒after second collision: \nchoose K from {0,1,2,3}…\n❒after ten collisions, choose \nK from {0,1,2,3,4,…,1023}\nSee/interact with Java\napplet on AWL Web site:\nhighly recommended !\n5: DataLink Layer 5a-54\nCSMA/CD efficiency\n❒Tprop = max prop between 2 nodes in LAN\n❒ttrans = time to transmit max-size frame\n❒Efficiency goes to 1 as tprop goes to 0\n❒Goes to 1 as ttrans goes to infinity\n❒Much better than ALOHA, but still decentralized, \nsimple, and cheap\n5: DataLink Layer 5a-55\nEthernet Technologies: 10Base2\n❒10: 10Mbps; 2: under 200 meters max  cable length\n❒thin coaxial cable in a bus topology\n❒repeaters used to connect up to multiple segments\n❒repeater repeats bits it hears on one interface to \nits other interfaces: physical layer device only!\n❒has become a legacy technology\n5: DataLink Layer 5a-56\n10BaseT and 100BaseT\n❒10/100 Mbps rate; latter called “fast ethernet”\n❒T stands for Twisted Pair\n❒Nodes connect to a hub: “star topology”; 100 m \nmax distance between nodes and hub\n❒Hubs are essentially physical-layer repeaters:\n❍bits coming in one link go out all other links\n❍no frame buffering\n❍no CSMA/CD at hub: adapters detect collisions\n❍provides net management functionality\n5: DataLink Layer 5a-57\nManchester encoding\n❒Used in 10BaseT, 10Base2\n❒Each bit has a transition\n❒Allows clocks in sending and receiving nodes to \nsynchronize to each other\n❍no need for a centralized, global clock among nodes!\n❒Hey, this is physical-layer stuff!\n5: DataLink Layer 5a-58\nGbit Ethernet\n❒use standard Ethernet frame format\n❒allows for point-to-point links and shared \nbroadcast channels\n❒in shared mode, CSMA/CD is used; short distances \nbetween nodes to be efficient\n❒uses hubs, called here “Buffered Distributors”\n❒Full-Duplex at 1 Gbps for point-to-point links\n❒10 Gbps now !\n5: DataLink Layer 5a-59\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction \n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer 5a-60\nInterconnecting LAN segments\n❍Remark: switches are essentially multi-port \n❍What we say about bridges also holds for \n5: DataLink Layer 5a-61\nInterconnecting with hubs\n❒Backbone hub interconnects LAN segments\n❒Extends max distance between nodes\n❒But individual segment collision domains become one \nlarge collision domain\n❍if a node in CS and a node EE transmit at same time: collision\n❒Can’t interconnect 10BaseT & 100BaseT\n5: DataLink Layer 5a-62\n❒Link layer device\n❍stores and forwards Ethernet frames\n❍examines frame header and selectively\nforwards  frame based on MAC dest address\n❍when frame is to be forwarded on segment, \nuses CSMA/CD to access segment\n❒transparent\n❍hosts are unaware of presence of bridges\n❒plug-and-play, self-learning\n❍bridges do not need to be configured\n5: DataLink Layer 5a-63\nBridges: traffic isolation\n❒Bridge installation breaks LAN into LAN segments\n❒bridges filter packets:\n❍same-LAN-segment frames not usually \nforwarded onto other LAN segments\n❍segments become separate collision  domains\nLAN (IP network)\nLAN segment\nLAN segment\n5: DataLink Layer 5a-64\nHow do determine to which LAN segment to \nforward frame?\n• Looks like a routing problem...\n5: DataLink Layer 5a-65\nSelf learning\n❒A bridge has a bridge table\n❒entry in bridge table: \n❍(Node LAN Address, Bridge Interface, Time Stamp)\n❍stale entries in table dropped (TTL can be 60 min) \n❒bridges learn which hosts can be reached through \nwhich interfaces\n❍when frame received, bridge “learns”  location of \nsender: incoming LAN segment\n❍records sender/location pair in bridge table\n5: DataLink Layer 5a-66\nFiltering/Forwarding\nWhen bridge receives a frame:\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction \n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer 5a-81\nIEEE 802.11 Wireless LAN\n❍2.4-5 GHz unlicensed \nradio spectrum\n❍up to 11 Mbps\n❍direct sequence spread \nspectrum (DSSS) in \nphysical layer\n• all hosts use same \nchipping code\n❍widely deployed, using \nbase stations\n❍5-6 GHz range\n❍up to 54 Mbps\n❍2.4-5 GHz range\n❍up to 54 Mbps\n❒All use CSMA/CA for \nmultiple access\n❒All have base-station \nand ad-hoc network \n5: DataLink Layer 5a-82\nBase station approach\n❒Wireless host communicates with a base station\n❍base station = access point (AP)\n❒Basic Service Set (BSS) (a.k.a. “cell”) contains:\n❍wireless hosts\n❍access point (AP): base station\n❒BSSs combined to form distribution system (DS)\n5: DataLink Layer 5a-83\nAd Hoc Network approach\n❒No AP (i.e., base station)\n❒wireless hosts communicate with each other\n❍to get packet from wireless host A to B may \nneed to route through wireless hosts X,Y,Z\n❒Applications:\n❍“laptop” meeting in conference room, car\n❍interconnection of “personal” devices\n❍battlefield \n❒IETF MANET \n(Mobile Ad hoc Networks) \nworking group \n5: DataLink Layer 5a-84\nIEEE 802.11: multiple access\n❒Collision if 2 or more nodes transmit at same time\n❒CSMA makes sense:\n❍get all the bandwidth if you’re the only one transmitting\n❍shouldn’t cause a collision if you sense another transmission\n❒Collision detection doesn’t work: hidden terminal \n5: DataLink Layer 5a-85\nIEEE 802.11 MAC Protocol: CSMA/CA\n802.11 CSMA: sender\n- if sense channel idle for \nthen transmit entire frame \n(no collision detection)\n-if sense channel busy \nthen binary backoff\n802.11 CSMA receiver\n- if received OK\nreturn ACK after SIFS\n(ACK is needed due to \nhidden terminal problem) \n5: DataLink Layer 5a-86\nCollision avoidance mechanisms\n❍two nodes, hidden from each other, transmit complete \nframes to base station\n❍wasted bandwidth for long duration !\n❍small reservation packets\n❍nodes track reservation interval  with internal \n“network allocation vector” (NAV)\n5: DataLink Layer 5a-87\nCollision Avoidance: RTS-CTS \n❒sender transmits short \nRTS (request to send) \npacket: indicates \nduration of transmission\n❒receiver replies with \nshort CTS (clear to send) \n❍notifying (possibly hidden) \n❒hidden nodes will  not \ntransmit for specified \nduration: NAV\n5: DataLink Layer 5a-88\nCollision Avoidance: RTS-CTS \n❒RTS and CTS short:\n❍collisions less likely, of \nshorter duration\n❍end result similar to \ncollision detection\n❒IEEE 802.11 allows:\n❍CSMA/CA: reservations\n❍polling from AP\n5: DataLink Layer 5a-89\nA word about Bluetooth\n❒Low-power, small radius, \nwireless networking \n❍10-100 meters\n❒omnidirectional\n❍not line-of-sight infrared\n❒Interconnects gadgets\n❒2.4-2.5 GHz unlicensed \n❒up to 721 kbps\n❒Interference from \nwireless LANs, digital \ncordless phones, \nmicrowave ovens:\n❍frequency hopping helps\n❒MAC protocol supports:\n❍error correction\n❒Each node has a 12-bit \n5: DataLink Layer 5a-90\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction \n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer 5a-91\nPoint to Point Data Link Control\n❒one sender, one receiver, one link: easier than \nbroadcast link:\n❍no Media Access Control\n❍no need for explicit MAC addressing\n❍e.g., dialup link, ISDN line\n❒popular  point-to-point DLC protocols:\n❍PPP (point-to-point protocol)\n❍HDLC: High level data link control (Data link \nused to be considered “high layer” in protocol \n5: DataLink Layer 5a-92\nPPP Design Requirements [RFC 1557]\n❒packet framing: encapsulation of network-layer \ndatagram in data link frame \n❍carry network layer data of any network layer \nprotocol (not just IP) at same time\n❍ability to demultiplex upwards\n❒bit transparency: must carry any bit pattern in the \n❒error detection (no correction)\n❒connection liveness: detect, signal link failure to \nnetwork layer\n❒network layer address negotiation: endpoint can \nlearn/configure each other’s network address\n5: DataLink Layer 5a-93\nPPP non-requirements\n❒no error correction/recovery\n❒no flow control\n❒out of order delivery OK \n❒no need to support multipoint links (e.g., polling)\nError recovery, flow control, data re-ordering \nall relegated to higher layers!\n5: DataLink Layer 5a-94\nPPP Data Frame\n❒Flag: delimiter (framing)\n❒Address: does nothing (only one option)\n❒Control: does nothing; in the future possible \nmultiple control fields\n❒Protocol: upper layer protocol to which frame \ndelivered (eg, PPP-LCP, IP, IPCP, etc) \n5: DataLink Layer 5a-95\nPPP Data Frame\n❒info: upper layer data being carried\n❒check: cyclic redundancy check for error \n5: DataLink Layer 5a-96\nByte Stuffing\n❒“data transparency” requirement: data field must \nbe allowed to include flag pattern  <01111110>\n❍Q: is received <01111110> data or flag?\n❒Sender: adds (“stuffs”) extra < 01111110> byte \nafter each < 01111110> data  byte\n❍two 01111110 bytes in a row: discard first byte, \ncontinue data reception\n❍single 01111110: flag byte\n5: DataLink Layer 5a-97\nByte Stuffing\nflag byte pattern plus\nstuffed byte in \ntransmitted  data\n5: DataLink Layer 5a-98\nPPP Data Control Protocol\nBefore exchanging network-\nlayer data, data link peers \n❒configure PPP link (max. \nframe length, \nauthentication)\n❒learn/configure network\nlayer information\n❍for IP: carry IP Control \nProtocol (IPCP) msgs\n(protocol field: 8021) to \nconfigure/learn IP \n5: DataLink Layer 5a-99\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction \n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer\nAsynchronous Transfer Mode: ATM\n❒1990’s/00 standard for high-speed (155Mbps to \n622 Mbps and higher) Broadband Integrated \nService Digital Network architecture\n❒Goal: integrated, end-end transport of carry voice, \nvideo, data\n❍meeting timing/QoS requirements of voice, video \n(versus Internet best-effort model)\n❍“next generation” telephony: technical roots in \ntelephone world\n❍packet-switching (fixed length packets, called \n“cells”) using virtual circuits\n5: DataLink Layer\nATM architecture \n❒adaptation layer: only at edge of ATM network\n❍data segmentation/reassembly\n❍roughly analogous to Internet transport layer\n❒ATM layer: “network” layer\n❍cell switching, routing\n❒physical layer\n5: DataLink Layer\nATM:  network or link layer?\nVision: end-to-end \ntransport: “ATM from \ndesktop to desktop”\n❍ATM is a network \nReality: used to connect \nIP backbone routers  \n❍“IP over ATM”\n❍ATM as switched \nlink layer, \nconnecting IP \n5: DataLink Layer\nATM Adaptation Layer (AAL)\n❒ATM Adaptation Layer (AAL): “adapts” upper \nlayers (IP or native ATM applications)  to ATM \nlayer below\n❒AAL present only in end systems, not in switches\n❒AAL layer segment (header/trailer fields, data) \nfragmented across multiple ATM cells \n❍analogy: TCP segment in many IP packets\n5: DataLink Layer\nATM Adaptation Layer (AAL) [more]\nDifferent versions of AAL layers, depending on ATM \nservice class:\n❒AAL1: for CBR (Constant Bit Rate) services, e.g. circuit emulation\n❒AAL2: for VBR (Variable Bit Rate) services, e.g., MPEG video\n❒AAL5: for data (e.g., IP datagrams)\n5: DataLink Layer\nAAL5 - Simple And Efficient \n❒AAL5: low overhead AAL used to carry IP \n❍4 byte cyclic redundancy check \n❍PAD ensures payload multiple of 48bytes \n❍large AAL5 data unit to be fragmented into 48-\nbyte ATM cells\n5: DataLink Layer\nService: transport cells across ATM network\n❒analogous to IP network layer\n❒very different services than IP network layer\nArchitecture\nbest effort\nno (inferred\nGuarantees ?\n5: DataLink Layer\nATM Layer: Virtual Circuits\n❒VC transport: cells carried on VC from source to dest\n❍call setup, teardown for each call before data can flow\n❍each packet carries VC identifier (not destination ID)\n❍every switch on source-dest path maintain “state” for each \npassing connection\n❍link,switch resources (bandwidth, buffers) may be allocated to \nVC: to get circuit-like perf.\n❒Permanent VCs (PVCs)\n❍long lasting connections\n❍typically: “permanent” route between to IP routers\n❒Switched VCs (SVC):\n❍dynamically set up on per-call basis\n5: DataLink Layer\n❒Advantages of ATM VC approach:\n❍QoS performance guarantee for connection \nmapped to VC (bandwidth, delay, delay jitter)\n❒Drawbacks of ATM VC approach:\n❍Inefficient support of datagram traffic\n❍one PVC between each source/dest pair) does \nnot scale (N*2 connections needed) \n❍SVC introduces call setup latency, processing \noverhead for short lived connections\n5: DataLink Layer\nATM Layer: ATM cell\n❒5-byte ATM cell header\n❒48-byte payload\n❍Why?: small payload -> short cell-creation delay \nfor digitized voice\n❍halfway between 32 and 64 (compromise!)\nCell header\nCell format\n5: DataLink Layer\nATM cell header\n❒VCI: virtual channel ID\n❍will change from link to link thru net\n❒PT: Payload type (e.g. RM cell versus data cell) \n❒CLP: Cell Loss Priority bit\n❍CLP = 1 implies low priority cell, can be \ndiscarded if congestion\n❒HEC: Header Error Checksum\n❍cyclic redundancy check\n5: DataLink Layer\nATM Physical Layer (more)\nTwo pieces (sublayers) of physical layer:\n❒Transmission Convergence Sublayer (TCS): adapts \nATM layer above to PMD sublayer below\n❒Physical Medium Dependent: depends on physical \nmedium being used\nTCS Functions:\n❍Header checksum generation: 8 bits CRC \n❍Cell delineation\n❍With “unstructured” PMD sublayer, transmission \nof idle cells when no data cells to send\n5: DataLink Layer\nATM Physical Layer\nPhysical Medium Dependent (PMD) sublayer\n❒SONET/SDH: transmission frame structure (like a \ncontainer carrying bits); \n❍bit synchronization; \n❍bandwidth partitions (TDM); \n❍several speeds: OC3 = 155.52 Mbps; OC12 = 622.08 \nMbps; OC48 = 2.45 Gbps, OC192 = 9.6 Gbps\n❒TI/T3: transmission frame structure (old \ntelephone hierarchy): 1.5 Mbps/ 45 Mbps\n❒unstructured: just cells (busy/idle)\n5: DataLink Layer\nIP-Over-ATM\nClassic IP only\n❒3 “networks” (e.g., \nLAN segments)\n❒MAC (802.3) and IP \nIP over ATM\n❒replace “network” \n(e.g., LAN segment) \nwith ATM network\n❒ATM addresses, IP \n5: DataLink Layer\nIP-Over-ATM\n❒IP datagrams into \nATM AAL5 PDUs\n❒from IP addresses \nto ATM addresses\n❍just like IP \naddresses to \n5: DataLink Layer\nDatagram Journey in IP-over-ATM Network\n❒at Source Host:\n❍IP layer maps between IP, ATM dest address (using ARP)\n❍passes datagram to AAL5\n❍AAL5 encapsulates data, segments cells, passes to ATM layer \n❒ATM network: moves cell along VC to destination\n❒at Destination Host:\n❍AAL5  reassembles cells into original datagram\n❍if CRC OK, datagram is passed to IP\n5: DataLink Layer\nChapter 5 outline\n❒5.1 Introduction and \n❒5.2 Error detection \nand correction \n❒5.3Multiple access \n❒5.4 LAN addresses \n❒5.5 Ethernet\n❒5.6 Hubs, bridges, and \n❒5.7 Wireless links and \n❒5.10 Frame Relay\n5: DataLink Layer\nFrame Relay\n❒wide area network technologies \n❒Virtual-circuit oriented \n❒origins in telephony world\n❒can be used to carry IP datagrams\n❍can thus be viewed as link layers by IP \n5: DataLink Layer\nFrame Relay\n❒Designed in late ‘80s, widely deployed in the ‘90s\n❒Frame relay service:\n❍no error control\n❍end-to-end congestion control\n5: DataLink Layer\nFrame Relay (more)\n❒Designed to interconnect corporate customer LANs\n❍typically permanent VC’s: “pipe” carrying aggregate \ntraffic between two routers \n❍switched VC’s: as in ATM\n❒corporate customer leases FR service from public \nFrame Relay network (e.g., Sprint, ATT)\n5: DataLink Layer\nFrame Relay (more)\n❒Flag bits, 01111110, delimit frame\n❍10 bit VC ID field\n❍3 congestion control bits\n• FECN: forward explicit congestion \nnotification (frame experienced congestion \n• BECN: congestion on reverse path\n• DE: discard eligibility\n5: DataLink Layer\nFrame Relay -VC Rate Control\n❒Committed Information Rate (CIR)\n❍defined, “guaranteed” for each VC\n❍negotiated at VC set up time\n❍customer pays based on CIR\n❒DE bit: Discard Eligibility bit\n❍Edge FR switch measures traffic rate for each VC; \nmarks DE bit\n❍DE = 0: high priority, rate compliant frame; deliver \nat “all costs”\n❍DE = 1: low priority, eligible for congestion discard\n5: DataLink Layer\nFrame Relay - CIR & Frame Marking\n❒Access Rate: rate R of the access link between \nsource router (customer) and edge FR switch\n(provider); 64Kbps < R < 1,544Kbps\n❒Typically, many VCs (one per destination router) \nmultiplexed on the same access trunk; each VC has \n❒Edge FR switch measures traffic rate for each \nVC; it marks (i.e. DE = 1) frames which exceed CIR \n(these may be later dropped)\n❒Internet’s more recent differentiated service\nuses similar ideas\n5: DataLink Layer\nChapter 5: Summary\n❒principles behind data link layer services:\n❍error detection, correction\n❍sharing a broadcast channel: multiple access\n❍link layer addressing, ARP\n❒link layer technologies: Ethernet, hubs, \nbridges, switches,IEEE 802.11 LANs, PPP, \nATM, Frame Relay\n❒journey down the protocol stack now OVER!\n❍next stops: multimedia, security, network",
    "unit": "Unit 2",
    "source_type": "notes",
    "book_priority": 0,
    "source_file": "Data Link Layer",
    "chunk_id": "Data Link Layer_chunk_0"
  },
  {
    "text": "Network Layer \nNetworking: A Top \nDown Approach  \n6th edition  \nJim Kurose, Keith Ross \nAddison-Wesley \nA note on the use of these ppt slides: \nWe’re making these slides freely available to all (faculty, students, readers). \nThey’re in PowerPoint form so you see the animations; and can add, modify, \nand delete slides  (including this one) and slide content to suit your needs. \nThey obviously represent a lot of work on our part. In return for use, we only \nask the following: \nv  If you use these slides (e.g., in a class) that you mention their source \n(after all, we’d like people to use our book!) \nv  If you post any slides on a www site, that you note that they are adapted \nChapter 4: network layer \nchapter goals:  \nv  understand principles behind network layer \n§  network layer service models \n§  forwarding versus routing \n§  how a router works \n§  routing (path selection) \n§  broadcast, multicast \nv  instantiation, implementation in the Internet \nNetwork Layer 4-3 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-4 \nNetwork layer \nv  transport segment from \nsending to receiving host  \nv  on sending side \nencapsulates segments \ninto datagrams \nv  on receiving side, delivers \nsegments to transport \nv  network layer protocols \nin every host, router \nv  router examines header \nfields in all IP datagrams \npassing through it \napplication \napplication \nNetwork Layer 4-5 \nTwo key network-layer functions \nv  forwarding: move packets \nfrom router’s input to \nappropriate router \nv  routing: determine route \ntaken by packets from \nsource to dest.  \n§  routing algorithms \nv  routing: process of \nplanning trip from source \nv  forwarding: process of \ngetting through single \ninterchange \nNetwork Layer 4-6 \nvalue in arriving \npacket’s header \nrouting algorithm \nlocal forwarding table \nheader value output link \nInterplay between routing and forwarding \nrouting algorithm determines \nend-end-path through network \nforwarding table determines \nlocal forwarding at this router \nNetwork Layer 4-7 \nConnection setup \nv  3rd important function in some network \narchitectures: \n§  ATM, frame relay, X.25 \nv  before datagrams flow, two end hosts and \nintervening routers establish virtual connection \n§  routers get involved \nv  network vs transport layer connection service: \n§  network: between two hosts (may also involve intervening \nrouters in case of VCs) \n§  transport: between two processes \nNetwork Layer 4-8 \nNetwork service model \nQ: What service model for “channel” transporting \ndatagrams from sender to receiver? \nexample services for \nindividual datagrams: \nv  guaranteed delivery \nv  guaranteed delivery with \nless than 40 msec delay \nexample services for a flow \nof datagrams: \nv  in-order datagram \nv  guaranteed minimum \nbandwidth to flow \nv  restrictions on changes in \ninter-packet spacing \nNetwork Layer 4-9 \nNetwork layer service models: \nArchitecture \nbest effort \nno (inferred \nGuarantees ? \nNetwork Layer 4-10 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-11 \nConnection, connection-less service \nv  datagram network provides network-layer \nconnectionless service \nv  virtual-circuit network provides network-layer \nconnection service \nv  analogous to TCP/UDP connecton-oriented / \nconnectionless transport-layer services, but: \n§  service: host-to-host \n§  no choice: network provides one or the other \n§  implementation: in network core \nNetwork Layer 4-12 \nVirtual circuits \nv  call setup, teardown for each call before data can flow \nv  each packet carries VC identifier (not destination host \nv  every router on source-dest path maintains “state” for \neach passing connection \nv  link, router resources (bandwidth, buffers) may be \nallocated to VC (dedicated resources = predictable \n“source-to-dest path behaves much like telephone \n§  performance-wise \n§  network actions along source-to-dest path \nNetwork Layer 4-13 \nVC implementation \na VC consists of: \n1.  path from source to destination \n2.  VC numbers, one number for each link along path \n3.  entries in forwarding tables in routers along path \npacket belonging to VC carries VC number \n(rather than dest address) \nVC number can be changed on each link. \nnew VC number comes from forwarding table \nNetwork Layer 4-14 \nVC forwarding table \nIncoming interface    Incoming VC #     Outgoing interface    Outgoing VC # \n1                          12                               3                          22 \n2                          63                               1                          18  \n3                           7                                2                          17 \n1                          97                               3                           87 \n…                          …                                …                            … \nforwarding table in \nnorthwest router: \nVC routers maintain connection state information! \nNetwork Layer 4-15 \napplication \nVirtual circuits: signaling protocols \nv  used to setup, maintain  teardown VC \nv  used in ATM, frame-relay, X.25 \nv  not used in today’s Internet \n1. initiate call \n2. incoming call \n3. accept call \n4. call connected \n5. data flow begins \n6. receive data \napplication \nNetwork Layer 4-16 \nDatagram networks \nv  no call setup at network layer \nv  routers: no state about end-to-end connections \n§  no network-level concept of “connection” \nv  packets forwarded using destination host address \n1. send datagrams \napplication \napplication \n2. receive datagrams \nNetwork Layer 4-17 \nDatagram forwarding  table \nIP destination address in  \narriving packet’s header \nrouting algorithm \nlocal forwarding table \ndest address output  link \naddress-range 1 \naddress-range 2 \naddress-range 3 \naddress-range 4 \n4 billion IP addresses, so \nrather than list individual \ndestination address \nlist range of addresses \n(aggregate table entries) \nNetwork Layer 4-18 \nDestination Address Range \n11001000 00010111 00010000 00000000 \n11001000 00010111 00010111 11111111 \n11001000 00010111 00011000 00000000 \n11001000 00010111 00011000 11111111   \n11001000 00010111 00011001 00000000 \n11001000 00010111 00011111 11111111   \nLink Interface \nQ: but what happens if ranges don’t divide up so nicely?  \nDatagram forwarding  table \nNetwork Layer 4-19 \nLongest prefix matching \nDestination Address Range                         \n11001000 00010111 00010*** *********  \n11001000 00010111 00011000 ********* \n11001000 00010111 00011*** ********* \nDA: 11001000  00010111  00011000  10101010  \nDA: 11001000  00010111  00010110  10100001  \nwhich interface? \nwhich interface? \nwhen looking for forwarding table entry for given \ndestination address, use longest address prefix that \nmatches destination address. \nlongest prefix matching \nLink interface \nNetwork Layer 4-20 \nDatagram or VC network: why? \nInternet (datagram) \nv  data exchange among \n§  “elastic” service, no strict \ntiming req.  \nv  many link types  \n§  different characteristics \n§  uniform service difficult \nv  “smart” end systems \n(computers) \n§  can adapt, perform control, \nerror recovery \n§  simple inside network, \ncomplexity at “edge” \nv  evolved from telephony \nv  human conversation:  \n§  strict timing, reliability \nrequirements \n§  need for guaranteed service \nv  “dumb” end systems \n§  telephones \n§  complexity inside \nNetwork Layer 4-21 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-22 \nRouter architecture overview \ntwo key router functions:  \nv  run routing algorithms/protocol (RIP, OSPF, BGP) \nv  forwarding datagrams from incoming to outgoing link \nrouter input ports \nrouter output ports \nforwarding data \nplane  (hardware) \nrouting, management \ncontrol plane (software) \nforwarding tables computed, \npushed to input ports \nNetwork Layer 4-23 \ntermination \nInput port functions \ndecentralized switching:  \nv  given datagram dest., lookup output port \nusing forwarding table in input port \nmemory (“match plus action”) \nv  goal: complete input port processing at \n‘line speed’ \nv  queuing: if datagrams arrive faster than \nforwarding rate into switch fabric \nphysical layer: \nbit-level reception \ndata link layer: \ne.g., Ethernet \nsee chapter 5 \nNetwork Layer 4-24 \nSwitching fabrics \nv  transfer packet from input buffer to appropriate \noutput buffer \nv  switching rate: rate at which packets can be \ntransfer from inputs to outputs \n§  often measured as multiple of input/output line rate \n§  N inputs: switching rate N times line rate desirable \nv  three types of switching fabrics \nNetwork Layer 4-25 \nSwitching via memory \nfirst generation routers: \nv traditional computers with switching under direct control \nv packet copied to system’s memory \nv  speed limited by memory bandwidth (2 bus crossings per \nNetwork Layer 4-26 \nSwitching via a bus \nv  datagram from input port memory \n    to output port memory via a \nv  bus contention:  switching speed \nlimited by bus bandwidth \nv  32 Gbps bus, Cisco 5600: sufficient \nspeed for access and enterprise \nNetwork Layer 4-27 \nSwitching via interconnection network \nv  overcome  bus bandwidth limitations \nv  banyan networks, crossbar, other \ninterconnection nets initially \ndeveloped to connect processors in \nmultiprocessor \nv  advanced design: fragmenting \ndatagram into fixed length cells, \nswitch cells through the fabric.  \nv  Cisco 12000: switches 60 Gbps \nthrough the interconnection \nNetwork Layer 4-28 \nOutput ports \nv  buffering required when datagrams arrive \nfrom fabric faster than the transmission \nv  scheduling discipline chooses among queued \ndatagrams for transmission \ntermination \nThis slide in HUGELY important! \nDatagram (packets) can be lost \ndue to congestion, lack of buffers \nPriority scheduling – who gets best \nperformance, network neutrality \nNetwork Layer 4-29 \nOutput port queueing \nv  buffering when arrival rate via switch exceeds \noutput line speed \nv  queueing (delay) and loss due to output port buffer \nat t, packets more \nfrom input to output \none packet time later \nNetwork Layer 4-30 \nHow much buffering? \nv  RFC 3439 rule of thumb: average buffering equal \nto “typical” RTT (say 250 msec) times link \n§  e.g., C = 10 Gpbs link: 2.5 Gbit buffer \nv  recent recommendation: with N flows, buffering \nNetwork Layer 4-31 \nInput port queuing \nv  fabric slower than input ports combined -> queueing may \noccur at input queues  \n§  queueing delay and loss due to input buffer overflow! \nv  Head-of-the-Line (HOL) blocking: queued datagram at front \nof queue prevents others in queue from moving forward \noutput port contention: \nonly one red datagram can be \ntransferred. \nlower red packet is blocked \none packet time \nlater: green packet \nexperiences HOL \nNetwork Layer 4-32 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-33 \nThe Internet network layer \nhost, router network layer functions: \nrouting protocols \n•  path selection \n•  RIP, OSPF, BGP \nIP protocol \n•  addressing conventions \n•  datagram format \n•  packet handling conventions \nICMP protocol \n•  error reporting \n•  router “signaling” \ntransport layer: TCP, UDP \nphysical layer \nNetwork Layer 4-34 \n(variable length, \ntypically a TCP  \nor UDP segment) \n16-bit identifier \n32 bit source IP address \n32 bit destination IP address \noptions (if any) \nIP datagram format \nIP protocol version \nheader length \nupper layer protocol \nto deliver payload to \ntotal datagram \nlength (bytes) \n“type” of data  \nfragmentation/ \nremaining hops \n(decremented at  \neach router) \ne.g. timestamp, \nrecord route \ntaken, specify \nlist of routers  \nhow much overhead? \nv  20 bytes of TCP \nv  20 bytes of IP \nv  = 40 bytes + app \nlayer overhead \nNetwork Layer 4-35 \nIP fragmentation, reassembly \nv  network links have MTU \n(max.transfer size) - \nlargest possible link-level \n§  different link types, \ndifferent MTUs  \nv  large IP datagram divided \n(“fragmented”) within net \n§  one datagram becomes \nseveral datagrams \n§  “reassembled” only at \nfinal destination \n§  IP header bits used to \nidentify, order related \nfragmentation:  \nin: one large datagram \nout: 3 smaller datagrams \nNetwork Layer 4-36 \none large datagram becomes \nseveral smaller datagrams \nv  4000 byte datagram \nv  MTU = 1500 bytes \n1480 bytes in  \nIP fragmentation, reassembly \nNetwork Layer 4-37 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-38 \nIP addressing: introduction \nv  IP address: 32-bit \nidentifier for host, router \nv  interface: connection \nbetween host/router and \nphysical link \n§  router’s typically have \nmultiple interfaces \n§  host typically has one or \ntwo interfaces (e.g., wired \nEthernet, wireless 802.11) \nv  IP addresses associated \nwith each interface \n223.1.1.1 = 11011111 00000001 00000001 00000001 \nNetwork Layer 4-39 \nIP addressing: introduction \nQ: how are interfaces \nactually connected? \nA: we’ll learn about that \nin chapter 5, 6. \nA: wired Ethernet interfaces \nconnected by Ethernet switches \nA: wireless WiFi interfaces \nconnected by WiFi base station \nFor now: don’t need to worry \nabout how one interface is \nconnected to another (with no \nintervening router)  \nNetwork Layer 4-40 \nv IP address:  \n§ subnet part - high order \n§ host part - low order \nv what’s a subnet ? \n§ device interfaces with \nsame subnet part of IP \n§ can physically reach \neach other without \nintervening router \nnetwork consisting of 3 subnets \nNetwork Layer 4-41 \nv  to determine the \nsubnets, detach each \ninterface from its host \nor router, creating \nislands of isolated \nv  each isolated network \nis called a subnet \nsubnet mask: /24 \n223.1.1.0/24 \n223.1.2.0/24 \n223.1.3.0/24 \nNetwork Layer 4-42 \nNetwork Layer 4-43 \nIP addressing: CIDR \nCIDR: Classless InterDomain Routing \n§  subnet portion of address of arbitrary length \n§  address format: a.b.c.d/x, where x is # bits in \nsubnet portion of address \n11001000  00010111  00010000  00000000 \n200.23.16.0/23 \nNetwork Layer 4-44 \nIP addresses: how to get one? \nQ: How does a host get IP address? \nv  hard-coded by system admin in a file \n§  Windows: control-panel->network->configuration->tcp/\nip->properties \n§  UNIX: /etc/rc.config \nv  DHCP: Dynamic Host Configuration Protocol: \ndynamically get address from as server \n§  “plug-and-play”  \nNetwork Layer 4-45 \nDHCP: Dynamic Host Configuration Protocol \ngoal: allow host to dynamically obtain its IP address from network \nserver when it joins network \n§  can renew its lease on address in use \n§  allows reuse of addresses (only hold address while \nconnected/“on”) \n§  support for mobile users who want to join network (more \nDHCP overview: \n§  host broadcasts “DHCP discover” msg [optional] \n§  DHCP server responds with “DHCP offer” msg [optional] \n§  host requests IP address: “DHCP request” msg \n§  DHCP server sends address: “DHCP ack” msg  \nNetwork Layer 4-46 \nDHCP client-server scenario \n223.1.1.0/24 \n223.1.2.0/24 \n223.1.3.0/24 \narriving DHCP \nclient needs  \naddress in this \nNetwork Layer 4-47 \nDHCP server: 223.1.2.5 \nDHCP discover \nsrc : 0.0.0.0, 68     \ndest.: 255.255.255.255,67 \nyiaddr:    0.0.0.0 \ntransaction ID: 654 \nsrc: 223.1.2.5, 67      \ndest:  255.255.255.255, 68 \nyiaddrr: 223.1.2.4 \ntransaction ID: 654 \nlifetime: 3600 secs \nDHCP request \nsrc:  0.0.0.0, 68     \ndest::  255.255.255.255, 67 \nyiaddrr: 223.1.2.4 \ntransaction ID: 655 \nlifetime: 3600 secs \nsrc: 223.1.2.5, 67      \ndest:  255.255.255.255, 68 \nyiaddrr: 223.1.2.4 \ntransaction ID: 655 \nlifetime: 3600 secs \nDHCP client-server scenario \nBroadcast: is there a \nDHCP server out there? \nBroadcast: I’m a DHCP \nserver! Here’s an IP \naddress you can use \nBroadcast: OK.  I’ll take \nthat IP address! \nBroadcast: OK.  You’ve \ngot that IP address! \nNetwork Layer 4-48 \nDHCP: more than IP addresses \nDHCP can return more than just allocated IP \naddress on subnet: \n§  address of first-hop router for client \n§  name and IP address of DNS sever \n§  network mask (indicating network versus host portion \nof address) \nNetwork Layer 4-49 \nv  connecting laptop needs \nits IP address, addr of \nfirst-hop router, addr of \nDNS server: use DHCP \nrouter with DHCP  \nserver built into  \nv  DHCP request encapsulated \nin UDP, encapsulated in IP, \nencapsulated in 802.1 \n v  Ethernet frame broadcast \n(dest: FFFFFFFFFFFF) on LAN, \nreceived at router running \nDHCP server \nv  Ethernet demuxed to IP \ndemuxed, UDP demuxed to \nDHCP: example \nNetwork Layer 4-50 \nv  DCP server formulates \nDHCP ACK containing \nclient’s IP address, IP \naddress of first-hop \nrouter for client, name & \nIP address of DNS server \nv  encapsulation of DHCP \nserver, frame forwarded \nto client, demuxing up to \nDHCP at client \nDHCP: example \nrouter with DHCP  \nserver built into  \nv  client now knows its IP \naddress, name and IP \naddress of DSN server, IP \naddress of its first-hop \nNetwork Layer 4-51 \nDHCP: Wireshark \noutput (home LAN) \nMessage type: Boot Reply (2) \nHardware type: Ethernet \nHardware address length: 6 \nTransaction ID: 0x6b3a11b7 \nSeconds elapsed: 0 \nBootp flags: 0x0000 (Unicast) \nClient IP address: 192.168.1.101 (192.168.1.101) \nYour (client) IP address: 0.0.0.0 (0.0.0.0) \nNext server IP address: 192.168.1.1 (192.168.1.1) \nRelay agent IP address: 0.0.0.0 (0.0.0.0) \nClient MAC address: Wistron_23:68:8a (00:16:d3:23:68:8a) \nServer host name not given \nBoot file name not given \nMagic cookie: (OK) \nOption: (t=53,l=1) DHCP Message Type = DHCP ACK \nOption: (t=54,l=4) Server Identifier = 192.168.1.1 \nOption: (t=1,l=4) Subnet Mask = 255.255.255.0 \nOption: (t=3,l=4) Router = 192.168.1.1 \nOption: (6) Domain Name Server \n     Length: 12; Value: 445747E2445749F244574092;  \n      IP Address: 68.87.71.226; \n      IP Address: 68.87.73.242;  \n      IP Address: 68.87.64.146 \nOption: (t=15,l=20) Domain Name = \"hsd1.ma.comcast.net.\" \nMessage type: Boot Request (1) \nHardware type: Ethernet \nHardware address length: 6 \nTransaction ID: 0x6b3a11b7 \nSeconds elapsed: 0 \nBootp flags: 0x0000 (Unicast) \nClient IP address: 0.0.0.0 (0.0.0.0) \nYour (client) IP address: 0.0.0.0 (0.0.0.0) \nNext server IP address: 0.0.0.0 (0.0.0.0) \nRelay agent IP address: 0.0.0.0 (0.0.0.0) \nClient MAC address: Wistron_23:68:8a (00:16:d3:23:68:8a) \nServer host name not given \nBoot file name not given \nMagic cookie: (OK) \nOption: (t=53,l=1) DHCP Message Type = DHCP Request \nOption: (61) Client identifier \n     Length: 7; Value: 010016D323688A;  \n     Hardware type: Ethernet \n     Client MAC address: Wistron_23:68:8a (00:16:d3:23:68:8a) \nOption: (t=50,l=4) Requested IP Address = 192.168.1.101 \nOption: (t=12,l=5) Host Name = \"nomad\" \nOption: (55) Parameter Request List \n     Length: 11; Value: 010F03062C2E2F1F21F92B \n     1 = Subnet Mask; 15 = Domain Name \n     3 = Router; 6 = Domain Name Server \n     44 = NetBIOS over TCP/IP Name Server \nNetwork Layer 4-52 \nIP addresses: how to get one? \nQ: how does network get subnet part of IP addr? \nA: gets allocated portion of its provider ISP’s address \nISP's block          11001000  00010111  00010000  00000000    200.23.16.0/20  \nOrganization 0    11001000  00010111  00010000  00000000    200.23.16.0/23  \nOrganization 1    11001000  00010111  00010010  00000000    200.23.18.0/23  \nOrganization 2    11001000  00010111  00010100  00000000    200.23.20.0/23  \n   ...                                          …..                                   ….                …. \nOrganization 7    11001000  00010111  00011110  00000000    200.23.30.0/23  \nNetwork Layer 4-53 \nHierarchical addressing: route aggregation \n“Send me anything \nwith addresses  \n200.23.16.0/20” \n200.23.16.0/23 \n200.23.18.0/23 \n200.23.30.0/23 \nFly-By-Night-ISP \nOrganization 0 \nOrganization 7 \nOrganization 1 \n“Send me anything \nwith addresses  \n199.31.0.0/16” \n200.23.20.0/23 \nOrganization 2 \nhierarchical addressing allows efficient advertisement of routing  \ninformation: \nNetwork Layer 4-54 \nISPs-R-Us has a more specific route to Organization 1 \n“Send me anything \nwith addresses  \n200.23.16.0/20” \n200.23.16.0/23 \n200.23.18.0/23 \n200.23.30.0/23 \nFly-By-Night-ISP \nOrganization 0 \nOrganization 7 \nOrganization 1 \n“Send me anything \nwith addresses  \nbeginning 199.31.0.0/16 \nor 200.23.18.0/23” \n200.23.20.0/23 \nOrganization 2 \nHierarchical addressing: more specific routes \nNetwork Layer 4-55 \nIP addressing: the last word... \nQ: how does an ISP get block of addresses? \nA: ICANN: Internet Corporation for Assigned  \n     Names and Numbers http://www.icann.org/ \n§  allocates addresses \n§  manages DNS \n§  assigns domain names, resolves disputes \nNetwork Layer 4-56 \nNAT: network address translation \n138.76.29.7 \nlocal network \n(e.g., home network) \ndatagrams with source or  \ndestination in this network \nhave 10.0.0/24 address for  \nsource, destination (as usual) \nall datagrams leaving local \nnetwork have same single \nsource NAT IP address: \n138.76.29.7,different source \nport numbers \nNetwork Layer 4-57 \nmotivation: local network uses just one IP address as far \nas outside world is concerned: \n§  range of addresses not needed from ISP:  just one \nIP address for all devices \n§  can change addresses of devices in local network \nwithout notifying outside world \n§  can change ISP without changing addresses of \ndevices in local network \n§  devices inside local net not explicitly addressable, \nvisible by outside world (a security plus) \nNAT: network address translation \nNetwork Layer 4-58 \n   implementation: NAT router must: \n§  outgoing datagrams: replace (source IP address, port #) of \nevery outgoing datagram to (NAT IP address, new port #) \n. . . remote clients/servers will respond using (NAT IP \naddress, new port #) as destination addr \n§  remember (in NAT translation table) every (source IP address, \nport #)  to (NAT IP address, new port #) translation pair \n§  incoming datagrams: replace (NAT IP address, new port #) in \ndest fields of every incoming datagram with corresponding \n(source IP address, port #) stored in NAT table \nNAT: network address translation \nNetwork Layer 4-59 \nS: 10.0.0.1, 3345 \nD: 128.119.40.186, 80 \n138.76.29.7 \n1: host 10.0.0.1  \nsends datagram to  \n128.119.40.186, 80 \nNAT translation table \nWAN side addr        LAN side addr \n138.76.29.7, 5001   10.0.0.1, 3345 \n……                                         …… \nS: 128.119.40.186, 80  \nD: 10.0.0.1, 3345 \nS: 138.76.29.7, 5001 \nD: 128.119.40.186, 80 \n2: NAT router \nchanges datagram \nsource addr from \n10.0.0.1, 3345 to \n138.76.29.7, 5001, \nupdates table \nS: 128.119.40.186, 80  \nD: 138.76.29.7, 5001 \n3: reply arrives \n dest. address: \n 138.76.29.7, 5001 \n4: NAT router \nchanges datagram \ndest addr from \n138.76.29.7, 5001 to 10.0.0.1, 3345  \nNAT: network address translation \nNetwork Layer 4-60 \nv  16-bit port-number field:  \n§  60,000 simultaneous connections with a single \nLAN-side address! \nv  NAT is controversial: \n§  routers should only process up to layer 3 \n§  violates end-to-end argument \n•  NAT possibility must be taken into account by app \ndesigners, e.g., P2P applications \n§  address shortage should instead be solved by \nNAT: network address translation \nNetwork Layer 4-61 \nNAT traversal problem \nv  client wants to connect to \nserver with address 10.0.0.1 \n§  server address 10.0.0.1 local to \nLAN (client can’t use it as \ndestination addr) \n§  only one externally visible NATed \naddress: 138.76.29.7 \nv  solution1: statically configure \nNAT to forward incoming \nconnection requests at given \nport to server \n§  e.g., (123.76.29.7, port 2500) \nalways forwarded to 10.0.0.1 port \n138.76.29.7 \nNetwork Layer 4-62 \nNAT traversal problem \nv  solution 2: Universal Plug and Play \n(UPnP) Internet Gateway Device \n(IGD) Protocol.  Allows NATed \nv  learn public IP address \n(138.76.29.7) \nv  add/remove port mappings \n(with lease times) \ni.e., automate static NAT port \nmap configuration \nNetwork Layer 4-63 \nNAT traversal problem \nv  solution 3: relaying (used in Skype) \n§  NATed client establishes connection to relay \n§  external client connects to relay \n§  relay bridges packets between to connections \n138.76.29.7 \n1. connection to \nrelay initiated \nby NATed host \n2. connection to \nrelay initiated \n3. relaying  \nestablished \nNetwork Layer 4-64 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-65 \nICMP: internet control message protocol \nv  used by hosts & routers \nto communicate network-\nlevel information \n§  error reporting: \nunreachable host, network, \nport, protocol \n§  echo request/reply (used by \nv  network-layer “above” IP: \n§  ICMP msgs carried in IP \nv  ICMP message: type, code \nplus first 8 bytes of IP \ndatagram causing error \nType  Code  description \n0        0         echo reply (ping) \n3        0         dest. network unreachable \n3        1         dest host unreachable \n3        2         dest protocol unreachable \n3        3         dest port unreachable \n3        6         dest network unknown \n3        7         dest host unknown \n4        0         source quench (congestion \n                     control - not used) \n8        0         echo request (ping) \n9        0         route advertisement \n10      0         router discovery \n11      0         TTL expired \n12      0         bad IP header \nNetwork Layer 4-66 \nTraceroute and ICMP \nv  source sends series of \nUDP segments to dest \n§  first set has TTL =1 \n§  second set has TTL=2, etc. \n§  unlikely port number \nv  when nth set of datagrams \narrives to nth router: \n§  router discards datagrams \n§  and sends source ICMP \nmessages (type 11, code 0) \n§  ICMP messages includes \nname of router & IP address \nv  when ICMP messages \narrives, source records \nstopping criteria: \nv  UDP segment eventually \narrives at destination host \nv  destination returns ICMP \n“port unreachable” \nmessage (type 3, code 3) \nv  source stops \nNetwork Layer 4-67 \nIPv6: motivation \nv  initial motivation: 32-bit address space soon to be \ncompletely allocated.   \nv  additional motivation: \n§  header format helps speed processing/forwarding \n§  header changes to facilitate QoS  \nIPv6 datagram format:  \n§  fixed-length 40 byte header \n§  no fragmentation allowed \nNetwork Layer 4-68 \nIPv6 datagram format \npriority:  identify priority among datagrams in flow \nflow Label: identify datagrams in same “flow.”  \n                    (concept of“flow” not well defined). \nnext header: identify upper layer protocol for data  \ndestination address \nsource address \npayload len \nNetwork Layer 4-69 \nOther changes from IPv4 \nv  checksum: removed entirely to reduce processing \ntime at each hop \nv  options: allowed, but outside of header, indicated \nby “Next Header” field \nv  ICMPv6: new version of ICMP \n§  additional message types, e.g. “Packet Too Big” \n§  multicast group management functions \nNetwork Layer 4-70 \nTransition from IPv4 to IPv6 \nv  not all routers can be upgraded simultaneously \n§  no “flag days” \n§  how will network operate with mixed IPv4 and \nIPv6 routers?  \nv  tunneling: IPv6 datagram carried as payload in IPv4 \ndatagram among IPv4 routers \nIPv4 source, dest addr  \nIPv4 header fields  \nIPv4 datagram \nIPv6 datagram \nIPv4 payload  \nUDP/TCP payload \nIPv6 source dest addr \nIPv6 header fields \nNetwork Layer 4-71 \nphysical view: \nlogical view: \nIPv4 tunnel  \nconnecting IPv6 routers \nNetwork Layer 4-72 \nIPv6 inside \nIPv6 inside \nphysical view: \nlogical view: \nIPv4 tunnel  \nconnecting IPv6 routers \nNetwork Layer 4-73 \nIPv6: adoption \nv  US National Institutes of Standards estimate [2013]: \n§  ~3% of industry IP routers \n§  ~11% of US gov’t routers \nv  Long (long!) time for deployment, use \n§  20 years and counting! \n§  think of application-level changes in last 20 years: WWW, \nFacebook, … \nNetwork Layer 4-74 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-75 \nIP destination address in  \narriving packet’s header \nrouting algorithm \nlocal forwarding table \ndest address output  link \naddress-range 1 \naddress-range 2 \naddress-range 3 \naddress-range 4 \nInterplay between routing, forwarding \nrouting algorithm determines \nend-end-path through network \nforwarding table determines \nlocal forwarding at this router \nNetwork Layer 4-76 \ngraph: G = (N,E) \nN = set of routers = { u, v, w, x, y, z } \nE = set of links ={ (u,v), (u,x), (v,x), (v,w), (x,w), (x,y), (w,y), (w,z), (y,z) } \nGraph abstraction \naside: graph abstraction is useful in other network contexts, e.g.,  \nP2P, where N is set of peers and E is set of TCP connections \nNetwork Layer 4-77 \nGraph abstraction: costs \nc(x,x’) = cost of link (x,x’) \n      e.g., c(w,z) = 5 \ncost could always be 1, or  \ninversely related to bandwidth, \nor inversely related to  \ncost of path (x1, x2, x3,…, xp) = c(x1,x2) + c(x2,x3) + … + c(xp-1,xp)   \nkey question: what is the least-cost path between u and z ? \nrouting algorithm: algorithm that finds that least cost path \nNetwork Layer 4-78 \nRouting algorithm classification \nQ: global or decentralized \ninformation? \nv  all routers have complete \ntopology, link cost info \nv  “link state” algorithms \ndecentralized:  \nv  router knows physically-\nconnected neighbors, link \ncosts to neighbors \nv  iterative process of \ncomputation, exchange of \ninfo with neighbors \nv  “distance vector” algorithms \nQ: static or dynamic? \nv  routes change slowly over \nv  routes change more \n§  periodic update \n§  in response to link \ncost changes \nNetwork Layer 4-79 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-80 \nA Link-State Routing Algorithm \nDijkstra’s algorithm \nv  net topology, link costs \nknown to all nodes \n§  accomplished via “link state \n§  all nodes have same info \nv  computes least cost paths \nfrom one node (‘source”) \nto all other nodes \n§  gives forwarding table for \nv  iterative: after k \niterations, know least cost \npath to k dest.’s \nv  c(x,y): link cost from \nnode x to y;  = ∞ if not \ndirect neighbors \nv  D(v): current value of \ncost of path from source \nv  p(v): predecessor node \nalong path from source to \nv  N': set of nodes whose \nleast cost path definitively \nNetwork Layer 4-81 \nDijsktra’s Algorithm \n1  Initialization:  \n2    N' = {u}  \n3    for all nodes v  \n4      if v adjacent to u  \n5          then D(v) = c(u,v)  \n6      else D(v) = ∞  \n9     find w not in N' such that D(w) is a minimum  \n10    add w to N'  \n11    update D(v) for all v adjacent to w and not in N' :  \n12       D(v) = min( D(v), D(w) + c(w,v) )  \n13    /* new cost to v is either old cost to v or known  \n14     shortest path cost to w plus cost from w to v */  \n15  until all nodes in N'  \nNetwork Layer 4-82 \nDijkstra’s algorithm: example \nv  construct shortest path tree by \ntracing predecessor nodes \nv  ties can exist (can be broken \narbitrarily) \nNetwork Layer 4-83 \nDijkstra’s algorithm: another example \nNetwork Layer 4-84 \nDijkstra’s algorithm: example (2)  \nresulting shortest-path tree from u: \ndestination \nresulting forwarding table in u: \nNetwork Layer 4-85 \nDijkstra’s algorithm, discussion \nalgorithm complexity: n nodes \nv  each iteration: need to check all nodes, w, not in N \nv  n(n+1)/2 comparisons: O(n2) \nv  more efficient implementations possible: O(nlogn) \noscillations possible: \nv  e.g., support link cost equals amount of carried traffic: \ngiven these costs, \nfind new routing…. \nresulting in new costs \ngiven these costs, \nfind new routing…. \nresulting in new costs \ngiven these costs, \nfind new routing…. \nresulting in new costs \nNetwork Layer 4-86 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-87 \nDistance vector algorithm  \nBellman-Ford equation (dynamic programming) \n   dx(y) := cost of least-cost path from x to y \n   dx(y) = min {c(x,v) + dv(y) } \ncost to neighbor v \nmin taken over all neighbors v of x \ncost from neighbor v to destination y \nNetwork Layer 4-88 \nBellman-Ford example  \nclearly, dv(z) = 5, dx(z) = 3, dw(z) = 3 \ndu(z) = min { c(u,v) + dv(z), \n                    c(u,x) + dx(z), \n                    c(u,w) + dw(z) } \n         = min {2 + 5, \n                    5 + 3}  = 4 \nnode achieving minimum is next \nhop in shortest path, used in forwarding table \nB-F equation says: \nNetwork Layer 4-89 \nDistance vector algorithm  \nv  Dx(y) = estimate of least cost from x to y \n§  x maintains  distance vector Dx = [Dx(y): y є N ] \n§  knows cost to each neighbor v: c(x,v) \n§  maintains its neighbors’ distance vectors. For \neach neighbor v, x maintains  \nDv = [Dv(y): y є N ] \nNetwork Layer 4-90 \nv  from time-to-time, each node sends its own \ndistance vector estimate to neighbors \nv  when x receives new DV estimate from neighbor, \nit updates its own DV using B-F equation: \nDx(y) ← minv{c(x,v) + Dv(y)}  for each node y ∊ N \nv  under minor, natural conditions, the estimate Dx(y) \nconverge to the actual least cost dx(y)  \nDistance vector algorithm  \nNetwork Layer 4-91 \niterative, asynchronous: \neach local iteration \nv  local link cost change  \nv  DV update message from \ndistributed: \nv  each node notifies \nneighbors only when its \n§  neighbors then notify their \nneighbors if necessary \nwait for (change in local link \ncost or msg from neighbor) \nrecompute estimates \nif DV to any dest has \nchanged, notify neighbors  \nDistance vector algorithm  \nNetwork Layer 4-92 \nDx(y) = min{c(x,y) + Dy(y), c(x,z) + Dz(y)}  \n             = min{2+0 , 7+1} = 2 \nDx(z) = min{c(x,y) +  \n      Dy(z), c(x,z) + Dz(z)}  \n= min{2+1 , 7+0} = 3 \nNetwork Layer 4-93 \nDx(y) = min{c(x,y) + Dy(y), c(x,z) + Dz(y)}  \n             = min{2+0 , 7+1} = 2 \nDx(z) = min{c(x,y) +  \n      Dy(z), c(x,z) + Dz(z)}  \n= min{2+1 , 7+0} = 3 \nNetwork Layer 4-94 \nDistance vector: link cost changes \nlink cost changes: \nv  node detects local link cost change \nv  updates routing info, recalculates  \ndistance vector \nv  if DV changes, notify neighbors  \nt0 : y detects link-cost change, updates its DV, informs its \n t1 : z receives update from y, updates its table, computes new \nleast cost to x , sends its neighbors its DV. \nt2 : y receives z’s update, updates its distance table.  y’s least costs \ndo not change, so y  does not send a message to z.  \nNetwork Layer 4-95 \nDistance vector: link cost changes \nlink cost changes: \nv  node detects local link cost change \nv  bad news travels slow - “count to \ninfinity” problem! \nv  44 iterations before algorithm \nstabilizes: see text \npoisoned reverse:  \nv  If Z routes through Y to get to X : \n§  Z tells Y its (Z’s) distance to X is infinite (so Y won’t route \nto X via Z) \nv  will this completely solve count to infinity problem? \nNetwork Layer 4-96 \nComparison of LS and DV algorithms \nmessage complexity \nv  LS: with n nodes, E links, O(nE) \nv  DV: exchange between neighbors \n§  convergence time varies \nspeed of convergence \nv  LS: O(n2) algorithm requires \n§  may have oscillations \nv  DV: convergence time varies \n§  may be routing loops \n§  count-to-infinity problem \nrobustness: what happens if \nrouter malfunctions? \n§  node can advertise incorrect \n§  each node computes only its \n§  DV node can advertise \nincorrect path cost \n§  each node’s table used by \n•  error propagate thru \nNetwork Layer 4-97 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-98 \nHierarchical routing \nscale: with 600 million \ndestinations: \nv  can’t store all dest’s in \nrouting tables! \nv  routing table exchange \nwould swamp links!  \nadministrative autonomy \nv  internet = network of \nv  each network admin may \nwant to control routing in \nits own network \nour routing study thus far - idealization  \nv  all routers identical \nv  network “flat” \n… not true in practice \nNetwork Layer 4-99 \nv  aggregate routers into \nregions, “autonomous \nsystems” (AS) \nv  routers in same AS \nrun same routing \n§  “intra-AS” routing \n§  routers in different AS \ncan run different intra-\nAS routing protocol \ngateway router: \nv  at “edge” of its own AS \nv  has  link to router in \nHierarchical routing \nNetwork Layer 4-100 \nInterconnected ASes \nv  forwarding table  \nconfigured by both intra- \nand inter-AS routing \n§  intra-AS sets entries \nfor internal dests \n§  inter-AS & intra-AS \nsets entries for \nexternal dests  \nNetwork Layer 4-101 \nInter-AS tasks \nv  suppose router in AS1 \nreceives datagram \ndestined outside of AS1: \n§  router should forward \npacket to gateway \nrouter, but which one? \nlearn which dests are \nreachable through AS2, \nwhich through AS3 \npropagate this \nreachability info to all \nrouters in AS1 \njob of inter-AS routing! \nNetwork Layer 4-102 \nExample: setting forwarding table in router 1d \nv  suppose AS1 learns (via inter-AS protocol) that subnet x \nreachable via AS3 (gateway 1c), but not via AS2 \n§  inter-AS protocol propagates reachability info to all internal \nv  router 1d determines from intra-AS routing info that its \ninterface I  is on the least cost path to 1c \n§  installs forwarding table entry (x,I) \nNetwork Layer 4-103 \nExample: choosing among multiple ASes \nv  now suppose AS1 learns from inter-AS protocol that subnet \nx is reachable from AS3 and from AS2. \nv  to configure forwarding table, router 1d must determine \nwhich gateway it should forward packets towards for dest x \n§  this is also job of inter-AS routing protocol! \nNetwork Layer 4-104 \nlearn from inter-AS  \nprotocol that subnet  \nx is reachable via  \nmultiple gateways \nuse routing info \nfrom intra-AS  \nprotocol to determine \ncosts of least-cost  \npaths to each \nof the gateways \nhot potato routing: \nchoose the gateway \nthat has the  \nsmallest least cost \ndetermine from \nforwarding table the  \ninterface I that leads  \nto least-cost gateway.  \nEnter (x,I) in  \nforwarding table \nExample: choosing among multiple ASes \nv  now suppose AS1 learns from inter-AS protocol that subnet \nx is reachable from AS3 and from AS2. \nv  to configure forwarding table, router 1d must determine \ntowards which gateway it should forward packets for dest x \n§  this is also job of inter-AS routing protocol! \nv  hot potato routing: send packet towards closest of two \nNetwork Layer 4-105 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-106 \nIntra-AS Routing \nv  also known as interior gateway protocols (IGP) \nv  most common intra-AS routing protocols: \n§  RIP: Routing Information Protocol \n§  OSPF: Open Shortest Path First \n§  IGRP: Interior Gateway Routing Protocol \n(Cisco proprietary) \nNetwork Layer 4-107 \nRIP ( Routing Information Protocol) \nv  included in BSD-UNIX distribution in 1982 \nv  distance vector algorithm \n§  distance metric: # hops (max = 15 hops), each link has cost 1 \n§  DVs exchanged with neighbors every 30 sec in response message (aka \nadvertisement) \n§  each advertisement: list of up to 25 destination subnets (in IP addressing \nsubnet    hops \n      u         1 \n      v         2 \n      x         3 \n      y         3 \n      z         2 \nfrom router A to destination subnets: \nNetwork Layer 4-108 \nRIP: example  \ndestination subnet\n   next  router      # hops to dest \nrouting table in router D \nNetwork Layer 4-109 \ndestination subnet\n   next  router      # hops to dest \nrouting table in router D \n dest     next  hops \nA-to-D advertisement \nRIP: example  \nNetwork Layer 4-110 \nRIP: link failure, recovery  \nif no advertisement heard after 180 sec --> neighbor/\nlink declared dead \n§  routes via neighbor invalidated \n§  new advertisements sent to neighbors \n§  neighbors in turn send out new advertisements (if tables \n§  link failure info quickly (?) propagates to entire net \n§  poison reverse used to prevent ping-pong loops (infinite \ndistance = 16 hops) \nNetwork Layer 4-111 \nRIP table processing \nv  RIP routing tables managed by application-level \nprocess called route-d (daemon) \nv  advertisements sent in UDP packets, periodically \nnetwork       forwarding \n   (IP)             table \nNetwork Layer 4-112 \nOSPF (Open Shortest Path First) \nv  “open”: publicly available \nv  uses link state algorithm  \n§  LS packet dissemination \n§  topology map at each node \n§  route computation using Dijkstra’s algorithm \nv  OSPF advertisement carries one entry per neighbor  \nv  advertisements flooded to entire AS \n§  carried in OSPF messages directly over IP (rather than \nv  IS-IS routing protocol: nearly identical to OSPF \nNetwork Layer 4-113 \nOSPF “advanced” features (not in RIP) \nv  security: all OSPF messages authenticated (to prevent \nmalicious intrusion)  \nv  multiple same-cost paths allowed (only one path in \nv  for each link, multiple cost metrics for different TOS \n(e.g., satellite link cost set “low” for best effort ToS; \nhigh for real time ToS) \nv  integrated uni- and multicast support:  \n§  Multicast OSPF (MOSPF) uses same topology data \nbase as OSPF \nv  hierarchical OSPF in large domains. \nNetwork Layer 4-114 \nHierarchical OSPF \nboundary router \nbackbone router \nNetwork Layer 4-115 \nv  two-level hierarchy: local area, backbone. \n§  link-state advertisements only in area  \n§  each nodes has detailed area topology; only know \ndirection (shortest path) to nets in other areas. \nv  area border routers: “summarize” distances  to nets in \nown area, advertise to other Area Border routers. \nv  backbone routers: run OSPF routing limited to \nv  boundary routers: connect to other AS’s. \nHierarchical OSPF \nNetwork Layer 4-116 \nInternet inter-AS routing: BGP \nv  BGP (Border Gateway Protocol): the de facto \ninter-domain routing protocol \n§  “glue that holds the Internet together” \nv  BGP provides each AS a means to: \n§  eBGP: obtain subnet reachability information from \nneighboring ASs. \n§  iBGP: propagate reachability information to all AS-\ninternal routers. \n§  determine “good” routes to other networks based on \nreachability information and policy. \nv  allows subnet to advertise its existence to rest of \nInternet: “I am here” \nNetwork Layer 4-117 \nv  when AS3 advertises a prefix to AS1: \n§  AS3 promises it will forward datagrams towards that prefix \n§  AS3 can aggregate prefixes in its advertisement \nv  BGP session: two BGP routers (“peers”) exchange BGP \n§  advertising paths to different destination network prefixes (“path vector” \n§  exchanged over semi-permanent TCP connections \nNetwork Layer 4-118 \nBGP basics: distributing path information \nv  using eBGP session between 3a and 1c, AS3 sends prefix \nreachability info to AS1. \n§  1c can then use iBGP do distribute new prefix info to all routers \n§  1b can then re-advertise new reachability info to AS2 over 1b-\nto-2a eBGP session \nv  when router learns of new prefix, it creates entry for \nprefix in its forwarding table. \neBGP session \niBGP session \nNetwork Layer 4-119 \nPath attributes and BGP routes \nv  advertised prefix includes BGP attributes  \n§  prefix + attributes = “route” \nv  two important attributes: \n§  AS-PATH: contains ASs through which prefix \nadvertisement has passed: e.g., AS 67, AS 17  \n§  NEXT-HOP: indicates specific internal-AS router to next-\nhop AS. (may be multiple links from current AS to next-\nv  gateway router receiving route advertisement uses \nimport policy to accept/decline \n§  e.g., never route through AS x \n§  policy-based routing \nNetwork Layer 4-120 \nBGP route selection \nv  router may learn about more than 1 route to \ndestination AS, selects route based on: \n1.  local preference value attribute: policy decision \n2.  shortest AS-PATH  \n3.  closest NEXT-HOP router: hot potato routing \n4.  additional criteria  \nNetwork Layer 4-121 \nBGP messages \nv  BGP messages exchanged between peers over TCP \nv  BGP messages: \n§  OPEN: opens TCP connection to peer and authenticates \n§  UPDATE: advertises new path (or withdraws old) \n§  KEEPALIVE: keeps connection alive in absence of \nUPDATES; also ACKs OPEN request \n§  NOTIFICATION: reports errors in previous msg; also \nused to close connection \nPutting it Altogether: \nHow Does an Entry Get Into a \nRouter’s Forwarding Table? \nv  Answer is complicated! \nv  Ties together hierarchical routing (Section 4.5.3) \nwith BGP (4.6.3) and OSPF (4.6.2). \nv  Provides nice overview of BGP! \nrouting algorithms \nlocal forwarding table \noutput port \n138.16.64/22 \nHow does entry get in forwarding table? \nAssume prefix is \nin another AS. \nHigh-level overview \nRouter becomes aware of prefix \nRouter determines output port for prefix \nRouter enters prefix-port in forwarding table \nHow does entry get in forwarding table? \nRouter becomes aware of prefix \nv  BGP message contains “routes”  \nv  “route” is a prefix and attributes: AS-PATH, NEXT-HOP,\nv  Example: route: \nv  Prefix:138.16.64/22 ;  AS-PATH:  AS3  AS131 ;  \nNEXT-HOP:  201.44.13.125 \nRouter may receive multiple routes \nv  Router may receive multiple routes for same prefix \nv  Has to select one route \nv  Router selects route based on shortest AS-PATH \nSelect best BGP route to prefix \nv  Example: \nv  AS2 AS17  to 138.16.64/22  \nv  AS3 AS131 AS201 to 138.16.64/22  \nv  What if there is a tie? We’ll come back to that! \nFind best intra-route to BGP route \nv  Use selected route’s NEXT-HOP attribute  \n§  Route’s NEXT-HOP attribute is the IP address of the \nrouter interface that begins the AS PATH.  \nv  Example:  \nv  AS-PATH:  AS2  AS17 ;  NEXT-HOP: 111.99.86.55 \nv  Router uses OSPF to find shortest path from 1c to \n111.99.86.55 \n111.99.86.55 \nRouter identifies port for route \nv  Identifies port along the OSPF shortest path \nv  Adds prefix-port entry to its forwarding table: \n§  (138.16.64/22 , port 4)  \nHot Potato Routing \nv  Suppose there two or more best inter-routes. \nv  Then choose route with closest NEXT-HOP \n§  Use OSPF to determine which gateway is closest \n§  Q: From 1c, chose AS3 AS131 or AS2 AS17? \n§   A: route AS3 AS201 since it is closer  \nRouter becomes aware of prefix \nvia BGP route advertisements from other routers \nDetermine router output port for prefix \nUse BGP route selection to find best inter-AS route \nUse OSPF to find best intra-AS route  leading to best \ninter-AS route \nRouter identifies router port for that best route \nEnter prefix-port entry in forwarding table \nHow does entry get in forwarding table? \nNetwork Layer 4-132 \nBGP routing policy \nv  A,B,C are provider networks \nv  X,W,Y are customer (of provider networks) \nv  X is dual-homed: attached to two networks \n§  X does not want to route from B via X to C \n§  .. so X will not advertise to B a route to C \nNetwork Layer 4-133 \nBGP routing policy (2) \nv  A advertises path AW  to B \nv  B advertises path BAW to X  \nv  Should B advertise path BAW to C? \n§  No way! B gets no “revenue” for routing CBAW since neither W nor \nC are B’s customers  \n§  B wants to force C to route to w via A \n§  B wants to route only to/from its customers! \nNetwork Layer 4-134 \nWhy different Intra-, Inter-AS routing ?  \nv  inter-AS: admin wants control over how its traffic \nrouted, who routes through its net.  \nv  intra-AS: single admin, so no policy decisions needed \nv  hierarchical routing saves table size, reduced update \nperformance:  \nv  intra-AS: can focus on performance \nv  inter-AS: policy may dominate over performance \nNetwork Layer 4-135 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format \n§  IPv4 addressing \n4.5 routing algorithms \n§  link state \n§  distance vector \n§  hierarchical routing \n4.6 routing in the Internet \n4.7 broadcast and multicast \nChapter 4: outline \nNetwork Layer 4-136 \nduplication \nduplication \ncreation/transmission \nBroadcast routing \nv  deliver packets from source to all other nodes \nv  source duplication is inefficient: \nv  source duplication: how does source determine \nrecipient addresses? \nNetwork Layer 4-137 \nIn-network duplication \nv  flooding: when node receives broadcast packet, \nsends copy to all neighbors \n§  problems: cycles & broadcast storm \nv  controlled flooding: node only broadcasts pkt if it \nhasn’t broadcast same packet before \n§  node keeps track of packet ids already broadacsted \n§  or reverse path forwarding (RPF): only forward packet \nif it arrived on shortest path between node and source \nv  spanning tree: \n§  no redundant packets received by any node \nNetwork Layer 4-138 \n(a) broadcast initiated at A \n(b) broadcast initiated at D \nSpanning tree \nv  first construct a spanning tree \nv  nodes then forward/make copies only along \nspanning tree \nNetwork Layer 4-139 \n(a) stepwise construction of \nspanning tree (center: E) \n(b) constructed spanning \nSpanning tree: creation \nv  center node \nv  each node sends unicast join message to center \n§  message forwarded until it arrives at a node already \nbelonging to spanning tree \nNetwork Layer 4-140 \nMulticast routing: problem statement \ngoal: find a tree (or trees) connecting routers having \nlocal mcast group members  \nv  tree: not all paths between routers used \nv  shared-tree: same tree used by all group members \nshared tree \nsource-based trees \nv  source-based: different tree from each sender to rcvrs \nNetwork Layer 4-141 \nApproaches for building mcast trees \napproaches: \nv  source-based tree: one tree per source \n§  shortest path trees \n§  reverse path forwarding \nv  group-shared tree: group uses one tree \n§  minimal spanning (Steiner)  \n§  center-based trees \n…we first look at basic approaches, then specific protocols \nadopting these approaches \nNetwork Layer 4-142 \nShortest path tree \nv  mcast forwarding tree: tree of shortest path \nroutes from source to all receivers \n§  Dijkstra’s algorithm \nrouter with attached \ngroup member \nrouter with no attached \ngroup member \nlink used for forwarding, \ni indicates order link \nadded by algorithm \nNetwork Layer 4-143 \nReverse path forwarding \nif (mcast datagram received on incoming link on \nshortest path back to center) \n   then flood datagram onto all outgoing links \n   else ignore datagram \nv  rely on router’s knowledge of unicast shortest \npath from it  to sender \nv  each router has simple forwarding behavior: \nNetwork Layer 4-144 \nReverse path forwarding: example \nv  result is a source-specific reverse SPT \n§  may be a bad choice with asymmetric links \nrouter with attached \ngroup member \nrouter with no attached \ngroup member \ndatagram will be  forwarded \ndatagram will not be  \nNetwork Layer 4-145 \nReverse path forwarding: pruning \nv  forwarding tree contains subtrees with no mcast group \n§  no need to forward datagrams down subtree \n§  “prune” msgs sent upstream by router with no \ndownstream group members \nrouter with attached \ngroup member \nrouter with no attached \ngroup member \nprune message \nlinks with multicast \nNetwork Layer 4-146 \nShared-tree: steiner tree \nv  steiner tree: minimum cost tree connecting all \nrouters with attached group members \nv  problem is NP-complete \nv  excellent heuristics exists \nv  not used in practice: \n§  computational complexity \n§  information about entire network needed \n§  monolithic: rerun whenever a router needs to join/\nNetwork Layer 4-147 \nCenter-based trees \nv  single delivery tree shared by all \nv  one router identified as “center” of tree \nv  to join: \n§  edge router sends unicast join-msg addressed to center \n§  join-msg “processed” by intermediate routers and \nforwarded towards center \n§  join-msg either hits existing tree branch for this center, \nor arrives at center \n§  path taken by join-msg becomes new branch of tree for \nthis router \nNetwork Layer 4-148 \nCenter-based trees: example \nsuppose R6 chosen as center: \nrouter with attached \ngroup member \nrouter with no attached \ngroup member \npath order in which join \nmessages generated \nNetwork Layer 4-149 \nInternet Multicasting Routing: DVMRP \nv  DVMRP: distance vector multicast routing \nprotocol, RFC1075 \nv  flood and prune:  reverse path forwarding, source-\n§  RPF tree based on DVMRP’s own routing tables \nconstructed by communicating DVMRP routers  \n§  no assumptions about underlying unicast \n§  initial datagram to mcast group flooded  everywhere \n§  routers not wanting group: send upstream prune msgs \nNetwork Layer 4-150 \nDVMRP: continued… \nv  soft state: DVMRP router periodically (1 min.) \n“forgets”  branches are pruned:  \n§  mcast data again flows down unpruned branch \n§  downstream router: reprune or else continue to receive \nv  routers can quickly regraft to tree  \n§  following IGMP join at leaf \nv  odds and ends \n§  commonly implemented in commercial router \nNetwork Layer 4-151 \nQ: how to connect “islands” of multicast  routers in a \n“sea” of unicast routers?  \nv  mcast datagram encapsulated inside “normal” (non-\nmulticast-addressed) datagram \nv  normal IP datagram sent thru “tunnel” via regular IP unicast \nto receiving mcast router (recall IPv6 inside IPv4 tunneling) \nv  receiving mcast router unencapsulates to get mcast \nphysical topology \nlogical topology \nNetwork Layer 4-152 \nPIM: Protocol Independent Multicast \nv  not dependent on any specific underlying unicast \nrouting algorithm (works with all) \nv  two different multicast distribution scenarios : \nv  group members densely \npacked, in “close” \nv  bandwidth more plentiful \nv  # networks with group \nmembers small wrt # \ninterconnected networks \nv  group members “widely \nv  bandwidth not plentiful \nNetwork Layer 4-153 \nConsequences of sparse-dense dichotomy:  \nv  group membership by \nrouters assumed until \nrouters explicitly prune \nv  data-driven construction on \nmcast tree (e.g., RPF) \nv  bandwidth and non-group-\nrouter processing profligate \nv  no membership until routers \nexplicitly join \nv  receiver- driven construction \nof mcast tree (e.g., center-\nv  bandwidth and non-group-\nrouter processing conservative \nNetwork Layer 4-154 \nPIM- dense mode \nflood-and-prune RPF: similar to DVMRP but… \nv  underlying unicast protocol provides RPF info \nfor incoming datagram \nv  less complicated (less efficient) downstream \nflood than DVMRP reduces reliance on \nunderlying routing algorithm \nv  has protocol mechanism for router to detect it \nis a leaf-node router \nNetwork Layer 4-155 \nPIM - sparse mode \nv  center-based approach \nv  router sends join msg to \nrendezvous point (RP) \n§  intermediate routers \nupdate state and \nforward join \nv  after joining via RP, router \ncan switch to source-\nspecific tree \n§  increased \nperformance: less \nconcentration, shorter \nall data multicast \nfrom rendezvous \nNetwork Layer 4-156 \nv  unicast data to RP, \nwhich distributes \ndown RP-rooted tree \nv  RP can extend mcast \ntree upstream to \nv  RP can send stop msg \nif no attached \n§  “no one is listening!” \nall data multicast \nfrom rendezvous \nPIM - sparse mode \nNetwork Layer 4-157 \n4.1 introduction \n4.2 virtual circuit and \ndatagram networks \n4.3 what’s inside a router \n4.4 IP: Internet Protocol \n§  datagram format, IPv4 \naddressing, ICMP, IPv6 \n4.5 routing algorithms \n§  link state, distance vector, \nhierarchical routing \n4.6 routing in the Internet \n§  RIP, OSPF, BGP \n4.7 broadcast and multicast \nChapter 4: done! \nv  understand principles behind network layer services: \n§  network layer service models, forwarding versus routing \nhow a router works, routing (path selection), broadcast, \nv  instantiation, implementation in the Internet",
    "unit": "Unit 1",
    "source_type": "notes",
    "book_priority": 0,
    "source_file": "Network Layer",
    "chunk_id": "Network Layer_chunk_0"
  },
  {
    "text": "Packet switching - Routing – Distance Vector and Link State Algorithms – RIP, OSPF and BGP - IPV4 \nPacket Format and Addressing – Effective IP address management techniques – Subnetting – CIDR – \nVLSM – DHCP – NAT – ICMP – Need for IPv6 – Addressing methods and types in IPv6 – IPv6 header – \nAdvantages of IPv6 – Transition from IPv4 to IPv6. \n1. NETWORK LAYER \n• The transport segment from the sending host is encapsulated into datagrams. \n• On the receiving side, the segments are delivered to the transport layer by the network layer \nprotocols in every host. \n• Routers examine header fields in all IP datagrams passing through them. \nForwarding: (Getting through a single interchange) packets from a router's input to the appropriate \nrouter output. \nRouting: (Planning a trip from source to destination) Determines the route taken by packets from source \nto destination. \nThe network layer is the third layer in the OSI model and plays a critical role in end-to-end \ncommunication by providing services such as: \n• Logical addressing: This includes assigning IP addresses to devices on a network to identify them \nuniquely and ensure proper packet delivery. \n• Routing: It involves determining the best path for data packets to travel from source to \ndestination across multiple networks (e.g., the internet). \n• Packet forwarding: This refers to the movement of packets across network devices (routers and \nswitches) based on the routing table and logical address. \n• Fragmentation and reassembly: This process breaks large packets into smaller fragments for \ntransmission over networks with different Maximum Transmission Unit (MTU) sizes, then \nreassembles them at the destination. \nPacket Switching \nDefinition: A method of breaking data into packets for transmission across networks, reassembled at \nthe destination. \n1. Datagram Switching: (Connectionless) Packets take different paths to the destination (e.g., IP \n• Advantages: Flexibility, fault tolerance, efficient use of network resources. \n• Disadvantages: Possibility of packet loss, out-of-order delivery, and delays due to congestion. \n2. Virtual Circuit Switching: (Connection-Oriented) A fixed path is established before transmission \n(e.g., MPLS – Multi Protocol Label Switching). \n• Advantages: More reliable, predictable performance. \n• Disadvantages: Less flexible, as the path must be predefined and maintained. \nVirtual Circuit Networks and Datagram Networks: \nVirtual Circuit Networks \nDatagram Networks \nConnection-oriented (path established \nbefore communication) \nConnectionless (no path established \nbeforehand) \nReliability \nMore reliable (due to fixed path and \nconnection) \nLess reliable (packets may be lost, \nduplicated, or unordered) \nFixed path for the entire communication \nIndependent routing for each packet \nPackets follow the same path and are \ndelivered in order \nPackets can take different paths and may \narrive out of order \nHandled by the network (dedicated path \nallows easy management) \nHandled by upper layers (e.g., transport \nRequires setup time to establish the \nNo setup time required for communication \nATM (Asynchronous Transfer Mode), \nFrame Relay, X.25 \nIP (Internet Protocol), UDP \nForwarding Table in Virtual Circuits (VC) and Datagram Networks \nVirtual Circuits (VC) \nForwarding Table: In a Virtual Circuit network, each router maintains a forwarding table that maps \nincoming VC numbers to outgoing VC numbers and interfaces. This table is essential for directing the \npackets along the pre-established path. \nExample Forwarding Table:  \nExplanation: \n• Incoming Interface: The interface where the \npacket arrives. \n• Incoming VC: The virtual circuit number of \nthe incoming packet. \n• Outgoing Interface: The interface where the \npacket should be forwarded. \n• Outgoing VC: The virtual circuit number \nassigned to the outgoing packet. \nDatagram Networks \nForwarding Table: In Datagram Networks, each router maintains a forwarding table that maps IP \naddress prefixes to outgoing interfaces. Each packet is routed independently based on the destination IP \nSignaling Protocols in Virtual Circuits (VC) and Datagram Networks \nRouting and Forwarding: \n• Routing is about selecting the best path to take across the network. \n• Forwarding is about sending a packet along that path based on the destination address. \nThe process of determining the best path \nfor packets to take. \nThe process of moving a packet from input to \noutput based on the destination. \nRouters use routing protocols and \nalgorithms to decide on paths. \nRouters or network devices forward packets \nusing forwarding tables. \nOccurs when a router learns the network \ntopology and updates routing tables. \nHappens continuously when a packet arrives \nat the router. \nInvolves choosing the overall route from \nsource to destination. \nInvolves deciding the next hop for a specific \nOSPF, BGP, RIP \nIP forwarding (in routers), MPLS forwarding \nFlowchart Structure – Routing \nGraph Extraction: \ngraph: G = (N,E) \nN = set of routers = { u, v, w, x, y, z } \nE = set of links ={ (u,v), (u,x), (v,x), (v,w), (x,w), (x,y), (w,y), \n(w,z), (y,z) } \nc(x,x’) = cost of link (x,x’) \ne.g., c(w,z) = 5 \ncost could always be 1, or inversely related to bandwidth, or inversely related to congestion \ncost of path (x1, x2, x3,…, xp) = c(x1,x2) + c(x2,x3) + … + c(xp-1,xp) \nRouting algorithm classification \nBased on Path Calculation Method: \n(i) Static Routing Algorithms: \n• The paths are preconfigured by the network administrator and do not change unless manually \n• Example: Manual configuration of routes in a router. \n• Pros: Simple, predictable. \n• Cons: Not adaptive to network changes, requires manual intervention. \n(ii) Dynamic Routing Algorithms: \n• The paths are automatically determined and adjusted based on network topology changes and \ntraffic conditions. Routers exchange routing information to adapt to network changes. \n• Example: RIP, OSPF, BGP. \n• Pros: Adaptive to changes, can handle network failures and congestion. \n• Cons: More complex, requires more resources (e.g., processing power and memory). \nStatic Routing \nDynamic Routing \nConfiguration \nManually set by the admin. \nAutomatically updated by routers. \nAdaptability \nNo automatic updates. \nUpdates automatically with network \nScalability \nBest for small networks. \nBest for large networks. \nLow; no need for updates. \nHigher due to constant updates. \nFault Tolerance \nLimited; manual fixes needed. \nHigh; reroutes automatically. \nManual routes, Static IP routing. \nRIP, OSPF, EIGRP, BGP. \nSimple, low overhead. \nFlexible, adapts to changes. \nInflexible, hard to manage large \nMore complex, higher overhead. \nRouting protocols are vital for ensuring efficient data transmission across networks. This assignment \ndelves into three critical routing protocols: Routing Information Protocol (RIP), Open Shortest Path First \n(OSPF), and Border Gateway Protocol (BGP). While RIP and OSPF are primarily intra-domain protocols, \nBGP operates at the inter-domain level, managing routing across Autonomous Systems (AS). \n1. Routing Information Protocol (RIP) \nRIP, one of the earliest intra-AS routing protocols, is still utilized in smaller networks due to its simplicity. \nDefined in: \n• RIP Version 1: [RFC 1058] \n• RIP Version 2: [RFC 2453] (adds backward compatibility and route aggregation). \nIt is a distance-vector protocol based on hop count as the cost metric. \nKey Mechanisms \n1. Hop Count Metric \na. Each link has a cost of 1 hop. \nb. Maximum allowable cost: 15 hops, limiting RIP to smaller ASs. \n2. Routing Table \nEach router maintains a table with: \na. Destination subnet. \nb. Next hop router. \nc. Number of hops to destination. \n3. Routing Updates \na. Routers exchange distance vectors (RIP advertisements) every 30 seconds. \nb. Advertisements include up to 25 subnets. \n4. Failure Detection \na. No update within 180 seconds marks the neighbor unreachable. \n5. Implementation \na. Application-layer protocol using UDP on port 520. \nb. UNIX uses a routed process for routing updates. \nRIP Message Format \nThe message format is used to share information among different routers. The RIP contains the following \nfields in a message: \no Command: It is an 8-bit field that is used for request or reply. The value of the request is 1, and the \nvalue of the reply is 2. \no Version: Here, version means that which version of the protocol we are using. Suppose we are \nusing the protocol of version1, then we put the 1 in this field. \no Reserved: This is a reserved field, so it is filled with zeroes. \no Family: It is a 16-bit field. As we are using the TCP/IP family, so we put 2 value in this field. \no Network Address: It is defined as 14 bytes field. If we use the IPv4 version, then we use 4 bytes, \nand the other 10 bytes are all zeroes. \no Distance: The distance field specifies the hop count, i.e., the number of hops used to reach the \ndestination. \nHow does the RIP work? \nIf there are 8 routers in a network where Router 1 wants to send the data to Router 3. If the network is \nconfigured with RIP, it will choose the route which has the least number of hops. There are three routes in \nthe above network, i.e., Route 1, Route 2, and Route 3. The Route 2 contains the least number of hops, \ni.e., 2 where Route 1 contains 3 hops, and Route 3 contains 4 hops, so RIP will choose Route 2. \nAdvantages and Disadvantages \nDisadvantages \nSimplicity, easy setup \nLimited to 15-hop networks \nWidely supported \nSlow convergence (30-second updates) \nDecentralized operations \nVulnerable to count-to-infinity issue \nComparison with OSPF \n• Routing Type: RIP uses distance-vector; OSPF uses link-state. \n• Scalability: RIP is limited by 15 hops, while OSPF is more scalable. \n• Convergence: OSPF converges faster than RIP. \n2. Open Shortest Path First (OSPF) \nOSPF is a robust and scalable intra-AS link-state routing protocol designed to replace RIP. Defined in \nRFC 2328, it uses Dijkstra’s shortest-path algorithm. \nKey Mechanisms \n1. Link-State Flooding \na. Broadcasts link-state updates to all routers within an AS. \nb. Updates are triggered by topology changes or every 30 minutes. \n2. Routing Table Calculation \na. Builds a complete AS topology map. \nb. Shortest-path tree computed using Dijkstra's algorithm. \n3. Link Costs \na. Configurable by administrators to reflect hop count, bandwidth, etc. \n4. Integration with IP \na. OSPF uses IP protocol 89 and manages its own reliability and authentication. \nOSPF Message Format \nThe following are the fields in an OSPF message format: \no Version: It is an 8-bit field that specifies the OSPF protocol version. \no Type: It is an 8-bit field. It specifies the type of the OSPF packet. \no Message: It is a 16-bit field that defines the total length of the message, including the header. \nTherefore, the total length is equal to the sum of the length of the message and header. \no Source IP address: It defines the address from which the packets are sent. It is a sending routing \nIP address. \no Area identification: It defines the area within which the routing takes place. \no Checksum: It is used for error correction and error detection. \no Authentication type: There are two types of authentication, i.e., 0 and 1. Here, 0 means for none \nthat specifies no authentication is available and 1 means for pwd that specifies the password-\nbased authentication. \no Authentication: It is a 32-bit field that contains the actual value of the authentication data. \nAdvanced Features \n• Security: MD5-based authentication ensures message authenticity. \n• Multiple Same-Cost Paths: Enables load balancing. \n• Hierarchical Routing: Divides AS into areas connected by Area Border Routers (ABRs). \nAdvantages and Disadvantages \nDisadvantages \nEfficient convergence \nMore complex configuration \nSuitable for large networks \nHigher memory and CPU usage \nCustomizable link costs \n3. Border Gateway Protocol (BGP) \nBGP is an inter-domain routing protocol that manages routing between ASs. It is defined in RFC 4271 and \nensures efficient and policy-driven routing on the global Internet. \nKey Features \n1. Path Vector Protocol \na. Maintains the path information (AS Path) for each destination. \nb. Avoids routing loops by rejecting routes with its own AS in the path. \n2. Policy-Based Routing \na. Allows network operators to define routing policies based on business or technical \nrequirements. \n3. Message Types \na. OPEN: Establishes peer connections. \nb. UPDATE: Communicates route additions/removals. \nc. KEEPALIVE: Ensures session liveness. \nd. NOTIFICATION: Indicates errors or session termination. \n4. Scalability \na. Suitable for large-scale networks with thousands of ASs. \nb. Supports CIDR for efficient route aggregation. \nBGP Packet Format \nNow we will see the format in which the packet travels. The following are the fields in a BGP packet \n1. Marker: It is a 32-bit field which is used for the authentication purpose. \n2. Length: It is a 16-bit field that defines the total length of the message, including the header. \n3. Type: It is an 8-bit field that defines the type of the packe \nAdvantages and Disadvantages \nDisadvantages \nScalable for large networks \nConfiguration complexity \nPolicy-driven routing \nVulnerable to misconfigurations \nAvoids loops with AS Path \nBased on Routing Information Exchange: \n (i) Link-State Routing Algorithm: (GLOBAL): \n• Routers maintain a complete map of the network topology and use this information to calculate the \nbest path. Every router exchanges information about its directly connected links with all other \n• Example: OSPF (Open Shortest Path First), IS-IS (Intermediate System to Intermediate System). \n• Working: Routers send link-state advertisements (LSAs) to all other routers, which then use \nalgorithms (like Dijkstra's algorithm) to calculate the shortest paths. \n• Pros: Faster convergence, avoids routing loops. \n• Cons: Requires more resources (memory, CPU) for maintaining the topology map. \n Dijkstra's Algorithm \nDijkstra's algorithm is a fundamental algorithm used in link-state routing protocols like Open Shortest \nPath First (OSPF). It finds the shortest path from a source node to all other nodes in a weighted graph. \nSteps of Dijkstra's Algorithm \n1. Initialization: \na. Set the distance to the source node as 0. \nb. Set the distance to all other nodes as infinity (∞). \nc. Mark all nodes as unvisited. \n2. Select Node: \na. Pick the unvisited node with the smallest tentative distance (initially the source node). \n3. Update Neighbors: \na. For each unvisited neighbor of the current node: \ni. Calculate the tentative distance: Tentative Distance=Current Distance+Edge \nii. If the calculated distance is smaller than the known distance, update it. \n4. Mark Visited: \na. Once all neighbors are considered, mark the current node as visited. \na. Repeat steps 2–4 until all nodes are visited or the shortest path to the destination is found. \nNotation Used in Dijkstra's Algorithm \n• G(V,E): A graph with vertices V and edges E. \n• d[u]: The tentative distance from the source node s to node u. \n• w(u,v): The weight of the edge between nodes u and v. \n• : A priority queue (min-heap) of unvisited nodes, sorted by tentative distance. \nExample of Dijkstra's Algorithm \nGraph Details: \n• Nodes: A,B,C,D,E,F \n• Edges and Weights: \nInitial State: \nNode Distance from A \n1. Start at A: \na. Tentative distances: d[B]=4,d[C]=2. \nb. Mark A as visited. \n2. Move to C (smallest tentative distance): \na. Tentative distances: d[E]=5 (via C). \nb. Mark C as visited. \n3. Move to B: \na. Tentative distances: d[D]=14 (via B). \nb. Mark B as visited. \n4. Move to E: \na. Tentative distances: d[D]=9,d[F]=11. \nb. Mark E as visited. \n5. Move to D: \na. Tentative distances: d[F]=10 (via D). \nb. Mark D as visited. \n6. Move to F: \na. All nodes are visited. \nFinal Result: \nDistance from A \nDiscussion on Dijkstra's Algorithm \nAdvantages: \n1. Guaranteed Shortest Path: Always finds the shortest path in graphs with non-negative edge \n2. Efficient: Runs in O(V2) for dense graphs or O((V+E)logV) with a priority queue. \n3. Widely Used: Employed in real-world routing protocols like OSPF. \nLimitations: \n1. Non-Negative Weights Only: Does not work correctly with graphs containing negative weight \n2. Not Suitable for Frequent Updates: In networks with highly dynamic topology, repeated \nrecalculations can be computationally expensive. \nApplications: \n• Used in routing protocols (e.g., OSPF) to compute the shortest path. \n• Traffic engineering and navigation systems. \n• Network optimization and resource allocation. \n(ii) Distance-Vector Routing Algorithms (DECENTRALIZED): \n• Routers send updates about their routing tables to their neighbors periodically. The distance to a \ndestination is typically measured by the number of hops. \n• Example: RIP (Routing Information Protocol), BGP (Border Gateway Protocol) (in some modes). \n• Working: Routers exchange routing tables, and each router updates its table based on information \nfrom neighbors. They calculate the shortest path based on distance metrics (like hop count). \n• Pros: Simple to implement. \n• Cons: Slow to converge, prone to routing loops (especially in large networks). \nDistance vector algorithm \nBellman-Ford Algorithm \nclearly, dv(z) = 5, dx(z) = 3, dw(z) = 3B-F equation says: \ndu(z) = min { c(u,v) + dv(z), \nc(u,x) + dx(z), \nc(u,w) + dw(z) } \n= min {2 + 5,1 + 3, 5 + 3} = 4 \nnode achieving minimum is next \nhop in shortest path, used in forwarding table \nComparison Table \nLink State (LS) \nDistance Vector (DV) \n- Each node floods link-state packets \n(LSPs) to all other nodes.  \n- With n nodes and E links: O(nE) \nmessages are sent. \n- Only exchanges information with direct \n- Message complexity depends on \nconvergence time and varies with \nnetwork conditions. \nConvergence \n- Dijkstra’s algorithm runs in O(n2) time \n(or O(nlogn) with optimized \nimplementations).  \n- Requires O(nE) messages for global \ntopology updates.  \n- May exhibit oscillations under certain \nconditions. \n- Convergence time is variable and \ndepends on network size and changes.  \n- Susceptible to issues like routing loops \nand count-to-infinity problems, which \nslow down convergence. \n- A malfunctioning node can advertise \nincorrect link costs.  \n- Each node computes its own table \nindependently, so errors are localized. \n- A faulty node can advertise incorrect \npath costs.  \n- Errors can propagate through the \nnetwork as other nodes rely on shared \ndistance vectors. \n- Oscillations may occur during frequent \nupdates or rapid topology changes. \n- Vulnerable to routing loops, especially \nin the absence of mechanisms like split \nhorizon or poison reverse.  \n- Count-to-infinity problem leads to slow \nerror correction. \nPropagation \n- Errors remain localized since each \nnode computes its table based on its \nown topology knowledge. \n- Errors spread across the network \nbecause each node’s table is used by \nothers to compute paths.",
    "unit": "Unit 3",
    "source_type": "notes",
    "book_priority": 0,
    "source_file": "UNIT III NETWORK LAYER1",
    "chunk_id": "UNIT III NETWORK LAYER1_chunk_0"
  },
  {
    "text": "Packet switching - Routing – Distance Vector and Link State Algorithms – RIP, OSPF and \nBGP - IPV4 Packet Format and Addressing – Effective IP address management techniques – \nSubnetting – CIDR – VLSM – DHCP – NAT – ICMP – Need for IPv6 – Addressing methods and \ntypes in IPv6 – IPv6 header – Advantages of IPv6 – Transition from IPv4 to IPv6. \nIPv4 Datagram Header \nCharacteristics of IPv4 \n• IPv4 could be a 32-Bit IP Address. \n• IPv4 could be a numeric address, and its bits are separated by a dot. \n• The number of header fields is twelve and the length of the header \nfield is twenty. \n• It has Unicast, broadcast, and multicast style of addresses. \n• IPv4 supports VLSM (Virtual Length Subnet Mask). \n• IPv4 uses the Post Address Resolution Protocol to map to the MAC \n• RIP may be a routing protocol supported by the routed daemon. \n• Networks ought to be designed either manually or with DHCP. \n• Packet fragmentation permits from routers and causing host. \nIPv4 Datagram Header \n• VERSION: Version of the IP protocol (4 bits), which is 4 for IPv4  \n• HLEN: IP header length (4 bits), which is the number of 32 bit words \nin the header. The minimum value for this field is 5 and the \nmaximum is 15.  \n• Type of service: Low Delay, High Throughput, Reliability (8 bits)  \n• Total Length: Length of header + Data (16 bits), which has a \nminimum value 20 bytes and the maximum is 65,535 bytes.  \n• Identification: Unique Packet Id for identifying the group of \nfragments of a single IP datagram (16 bits)  \n• Flags: 3 flags of 1 bit each : reserved bit (must be zero), do not \nfragment flag, more fragments flag (same order)  \n• Fragment Offset: Represents the number of Data Bytes ahead of \nthe particular fragment in the particular Datagram. Specified in terms \nof number of 8 bytes, which has the maximum value of 65,528 \n• Time to live: Datagram’s lifetime (8 bits), It prevents the datagram \nto loop through the network by restricting the number of Hops taken \nby a Packet before delivering to the Destination. \n• Protocol: Name of the protocol to which the data is to be passed (8 \n• Header Checksum: 16 bits header checksum for checking errors in \nthe datagram header  \n• Source IP address: 32 bits IP address of the sender  \n• Destination IP address: 32 bits IP address of the receiver  \n• Option: Optional information such as source route, record route. \nUsed by the Network administrator to check whether a path is \nworking or not. \nThe IPv4 packet has a fixed header with a variable-length data section. Key fields in the \nDescription \nIPv4 (value = 4) \nHeader Length \nLength of the header in 32-bit words (min = 5, \nType of Service (TOS) \nPriority and quality of service options \nTotal Length \nTotal length of the packet (header + data) \nIdentification \nUnique identifier for fragmented packets \nControl fragmentation \nFragment Offset \nIndicates the fragment’s position in the \noriginal packet \nTime to Live (TTL) \nLimits the lifetime of the packet (hop count) \nIndicates the higher-layer protocol (e.g., TCP, \nHeader Checksum \nValidates the header’s integrity \nSource Address \nIP address of the sender \nDestination Address \nIP address of the receiver \nAdditional settings (rarely used) \nPayload containing upper-layer protocol \ninformation \nIPv6 Header \nSize (bits) \nDescription \nIPv6 (value = 6) \nTraffic Class \nSimilar to IPv4’s TOS; QoS priority. \nIdentifies packets requiring special \nPayload Length \nLength of the payload (data). \nNext Header \nIndicates the next protocol (e.g., TCP, \nSimilar to IPv4’s TTL; limits the number of \nSource Address \nIPv6 address of the sender. \nDestination \nIPv6 address of the receiver. \nNeed for IPv6 \nIPv4 has limitations that necessitated the development of IPv6: \nAddress Exhaustion: IPv4 provides ~4.3 billion addresses, which is insufficient for \nthe growing number of devices. \nNetwork Growth: IPv6 supports a virtually unlimited number of devices. \nImproved Security: IPv6 integrates IPsec for secure communication. \nBetter QoS: IPv6 offers improved Quality of Service (QoS) for real-time applications. \nSimplified Configuration: IPv6 includes autoconfiguration capabilities, reducing \nadministrative overhead. \nAddressing Methods and Types in IPv6: \nDescription \nIdentifies a single interface; packets are delivered to one specific \nIdentifies multiple interfaces; packets are delivered to all members \nof a group. \nIdentifies multiple interfaces; packets are delivered to the nearest \none in terms of routing distance. \nAddress Type \nDescription \nGlobal Unicast \nUnique addresses routable over the internet. \nFE80::/10 Automatically configured addresses for communication \nwithin a link; not routable beyond the link. \nUnique Local \nFor local communications; not routable over the internet \nbut unique within an organization. \nUsed for multicast groups; packets are delivered to \nmultiple destinations. \nUnspecified \nRepresents no address (used as a placeholder). \nUsed by a device to send packets to itself (loopback \nReserved for special purposes, including future use and \ntesting (e.g., FF02::1 for all nodes). \nAdvantages of IPv6 \nLarger Address Space: IPv6 provides 21282^{128}2128 addresses, far more than \nIPv4's 2322^{32}232. \nSimplified Header: Improves processing efficiency by routers. \nImproved Security: Built-in IPsec support for authentication and encryption. \nAutoconfiguration: Supports stateful (DHCPv6) and stateless (SLAAC) \nconfiguration. \nBetter Support for Mobile Devices: Allows seamless mobility with minimal \ndisruption. \nEliminates NAT: Direct addressing removes the need for Network Address \nTranslation. \nTransition from IPv4 to IPv6 \nChallenges in Transition \nIPv4 and IPv6 are not directly compatible. \nRequires dual-stack operation and gradual migration. \nTechniques for Transition \n1. Dual-Stack: Devices run both IPv4 and IPv6 simultaneously. \n2. Tunneling: IPv6 packets are encapsulated within IPv4 packets for transport over an \nIPv4 network. \no Examples: 6to4, Teredo, and ISATAP. \n3. Translation: Converts IPv4 packets to IPv6 and vice versa using NAT64/DNS64. \nEffective IP Address Management Techniques: \n1. Subnetting \na. Definition: Dividing a large IP network into smaller, manageable sub-\nnetworks (subnets). \nb. Purpose: Optimizes IP address usage, improves network performance, and \nenhances security. \nc. Example: Splitting 192.168.1.0/24 into two subnets: 192.168.1.0/25 \nand 192.168.1.128/25. \n2. CIDR (Classless Inter-Domain Routing) \na. Definition: A method to allocate IP addresses efficiently by allowing flexible \nprefix lengths, replacing the rigid class-based addressing. \nb. Purpose: Reduces wasted IP addresses and simplifies routing. \nc. Example: Instead of a classful network 192.168.1.0/24, CIDR can allocate \nsmaller networks like 192.168.1.0/28 for 16 addresses. \n3. VLSM (Variable Length Subnet Masking) \na. Definition: Extends subnetting by allowing different subnets to have varying \nsizes within the same network. \nb. Purpose: Maximizes efficient IP address allocation based on the number of \nrequired hosts. \nc. Example: A 192.168.1.0/24 network can be divided as 192.168.1.0/26 \nfor 64 hosts and 192.168.1.64/27 for 32 hosts. \n4. DHCP (Dynamic Host Configuration Protocol) \na. Definition: Automatically assigns IP addresses and other network \nconfigurations to devices. \nb. Purpose: Simplifies IP management in dynamic or large networks. \nc. Example: Assigns IP addresses from a pool (192.168.1.100 to \n192.168.1.200) to devices when they connect. \n5. NAT (Network Address Translation) \na. Definition: Allows multiple devices on a private network to access the \nInternet using a single public IP address. \nb. Purpose: Conserves public IPs, enhances security, and provides IP address \nc. Example: A private network (192.168.1.0/24) is translated to a single \npublic IP (203.0.113.1) for external communication. \n6. ICMP (Internet Control Message Protocol) \na. Definition: Used for diagnostic and error-reporting purposes in network \ncommunication. \nb. Purpose: Helps identify issues like unreachable hosts or network errors. \nc. Example: Ping uses ICMP to check the availability and response time of a \nDefinition: \nSubnetting is the process of dividing a larger IP network into smaller, more manageable \nsub-networks (subnets). Each subnet operates as an independent network while still being \npart of the original larger network. \n• Efficient IP Address Utilization: Allocates IP addresses based on specific needs \n(e.g., number of devices per subnet). \n• Improved Network Performance: Reduces broadcast traffic within each subnet. \n• Enhanced Security: Isolates sensitive parts of a network. \n• Simplified Management: Organizes networks logically. \nKey Concepts \n1. Subnet Mask: \na. Defines the division between the network and host portions of an IP address. \nb. Example: 255.255.255.0 (or /24) indicates the first 24 bits represent the \n2. CIDR Notation: \na. Specifies the subnet mask using a / followed by the number of network bits. \nb. Example: 192.168.1.0/25. \n3. Subnet ID and Broadcast Address: \na. Each subnet has a unique ID and broadcast address. \nb. Example: For 192.168.1.0/25, the subnet ID is 192.168.1.0, and the \nbroadcast address is 192.168.1.127. \nSubnetting Example \n• Given: Network 192.168.1.0/24. \n• Task: Create four subnets. \n• Solution: Increase subnet mask from /24 to /26 (adds 2 bits to the network \no Subnet Mask: 255.255.255.192 or /26. \no Each subnet has 2^6 = 64 addresses (62 usable for hosts). \nBroadcast Address \n192.168.1.0/26 \n192.168.1.1 \n192.168.1.62 \n192.168.1.63 \n192.168.1.64/26 \n192.168.1.65 \n192.168.1.126 \n192.168.1.127 \n192.168.1.128/26 192.168.1.129 \n192.168.1.190 \n192.168.1.191 \n192.168.1.192/26 192.168.1.193 \n192.168.1.254 \n192.168.1.255 \nBenefits of Subnetting: \n1. Conserves IP Addresses: Reduces wastage in larger networks. \n2. Limits Broadcast Domains: Ensures less traffic and better performance. \n3. Logical Organization: Facilitates departmental or geographical segregation. \nSupernetting \nDefinition: \nSupernetting is the process of combining multiple smaller, contiguous networks (subnets) \ninto a single larger network (supernet). It is the inverse of subnetting and is typically used in \nrouting to reduce the size of routing tables. \n• Simplifies routing by aggregating multiple routes into a single entry. \n• Optimizes the allocation of IP addresses. \n• Reduces the load on routers and improves efficiency in large networks \nKey Features: \n1. Combines Networks: Merges multiple networks with contiguous IP address \n2. Flexible Masking: Uses a shorter subnet mask to include more addresses in a \nsingle block. \n3. Classless: Like CIDR, supernetting ignores traditional class boundaries (Class A, B, \nSuppose you have four Class C networks: \n• 192.168.1.0/24 \n• 192.168.2.0/24 \n• 192.168.3.0/24 \n• 192.168.4.0/24 \nTo supernet these, a shorter prefix /22 can represent them as a single block: \n• Supernet: 192.168.0.0/22 \n• Address Range: 192.168.0.0 to 192.168.3.255. \n1. Reduces routing table size by advertising one route instead of four. \n2. Simplifies network management. \n3. Minimizes overhead in routers. \nApplications: \n• Used by ISPs for route aggregation to advertise fewer routes. \n• Optimizes large-scale enterprise networks with contiguous IP address blocks. \n1. CIDR (Classless Inter-Domain Routing) \n• Definition: A method for allocating IP addresses and routing by allowing flexible \nsubnet masks instead of fixed class-based masks (Class A, B, C). \n• Purpose: Optimizes IP address allocation and reduces routing table size. \n• Key Features: \no Uses prefix length to define networks (e.g., 192.168.1.0/24 for 256 \naddresses). \no Aggregates routes to minimize entries (e.g., 192.168.0.0/16 combines \nmultiple /24 subnets). \nInstead of allocating a full Class B network (172.16.0.0/16), CIDR allows creating a \nsmaller network, such as 172.16.0.0/20 for 4,096 addresses. \n1. Write the subnet mask in decimal notation. Example: 255.255.255.0. \n2. Convert the decimal subnet mask into binary. \no 255.255.255.0 becomes 11111111.11111111.11111111.00000000. \n3. Count the number of 1 bits in the binary subnet mask. \no Here, there are 24 ones. \n4. The CIDR notation is the number of 1s. \no For the example 255.255.255.0, the CIDR is /24. \n1. Subnet Mask: 255.255.255.128 \no Binary: 11111111.11111111.11111111.10000000 \no Number of 1s: 25 \no CIDR: /25 \n2. Subnet Mask: 255.255.252.0 \no Binary: 11111111.11111111.11111100.00000000 \no Number of 1s: 22 \no CIDR: /22 \nSteps to Convert CIDR to Subnet Mask \n1. Identify the CIDR Prefix \nThe number after the / in CIDR represents the number of 1s in the subnet mask. For \nexample, /20 means the subnet mask has 20 1s. \n2. Write the Binary Subnet Mask \nWrite 1s for the prefix length and fill the rest with 0s to make a total of 32 bits. \nExample for /20: 11111111.11111111.11110000.00000000. \n3. Convert Each Octet from Binary to Decimal \nBreak the 32-bit binary string into four 8-bit segments (octets) and convert each to \nExample: 11111111.11111111.11110000.00000000 → 255.255.240.0. \nExample Conversions \nCIDR Binary Subnet Mask \nDecimal Subnet Mask \n11111111.11111111.00000000.00000000 255.255.0.0 \n11111111.11111111.11111111.00000000 255.255.255.0 \n11111111.11111111.11111111.11100000 255.255.255.224 \n11111111.11111111.11111111.11111100 255.255.255.252 \n2. VLSM (Variable Length Subnet Masking) \n• Definition: Extends subnetting by allowing subnets of different sizes within the \nsame network. \n• Purpose: Efficiently uses IP addresses by matching the subnet size to the number \nof required hosts. \n• Key Features: \no Avoids IP wastage by creating subnets based on host requirements. \no Compatible with CIDR. \nFrom 192.168.1.0/24: \no /26 for 64 hosts (e.g., 192.168.1.0/26). \no /27 for 32 hosts (e.g., 192.168.1.64/27). \no /30 for point-to-point links (e.g., 192.168.1.96/30). \n3. DHCP (Dynamic Host Configuration Protocol) \n• Definition: A protocol that dynamically assigns IP addresses and other network \nsettings to devices. \n• Purpose: Simplifies IP management in networks with frequently changing devices. \n• Key Features: \no Centralized IP allocation. \no Prevents conflicts by ensuring unique IPs. \no Can also configure DNS, default gateway, and subnet masks. \n• Example Workflow: \no A device sends a DHCP Discover message. \no The DHCP server replies with a DHCP Offer. \no The device accepts using a DHCP Request. \no The server confirms with a DHCP Acknowledgement. \n4. NAT (Network Address Translation) \n• Definition: A technique where private IP addresses are translated to a public IP \naddress for external communication. \n• Purpose: Conserves public IP addresses and enhances security. \n• Key Features: \no Static NAT: Maps a private IP to a specific public IP. \no Dynamic NAT: Maps private IPs to a pool of public IPs. \no PAT (Port Address Translation): Multiple private IPs share a single public IP \nusing unique ports. \no Internal Network: 192.168.0.0/24. \no External IP: 203.0.113.1. \no Devices with private IPs access the internet using 203.0.113.1. \n5. ICMP (Internet Control Message Protocol) \n• Definition: A protocol used for diagnostic and error-reporting in IP networks. \n• Purpose: Helps identify network issues such as unreachable hosts or congestion. \n• Key Features: \no Works alongside IP but does not transfer data. \no Generates error messages (e.g., \"Destination Unreachable\"). \no Used by tools like ping and traceroute. \n• Example Uses: \no Ping: Sends ICMP Echo Request to check if a host is reachable. \no Traceroute: Uses ICMP to map the path packets take to a destinatio",
    "unit": "Unit 3",
    "source_type": "notes",
    "book_priority": 0,
    "source_file": "UNIT III NETWORK LAYER2",
    "chunk_id": "UNIT III NETWORK LAYER2_chunk_0"
  }
]