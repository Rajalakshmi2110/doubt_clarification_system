[
  {
    "text": "Cover Image by Andrii Yalanskyi/Shutterstock\nPearson Education Limited\nHockham Way\nUnited Kingdom\nand Associated Companies throughout the world\nVisit us on the World Wide Web at: www.pearsonglobaleditions.com\nPlease contact https://support.pearson.com/getsupport/s/ with any queries on this content.\n© Pearson Education Limited 2022\nThe rights of James F. Kurose and Keith W. Ross to be identified as the authors of this work\nof the network layer (Chapter 4) and a new chapter focused on the network\nlayer’s “control plane” (Chapter 5). That change turned out to be prescient,\nas software-defined networking (SDN), arguably the most important and\nexciting advance in networking in decades, has been rapidly adopted\nin practice—so much so that it’s already hard to imagine an introduction to\nmodern computer networking that doesn’t cover SDN. SDN has also\nenabled new advances in the practice of network management, which we\nalso cover in modernized and deeper detail in this edition. And as we’ll see\nin Chapter 7 of this eighth edition, the separation of the data and control\nplanes is now also deeply embedded in 4G/5G mobile cellular network\narchitectures, as is an “all-IP” approach to their core networks. The rapid\nadoption of 4G/5G networks and the mobile applications they enable are\nundoubtedly the most significant changes we’ve seen in networking since\nthe publication of our seventh edition. We’ve thus significantly updated and\ndeepened our treatment of this exciting area. Indeed, the ongoing wireless\nnetwork revolution is so important that we think it has become a critical\npart of an introductory networking course.\nIn addition to these changes, we’ve also updated many sections\nthroughout the book and added new material to reflect changes across the\nbreadth of networking. In some cases, we have also retired material from\nthe previous edition. As always, material that has been retired from the\nprinted text can always be found on our book’s Companion Website. The\nmost important changes in this eighth edition are the following:\nChapter 1 has been updated to reflect the ever-growing reach and use\nof the Internet, and of 4G/5G networks.\nChapter 2, which covers the application layer, has been significantly\nupdated, including material on the new HTTP/2 and HTTP/3 protocols\nfor the Web.\nChapter 3, has been updated to reflect advances in, and evolution in\nuse of, transport-layer congestion control and error-control protocols\nover the past five years. While this material had remained relatively\nstable for quite some time, there have been a number of important\nadvances since the seventh edition. Several new congestion-control\nalgorithms have been developed and deployed beyond the “classic”\nTCP algorithms. We provide a deeper coverage of TCP CUBIC, the\ndefault TCP protocol in many deployed systems, and examine delay-\nbased approaches to congestion control, including the new BBR\nprotocol, which is deployed in Google’s backbone network. We also\nstudy the QUIC protocol, which is being incorporated into the HTTP/3\nstandard. Although QUIC is technically not a transport-layer protocol—\nit provides application-layer reliability, congestion control, and\nconnection multiplexing services at the application layer—it uses many\nof the error- and congestion-control principles that we develop in the\nearly sections of Chapter 3.\nChapter 4, which covers the network-layer data plane, has general\nupdates throughout. We’ve added a new section on so-called\nmiddleboxes, which perform network-layer functions other than routing\nand forwarding, such as firewalling and load balancing. Middleboxes\nbuild naturally on the generalized “match plus action” forwarding\noperation of network-layer devices that we cover earlier in Chapter 4.\nWe’ve also added timely new material on topics such as the amount of\nbuffering that is “just right” in network routers, on net neutrality, and on\nthe architectural principles of the Internet.\nChapter 5, which cover the network-layer’s control plane, contains\nupdated material on SDN, and a significantly new treatment of network\nmanagement. The use of SDN has evolved beyond management of\npacket-forwarding tables to include configuration management of\nnetwork devices as well. We introduce two new protocols, NETCONF\nand YANG, whose adoption and use have fueled this new approach\ntoward network management.\nChapter 6, which covers the link layer, has been updated to reflect the\ncontinuing evolution of link-layer technologies such as Ethernet. We\nhave also updated and expanded our treatment of datacenter networks,\nwhich are at the heart of the technology driving much of today’s\nInternet commerce.\nAs noted earlier, Chapter 7 has been significantly updated and revised\nto reflect the many changes in wireless networking since the seventh\nedition, from short-range Bluetooth piconets, to medium-range wireless\n802.11 local area networks (WLANs), to wide-area 4G/5G wireless\ncellular networks. We have retired our coverage of earlier 2G and 3G\nnetworks in favor of a broader and deeper treatment of today’s 4G LTE\nnetworks and tomorrow’s 5G networks. We have also updated our\ncoverage of mobility issues, from the local issue of handover of mobile\ndevices between base stations to the global issue of identity\nmanagement and mobile device roaming among different global cellular\nChapter 8, which covers network security, has been updated to reflect\nchanges in wireless network security in particular, with new material on\nWPA3 security in WLANs, and mutual device/network mutual\nauthentication and confidentiality in 4G/5G networks.\nWe have also retired Chapter 9, on multimedia networking, from this\nedition. Over time, as multimedia applications became more prevalent, we\nhad already migrated Chapter 9 material on topics such as video streaming,\npacket scheduling, and content distribution networks into earlier chapters.\nAs noted earlier, all retired material from this and earlier editions can be\nfound on our book’s Companion Website.\nThis textbook is for a first course on computer networking. It can be used in\nboth computer science and electrical engineering departments. In terms of\nprogramming languages, the book assumes only that the student has\nexperience with C, C++, Java, or Python (and even then only in a few\nplaces). Although this book is more precise and analytical than many other\nintroductory computer networking texts, it rarely uses any mathematical\nconcepts that are not taught in high school. We have made a deliberate\neffort to avoid using any advanced calculus, probability, or stochastic\nprocess concepts (although we’ve included some homework problems for\nstudents with this advanced background). The book is therefore appropriate\nfor undergraduate courses and for first-year graduate courses. It should also\nbe useful to practitioners in the networking industry.\nWhat Is Unique About This Textbook?\nThe subject of computer networking is enormously complex, involving\nmany concepts, protocols, and technologies that are woven together in an\nintricate manner. To cope with this scope and complexity, many computer\nnetworking texts are often organized around the “layers” of a network\narchitecture. With a layered organization, students can see through the\ncomplexity of computer networking—they learn about the distinct concepts\nand protocols in one part of the architecture while seeing the big picture of\nhow all parts fit together. From a pedagogical perspective, our personal\nexperience has been that such a layered approach indeed works well.\nNevertheless, we have found that the traditional approach of teaching—\nbottom up; that is, from the physical layer toward the application layer—is\nnot the best approach for a modern course on computer networking.\nA Top-Down Approach\nOur book broke new ground 20 years ago by treating networking in a top-\ndown ­manner—that is, by beginning at the application layer and working its\nway down toward the physical layer. The feedback we received from\nteachers and students alike have confirmed that this top-down approach has\nmany advantages and does indeed work well pedagogically. First, it places\nemphasis on the application layer (a “high growth area” in networking).\nIndeed, many of the recent revolutions in computer networking—including\nthe Web, and media streaming—have taken place at the ­application layer.\nAn early emphasis on application-layer issues differs from the ­approaches\ntaken in most other texts, which have only a small amount of material on\nnetwork applications, their requirements, application-layer paradigms (e.g.,\nclient-server and peer-to-peer), and application programming interfaces.\nSecond, our experience as instructors (and that of many instructors who\nhave used this text) has been that teaching networking applications near the\nbeginning of the course is a powerful motivational tool. Students are\nthrilled to learn about how networking applications work—applications\nsuch as e-mail, streaming video, and the Web, which most students use on a\ndaily basis. Once a student understands the applications, the student can\nthen understand the network services needed to support these applications.\nThe student can then, in turn, examine the various ways in which such\nservices might be provided and implemented in the lower layers. Covering\napplications early thus provides motivation for the remainder of the text.\nThird, a top-down approach enables instructors to introduce network\napplication development at an early stage. Students not only see how\npopular applications and protocols work, but also learn how easy it is to\ncreate their own network ­applications and application-layer protocols. With\nthe top-down approach, students get early ­exposure to the notions of socket\nprogramming, service models, and ­protocols—important concepts that\nresurface in all subsequent layers. By providing socket programming\nexamples in Python, we highlight the central ideas without confusing\nstudents with complex code. Undergraduates in electrical engineering and\ncomputer science will have no difficulty following the Python code.\nAn Internet Focus\nAlthough we dropped the phrase “Featuring the Internet” from the title of\nthis book with the fourth edition, this doesn’t mean that we dropped our\nfocus on the Internet. Indeed, nothing could be further from the case!\nInstead, since the Internet has become so pervasive, we felt that any\nnetworking textbook must have a significant focus on the Internet, and thus\nthis phrase was somewhat unnecessary. We continue to use the Internet’s\narchitecture and protocols as primary vehicles for studying fundamental\ncomputer networking concepts. Of course, we also include concepts and\nprotocols from other network architectures. But the spotlight is clearly on\nthe Internet, a fact reflected in our organizing the book around the Internet’s\nfive-layer architecture: the application, transport, network, link, and\nphysical layers.\nAnother benefit of spotlighting the Internet is that most computer\nscience and electrical engineering students are eager to learn about the\nInternet and its protocols. They know that the Internet has been a\nrevolutionary and disruptive technology and can see that it is profoundly\nchanging our world. Given the enormous relevance of the Internet, students\nare naturally curious about what is “under the hood.” Thus, it is easy for an\ninstructor to get students excited about basic principles when using the\nInternet as the guiding focus.\nTeaching Networking Principles\nTwo of the unique features of the book—its top-down approach and its\nfocus on the Internet—have appeared in the titles of our book. If we could\nhave squeezed a third phrase into the subtitle, it would have contained the\nword principles. The field of networking is now mature enough that a\nnumber of fundamentally important issues can be identified. For example,\nin the transport layer, the fundamental issues include reliable\ncommunication over an unreliable network layer, connection establishment/\nteardown and handshaking, congestion and flow control, and multiplexing.\nThree fundamentally important network-layer issues are determining\n“good” paths between two routers, interconnecting a large number of\nheterogeneous networks, and managing the complexity of a modern\nnetwork. In the link layer, a fundamental problem is sharing a multiple\nconfidentiality, authentication, and message integrity are all based on\ncryptographic fundamentals. This text identifies fundamental networking\nissues and studies approaches toward ­addressing these issues. The student\nlearning these principles will gain knowledge with a long “shelf life”—long\nafter many of today’s network standards and protocols have become\nobsolete, the principles they embody will remain important and relevant.\nWe believe that the combination of using the Internet to get the student’s\nfoot in the door and then emphasizing fundamental issues and solution\napproaches will allow the student to quickly understand just about any\nnetworking technology.\nStudent Resources\nStudent resources are available on the Companion Website (CW) at www.­-\npearsonglobaleditions.com. Resources include:\nInteractive learning material. The book’s Website contains ­VideoNotes\n—video presentations of important topics throughout the book done by\nthe authors, as well as walkthroughs of solutions to problems similar to\nthose at the end of the chapter. We’ve seeded the Website with\nVideoNotes and online problems for Chapters 1 through 5. As in earlier\neditions, the Website contains the interactive animations that illustrate\nmany key networking concepts. Professors can integrate these\ninteractive features into their lectures or use them as mini labs.\nAdditional technical material. As we have added new material in each\nedition of our book, we’ve had to remove coverage of some existing\ntopics to keep the book at manageable length. Material that appeared in\nearlier editions of the text is still of ­interest, and thus can be found on\nthe book’s Website.\nProgramming assignments. The Website also provides a number of\ndetailed programming assignments, which include building a\nmultithreaded Web ­server, building an e-mail client with a GUI\ninterface, programming the sender and ­receiver sides of a reliable data\ntransport protocol, programming a distributed routing algorithm, and\nWireshark labs. One’s understanding of network protocols can be\ngreatly ­deepened by seeing them in action. The Website provides\nnumerous Wireshark assignments that enable students to actually\nobserve the sequence of messages exchanged between two protocol\nentities. The Website includes separate Wireshark labs on HTTP, DNS,\nTCP, UDP, IP, ICMP, Ethernet, ARP, WiFi, TLS and on tracing all\nprotocols involved in satisfying a request to fetch a Web page. We’ll\ncontinue to add new labs over time.\nPedagogical Features\nWe have each been teaching computer networking for more than 30 years.\nTogether, we bring more than 60 years of teaching experience to this text,\nduring which time we have taught many thousands of students. We have\nalso been active researchers in computer networking during this time. (In\nfact, Jim and Keith first met each other as master’s students in a computer\nnetworking course taught by Mischa Schwartz in 1979 at Columbia\nUniversity.) We think all this gives us a good perspective on where\nnetworking has been and where it is likely to go in the future. Nevertheless,\nwe have resisted temptations to bias the material in this book toward our\nown pet research projects. We figure you can visit our personal Websites if\nyou are interested in our research. Thus, this book is about modern\ncomputer networking—it is about contemporary protocols and technologies\nas well as the underlying principles behind these protocols and\ntechnologies. We also believe that learning (and teaching!) about\nnetworking can be fun. A sense of humor, use of analogies, and real-world\nexamples in this book will hopefully make this material more fun.\nSupplements for Instructors\nWe provide a complete supplements package to aid instructors in teaching\nthis course. This material can be accessed from Pearson’s Instructor\n(http://www.pearsonglobaleditions.com). \nInstructor Resource Center for ­information about accessing these\ninstructor’s supplements.\nPowerPoint® slides. We provide PowerPoint slides for all eight\nchapters. The slides have been completely updated with this eighth\nedition. The slides cover each chapter in detail. They use graphics and\nanimations (rather than relying only on monotonous text bullets) to\nmake the slides interesting and visually appealing. We provide the\noriginal PowerPoint slides so you can customize them to best suit your\nown teaching needs. Some of these slides have been contributed by\nother instructors who have taught from our book.\nHomework solutions. We provide a solutions manual for the homework\nproblems in the text, programming assignments, and Wireshark labs. As\nnoted ­earlier, we’ve introduced many new homework problems at each\nchapter’s end. For additional interactive problems and solutions, an\ninstructor (and students) can consult this books Companion Website at\nChapter Dependencies\nThe first chapter of this text presents a self-contained overview of computer\nnetworking. Introducing many key concepts and terminology, this chapter\nsets the stage for the rest of the book. All of the other chapters directly\ndepend on this first chapter. After completing Chapter 1, we recommend\ninstructors cover Chapters 2 through 6 in sequence, following our top-down\nphilosophy. Each of these five chapters leverages material from the\npreceding chapters. After completing the first six chapters, the instructor\nhas quite a bit of flexibility. There are no interdependencies among the last\ntwo chapters, so they can be taught in any order. However, the last two\nchapters depends on the material in the first six chapters. Many instructors\nfirst teach the first six chapters and then teach one of the last two chapters\nfor “dessert.”\nOne Final Note: We’d Love to Hear from You\nWe encourage students and instructors to e-mail us with any comments they\nmight have about our book. It’s been wonderful for us to hear from so many\ninstructors and students from around the world about our first seven\neditions. We’ve incorporated many of these suggestions into later editions\nof the book. We also encourage instructors to send us new homework\nproblems (and solutions) that would complement the current homework\nproblems. We’ll post these on the instructor-only portion of the Website. We\nalso encourage instructors and students to create new interactive animations\nthat illustrate the concepts and protocols in this book. If you have an\nanimation that you think would be appropriate for this text, please submit it\nto us. If the animation (including notation and terminology) is appropriate,\nwe’ll be happy to include it on the text’s Website, with an appropriate\nreference to the animation’s authors.\nSo, as the saying goes, “Keep those cards and letters coming!”\nSeriously, please do continue to send us interesting URLs, point out typos,\ndisagree with any of our claims, and tell us what works and what doesn’t\nwork. Tell us what you think should or shouldn’t be included in the next\nkurose@cs.umass.edu \nkeithwross@nyu.edu.\nChapter 1 Computer Networks and the Internet\nChapter 2 Application Layer\nChapter 3 Transport Layer\nChapter 4 The Network Layer: Data Plane\nChapter 5 The Network Layer: Control Plane\nChapter 6 The Link Layer and LANs\nChapter 7 Wireless and Mobile Networks\nChapter 8 Security in Computer Networks\nChapter 1 Computer Networks and the Internet\nWhat Is the Internet?\nA Nuts-and-Bolts Description\nA Services Description\nWhat Is a Protocol?\nThe Network Edge\nAccess Networks\nPhysical Media\nThe Network Core\nPacket Switching\nCircuit Switching\nA Network of Networks\nDelay, Loss, and Throughput in Packet-Switched\nOverview of Delay in Packet-Switched\nQueuing Delay and Packet Loss\nEnd-to-End Delay\nThroughput in Computer Networks\nProtocol Layers and Their Service Models\nLayered Architecture\nEncapsulation\nNetworks Under Attack\nHistory of Computer Networking and the Internet\nThe Development of Packet Switching:\nProprietary Networks and Internetworking:\nA Proliferation of Networks: 1980–1990\nThe Internet Explosion: The 1990s\nThe New Millennium\nHomework Problems and Questions\nWireshark Lab\nInterview: Leonard Kleinrock\nChapter 2 Application Layer\nPrinciples of Network Applications\nNetwork Application Architectures\nProcesses Communicating\nTransport Services Available to Applications\nTransport Services Provided by the Internet\nApplication-Layer Protocols\nNetwork Applications Covered in This Book\nThe Web and HTTP\nOverview of HTTP\nNon-Persistent and Persistent Connections\nHTTP Message Format\nUser-Server Interaction: Cookies\nWeb Caching\nElectronic Mail in the Internet\nMail Message Formats\nMail Access Protocols\nDNS—The Internet’s Directory Service\nServices Provided by DNS\nOverview of How DNS Works\nDNS Records and Messages\nPeer-to-Peer File Distribution\nVideo Streaming and Content Distribution\nInternet Video\nHTTP Streaming and DASH\nContent Distribution Networks\nCase Studies: Netflix and YouTube\nSocket Programming: Creating Network\nApplications\nSocket Programming with UDP\nSocket Programming with TCP\nHomework Problems and Questions\nSocket Programming Assignments\nWireshark Labs: HTTP, DNS\nInterview: Tim Berners-Lee\nChapter 3 Transport Layer\nIntroduction and Transport-Layer Services\nRelationship Between Transport and\nNetwork Layers\nOverview of the Transport Layer in the\nMultiplexing and Demultiplexing\nConnectionless Transport: UDP\nUDP Segment Structure\nUDP Checksum\nPrinciples of Reliable Data Transfer\nBuilding a Reliable Data Transfer Protocol\nPipelined Reliable Data Transfer Protocols\nGo-Back-N (GBN)\nSelective Repeat (SR)\nConnection-Oriented Transport: TCP\nThe TCP Connection\nTCP Segment Structure\nRound-Trip Time Estimation and Timeout\nReliable Data Transfer\nFlow Control\nTCP Connection Management\nPrinciples of Congestion Control\nThe Causes and the Costs of Congestion\nApproaches to Congestion Control\nTCP Congestion Control\nClassic TCP Congestion Control\nNetwork-Assisted Explicit Congestion\nNotification and Delayed-based Congestion\nEvolution of Transport-Layer Functionality\nHomework Problems and Questions\nProgramming Assignments\nWireshark Labs: Exploring TCP, UDP\nInterview: Van Jacobson\nChapter 4 The Network Layer: Data Plane\nOverview of Network Layer\nForwarding and Routing: The Data and\nControl Planes\nNetwork Service Model\nWhat’s Inside a Router?\nInput Port Processing and Destination-\nBased Forwarding\nOutput Port Processing\nWhere Does Queuing Occur?\nPacket Scheduling\nThe Internet Protocol (IP): IPv4, Addressing, IPv6,\nIPv4 Datagram Format\nIPv4 Addressing\nNetwork Address Translation (NAT)\nGeneralized Forwarding and SDN\nOpenFlow Examples of Match-plus-action\nMiddleboxes\nHomework Problems and Questions\nWireshark Lab: IP\nInterview: Vinton G. Cerf\nChapter 5 The Network Layer: Control Plane\nIntroduction\nRouting Algorithms\nThe Link-State (LS) Routing Algorithm\nThe Distance-Vector (DV) Routing\nIntra-AS Routing in the Internet: OSPF\nRouting Among the ISPs: BGP\nThe Role of BGP\nAdvertising BGP Route Information\nDetermining the Best Routes\nRouting Policy\nPutting the Pieces Together: Obtaining\nInternet Presence\nThe SDN Control Plane\nThe SDN Control Plane: SDN Controller\nand SDN Network-control Applications\nOpenFlow Protocol\nData and Control Plane Interaction: An\nSDN: Past and Future\nICMP: The Internet Control Message Protocol\nNetwork Management and SNMP,\nNETCONF/YANG\nThe Network Management Framework\nThe Simple Network Management Protocol\n(SNMP) and the Management Information\nThe Network Configuration Protocol\n(NETCONF) and YANG\nHomework Problems and Questions\nSocket Programming Assignment 5: ICMP Ping\nProgramming Assignment: Routing\nWireshark Lab: ICMP\nInterview: Jennifer Rexford\nChapter 6 The Link Layer and LANs\nIntroduction to the Link Layer\nThe Services Provided by the Link Layer\nWhere Is the Link Layer Implemented?\nError-Detection and -Correction Techniques\nParity Checks\nChecksumming Methods\nCyclic Redundancy Check (CRC)\nMultiple Access Links and Protocols\nChannel Partitioning Protocols\nRandom Access Protocols\nTaking-Turns Protocols\nDOCSIS: The Link-Layer Protocol for\nCable Internet Access\nSwitched Local Area Networks\nLink-Layer Addressing and ARP\nLink-Layer Switches\nVirtual Local Area Networks (VLANs)\nLink Virtualization: A Network as a Link Layer\nMultiprotocol Label Switching (MPLS)\nData Center Networking\nData Center Architectures\nTrends in Data Center Networking\nRetrospective: A Day in the Life of a Web Page\nGetting Started: DHCP, UDP, IP, and\nStill Getting Started: DNS and ARP\nStill Getting Started: Intra-Domain Routing\nto the DNS Server\nWeb Client-Server Interaction: TCP and\nHomework Problems and Questions\nWireshark Labs: 802.11 Ethernet\nInterview: Albert Greenberg\nChapter 7 Wireless and Mobile Networks\nIntroduction\nWireless Links and Network Characteristics\nWiFi: 802.11 Wireless LANs\nThe 802.11 Wireless LAN Architecture\nThe 802.11 MAC Protocol\nThe IEEE 802.11 Frame\nMobility in the Same IP Subnet\nAdvanced Features in 802.11\nPersonal Area Networks: Bluetooth\nCellular Networks: 4G and 5G\n4G LTE Cellular Networks: Architecture\nand Elements\nLTE Protocols Stacks\nLTE Radio Access Network\nAdditional LTE Functions: Network\nAttachment and Power Management\nThe Global Cellular Network: A Network of\n5G Cellular Networks\nMobility Management: Principles\nDevice Mobility: a Network-layer\nPerspective\nHome Networks and Roaming on Visited\nDirect and Indirect Routing to/from a\nMobile Device\nMobility Management in Practice\nMobility Management in 4G/5G Networks\nWireless and Mobility: Impact on Higher-Layer\nHomework Problems and Questions\nWireshark Lab: WiFi\nInterview: Deborah Estrin\nChapter 8 Security in Computer Networks\nWhat Is Network Security?\nPrinciples of Cryptography\nSymmetric Key Cryptography\nPublic Key Encryption\nMessage Integrity and Digital Signatures\nCryptographic Hash Functions\nMessage Authentication Code\nDigital Signatures\nEnd-Point Authentication\nSecuring E-Mail\nSecure E-Mail\nSecuring TCP Connections: TLS\nThe Big Picture\nA More Complete Picture\nNetwork-Layer Security: IPsec and Virtual Private\nIPsec and Virtual Private Networks (VPNs)\nThe AH and ESP Protocols\nSecurity Associations\nThe IPsec Datagram\nIKE: Key Management in IPsec\nSecuring Wireless LANs and 4G/5G Cellular\nAuthentication and Key Agreement in\n802.11 Wireless LANs\nAuthentication and Key Agreement in\n4G/5G Cellular Networks\nOperational Security: Firewalls and Intrusion\nDetection Systems\nIntrusion Detection Systems\nHomework Problems and Questions\nWireshark Lab: SSL\nInterview: Steven M. Bellovin\nChapter 2. For now, let’s draw upon a simple analogy, one that we will\nfrequently use in this book. Suppose Alice wants to send a letter to Bob\nusing the postal service. Alice, of course, can’t just write the letter (the data)\nand drop the letter out her window. Instead, the postal service requires that\nAlice put the letter in an envelope; write Bob’s full name, address, and zip\ncode in the center of the envelope; seal the envelope; put a stamp in the\nupper-right-hand corner of the envelope; and finally, drop the envelope into\nan official postal service mailbox. Thus, the postal service has its own\n“postal service interface,” or set of rules, that Alice must follow to have the\npostal service deliver her letter to Bob. In a similar manner, the Internet has\na socket interface that the program sending data must follow to have the\nInternet deliver the data to the program that will receive the data.\nThe postal service, of course, provides more than one service to its\ncustomers. It provides express delivery, reception confirmation, ordinary\nuse, and many more services. In a similar manner, the Internet provides\nmultiple services to its applications. When you develop an Internet\napplication, you too must choose one of the Internet’s services for your\napplication. We’ll describe the Internet’s services in Chapter 2.\nWe have just given two descriptions of the Internet; one in terms of its\nhardware and software components, the other in terms of an infrastructure\nfor providing services to distributed applications. But perhaps you are still\nconfused as to what the Internet is. What are packet switching and TCP/IP?\nWhat are routers? What kinds of communication links are present in the\nInternet? What is a distributed application? How can a thermostat or body\nscale be attached to the Internet? If you feel a bit overwhelmed by all of this\nnow, don’t worry—the purpose of this book is to introduce you to both the\nnuts and bolts of the Internet and the principles that govern how and why it\nworks. We’ll explain these important terms and questions in the following\nsections and chapters.\n1.1.3 What Is a Protocol?\nNow that we’ve got a bit of a feel for what the Internet is, let’s consider\nanother important buzzword in computer networking: protocol. What is a\nprotocol? What does a protocol do?\nA Human Analogy\nIt is probably easiest to understand the notion of a computer network\nprotocol by first considering some human analogies, since we humans\nexecute protocols all of the time. Consider what you do when you want to\nask someone for the time of day. A typical exchange is shown in Figure 1.2.\nHuman protocol (or good manners, at least) dictates that one first offer a\ngreeting (the first “Hi” in Figure 1.2) to initiate communication with\nsomeone else. The typical response to a “Hi” is a returned “Hi” message.\nImplicitly, one then takes a cordial “Hi” response as an indication that one\ncan proceed and ask for the time of day. A different response to the initial\n“Hi” (such as “Don’t bother me!” or “I don’t speak English,” or some\nunprintable reply) might indicate an unwillingness or inability to\ncommunicate. In this case, the human protocol would be not to ask for the\ntime of day. Sometimes one gets no response at all to a question, in which\ncase one typically gives up asking that person for the time. Note that in our\nhuman protocol, there are specific messages we send, and specific actions\nwe take in response to the received reply messages or other events (such as\nno reply within some given amount of time). Clearly, transmitted and\nreceived messages, and actions taken when these messages are sent or\nreceived or other events occur, play a central role in a human protocol. If\npeople run different protocols (for example, if one person has manners but\nthe other does not, or if one understands the concept of time and the other\ndoes not) the protocols do not interoperate and no useful work can be\naccomplished. The same is true in networking—it takes two (or more)\ncommunicating entities running the same protocol in order to accomplish a\nFigure 1.2 ♦A human protocol and a computer network protocol\nLet’s consider a second human analogy. Suppose you’re in a college\nclass (a computer networking class, for example!). The teacher is droning\non about protocols and you’re confused. The teacher stops to ask, “Are\nthere any questions?” (a message that is transmitted to, and received by, all\nstudents who are not sleeping). You raise your hand (transmitting an\nimplicit message to the teacher). Your teacher acknowledges you with a\nsmile, saying “Yes . . .” (a transmitted message encouraging you to ask your\nquestion—teachers love to be asked questions), and you then ask your\nquestion (that is, transmit your message to your teacher). Your teacher hears\nyour question (receives your question message) and answers (transmits a\nreply to you). Once again, we see that the transmission and receipt of\nmessages, and a set of conventional actions taken when these messages are\nsent and received, are at the heart of this question-and-answer protocol.\nNetwork Protocols\nA network protocol is similar to a human protocol, except that the entities\nexchanging messages and taking actions are hardware or software\ncomponents of some device (for example, computer, smartphone, tablet,\nrouter, or other network-capable device). All activity in the Internet that\ninvolves two or more communicating remote entities is governed by a\nprotocol. For example, hardware-implemented protocols in two physically\nconnected computers control the flow of bits on the “wire” between the two\nnetwork interface cards; congestion-control protocols in end systems\ncontrol the rate at which packets are transmitted between sender and\nreceiver; protocols in routers determine a packet’s path from source to\ndestination. Protocols are running everywhere in the Internet, and\nconsequently much of this book is about computer network protocols.\nAs an example of a computer network protocol with which you are\nprobably familiar, consider what happens when you make a request to a\nWeb server, that is, when you type the URL of a Web page into your Web\nbrowser. The scenario is illustrated in the right half of Figure 1.2. First, your\ncomputer will send a connection request message to the Web server and\nwait for a reply. The Web server will eventually receive your connection\nrequest message and return a connection reply message. Knowing that it is\nnow OK to request the Web document, your computer then sends the name\nof the Web page it wants to fetch from that Web server in a GET message.\nFinally, the Web server returns the Web page (file) to your computer.\nGiven the human and networking examples above, the exchange of\nmessages and the actions taken when these messages are sent and received\nare the key defining elements of a protocol:\nA protocol defines the format and the order of messages exchanged\nbetween two or more communicating entities, as well as the actions\ntaken on the transmission and/or receipt of a message or other event.\nThe Internet, and computer networks in general, make extensive use of\ncommunication tasks. As you read through this book, you will learn that\nsome protocols are simple and straightforward, while others are complex\nand intellectually deep. Mastering the field of computer networking is\nequivalent to understanding the what, why, and how of networking\n1.2 The Network Edge\nIn the previous section, we presented a high-level overview of the Internet\nand ­networking protocols. We are now going to delve a bit more deeply into\nthe components of the Internet. We begin in this section at the edge of the\nnetwork and look at the components with which we are most ­familiar—\nnamely, the computers, smartphones and other devices that we use on a\ndaily basis. In the next section, we’ll move from the network edge to the\nnetwork core and examine switching and routing in computer networks.\nRecall from the previous section that in computer networking jargon,\nthe computers and other devices connected to the Internet are often referred\nto as end systems. They are referred to as end systems because they sit at\nthe edge of the Internet, as shown in Figure 1.3. The Internet’s end systems\ninclude desktop computers (e.g., desktop PCs, Macs, and Linux boxes),\nservers (e.g., Web and e-mail servers), and mobile devices (e.g., laptops,\nsmartphones, and tablets). Furthermore, an increasing number of non-\ntraditional “things” are being attached to the Internet as end ­systems (see\nthe Case History feature).\nFigure 1.3 ♦End-system interaction\nEnd systems are also referred to as hosts because they host (that is, run)\napplication programs such as a Web browser program, a Web server\nprogram, an e-mail client program, or an e-mail server program.\nThroughout this book we will use the terms hosts and end systems\ninterchangeably; that is, host = end system. Hosts are sometimes further\ndivided into two categories: clients and servers. Informally, clients tend to\nbe desktops, laptops, smartphones, and so on, whereas servers tend to be\nmore powerful machines that store and distribute Web pages, stream video,\nrelay e-mail, and so on. Today, most of the servers from which we receive\nsearch results, e-mail, Web pages, videos and mobile app content reside in\nlarge data centers. For example, as of 2020, Google has 19 data centers on\nfour continents, collectively containing several million servers. Figure 1.3\nincludes two such data centers, and the Case History sidebar describes data\ncenters in more detail.\nInternet companies such as Google, Microsoft, Amazon, and Alibaba have built\nmassive data centers, each housing tens to hundreds of thousands of hosts. These\ndata centers are not only connected to the Internet, as shown in Figure 1.1, but also\ninternally include complex computer networks that interconnect the datacenter’s hosts.\nThe data centers are the engines behind the Internet applications that we use on a\ndaily basis.\nBroadly speaking, data centers serve three purposes, which we describe here in the\ncontext of Amazon for concreteness. First, they serve Amazon e-commerce pages to\nusers, for example, pages describing products and purchase information. Second, they\nserve as massively parallel computing infrastructures for Amazon-specific data\nprocessing tasks. Third, they provide cloud computing to other companies. Indeed,\ntoday a major trend in computing is for companies to use a cloud provider such as\nAmazon to handle essentially all of their IT needs. For example, Airbnb and many other\nInternet-based companies do not own and manage their own data centers but instead\nrun their entire Web-based services in the Amazon cloud, called Amazon Web Services\nThe worker bees in a data center are the hosts. They serve content (e.g., Web\npages and videos), store e-mails and documents, and collectively perform massively\ndistributed computations. The hosts in data centers, called blades and resembling\npizza boxes, are generally commodity hosts that include CPU, memory, and disk\nstorage. The hosts are stacked in racks, with each rack typically having 20 to\n40 blades. The racks are then interconnected using sophisticated and evolving data\ncenter network designs. Data center networks are discussed in greater detail in\n1.2.1 Access Networks\nHaving considered the applications and end systems at the “edge of the\nnetwork,” let’s next consider the access network—the network that\nphysically connects an end system to the first router (also known as the\n“edge router”) on a path from the end system to any other distant end\nsystem. Figure 1.4 shows several types of access networks with thick,\nshaded lines and the settings (home, enterprise, and wide-area mobile\nwireless) in which they are used.\nFigure 1.4 ♦Access networks\nHome Access: DSL, Cable, FTTH, and 5G Fixed Wireless\nAs of 2020, more than 80% of the households in Europe and the USA have\nInternet access [Statista 2019]. Given this widespread use of home access\nnetworks let’s begin our overview of access networks by considering how\nhomes connect to the Internet.\nToday, the two most prevalent types of broadband residential access are\ndigital subscriber line (DSL) and cable. A residence typically obtains DSL\nInternet access from the same local telephone company (telco) that provides\nits wired local phone access. Thus, when DSL is used, a customer’s telco is\nalso its ISP. As shown in Figure 1.5, each customer’s DSL modem uses the\nexisting telephone line exchange data with a digital subscriber line access\nmultiplexer (DSLAM) located in the telco’s local central office (CO). The\nhome’s DSL modem takes digital data and translates it to high-­frequency\ntones for transmission over telephone wires to the CO; the analog signals\nfrom many such houses are translated back into digital format at the\nFigure 1.5 ♦DSL Internet access\nThe residential telephone line carries both data and traditional\ntelephone signals simultaneously, which are encoded at different\nfrequencies:\nA high-speed downstream channel, in the 50 kHz to 1 MHz band\nA medium-speed upstream channel, in the 4 kHz to 50 kHz band\nAn ordinary two-way telephone channel, in the 0 to 4 kHz band\nThis approach makes the single DSL link appear as if there were three\nseparate links, so that a telephone call and an Internet connection can share\nthe DSL link at the same time. (We’ll describe this technique of frequency-\ndivision multiplexing in Section 1.3.1.) On the customer side, a splitter\nseparates the data and telephone signals arriving to the home and forwards\nthe data signal to the DSL modem. On the telco side, in the CO, the\nDSLAM separates the data and phone signals and sends the data into the\nInternet. Hundreds or even thousands of households connect to a single\nThe DSL standards define multiple transmission rates, including\ndownstream transmission rates of 24 Mbs and 52 Mbs, and upstream rates\nof 3.5 Mbps and 16  Mbps; the newest standard provides for aggregate\nupstream plus downstream rates of 1 Gbps [ITU 2014]. Because the\ndownstream and upstream rates are different, the access is said to be\nasymmetric. The actual downstream and upstream transmission rates\nachieved may be less than the rates noted above, as the DSL provider may\npurposefully limit a residential rate when tiered service (different rates,\navailable at different prices) are offered. The maximum rate is also limited\nby the distance between the home and the CO, the gauge of the twisted-pair\nline and the degree of electrical interference. Engineers have expressly\ndesigned DSL for short distances between the home and the CO; generally,\nif the residence is not located within 5 to 10 miles of the CO, the residence\nmust resort to an alternative form of Internet access.\nWhile DSL makes use of the telco’s existing local telephone\ninfrastructure, cable Internet access makes use of the cable television\ncompany’s existing cable television infrastructure. A residence obtains cable\nInternet access from the same company that provides its cable television. As\nillustrated in Figure 1.6, fiber optics connect the cable head end to\nneighborhood-level junctions, from which traditional coaxial cable is then\nused to reach individual houses and apartments. Each neighborhood\njunction typically supports 500 to 5,000 homes. Because both fiber and\ncoaxial cable are employed in this system, it is often referred to as hybrid\nfiber coax (HFC).\nFigure 1.6 ♦A hybrid fiber-coaxial access network\nCable internet access requires special modems, called cable modems.\nAs with a DSL modem, the cable modem is typically an external device and\nconnects to the home PC through an Ethernet port. (We will discuss\nEthernet in great detail in Chapter 6.) At the cable head end, the cable\nmodem termination system (CMTS) serves a similar function as the DSL\nnetwork’s DSLAM—turning the analog signal sent from the cable modems\nin many downstream homes back into digital format. Cable modems divide\nthe HFC network into two channels, a downstream and an upstream\nchannel. As with DSL, access is typically asymmetric, with the downstream\nchannel typically allocated a higher transmission rate than the upstream\nchannel. The DOCSIS 2.0 and 3.0 standards define downstream bitrates of\n40 Mbps and 1.2 Gbps, and upstream rates of  30  Mbps and 100 Mbps,\nrespectively. As in the case of DSL networks, the ­maximum achievable rate\nmay not be realized due to lower contracted data rates or media\nimpairments.\nOne important characteristic of cable Internet access is that it is a\nshared broadcast medium. In particular, every packet sent by the head end\ntravels downstream on every link to every home and every packet sent by a\nhome travels on the upstream channel to the head end. For this reason, if\nseveral users are simultaneously downloading a video file on the\ndownstream channel, the actual rate at which each user receives its video\nfile will be significantly lower than the aggregate cable downstream rate.\nOn the other hand, if there are only a few active users and they are all Web\nsurfing, then each of the users may actually receive Web pages at the full\ncable downstream rate, because the users will rarely request a Web page at\nexactly the same time. Because the upstream channel is also shared, a\ndistributed multiple access protocol is needed to coordinate transmissions\nand avoid collisions. (We’ll discuss this collision issue in some detail in\nChapter 6.)\nAlthough DSL and cable networks currently represent the majority of\nresidential broadband access in the United States, an up-and-coming\ntechnology that provides even higher speeds is fiber to the home (FTTH)\n[Fiber Broadband 2020]. As the name suggests, the FTTH concept is simple\n—provide an optical fiber path from the CO directly to the home. FTTH\ncan potentially provide Internet access rates in the gigabits per second\nThere are several competing technologies for optical distribution from\nthe CO to the homes. The simplest optical distribution network is called\ndirect fiber, with one fiber leaving the CO for each home. More commonly,\neach fiber leaving the central office is actually shared by many homes; it is\nnot until the fiber gets relatively close to the homes that it is split into\nindividual customer-specific fibers. There are two competing optical-\ndistribution network architectures that perform this splitting: active optical\nnetworks (AONs) and passive optical networks (PONs). AON is essentially\nswitched Ethernet, which is discussed in Chapter 6.\nHere, we briefly discuss PON, which is used in Verizon’s FiOS service.\nFigure 1.7 shows FTTH using the PON distribution architecture. Each\nhome has an optical network terminator (ONT), which is connected by\ndedicated optical fiber to a neighborhood splitter. The splitter combines a\nnumber of homes (typically less than 100) onto a single, shared optical\nfiber, which connects to an optical line ­terminator (OLT) in the telco’s CO.\nThe OLT, providing conversion between optical and electrical signals,\nconnects to the Internet via a telco router. At home, users connect a home\nrouter (typically a wireless router) to the ONT and access the ­Internet via\nthis home router. In the PON architecture, all packets sent from OLT to the\nsplitter are replicated at the splitter (similar to a cable head end).\nFigure 1.7 ♦FTTH Internet access\nIn addition to DSL, Cable, and FTTH, 5G fixed wireless is beginning\nto be deployed. 5G fixed wireless not only promises high-speed residential\naccess, but will do so without installing costly and failure-prone cabling\nfrom the telco’s CO to the home. With 5G fixed wireless, using beam-\nforming technology, data is sent wirelessly from a provider’s base station to\nthe a modem in the home. A WiFi wireless router is connected to the\nmodem (possibly bundled together), similar to how a WiFi wireless router\nis connected to a cable or DSL modem. 5G cellular networks are covered in\nAccess in the Enterprise (and the Home): Ethernet and WiFi\nOn corporate and university campuses, and increasingly in home settings, a\nlocal area  network (LAN) is used to connect an end system to the edge\nrouter. Although there are many types of LAN technologies, Ethernet is by\nfar the most prevalent access technology in corporate, university, and home\nnetworks. As shown in Figure 1.8, Ethernet users use twisted-pair copper\nwire to connect to an Ethernet switch, a technology discussed in detail in\nChapter 6. The Ethernet switch, or a network of such interconnected\nswitches, is then in turn connected into the larger Internet. With Ethernet\naccess, users typically have 100 Mbps to tens of Gbps access to the\nEthernet switch, whereas servers may have 1 Gbps 10 Gbps access.\nFigure 1.8 ♦Ethernet Internet access\nIncreasingly, however, people are accessing the Internet wirelessly from\nlaptops, smartphones, tablets, and other “things”. In a wireless LAN setting,\nwireless users transmit/receive packets to/from an access point that is\nconnected into the enterprise’s network (most likely using wired Ethernet),\nwhich in turn is connected to the wired Internet. A wireless LAN user must\ntypically be within a few tens of meters of the access point. Wireless LAN\naccess based on IEEE 802.11 technology, more colloquially known as WiFi,\nis now just about everywhere—universities, business offices, cafes, airports,\nhomes, and even in airplanes. As discussed in detail in Chapter 7, 802.11\ntoday provides a shared transmission rate of up to more than 100 Mbps.\nEven though Ethernet and WiFi access networks were initially\ndeployed in enterprise (corporate, university) settings, they are also\ncommon components of home networks. Many homes combine broadband\nresidential access (that is, cable modems or DSL) with these inexpensive\nwireless LAN technologies to create powerful home networks Figure 1.9\nshows a typical home network. This home network consists of a roaming\nlaptop, multiple Internet-connected home appliances, as well as a wired PC;\na base station (the wireless access point), which communicates with the\nwireless PC and other wireless devices in the home; and a home router that\nconnects the wireless access point, and any other wired home devices, to\nthe Internet. This network allows household members to have broadband\naccess to the Internet with one member roaming from the kitchen to the\nbackyard to the bedrooms.\nFigure 1.9 ♦A typical home network\nWide-Area Wireless Access: 3G and LTE 4G and 5G\nMobile devices such as iPhones and Android devices are being used to\nmessage, share photos in social networks, make mobile payments, watch\nmovies, stream music, and much more while on the run. These devices\nemploy the same wireless infrastructure used for cellular telephony to\nsend/receive packets through a base station that is operated by the cellular\nnetwork provider. Unlike WiFi, a user need only be within a few tens of\nkilometers (as opposed to a few tens of meters) of the base station.\nTelecommunications companies have made enormous investments in\nso-called fourth-generation (4G) wireless, which provides real-world\ndownload speeds of up to 60 Mbps. But even higher-speed wide-area access\ntechnologies—a fifth-generation (5G) of wide-area wireless networks—are\nalready being deployed. We’ll cover the basic principles of wireless\nnetworks and mobility, as well as WiFi, 4G and 5G technologies (and\nmore!) in Chapter 7.\n1.2.2 Physical Media\nIn the previous subsection, we gave an overview of some of the most\nimportant network access technologies in the Internet. As we described\nthese technologies, we also indicated the physical media used. For example,\nwe said that HFC uses a combination of fiber cable and coaxial cable. We\nsaid that DSL and Ethernet use copper wire. And we said that mobile access\nnetworks use the radio spectrum. In this subsection, we provide a brief\noverview of these and other transmission media that are commonly used in\nthe Internet.\nIn order to define what is meant by a physical medium, let us reflect on\nthe brief life of a bit. Consider a bit traveling from one end system, through\na series of links and routers, to another end system. This poor bit gets\nkicked around and transmitted many, many times! The source end system\nfirst transmits the bit, and shortly thereafter the first router in the series\nreceives the bit; the first router then transmits the bit, and shortly thereafter\nthe second router receives the bit; and so on. Thus our bit, when traveling\nfrom source to destination, passes through a series of transmitter-receiver\npairs. For each transmitter-receiver pair, the bit is sent by propagating\nelectromagnetic waves or optical pulses across a physical medium. The\nphysical medium can take many shapes and forms and does not have to be\nof the same type for each transmitter-receiver pair along the path. Examples\nof physical media include twisted-pair copper wire, coaxial cable,\nmultimode fiber-optic cable, terrestrial radio spectrum, and satellite radio\nspectrum. Physical media fall into two categories: guided media and\nunguided media. With guided media, the waves are guided along a solid\nmedium, such as a fiber-optic cable, a twisted-pair copper wire, or a coaxial\ncable. With unguided media, the waves propagate in the atmosphere and in\nouter space, such as in a wireless LAN or a digital satellite channel.\nBut before we get into the characteristics of the various media types, let\nus say a few words about their costs. The actual cost of the physical link\n(copper wire, fiber-optic cable, and so on) is often relatively minor\ncompared with other networking costs. In particular, the labor cost\nassociated with the installation of the physical link can be orders of\nmagnitude higher than the cost of the material. For this reason, many\nbuilders install twisted pair, optical fiber, and coaxial cable in every room in\na building. Even if only one medium is initially used, there is a good chance\nthat another medium could be used in the near future, and so money is\nsaved by not having to lay additional wires in the future.\nTwisted-Pair Copper Wire\nThe least expensive and most commonly used guided transmission medium\nis twisted-pair copper wire. For over a hundred years it has been used by\ntelephone networks. In fact, more than 99 percent of the wired connections\nfrom the telephone handset to the local telephone switch use twisted-pair\ncopper wire. Most of us have seen twisted pair in our homes (or those of\nour parents or grandparents!) and work environments. Twisted pair consists\nof two insulated copper wires, each about 1 mm thick, arranged in a regular\nspiral pattern. The wires are twisted together to reduce the electrical\ninterference from similar pairs close by. Typically, a number of pairs are\nbundled together in a cable by wrapping the pairs in a protective shield. A\nwire pair constitutes a single communication link. Unshielded twisted pair\n(UTP) is commonly used for computer networks within a building, that is,\nfor LANs. Data rates for LANs using twisted pair today range from 10\nMbps to 10 Gbps. The data rates that can be achieved depend on the\nthickness of the wire and the distance between transmitter and receiver.\nWhen fiber-optic technology emerged in the 1980s, many people\ndisparaged twisted pair because of its relatively low bit rates. Some people\neven felt that fiber-optic technology would completely replace twisted pair.\nBut twisted pair did not give up so easily. Modern twisted-pair technology,\nsuch as category 6a cable, can achieve data rates of 10 Gbps for distances\nup to a hundred meters. In the end, twisted pair has emerged as the\ndominant solution for high-speed LAN networking.\nAs discussed earlier, twisted pair is also commonly used for residential\nInternet access. We saw that dial-up modem technology enables access at\nrates of up to 56  kbps over twisted pair. We also saw that DSL (digital\nsubscriber line) technology has enabled residential users to access the\nInternet at tens of Mbps over twisted pair (when users live close to the ISP’s\ncentral office).\nCoaxial Cable\nLike twisted pair, coaxial cable consists of two copper conductors, but the\ntwo conductors are concentric rather than parallel. With this construction\nand special insulation and shielding, coaxial cable can achieve high data\ntransmission rates. Coaxial cable is quite common in cable television\nsystems. As we saw earlier, cable television systems have recently been\ncoupled with cable modems to provide residential users with Internet access\nat rates of hundreds of Mbps. In cable television and cable Internet access,\nthe transmitter shifts the digital signal to a specific frequency band, and the\nresulting analog signal is sent from the transmitter to one or more receivers.\nCoaxial cable can be used as a guided shared medium. Specifically, a\nnumber of end systems can be connected directly to the cable, with each of\nthe end systems receiving whatever is sent by the other end systems.\nFiber Optics\nAn optical fiber is a thin, flexible medium that conducts pulses of light,\nwith each pulse representing a bit. A single optical fiber can support\ntremendous bit rates, up to tens or even hundreds of gigabits per second.\nThey are immune to electromagnetic interference, have very low signal\nattenuation up to 100 kilometers, and are very hard to tap. These\ncharacteristics have made fiber optics the preferred long-haul guided\ntransmission media, particularly for overseas links. Many of the long-\ndistance telephone networks in the United States and elsewhere now use\nfiber optics exclusively. Fiber optics is also prevalent in the backbone of the\nInternet. However, the high cost of optical devices—such as transmitters,\nreceivers, and switches—has hindered their deployment for short-haul\ntransport, such as in a LAN or into the home in a residential access\nnetwork. The Optical Carrier (OC) standard link speeds range from\n51.8 Mbps to 39.8 Gbps; these specifications are often referred to as OC-n,\nwhere the link speed equals n × 51.8 Mbps. Standards in use today include\nOC-1, OC-3, OC-12, OC-24, OC-48, OC-96, OC-192, OC-768.\nTerrestrial Radio Channels\nRadio channels carry signals in the electromagnetic spectrum. They are an\nattractive medium because they require no physical wire to be installed, can\npenetrate walls, provide connectivity to a mobile user, and can potentially\ncarry a signal for long distances. The characteristics of a radio channel\ndepend significantly on the propagation environment and the distance over\nwhich a signal is to be carried. Environmental considerations determine\npath loss and shadow fading (which decrease the signal strength as the\nsignal travels over a distance and around/through obstructing objects),\nmultipath fading (due to signal reflection off of interfering objects), and\ninterference (due to other transmissions and electromagnetic signals).\nTerrestrial radio channels can be broadly classified into three groups:\nthose that operate over very short distance (e.g., with one or two meters);\nthose that operate in local areas, typically spanning from ten to a few\nhundred meters; and those that operate in the wide area, spanning tens of\nkilometers. Personal devices such as wireless headsets, keyboards, and\nmedical devices operate over short distances; the wireless LAN\ntechnologies described in Section 1.2.1 use local-area radio channels; the\ncellular access technologies use wide-area radio channels. We’ll discuss\nradio channels in detail in Chapter 7.\nSatellite Radio Channels\nA communication satellite links two or more Earth-based microwave\ntransmitter/receivers, known as ground stations. The satellite receives\ntransmissions on one frequency band, regenerates the signal using a\nrepeater (discussed below), and transmits the signal on another frequency.\nTwo types of satellites are used in  communications: geostationary\nsatellites and low-earth orbiting (LEO) satellites.\nGeostationary satellites permanently remain above the same spot on\nEarth. This stationary presence is achieved by placing the satellite in orbit at\n36,000 kilo­meters above Earth’s surface. This huge distance from ground\nstation through satellite back to ground station introduces a substantial\nsignal propagation delay of 280 milliseconds. Nevertheless, satellite links,\nwhich can operate at speeds of hundreds of Mbps, are often used in areas\nwithout access to DSL or cable-based Internet access.\nLEO satellites are placed much closer to Earth and do not remain\npermanently above one spot on Earth. They rotate around Earth (just as the\nMoon does) and may communicate with each other, as well as with ground\nstations. To provide continuous coverage to an area, many satellites need to\nbe placed in orbit. There are currently many low-altitude communication\nsystems in development. LEO satellite ­technology may be used for Internet\naccess sometime in the future.\n1.3 The Network Core\nHaving examined the Internet’s edge, let us now delve more deeply inside\nthe network core—the mesh of packet switches and links that interconnects\nthe Internet’s end systems. Figure 1.10 highlights the network core with\nthick, shaded lines.\nFigure 1.10 ♦The network core\n1.3.1 Packet Switching\nIn a network application, end systems exchange messages with each other.\nMessages can contain anything the application designer wants. Messages\nmay perform a control function (for example, the “Hi” messages in our\nhandshaking example in Figure 1.2) or can contain data, such as an e-mail\nmessage, a JPEG image, or an MP3 audio file. To send a message from a\nsource end system to a destination end system, the source breaks long\nmessages into smaller chunks of data known as packets. Between source\nand destination, each packet travels through communication links and\npacket switches (for which there are two predominant types, routers and\nlink-layer switches). Packets are transmitted over each communication link\nat a rate equal to the full transmission rate of the link. So, if a source end\nsystem or a packet switch is sending a packet of L bits over a link with\ntransmission rate R bits/sec, then the time to transmit the packet is L / R\nStore-and-Forward Transmission\nMost packet switches use store-and-forward transmission at the inputs to\nthe links. Store-and-forward transmission means that the packet switch\nmust receive the entire packet before it can begin to transmit the first bit of\nthe packet onto the outbound link. To explore store-and-forward\ntransmission in more detail, consider a simple network consisting of two\nend systems connected by a single router, as shown in Figure 1.11. A router\nwill typically have many incident links, since its job is to switch an\nincoming packet onto an outgoing link; in this simple example, the router\nhas the rather simple task of transferring a packet from one (input) link to\nthe only other attached link. In this example, the source has three packets,\neach consisting of L bits, to send to the destination. At the snapshot of time\nshown in Figure 1.11, the source has transmitted some of packet 1, and the\nfront of packet 1 has already arrived at the router. Because the router\nemploys store-and-forwarding, at this instant of time, the router cannot\ntransmit the bits it has received; instead it must first buffer (i.e., “store”) the\npacket’s bits. Only after the router has received all of the packet’s bits can it\nbegin to transmit (i.e., “forward”) the packet onto the outbound link. To\ngain some insight into store-and-forward transmission, let’s now calculate\nthe amount of time that elapses from when the source begins to send the\npacket until the destination has received the entire packet. (Here we will\nignore propagation delay—the time it takes for the bits to travel across the\nwire at near the speed of light—which will be discussed in Section 1.4.)\nThe source begins to transmit at time 0; at time L/R seconds, the source has\ntransmitted the entire packet, and the entire packet has been received and\nstored at the router (since there is no propagation delay). At time L/R\nseconds, since the router has just received the entire packet, it can begin to\ntransmit the packet onto the outbound link towards the destination; at time\n2L/R, the router has transmitted the entire packet, and the entire packet has\nbeen received by the destination. Thus, the total delay is 2L/R. If the switch\ninstead forwarded bits as soon as they arrive (without first receiving the\nentire packet), then the total delay would be L/R since bits are not held up at\nthe router. But, as we will discuss in Section 1.4, routers need to receive,\nstore, and process the entire packet before forwarding.\nFigure 1.11 ♦Store-and-forward packet switching\nNow let’s calculate the amount of time that elapses from when the\nsource begins to send the first packet until the destination has received all\nthree packets. As before, at time L/R, the router begins to forward the first\npacket. But also at time L/R the source will begin to send the second packet,\nsince it has just finished sending the entire first packet. Thus, at time 2L/R,\nthe destination has received the first packet and the router has received the\nsecond packet. Similarly, at time 3L/R, the destination has received the first\ntwo packets and the router has received the third packet. Finally, at time\n4L/R the destination has received all three packets!\nLet’s now consider the general case of sending one packet from source\nto destination over a path consisting of N links each of rate R (thus, there\nare N-1 routers between source and destination). Applying the same logic as\nabove, we see that the end-to-end delay is:\nYou may now want to try to determine what the delay would be for P\npackets sent over a series of N links.\nQueuing Delays and Packet Loss\nEach packet switch has multiple links attached to it. For each attached link,\nthe packet switch has an output buffer (also called an output queue),\nwhich stores packets that the router is about to send into that link. The\noutput buffers play a key role in packet switching. If an arriving packet\nneeds to be transmitted onto a link but finds the link busy with the\ntransmission of another packet, the arriving packet must wait in the output\nbuffer. Thus, in addition to the store-and-forward delays, packets suffer\ndend-to-end = N L\noutput buffer queuing delays. These delays are variable and depend on the\nlevel of congestion in the network. Since the amount of buffer space is\nfinite, an arriving packet may find that the buffer is completely full with\nother packets waiting for transmission. In this case, packet loss will occur\n—either the arriving packet or one of the already-queued packets will be\nFigure 1.12 illustrates a simple packet-switched network. As in Figure\n1.11, packets are represented by three-dimensional slabs. The width of a\nslab represents the number of bits in the packet. In this figure, all packets\nhave the same width and hence the same length. Suppose Hosts A and B are\nsending packets to Host E. Hosts A and B first send their packets along 100\nMbps Ethernet links to the first router. The router then directs these packets\nto the 15 Mbps link. If, during a short interval of time, the arrival rate of\npackets to the router (when converted to bits per second) exceeds 15 Mbps,\ncongestion will occur at the router as packets queue in the link’s output\nbuffer before being transmitted onto the link. For example, if Host A and B\neach send a burst of five packets back-to-back at the same time, then most\nof these packets will spend some time waiting in the queue. The situation is,\nin fact, entirely analogous to many common-day situations—for example,\nwhen we wait in line for a bank teller or wait in front of a tollbooth. We’ll\nexamine this queuing delay in more detail in Section 1.4.\nFigure 1.12 ♦Packet switching\nForwarding Tables and Routing Protocols\nEarlier, we said that a router takes a packet arriving on one of its attached\ncommunication links and forwards that packet onto another one of its\nattached communication links. But how does the router determine which\nlink it should forward the packet onto? Packet forwarding is actually done\nin different ways in different types of computer networks. Here, we briefly\ndescribe how it is done in the Internet.\nIn the Internet, every end system has an address called an IP address.\nWhen a source end system wants to send a packet to a destination end\nsystem, the source includes the destination’s IP address in the packet’s\nheader. As with postal addresses, this address has a hierarchical structure.\nWhen a packet arrives at a router in the network, the router examines a\nportion of the packet’s destination address and forwards the packet to an\nadjacent router. More specifically, each router has a forwarding table that\nmaps destination addresses (or portions of the destination addresses) to that\nrouter’s outbound links. When a packet arrives at a router, the router\nexamines the address and searches its forwarding table, using this\ndestination address, to find the appropriate outbound link. The router then\ndirects the packet to this outbound link.\nThe end-to-end routing process is analogous to a car driver who does\nnot use maps but instead prefers to ask for directions. For example, suppose\nJoe is driving from Philadelphia to 156 Lakeside Drive in Orlando, Florida.\nJoe first drives to his neighborhood gas station and asks how to get to 156\nLakeside Drive in Orlando, Florida. The gas station attendant extracts the\nFlorida portion of the address and tells Joe that he needs to get onto the\ninterstate highway I-95 South, which has an entrance just next to the gas\nstation. He also tells Joe that once he enters Florida, he should ask someone\nelse there. Joe then takes I-95 South until he gets to Jacksonville, Florida, at\nwhich point he asks another gas station attendant for directions. The\nattendant extracts the Orlando portion of the address and tells Joe that he\nshould continue on I-95 to Daytona Beach and then ask someone else. In\nDaytona Beach, another gas station attendant also extracts the Orlando\nportion of the address and tells Joe that he should take I-4 directly to\nOrlando. Joe takes I-4 and gets off at the Orlando exit. Joe goes to another\ngas station attendant, and this time the attendant extracts the Lakeside Drive\nportion of the address and tells Joe the road he must follow to get to\nLakeside Drive. Once Joe reaches Lakeside Drive, he asks a kid on a\nbicycle how to get to his destination. The kid extracts the 156 portion of the\naddress and points to the house. Joe finally reaches his ultimate destination.\nIn the above analogy, the gas station attendants and kids on bicycles are\nanalogous to routers.\nWe just learned that a router uses a packet’s destination address to\nChapter 5. But to whet your appetite here, we’ll note now that the Internet\nhas a number of special routing protocols that are used to automatically set\nthe forwarding tables. A routing protocol may, for example, determine the\nshortest path from each router to each destination and use the shortest path\nresults to configure the forwarding tables in the routers.\n1.3.2 Circuit Switching\nThere are two fundamental approaches to moving data through a network of\nlinks and switches: circuit switching and packet switching. Having\ncovered packet-switched networks in the previous subsection, we now turn\nour attention to circuit-switched networks.\nIn circuit-switched networks, the resources needed along a path\n(buffers, link transmission rate) to provide for communication between the\nend systems are reserved for the duration of the communication session\nbetween the end systems. In packet-switched networks, these resources are\nnot reserved; a session’s messages use the resources on demand and, as a\nconsequence, may have to wait (that is, queue) for access to a\ncommunication link. As a simple analogy, consider two restaurants, one that\nrequires reservations and another that neither requires reservations nor\naccepts them. For the restaurant that requires reservations, we have to go\nthrough the hassle of calling before we leave home. But when we arrive at\nthe restaurant we can, in principle, immediately be seated and order our\nmeal. For the restaurant that does not require reservations, we don’t need to\nbother to reserve a table. But when we arrive at the restaurant, we may have\nto wait for a table before we can be seated.\nTraditional telephone networks are examples of circuit-switched\nnetworks. ­Consider what happens when one person wants to send\ninformation (voice or facsimile) to another over a telephone network.\nBefore the sender can send the information, the network must establish a\nconnection between the sender and the receiver. This is a bona fide\nconnection for which the switches on the path between the sender and\nreceiver maintain connection state for that connection. In the jargon of\ntelephony, this connection is called a circuit. When the network establishes\nthe circuit, it also reserves a constant transmission rate in the network’s\nlinks (representing a fraction of each link’s transmission capacity) for the\nduration of the connection. Since a given transmission rate has been\nreserved for this sender-to-receiver connection, the sender can transfer the\ndata to the receiver at the guaranteed constant rate.\nFigure 1.13 illustrates a circuit-switched network. In this network, the\nfour circuit switches are interconnected by four links. Each of these links\nhas four circuits, so that each link can support four simultaneous\nconnections. The hosts (for example, PCs and workstations) are each\ndirectly connected to one of the switches. When two hosts want to\ncommunicate, the network establishes a dedicated end-to-end connection\nbetween the two hosts. Thus, in order for Host A to communicate with Host\nB, the network must first reserve one circuit on each of two links. In this\nexample, the dedicated end-to-end connection uses the second circuit in the\nfirst link and the fourth circuit in the second link. Because each link has\nfour circuits, for each link used by the end-to-end connection, the\nconnection gets one fourth of the link’s total transmission capacity for the\nduration of the connection. Thus, for example, if each link between adjacent\nswitches has a transmission rate of 1 Mbps, then each end-to-end circuit-\nswitch connection gets 250 kbps of dedicated transmission rate.\nFigure 1.13 ♦A simple circuit-switched network consisting of four\nswitches and four links\nIn contrast, consider what happens when one host wants to send a\npacket to another host over a packet-switched network, such as the Internet.\nAs with circuit switching, the packet is transmitted over a series of\ncommunication links. But different from circuit switching, the packet is sent\ninto the network without reserving any link resources whatsoever. If one of\nthe links is congested because other packets need to be transmitted over the\nlink at the same time, then the packet will have to wait in a buffer at the\nsending side of the transmission link and suffer a delay. The Internet makes\nits best effort to deliver packets in a timely manner, but it does not make\nany guarantees.\nMultiplexing in Circuit-Switched Networks\nA circuit in a link is implemented with either frequency-division\nmultiplexing (FDM) or time-division multiplexing (TDM). With FDM,\nthe frequency spectrum of a link is divided up among the connections\nestablished across the link. Specifically, the link dedicates a frequency band\nto each connection for the ­duration of the connection. In telephone\nnetworks, this frequency band typically has a width of 4 kHz (that is, 4,000\nhertz or 4,000 cycles per second). The width of the band is called, not\nsurprisingly, the bandwidth. FM radio stations also use FDM to share the\nfrequency spectrum between 88 MHz and 108 MHz, with each station\nbeing allocated a specific frequency band.\nFor a TDM link, time is divided into frames of fixed duration, and each\nframe is divided into a fixed number of time slots. When the network\nestablishes a connection across a link, the network dedicates one time slot\nin every frame to this connection. These slots are dedicated for the sole use\nof that connection, with one time slot available for use (in every frame) to\ntransmit the connection’s data.\nFigure 1.14 illustrates FDM and TDM for a specific network link\nsupporting up to four circuits. For FDM, the frequency domain is\nsegmented into four bands, each of bandwidth 4 kHz. For TDM, the time\ndomain is segmented into frames, with four time slots in each frame; each\ncircuit is assigned the same dedicated slot in the revolving TDM frames.\nFor TDM, the transmission rate of a circuit is equal to the frame rate\nmultiplied by the number of bits in a slot. For example, if the link transmits\n8,000 frames per second and each slot consists of 8 bits, then the\ntransmission rate of each circuit is 64 kbps.\nFigure 1.14 ♦With FDM, each circuit continuously gets a fraction of\nthe bandwidth. With TDM, each circuit gets all of the\nbandwidth periodically during brief intervals of time\n(that is, during slots)\nProponents of packet switching have always argued that circuit\nswitching is wasteful because the dedicated circuits are idle during silent\nperiods. For example, when one person in a telephone call stops talking,\nthe idle network resources (frequency bands or time slots in the links along\nthe connection’s route) cannot be used by other ongoing connections. As\nanother example of how these resources can be underutilized, consider a\nradiologist who uses a circuit-switched network to remotely access a series\nof x-rays. The radiologist sets up a connection, requests an image,\ncontemplates the image, and then requests a new image. Network resources\nare allocated to the connection but are not used (i.e., are wasted) during the\nradiologist’s contemplation periods. Proponents of packet switching also\nenjoy pointing out that establishing end-to-end circuits and reserving end-\nto-end transmission capacity is complicated and requires complex signaling\nsoftware to coordinate the operation of the switches along the end-to-end\nBefore we finish our discussion of circuit switching, let’s work through\na numerical example that should shed further insight on the topic. Let us\nconsider how long it takes to send a file of 640,000 bits from Host A to Host\nB over a circuit-switched network. Suppose that all links in the network use\nTDM with 24 slots and have a bit rate of 1.536 Mbps. Also suppose that it\ntakes 500 msec to establish an end-to-end circuit before Host A can begin to\ntransmit the file. How long does it take to send the file? Each circuit has a\ntransmission rate of (1.536 Mbps)/24 = 64 kbps, so it takes (640,000\nbits)/(64 kbps) = 10 seconds to transmit the file. To this 10 seconds we add\nthe circuit establishment time, giving 10.5 seconds to send the file. Note\nthat the transmission time is independent of the number of links: The\ntransmission time would be 10 seconds if the end-to-end circuit passed\nthrough one link or a hundred links. (The actual end-to-end delay also\nincludes a propagation delay; see Section 1.4.)\nPacket Switching Versus Circuit Switching\nHaving described circuit switching and packet switching, let us compare the\ntwo. Critics of packet switching have often argued that packet switching is\nnot suitable for real-time services (for example, telephone calls and video\nconference calls) because of its variable and unpredictable end-to-end\ndelays (due primarily to variable and unpredictable queuing delays).\nProponents of packet switching argue that (1) it offers better sharing of\ntransmission capacity than circuit switching and (2) it is simpler, more\nefficient, and less costly to implement than circuit switching. An interesting\ndiscussion of packet switching versus circuit switching is [Molinero-\nFernandez 2002]. Generally speaking, people who do not like to hassle with\n­restaurant reservations prefer packet switching to circuit switching.\nWhy is packet switching more efficient? Let’s look at a simple\nexample. Suppose users share a 1 Mbps link. Also suppose that each user\nalternates between periods of activity, when a user generates data at a\nconstant rate of 100 kbps, and periods of inactivity, when a user generates\nno data. Suppose further that a user is active only 10 percent of the time\n(and is idly drinking coffee during the remaining 90 percent of the time).\nWith circuit switching, 100 kbps must be reserved for each user at all times.\nFor example, with circuit-switched TDM, if a one-second frame is divided\ninto 10 time slots of 100 ms each, then each user would be allocated one\ntime slot per frame.\nThus, the circuit-switched link can support only 10 ( = 1 Mbps/100\nkbps) simultaneous users. With packet switching, the probability that a\nspecific user is active is 0.1 (that is, 10 percent). If there are 35 users, the\nprobability that there are 11 or more simultaneously active users is\napproximately 0.0004. (Homework Problem P8 outlines how this\nprobability is obtained.) When there are 10 or fewer simultaneously active\nusers (which happens with probability 0.9996), the aggregate arrival rate of\ndata is less than or equal to 1 Mbps, the output rate of the link. Thus, when\nthere are 10 or fewer active users, users’ packets flow through the link\nessentially without delay, as is the case with circuit switching. When there\nare more than 10 simultaneously active users, then the aggregate arrival rate\nof packets exceeds the output capacity of the link, and the output queue will\nbegin to grow. (It continues to grow until the aggregate input rate falls back\nbelow 1 Mbps, at which point the queue will begin to diminish in length.)\nBecause the probability of having more than 10 simultaneously active users\nis minuscule in this example, packet switching provides essentially the\nsame performance as circuit switching, but does so while allowing for more\nthan three times the number of users.\nLet’s now consider a second simple example. Suppose there are 10\nusers and that one user suddenly generates one thousand 1,000-bit packets,\nwhile other users remain quiescent and do not generate packets. Under\nTDM circuit switching with 10 slots per frame and each slot consisting of\n1,000 bits, the active user can only use its one time slot per frame to\ntransmit data, while the remaining nine time slots in each frame remain idle.\nIt will be 10 seconds before all of the active user’s one million bits of data\nhas been transmitted. In the case of packet switching, the active user can\ncontinuously send its packets at the full link rate of 1 Mbps, since there are\nno other users generating packets that need to be multiplexed with the\nactive user’s packets. In this case, all of the active user’s data will be\ntransmitted within 1 second.\nThe above examples illustrate two ways in which the performance of\npacket switching can be superior to that of circuit switching. They also\nhighlight the crucial difference between the two forms of sharing a link’s\ntransmission rate among multiple data streams. Circuit switching pre-\nallocates use of the transmission link regardless of demand, with allocated\nbut unneeded link time going unused. Packet switching on the other hand\nallocates link use on demand. Link transmission capacity will be shared on\na packet-by-packet basis only among those users who have packets that\nneed to be transmitted over the link.\nAlthough packet switching and circuit switching are both prevalent in\ntoday’s telecommunication networks, the trend has certainly been in the\ndirection of packet switching. Even many of today’s circuit-switched\ntelephone networks are slowly migrating toward packet switching. In\nparticular, telephone networks often use packet switching for the expensive\noverseas portion of a telephone call.\n1.3.3 A Network of Networks\nWe saw earlier that end systems (PCs, smartphones, Web servers, mail\nservers, and so on) connect into the Internet via an access ISP. The access\nISP can provide either wired or wireless connectivity, using an array of\naccess technologies including DSL, cable, FTTH, Wi-Fi, and cellular. Note\nthat the access ISP does not have to be a telco or a cable company; instead it\ncan be, for example, a university (providing Internet access to students,\nstaff, and faculty), or a company (providing access for its employees). But\nconnecting end users and content providers into an access ISP is only a\nsmall piece of solving the puzzle of connecting the billions of end systems\nthat make up the Internet. To complete this puzzle, the access ISPs\nthemselves must be interconnected. This is done by creating a network of\nnetworks—understanding this phrase is the key to understanding the\nOver the years, the network of networks that forms the Internet has\nevolved into a very complex structure. Much of this evolution is driven by\neconomics and national policy, rather than by performance considerations.\nIn order to understand today’s Internet network structure, let’s\nincrementally build a series of network structures, with each new structure\nbeing a better approximation of the complex Internet that we have today.\nRecall that the overarching goal is to interconnect the access ISPs so that all\nend systems can send packets to each other. One naive approach would be\nto have each access ISP directly connect with every other access ISP. Such a\nmesh design is, of course, much too costly for the access ISPs, as it would\nrequire each access ISP to have a separate communication link to each of\nthe hundreds of thousands of other access ISPs all over the world.\nOur first network structure, Network Structure 1, interconnects all of\nthe access ISPs with a single global transit ISP. Our (imaginary) global\ntransit ISP is a network of routers and communication links that not only\nspans the globe, but also has at least one router near each of the hundreds of\nthousands of access ISPs. Of course, it would be very costly for the global\nISP to build such an extensive network. To be profitable, it would naturally\ncharge each of the access ISPs for connectivity, with the pricing reflecting\n(but not necessarily directly proportional to) the amount of traffic an access\nISP exchanges with the global ISP. Since the access ISP pays the global\ntransit ISP, the access ISP is said to be a customer and the global transit ISP\nis said to be a provider.\nNow if some company builds and operates a global transit ISP that is\nprofitable, then it is natural for other companies to build their own global\ntransit ISPs and compete with the original global transit ISP. This leads to\nNetwork Structure 2, which consists of the hundreds of thousands of access\nISPs and multiple global ­transit ISPs. The access ISPs certainly prefer\nNetwork Structure 2 over Network Structure 1 since they can now choose\namong the competing global transit providers as a function of their pricing\nand services. Note, however, that the global transit ISPs themselves must\ninterconnect: Otherwise access ISPs connected to one of the global transit\nproviders would not be able to communicate with access ISPs connected to\nthe other global transit providers.\nNetwork Structure 2, just described, is a two-tier hierarchy with global\ntransit providers residing at the top tier and access ISPs at the bottom tier.\nThis assumes that global transit ISPs are not only capable of getting close to\neach and every access ISP, but also find it economically desirable to do so.\nIn reality, although some ISPs do have impressive global coverage and do\ndirectly connect with many access ISPs, no ISP has presence in each and\nevery city in the world. Instead, in any given region, there may be a\nregional ISP to which the access ISPs in the region connect. Each regional\nISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to our (imaginary)\nglobal transit ISP; but tier-1 ISPs, which actually do exist, do not have a\npresence in every city in the world. There are approximately a dozen tier-1\nISPs, including Level 3 Communications, AT&T, Sprint, and NTT.\nInterestingly, no group officially sanctions tier-1 status; as the saying goes\n—if you have to ask if you’re a member of a group, you’re probably not.\nReturning to this network of networks, not only are there multiple\ncompeting tier-1 ISPs, there may be multiple competing regional ISPs in a\nregion. In such a hierarchy, each access ISP pays the regional ISP to which\nit connects, and each regional ISP pays the tier-1 ISP to which it connects.\n(An access ISP can also connect directly to a tier-1 ISP, in which case it\npays the tier-1 ISP). Thus, there is customer-provider relationship at each\nlevel of the hierarchy. Note that the tier-1 ISPs do not pay anyone as they\nare at the top of the hierarchy. To further complicate matters, in some\nregions, there may be a larger regional ISP (possibly spanning an entire\ncountry) to which the smaller regional ISPs in that region connect; the\nlarger regional ISP then connects to a tier-1 ISP. For example, in China,\nthere are access ISPs in each city, which connect to provincial ISPs, which\nin turn connect to national ISPs, which finally connect to tier-1 ISPs [Tian\n2012]. We refer to this multi-tier hierarchy, which is still only a crude\napproximation of today’s Internet, as Network Structure 3.\nTo build a network that more closely resembles today’s Internet, we\nmust add points of presence (PoPs), multi-homing, peering, and Internet\nexchange points (IXPs) to the hierarchical Network Structure 3. PoPs exist\nin all levels of the hierarchy, except for the bottom (access ISP) level. A\nPoP is simply a group of one or more routers (at the same location) in the\nprovider’s network where customer ISPs can connect into the provider ISP.\nFor a customer network to connect to a provider’s PoP, it can lease a high-\nspeed link from a third-party telecommunications provider to directly\nconnect one of its routers to a router at the PoP. Any ISP (except for tier-1\nISPs) may choose to multi-home, that is, to connect to two or more\nprovider ISPs. So, for example, an access ISP may multi-home with two\nregional ISPs, or it may multi-home with two regional ISPs and also with a\ntier-1 ISP. Similarly, a regional ISP may multi-home with multiple tier-1\nISPs. When an ISP multi-homes, it can continue to send and receive packets\ninto the Internet even if one of its providers has a failure.\nAs we just learned, customer ISPs pay their provider ISPs to obtain\nglobal Internet interconnectivity. The amount that a customer ISP pays a\nprovider ISP reflects the amount of traffic it exchanges with the provider.\nTo reduce these costs, a pair of nearby ISPs at the same level of the\nhierarchy can peer, that is, they can directly connect their networks together\nso that all the traffic between them passes over the direct connection rather\nthan through upstream intermediaries. When two ISPs peer, it is typically\nsettlement-free, that is, neither ISP pays the other. As noted earlier, tier-1\nISPs also peer with one another, settlement-free. For a readable discussion\nof peering and customer-provider relationships, see [Van der Berg 2008].\nAlong these same lines, a third-party company can create an Internet\nExchange Point (IXP), which is a meeting point where multiple ISPs can\npeer together. An IXP is typically in a stand-alone building with its own\nswitches [Ager 2012]. There are over 600 IXPs in the Internet today\n[PeeringDB 2020]. We refer to this ecosystem—consisting of access ISPs,\nregional ISPs, tier-1 ISPs, PoPs, multi-homing, peering, and IXPs—as\nNetwork Structure 4.\nWe now finally arrive at Network Structure 5, which describes today’s\nInternet. Network Structure 5, illustrated in Figure 1.15, builds on top of\nNetwork Structure 4 by adding content-provider networks. Google is\ncurrently one of the leading examples of such a content-provider network.\nAs of this writing, it Google has 19 major data centers distributed across\nNorth America, Europe, Asia, South America, and Australia with each data\ncenter having tens or hundreds of thousands of servers. Additionally,\nGoogle has smaller data centers, each with a few hundred servers; these\nsmaller data centers are often located within IXPs. The Google data centers\nare all interconnected via Google’s private TCP/IP network, which spans\nthe entire globe but is nevertheless separate from the public Internet.\nImportantly, the Google private network only carries traffic to/from Google\nservers. As shown in Figure 1.15, the Google private network attempts to\n“bypass” the upper tiers of the Internet by peering (settlement free) with\nlower-tier ISPs, either by directly connecting with them or by connecting\nwith them at IXPs [Labovitz 2010]. However, because many access ISPs\ncan still only be reached by transiting through tier-1 networks, the Google\nnetwork also connects to tier-1 ISPs, and pays those ISPs for the traffic it\nexchanges with them. By creating its own network, a content provider not\nonly reduces its payments to upper-tier ISPs, but also has greater control of\nhow its services are ultimately delivered to end users. Google’s network\ninfrastructure is described in greater detail in Section 2.6.\nFigure 1.15 ♦Interconnection of ISPs\nIn summary, today’s Internet—a network of networks—is complex,\nconsisting of a dozen or so tier-1 ISPs and hundreds of thousands of lower-\ntier ISPs. The ISPs are diverse in their coverage, with some spanning\nmultiple continents and oceans, and others limited to narrow geographic\nregions. The lower-tier ISPs connect to the higher-tier ISPs, and the higher-\ntier ISPs interconnect with one another. Users and content providers are\ncustomers of lower-tier ISPs, and lower-tier ISPs are customers of higher-\ntier ISPs. In recent years, major content providers have also created their\nown networks and connect directly into lower-tier ISPs where possible.\n1.4 Delay, Loss, and Throughput in Packet-Switched Networks\nBack in Section 1.1 we said that the Internet can be viewed as an infrastructure that provides\nservices to distributed applications running on end systems. Ideally, we would like Internet\nservices to be able to move as much data as we want between any two end systems,\ninstantaneously, without any loss of data. Alas, this is a lofty goal, one that is unachievable\nin reality. Instead, computer networks necessarily constrain throughput (the amount of data\nper second that can be transferred) between end systems, introduce delays between end\nsystems, and can actually lose packets. On one hand, it is unfortunate that the physical laws\nof reality introduce delay and loss as well as constrain throughput. On the other hand,\nbecause computer networks have these problems, there are many fascinating issues\nsurrounding how to deal with the problems—more than enough issues to fill a course on\ncomputer networking and to motivate thousands of PhD theses! In this section, we’ll begin\nto examine and quantify delay, loss, and throughput in computer networks.\n1.4.1 Overview of Delay in Packet-Switched Networks\nRecall that a packet starts in a host (the source), passes through a series of routers, and ends\nits journey in another host (the destination). As a packet travels from one node (host or\nrouter) to the subsequent node (host or router) along this path, the packet suffers from\nseveral types of delays at each node along the path. The most important of these delays are\nthe nodal processing delay, queuing delay, transmission delay, and propagation delay;\ntogether, these delays accumulate to give a total nodal delay. The performance of many\nInternet applications—such as search, Web browsing, e-mail, maps, instant messaging, and\nvoice-over-IP—are greatly affected by network delays. In order to acquire a deep\nunderstanding of packet switching and computer networks, we must understand the nature\nand importance of these delays.\nTypes of Delay\nLet’s explore these delays in the context of Figure 1.16. As part of its end-to-end route\nbetween source and destination, a packet is sent from the upstream node through router A to\nrouter B. Our goal is to characterize the nodal delay at router A. Note that router A has an\noutbound link leading to router B. This link is preceded by a queue (also known as a buffer).\nWhen the packet arrives at router A from the upstream node, router A examines the packet’s\nheader to determine the appropriate outbound link for the packet and then directs the packet\nto this link. In this example, the outbound link for the packet is the one that leads to router B.\nA packet can be transmitted on a link only if there is no other packet currently being\ntransmitted on the link and if there are no other packets preceding it in the queue; if the link\nis ­currently busy or if there are other packets already queued for the link, the newly arriving\npacket will then join the queue.\nFigure 1.16 ♦The nodal delay at router A\nProcessing Delay\nThe time required to examine the packet’s header and determine where to direct the packet is\npart of the processing delay. The processing delay can also include other factors, such as the\ntime needed to check for bit-level errors in the packet that occurred in transmitting the\npacket’s bits from the upstream node to router A. Processing delays in high-speed routers are\ntypically on the order of microseconds or less. After this nodal processing, the router directs\nthe packet to the queue that precedes the link to router B. (In Chapter 4 we’ll study the\ndetails of how a router operates.)\nQueuing Delay\nAt the queue, the packet experiences a queuing delay as it waits to be transmitted onto the\nlink. The length of the queuing delay of a specific packet will depend on the number of\nearlier-arriving packets that are queued and waiting for transmission onto the link. If the\nqueue is empty and no other packet is currently being transmitted, then our packet’s queuing\ndelay will be zero. On the other hand, if the traffic is heavy and many other packets are also\nwaiting to be transmitted, the queuing delay will be long. We will see shortly that the\nnumber of packets that an arriving packet might expect to find is a function of the intensity\nand nature of the traffic arriving at the queue. ­Queuing delays can be on the order of\nmicroseconds to milliseconds in practice.\nTransmission Delay\nAssuming that packets are transmitted in a first-come-first-served manner, as is common in\npacket-switched networks, our packet can be transmitted only after all the packets that have\narrived before it have been transmitted. Denote the length of the packet by L bits, and denote\nthe transmission rate of the link from router A to router B by R bits/sec. For example, for a\n10 Mbps Ethernet link, the rate is R = 10 Mbps; for a 100 Mbps Ethernet link, the rate is R =\n100 Mbps. The transmission delay is L/R. This is the amount of time required to push (that\nis, transmit) all of the packet’s bits into the link. Transmission delays are typically on the\norder of microseconds to milliseconds in practice.\nPropagation Delay\nOnce a bit is pushed into the link, it needs to propagate to router B. The time required to\npropagate from the beginning of the link to router B is the propagation delay. The bit\npropagates at the propagation speed of the link. The propagation speed depends on the\nphysical medium of the link (that is, fiber optics, twisted-pair copper wire, and so on) and is\nin the range of\n2  ⋅ 108 meters/sec to 3  ⋅ 108 meters/sec\nwhich is equal to, or a little less than, the speed of light. The propagation delay is the\ndistance between two routers divided by the propagation speed. That is, the propagation\ndelay is d/s, where d is the distance between router A and router B and s is the propagation\nspeed of the link. Once the last bit of the packet propagates to node B, it and all the\npreceding bits of the packet are stored in router B. The whole process then continues with\nrouter B now performing the forwarding. In wide-area networks, propagation delays are on\nthe order of milliseconds.\nComparing Transmission and Propagation Delay\nExploring propagation delay and transmission delay\nNewcomers to the field of computer networking sometimes have difficulty understanding\nthe difference between transmission delay and propagation delay. The difference is subtle\nbut important. The transmission delay is the amount of time required for the router to push\nout the packet; it is a function of the packet’s length and the transmission rate of the link, but\nhas nothing to do with the distance between the two routers. The propagation delay, on the\nother hand, is the time it takes a bit to propagate from one router to the next; it is a function\nof the distance between the two routers, but has nothing to do with the packet’s length or the\ntransmission rate of the link.\nAn analogy might clarify the notions of transmission and propagation delay. Consider a\nhighway that has a tollbooth every 100 kilometers, as shown in Figure 1.17. You can think of\nthe highway segments between tollbooths as links and the tollbooths as routers. Suppose that\ncars travel (that is, propagate) on the highway at a rate of 100 km/hour (that is, when a car\nleaves a tollbooth, it instantaneously accelerates to 100 km/hour and maintains that speed\nbetween tollbooths). Suppose next that 10 cars, traveling together as a caravan, follow each\nother in a fixed order. You can think of each car as a bit and the caravan as a packet. Also\nsuppose that each tollbooth services (that is, transmits) a car at a rate of one car per 12\nseconds, and that it is late at night so that the caravan’s cars are the only cars on the highway.\nFinally, suppose that whenever the first car of the caravan arrives at a tollbooth, it waits at\nthe entrance until the other nine cars have arrived and lined up behind it. (Thus, the entire\ncaravan must be stored at the tollbooth before it can begin to be forwarded.) The time\nrequired for the tollbooth to push the entire caravan onto the highway is (10 cars)/(5\ncars/minute) = 2 minutes. This time is analogous to the transmission delay in a router. The\ntime required for a car to travel from the exit of one tollbooth to the next tollbooth is 100\nkm/(100 km/hour) = 1 hour. This time is analogous to propagation delay. Therefore, the time\nfrom when the caravan is stored in front of a tollbooth until the caravan is stored in front of\nthe next tollbooth is the sum of transmission delay and propagation delay—in this example,\n62 minutes.\nFigure 1.17 ♦Caravan analogy\nLet’s explore this analogy a bit more. What would happen if the tollbooth service time\nfor a caravan were greater than the time for a car to travel between tollbooths? For example,\nsuppose now that the cars travel at the rate of 1,000 km/hour and the tollbooth services cars\nat the rate of one car per minute. Then the traveling delay between two tollbooths is 6\nminutes and the time to serve a caravan is 10 minutes. In this case, the first few cars in the\ncaravan will arrive at the second tollbooth before the last cars in the caravan leave the first\ntollbooth. This situation also arises in packet-switched networks—the first bits in a packet\ncan arrive at a router while many of the remaining bits in the packet are still waiting to be\ntransmitted by the preceding router.\nIf a picture speaks a thousand words, then an animation must speak a million words. The\nWeb site for this textbook provides an interactive animation that nicely illustrates and\ncontrasts transmission delay and propagation delay. The reader is highly encouraged to visit\nthat animation. [Smith 2009] also provides a very readable discussion of propagation,\nqueuing, and transmission delays.\nIf we let d\n denote the processing, queuing, transmission, and\npropagation delays, then the total nodal delay is given by\ndnodal = dproc + dqueue + dtrans + dprop\nThe contribution of these delay components can vary significantly. For example, d\nnegligible (for example, a couple of microseconds) for a link connecting two routers on the\nsame university campus; however, d\n is hundreds of milliseconds for two routers\ninterconnected by a geostationary satellite link, and can be the dominant term in d\nSimilarly, d\n can range from negligible to significant. Its contribution is typically\nnegligible for transmission rates of 10 Mbps and higher (for example, for LANs); however,\nit can be hundreds of milliseconds for large Internet packets sent over low-speed dial-up\nmodem links. The processing delay, d\n, is often negligible; however, it strongly influences\na router’s maximum throughput, which is the maximum rate at which a router can forward\n1.4.2 Queuing Delay and Packet Loss\nThe most complicated and interesting component of nodal delay is the queuing delay, d\nIn fact, queuing delay is so important and interesting in computer networking that thousands\nof papers and numerous books have been written about it [Bertsekas 1991; Kleinrock 1975,\nKleinrock 1976]. We give only a high-level, intuitive discussion of queuing delay here; the\nmore curious reader may want to browse through some of the books (or even eventually\nwrite a PhD thesis on the subject!). Unlike the other three delays (namely, d\n), the queuing delay can vary from packet to packet. For example, if 10 packets arrive at\nan empty queue at the same time, the first packet transmitted will suffer no queuing delay,\nwhile the last packet transmitted will suffer a relatively large queuing delay (while it waits\nfor the other nine packets to be transmitted). Therefore, when characterizing queuing delay,\none typically uses statistical measures, such as average queuing delay, variance of queuing\ndelay, and the probability that the queuing delay exceeds some specified value.\nWhen is the queuing delay large and when is it insignificant? The answer to this\nquestion depends on the rate at which traffic arrives at the queue, the transmission rate of the\nlink, and the nature of the arriving traffic, that is, whether the traffic arrives periodically or\narrives in bursts. To gain some insight here, let a denote the average rate at which packets\narrive at the queue (a is in units of packets/sec). Recall that R is the transmission rate; that is,\nit is the rate (in bits/sec) at which bits are pushed out of the queue. Also suppose, for\nsimplicity, that all packets consist of L bits. Then the average rate at which bits arrive at the\nqueue is La bits/sec. Finally, assume that the queue is very big, so that it can hold essentially\nan infinite number of bits. The ratio La/R, called the traffic intensity, often plays an\nimportant role in estimating the extent of the queuing delay. If La/R > 1, then the average\nrate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted\nfrom the queue. In this unfortunate situation, the queue will tend to increase without bound\nand the queuing delay will approach infinity! Therefore, one of the golden rules in traffic\nengineering is: Design your system so that the traffic intensity is no greater than 1.\nNow consider the case La/R ≤ 1. Here, the nature of the arriving traffic impacts the\nqueuing delay. For example, if packets arrive periodically—that is, one packet arrives every\nL/R seconds—then every packet will arrive at an empty queue and there will be no queuing\ndelay. On the other hand, if packets arrive in bursts but periodically, there can be a\nsignificant average queuing delay. For example, suppose N packets arrive simultaneously\nevery (L/R)N seconds. Then the first packet transmitted has no queuing delay; the second\npacket transmitted has a queuing delay of L/R seconds; and more generally, the nth packet\ntransmitted has a queuing delay of (n − 1)L/R We leave it as an exercise for you to calculate\nthe average queuing delay in this example.\nThe two examples of periodic arrivals described above are a bit academic. ­Typically, the\narrival process to a queue is random; that is, the arrivals do not follow any pattern and the\npackets are spaced apart by random amounts of time. In this more realistic case, the quantity\nLa/R is not usually sufficient to fully characterize the queuing delay statistics. Nonetheless,\nit is useful in gaining an intuitive understanding of the extent of the queuing delay. In\nparticular, if the traffic intensity is close to zero, then packet arrivals are few and far between\nand it is unlikely that an arriving packet will find another packet in the queue. Hence, the\naverage queuing delay will be close to zero. On the other hand, when the traffic intensity is\nclose to 1, there will be intervals of time when the arrival rate exceeds the transmission\ncapacity (due to variations in packet arrival rate), and a queue will form during these periods\nof time; when the arrival rate is less than the transmission capacity, the length of the queue\nwill shrink. Nonetheless, as the traffic intensity approaches 1, the average queue length gets\nlarger and larger. The qualitative dependence of average queuing delay on the traffic\nintensity is shown in Figure 1.18.\nFigure 1.18 ♦Dependence of average queuing delay on traffic intensity\nOne important aspect of Figure 1.18 is the fact that as the traffic intensity approaches 1,\nthe average queuing delay increases rapidly. A small percentage increase in the intensity will\nresult in a much larger percentage-wise increase in delay. Perhaps you have experienced this\nphenomenon on the highway. If you regularly drive on a road that is typically congested, the\nfact that the road is typically congested means that its traffic intensity is close to 1. If some\nevent causes an even slightly larger-than-usual amount of traffic, the delays you experience\ncan be huge.\nTo really get a good feel for what queuing delays are about, you are encouraged once\nagain to visit the textbook Web site, which provides an interactive animation for a queue. If\nyou set the packet arrival rate high enough so that the traffic intensity exceeds 1, you will\nsee the queue slowly build up over time.\nPacket Loss\nIn our discussions above, we have assumed that the queue is capable of holding an infinite\nnumber of packets. In reality a queue preceding a link has finite capacity, although the\nqueuing capacity greatly depends on the router design and cost. Because the queue capacity\nis finite, packet delays do not really approach infinity as the traffic intensity approaches 1.\nInstead, a packet can arrive to find a full queue. With no place to store such a packet, a\nrouter will drop that packet; that is, the packet will be lost. This overflow at a queue can\nagain be seen in the interactive animation when the traffic intensity is greater than 1.\nFrom an end-system viewpoint, a packet loss will look like a packet having been\ntransmitted into the network core but never emerging from the network at the destination.\nThe fraction of lost packets increases as the traffic intensity increases. Therefore,\nperformance at a node is often measured not only in terms of delay, but also in terms of the\nprobability of packet loss. As we’ll discuss in the subsequent chapters, a lost packet may be\nretransmitted on an end-to-end basis in order to ensure that all data are eventually transferred\nfrom source to destination.\n1.4.3 End-to-End Delay\nOur discussion up to this point has focused on the nodal delay, that is, the delay at a single\nrouter. Let’s now consider the total delay from source to destination. To get a handle on this\nconcept, suppose there are N − 1 routers between the source host and the destination host.\nLet’s also suppose for the moment that the network is uncongested (so that queuing delays\nare negligible), the processing delay at each router and at the source host is d\ntransmission rate out of each router and out of the source host is R bits/sec, and the\npropagation on each link is d\n. The nodal delays accumulate and give an end-to-end delay,\nwhere, once again, d\n = L/R, where L is the packet size. Note that Equation 1.2 is a\ngeneralization of Equation 1.1, which did not take into account processing and propagation\ndelays. We leave it to you to generalize Equation 1.2 to the case of ­heterogeneous delays at\nthe nodes and to the presence of an average queuing delay at each node.\nUsing Traceroute to discover network paths and measure network delay\nTo get a hands-on feel for end-to-end delay in a computer network, we can make use of the\nTraceroute program. Traceroute is a simple program that can run in any Internet host. When\nthe user specifies a destination hostname, the program in the source host sends multiple,\nspecial packets toward that destination. As these packets work their way toward the\ndestination, they pass through a series of routers. When a router receives one of these special\npackets, it sends back to the source a short message that contains the name and address of\nthe router.\nMore specifically, suppose there are N − 1 routers between the source and the\ndestination. Then the source will send N special packets into the network, with each packet\naddressed to the ultimate destination. These N special packets are marked 1 through N, with\nthe first packet marked 1 and the last packet marked N. When the nth router receives the nth\npacket marked n, the router does not forward the packet toward its destination, but instead\nsends a message back to the source. When the destination host receives the Nth packet, it too\nreturns a message back to the source. The source records the time that elapses between when\nit sends a packet and when it receives the corresponding return message; it also records the\nname and address of the router (or the destination host) that returns the message. In this\nmanner, the source can reconstruct the route taken by packets flowing from source to\ndestination, and the source can determine the round-trip delays to all the intervening routers.\ndend-end = N(dproc + dtrans + dprop)\nTraceroute actually repeats the experiment just described three times, so the source actually\nsends 3 • N packets to the destination. RFC 1393 describes Traceroute in detail.\nHere is an example of the output of the Traceroute program, where the route was being\ntraced from the source host gaia.cs.umass.edu (at the University of ­Massachusetts) to a host\nin the computer science department at the University of Sorbonne in Paris (formerly the\nuniversity was known as UPMC). The output has six columns: the first column is the n value\ndescribed above, that is, the number of the router along the route; the second column is the\nname of the router; the third column is the address of the router (of the form\nxxx.xxx.xxx.xxx); the last three columns are the round-trip delays for three experiments. If\nthe source receives fewer than three messages from any given router (due to packet loss in\nthe network), Traceroute places an asterisk just after the router number and reports fewer\nthan three round-trip times for that router.\ngw-vlan-2451.cs.umass.edu (128.119.245.1) 1.899 ms 3.266 ms\nj-cs-gw-int-10-240.cs.umass.edu (10.119.240.254) 1.296 ms\n1.276 ms 1.245 ms\nn5-rt-1-1-xe-2-1-0.gw.umass.edu (128.119.3.33) 2.237 ms\n2.217 ms 2.187 ms\ncore1-rt-et-5-2-0.gw.umass.edu (128.119.0.9) 0.351 ms 0.392\nms 0.380 ms\nborder1-rt-et-5-0-0.gw.umass.edu (192.80.83.102) 0.345 ms\n0.345 ms 0.344 ms\nnox300gw1-umass-re.nox.org (192.5.89.101) 3.260 ms 0.416 ms\nnox300gw1-umass-re.nox.org (192.5.89.101) 3.165 ms 7.326 ms\n198.71.45.237 (198.71.45.237) 77.826 ms 77.246 ms 77.744 ms\nrenater-lb1-gw.mx1.par.fr.geant.net (62.40.124.70) 79.357\nms 77.729 79.152 ms\n193.51.180.109 (193.51.180.109) 78.379 ms 79.936 80.042 ms\n* 193.51.180.109 (193.51.180.109) 80.640 ms *\n* 195.221.127.182 (195.221.127.182) 78.408 ms *\n195.221.127.182 (195.221.127.182) 80.686 ms 80.796 ms\nr-upmc1.reseau.jussieu.fr (134.157.254.10) 78.399 ms *\nIn the trace above, there are 14 routers between the source and the destination. Most of these\nrouters have a name, and all of them have addresses. For example, the name of Router 4 is\ncore1-rt-et-5-2-0.gw.umass.edu and its address is 128.119.0.9. Looking at\nthe data provided for this same router, we see that in the first of the three trials the round-trip\ndelay between the source and the router was 0.351 msec. The round-trip delays for the\nsubsequent two trials were 0.392 and 0.380 msec. These round-trip delays include all of the\ndelays just discussed, including transmission delays, propagation delays, router processing\ndelays, and queuing delay.\nBecause the queuing delay is varying with time, the round-trip delay of packet n sent to\na router n can sometimes be longer than the round-trip delay of packet n+1 sent to router\nn+1. Indeed, we observe this phenomenon in the above example: the delay to Router 12 is\nsmaller than the delay to Router 11! Also note the big increase in the round-trip delay when\ngoing from router 7 to router 8. This is due to a transatlantic fiber-optic link between routers\n7 and 8, giving rise to a relatively large propagation delay. There are a number of free\nsoftware programs that provide a graphical interface to Traceroute; one of our favorites is\nPingPlotter [PingPlotter 2020].\nEnd System, Application, and Other Delays\nIn addition to processing, transmission, and propagation delays, there can be additional\nsignificant delays in the end systems. For example, an end system wanting to transmit a\npacket into a shared medium (e.g., as in a WiFi or cable modem scenario) may purposefully\ndelay its transmission as part of its protocol for sharing the medium with other end systems;\nwe’ll consider such protocols in detail in Chapter 6. Another important delay is media\npacketization delay, which is present in Voice-over-IP (VoIP) applications. In VoIP, the\nsending side must first fill a packet with encoded digitized speech before passing the packet\nto the Internet. This time to fill a packet—called the packetization delay—can be significant\nand can impact the user-perceived quality of a VoIP call. This issue will be further explored\nin a homework problem at the end of this chapter.\n1.4.4 Throughput in Computer Networks\nIn addition to delay and packet loss, another critical performance measure in computer\nnetworks is end-to-end throughput. To define throughput, consider transferring a large file\nfrom Host A to Host B across a computer network. This transfer might be, for example, a\nlarge video clip from one computer to another. The instantaneous throughput at any instant\nof time is the rate (in bits/sec) at which Host B is receiving the file. (Many applications\ndisplay the instantaneous throughput during downloads in the user interface—perhaps you\nhave observed this before! You might like to try measuring the end-to-end delay and\ndownload throughput between your and servers around the Internet using the speedtest\napplication [Speedtest 2020].) If the file consists of F bits and the transfer takes T seconds\nfor Host B to receive all F bits, then the average throughput of the file transfer is F/T\nbits/sec. For some applications, such as Internet telephony, it is desirable to have a low delay\nand an instantaneous throughput consistently above some threshold (for example, over 24\nkbps for some Internet telephony applications and over 256 kbps for some real-time video\napplications). For other applications, including those involving file transfers, delay is not\ncritical, but it is desirable to have the highest possible throughput.\nTo gain further insight into the important concept of throughput, let’s consider a few\nexamples. Figure 1.19(a) shows two end systems, a server and a client, connected by two\ncommunication links and a router. Consider the throughput for a file transfer from the server\nto the client. Let R  denote the rate of the link between the server and the router; and R\ndenote the rate of the link between the router and the client. Suppose that the only bits being\nsent in the entire network are those from the server to the client. We now ask, in this ideal\nscenario, what is the server-to-client throughput? To answer this question, we may think of\nbits as fluid and communication links as pipes. Clearly, the server cannot pump bits through\nits link at a rate faster than R  bps; and the router cannot forward bits at a rate faster than R\nbps. If R  < R , then the bits pumped by the server will “flow” right through the router and\narrive at the client at a rate of R  bps, giving a throughput of R  bps. If, on the other hand, R\n< R , then the router will not be able to forward bits as quickly as it receives them. In this\ncase, bits will only leave the router at rate R , giving an end-to-end throughput of R . (Note\nalso that if bits continue to arrive at the router at rate R , and continue to leave the router at\nR , the backlog of bits at the router waiting for transmission to the client will grow and grow\n—a most undesirable situation!) Thus, for this simple two-link network, the throughput is\nmin{R , R }, that is, it is the transmission rate of the bottleneck link. Having determined the\nthroughput, we can now approximate the time it takes to transfer a large file of F bits from\nserver to client as F/min{R , R }. For a specific example, suppose that you are downloading\nan MP3 file of F = 32 million bits, the server has a transmission rate of R  = 2 Mbps, and\nyou have an access link of R  = 1 Mbps. The time needed to transfer the file is then 32\nseconds. Of course, these expressions for throughput and transfer time are only\napproximations, as they do not account for store-and-forward and processing delays as well\nas protocol issues.\nFigure 1.19 ♦Throughput for a file transfer from server to client\nFigure 1.19(b) now shows a network with N links between the server and the client, with\nthe transmission rates of the N links being R , R , ..., R . Applying the same analysis as for\nthe two-link network, we find that the throughput for a file transfer from server to client is\nmin{R , R , ..., R }, which is once again the transmission rate of the bottleneck link along the\npath between server and client.\nNow consider another example motivated by today’s Internet. Figure 1.20(a) shows two\nend systems, a server and a client, connected to a computer network. Consider the\nthroughput for a file transfer from the server to the client. The server is connected to the\nnetwork with an access link of rate R  and the client is connected to the network with an\naccess link of rate R . Now suppose that all the links in the core of the communication\nnetwork have very high transmission rates, much higher than R  and R . Indeed, today, the\ncore of the Internet is over-provisioned with high speed links that experience little\ncongestion. Also suppose that the only bits being sent in the entire network are those from\nthe server to the client. Because the core of the computer network is like a wide pipe in this\nexample, the rate at which bits can flow from source to destination is again the minimum of\nR  and R , that is, throughput = min{R , R }. Therefore, the constraining factor for throughput\nin today’s Internet is typically the access network.\nFigure 1.20 ♦End-to-end throughput: (a) Client downloads a file from ­server; (b)\n10 clients ­downloading with 10 servers\nFor a final example, consider Figure 1.20(b) in which there are 10 servers and 10 clients\nconnected to the core of the computer network. In this example, there are 10 simultaneous\ndownloads taking place, involving 10 client-server pairs. Suppose that these 10 downloads\nare the only traffic in the network at the current time. As shown in the figure, there is a link\nin the core that is traversed by all 10 downloads. Denote R for the transmission rate of this\nlink R. Let’s suppose that all server access links have the same rate R , all client access links\nhave the same rate R , and the transmission rates of all the links in the core—except the one\ncommon link of rate R—are much larger than R , R , and R. Now we ask, what are the\nthroughputs of the downloads? Clearly, if the rate of the common link, R, is large—say a\nhundred times larger than both R  and R —then the throughput for each download will once\nagain be min{R , R }. But what if the rate of the common link is of the same order as R  and\nR ? What will the throughput be in this case? Let’s take a look at a specific example.\nSuppose R  = 2 Mbps, R  = 1 Mbps, R = 5 Mbps, and the common link divides its\ntransmission rate equally among the 10 downloads. Then the bottleneck for each download\nis no longer in the access network, but is now instead the shared link in the core, which only\nprovides each download with 500 kbps of throughput. Thus, the end-to-end throughput for\neach download is now reduced to 500 kbps.\nThe examples in Figure 1.19 and Figure 1.20(a) show that throughput depends on the\ntransmission rates of the links over which the data flows. We saw that when there is no other\nintervening traffic, the throughput can simply be approximated as the minimum transmission\nrate along the path between source and destination. The example in Figure 1.20(b) shows\nthat more generally the throughput depends not only on the transmission rates of the links\nalong the path, but also on the intervening traffic. In particular, a link with a high\ntransmission rate may nonetheless be the bottleneck link for a file transfer if many other data\nflows are also passing through that link. We will examine throughput in computer networks\nmore closely in the homework problems and in the subsequent chapters.\n1.5 Protocol Layers and Their Service Models\nFrom our discussion thus far, it is apparent that the Internet is an extremely\ncomplicated system. We have seen that there are many pieces to the\nInternet: numerous applications and protocols, various types of end\nsystems, packet switches, and various types of link-level media. Given this\nenormous complexity, is there any hope of organizing a network\narchitecture, or at least our discussion of network architecture? Fortunately,\nthe answer to both questions is yes.\n1.5.1 Layered Architecture\nBefore attempting to organize our thoughts on Internet architecture, let’s\nlook for a human analogy. Actually, we deal with complex systems all the\ntime in our everyday life. Imagine if someone asked you to describe, for\nexample, the airline system. How would you find the structure to describe\nthis complex system that has ticketing agents, baggage checkers, gate\npersonnel, pilots, airplanes, air traffic control, and a worldwide system for\nrouting airplanes? One way to describe this system might be to describe the\nseries of actions you take (or others take for you) when you fly on an\nairline. You purchase your ticket, check your bags, go to the gate, and\neventually get loaded onto the plane. The plane takes off and is routed to its\ndestination. After your plane lands, you deplane at the gate and claim your\nbags. If the trip was bad, you complain about the flight to the ticket agent\n(getting nothing for your effort). This scenario is shown in Figure 1.21.\nFigure 1.21 ♦Taking an airplane trip: actions\nAlready, we can see some analogies here with computer networking:\nYou are being shipped from source to destination by the airline; a packet is\nshipped from source host to destination host in the Internet. But this is not\nquite the analogy we are after. We are looking for some structure in Figure\n1.21. Looking at Figure 1.21, we note that there is a ticketing function at\neach end; there is also a baggage function for already-ticketed passengers,\nand a gate function for already-ticketed and already-baggage-checked\npassengers. For passengers who have made it through the gate (that is,\npassengers who are already ticketed, baggage-checked, and through the\ngate), there is a takeoff and landing function, and while in flight, there is an\nairplane-routing function. This suggests that we can look at the\nfunctionality in Figure 1.21 in a horizontal manner, as shown in Figure\nFigure 1.22 ♦Horizontal layering of airline functionality\nFigure 1.22 has divided the airline functionality into layers, providing a\nframework in which we can discuss airline travel. Note that each layer,\ncombined with the layers below it, implements some functionality, some\nservice. At the ticketing layer and below, airline-counter-to-airline-counter\ntransfer of a person is accomplished. At the baggage layer and below,\nbaggage-check-to-baggage-claim transfer of a person and bags is\naccomplished. Note that the baggage layer provides this service only to an\nalready-ticketed person. At the gate layer, departure-gate-to-arrival-gate\ntransfer of a person and bags is accomplished. At the takeoff/landing layer,\nrunway-to-runway transfer of people and their bags is accomplished. Each\nlayer provides its service by (1) performing certain actions within that layer\n(for example, at the gate layer, loading and unloading people from an\nairplane) and by (2) using the services of the layer directly below it (for\nexample, in the gate layer, using the runway-to-runway passenger transfer\nservice of the takeoff/landing layer).\nA layered architecture allows us to discuss a well-defined, specific part\nof a large and complex system. This simplification itself is of considerable\nvalue by providing modularity, making it much easier to change the\nimplementation of the service provided by the layer. As long as the layer\nprovides the same service to the layer above it, and uses the same services\nfrom the layer below it, the remainder of the system remains unchanged\nwhen a layer’s implementation is changed. (Note that changing the\nimplementation of a service is very different from changing the service\nitself!) For example, if the gate functions were changed (for instance, to\nhave people board and disembark by height), the remainder of the airline\nsystem would remain unchanged since the gate layer still provides the same\nfunction (loading and unloading people); it simply implements that function\nin a different manner after the change. For large and complex systems that\nare constantly being updated, the ability to change the implementation of a\nservice without affecting other components of the system is another\nimportant advantage of layering.\nProtocol Layering\nBut enough about airlines. Let’s now turn our attention to network\nprotocols. To provide structure to the design of network protocols, network\ndesigners organize protocols—and the network hardware and software that\nimplement the protocols—in layers. Each protocol belongs to one of the\nlayers, just as each function in the airline architecture in Figure 1.22\nbelonged to a layer. We are again interested in the services that a layer\noffers to the layer above—the so-called service model of a layer. Just as in\nthe case of our airline example, each layer provides its service by (1)\nperforming certain actions within that layer and by (2) using the services of\nthe layer directly below it. For example, the services provided by layer n\nmay include reliable delivery of messages from one edge of the network to\nthe other. This might be implemented by using an unreliable edge-to-edge\nmessage delivery service of layer n − 1, and adding layer n functionality to\ndetect and retransmit lost messages.\nA protocol layer can be implemented in software, in hardware, or in a\ncombination of the two. Application-layer protocols—such as HTTP and\nSMTP—are almost always implemented in software in the end systems; so\nare transport-layer protocols. Because the physical layer and data link layers\nare responsible for handling communication over a specific link, they are\ntypically implemented in a network interface card (for example, Ethernet or\nWiFi interface cards) associated with a given link. The network layer is\noften a mixed implementation of hardware and software. Also note that just\nas the functions in the layered airline architecture were distributed among\nthe various airports and flight control centers that make up the system, so\ntoo is a layer n protocol distributed among the end systems, packet\nswitches, and other components that make up the network. That is, there’s\noften a piece of a layer n protocol in each of these network components.\nProtocol layering has conceptual and structural advantages [RFC 3439].\nAs we have seen, layering provides a structured way to discuss system\ncomponents. Modularity makes it easier to update system components. We\nmention, however, that some researchers and networking engineers are\nvehemently opposed to layering [Wakeman 1992]. One potential drawback\nof layering is that one layer may duplicate lower-layer functionality. For\nexample, many protocol stacks provide error recovery on both a per-link\nbasis and an end-to-end basis. A second potential drawback is that\nfunctionality at one layer may need information (for example, a timestamp\nvalue) that is present only in another layer; this violates the goal of\nseparation of layers.\nWhen taken together, the protocols of the various layers are called the\nprotocol stack. The Internet protocol stack consists of five layers: the\nphysical, link, network, transport, and application layers, as shown in\n(DNS). We’ll see in Chapter 2 that it is very easy to create and deploy our\nown new application-layer protocols.\nAn application-layer protocol is distributed over multiple end systems,\nwith the application in one end system using the protocol to exchange\npackets of information with the application in another end system. We’ll\nrefer to this packet of information at the application layer as a message.\nTransport Layer\nThe Internet’s transport layer transports application-layer messages between\napplication endpoints. In the Internet, there are two transport protocols,\nTCP and UDP, either of which can transport application-layer messages.\nTCP provides a ­connection-oriented service to its applications. This service\nincludes guaranteed delivery of application-layer messages to the\ndestination and flow control (that is, sender/receiver speed matching). TCP\nalso breaks long messages into shorter ­segments and provides a congestion-\ncontrol mechanism, so that a source throttles its transmission rate when the\nnetwork is congested. The UDP protocol provides a connectionless service\nto its applications. This is a no-frills service that provides no reliability, no\nflow control, and no congestion control. In this book, we’ll refer to a\ntransport-layer packet as a segment.\nNetwork Layer\nThe Internet’s network layer is responsible for moving network-layer\npackets known as datagrams from one host to another. The Internet\ntransport-layer protocol (TCP or UDP) in a source host passes a transport-\nlayer segment and a destination address to the network layer, just as you\nwould give the postal service a letter with a destination address. The\nnetwork layer then provides the service of delivering the segment to the\ntransport layer in the destination host.\nThe Internet’s network layer includes the celebrated IP protocol, which\ndefines the fields in the datagram as well as how the end systems and\nrouters act on these fields. There is only one IP protocol, and all Internet\ncomponents that have a network layer must run the IP protocol. The\nInternet’s network layer also contains routing protocols that determine the\nroutes that datagrams take between sources and destinations. The Internet\nhas many routing protocols. As we saw in Section 1.3, the Internet is a\nnetwork of networks, and within a network, the network administrator can\nrun any routing protocol desired. Although the network layer contains both\nthe IP protocol and numerous routing protocols, it is often simply referred\nto as the IP layer, reflecting the fact that IP is the glue that binds the Internet\nThe Internet’s network layer routes a datagram through a series of routers\nbetween the source and destination. To move a packet from one node (host\nor router) to the next node in the route, the network layer relies on the\nservices of the link layer. In particular, at each node, the network layer\npasses the datagram down to the link layer, which delivers the datagram to\nthe next node along the route. At this next node, the link layer passes the\ndatagram up to the network layer.\nThe services provided by the link layer depend on the specific link-\nlayer protocol that is employed over the link. For example, some link-layer\nprotocols provide reliable delivery, from transmitting node, over one link, to\nreceiving node. Note that this reliable delivery service is different from the\nreliable delivery service of TCP, which provides reliable delivery from one\nend system to another. Examples of link-layer protocols include Ethernet,\nWiFi, and the cable access network’s DOCSIS protocol. As datagrams\ntypically need to traverse several links to travel from source to destination,\na datagram may be handled by different link-layer protocols at different\nlinks along its route. For example, a datagram may be handled by Ethernet\non one link and by PPP on the next link. The network layer will receive a\ndifferent service from each of the different link-layer protocols. In this\nbook, we’ll refer to the link-layer packets as frames.\nPhysical Layer\nWhile the job of the link layer is to move entire frames from one network\nelement to an adjacent network element, the job of the physical layer is to\nmove the individual bits within the frame from one node to the next. The\nprotocols in this layer are again link dependent and further depend on the\nactual transmission medium of the link (for example, twisted-pair copper\nwire, single-mode fiber optics). For example, Ethernet has many physical-\nlayer protocols: one for twisted-pair copper wire, another for coaxial cable,\nanother for fiber, and so on. In each case, a bit is moved across the link in a\ndifferent way.\n1.5.2 Encapsulation\nFigure 1.24 shows the physical path that data takes down a sending end\nsystem’s protocol stack, up and down the protocol stacks of an intervening\nlink-layer switch and router, and then up the protocol stack at the receiving\nend system. As we discuss later in this book, routers and link-layer switches\nare both packet switches. Similar to end systems, routers and link-layer\nswitches organize their networking hardware and software into layers. But\nrouters and link-layer switches do not implement all of the layers in the\nprotocol stack; they typically implement only the bottom layers. As shown\nin Figure 1.24, link-layer switches implement layers 1 and 2; routers\nimplement layers 1 through 3. This means, for example, that Internet routers\nare capable of implementing the IP protocol (a layer 3 protocol), while link-\nlayer switches are not. We’ll see later that while link-layer switches do not\nrecognize IP addresses, they are capable of recognizing layer 2 addresses,\nsuch as Ethernet addresses. Note that hosts implement all five layers; this is\nconsistent with the view that the Internet architecture puts much of its\ncomplexity at the edges of the network.\nFigure 1.24 ♦Hosts, routers, and link-layer switches; each contains\na ­different set of layers, reflecting their differences in\nfunctionality\nFigure 1.24 also illustrates the important concept of encapsulation. At\nthe sending host, an application-layer message (M in Figure 1.24) is\npassed to the transport layer. In the simplest case, the transport layer takes\nthe message and appends additional information (so-called transport-layer\nheader information, H  in Figure 1.24) that will be used by the receiver-side\ntransport layer. The application-layer message and the transport-layer\nheader information together constitute the transport-layer segment. The\ntransport-layer segment thus encapsulates the application-layer message.\nThe added information might include information allowing the receiver-side\ntransport layer to deliver the message up to the appropriate application, and\nerror-detection bits that allow the receiver to determine whether bits in the\nmessage have been changed in route. The transport layer then passes the\nsegment to the network layer, which adds network-layer header information\n(H  in Figure 1.24) such as source and destination end system addresses,\ncreating a network-layer datagram. The datagram is then passed to the\nlink layer, which (of course!) will add its own link-layer header information\nand create a link-layer frame. Thus, we see that at each layer, a packet has\ntwo types of fields: header fields and a payload field. The payload is\ntypically a packet from the layer above.\nA useful analogy here is the sending of an interoffice memo from one\ncorporate branch office to another via the public postal service. Suppose\nAlice, who is in one branch office, wants to send a memo to Bob, who is in\nanother branch office. The memo is analogous to the application-layer\nmessage. Alice puts the memo in an interoffice envelope with Bob’s name\nand department written on the front of the envelope. The interoffice\nenvelope is analogous to a transport-layer segment—it contains header\ninformation (Bob’s name and department number) and it encapsulates the\napplication-layer message (the memo). When the sending branch-office\nmailroom receives the interoffice envelope, it puts the interoffice envelope\ninside yet another envelope, which is suitable for sending through the\npublic postal service. The sending mailroom also writes the postal address\nof the sending and receiving branch offices on the postal envelope. Here,\nthe postal envelope is analogous to the datagram—it encapsulates the\ntransport-layer segment (the interoffice envelope), which encapsulates the\noriginal message (the memo). The postal service delivers the postal\nenvelope to the receiving branch-office mailroom. There, the process of de-\nencapsulation is begun. The mailroom extracts the interoffice memo and\nforwards it to Bob. Finally, Bob opens the envelope and removes the memo.\nThe process of encapsulation can be more complex than that described\nabove. For example, a large message may be divided into multiple\ntransport-layer segments (which might themselves each be divided into\nmultiple network-layer datagrams). At the receiving end, such a segment\nmust then be reconstructed from its constituent datagrams.\n1.6 Networks Under Attack\nThe Internet has become mission critical for many institutions today,\nincluding large and small companies, universities, and government\nagencies. Many individuals also rely on the Internet for many of their\nprofessional, social, and personal activities. Billions of “things,” including\nwearables and home devices, are currently being connected to the Internet.\nBut behind all this utility and excitement, there is a dark side, a side where\n“bad guys” attempt to wreak havoc in our daily lives by damaging our\nInternet-connected computers, violating our privacy, and rendering\ninoperable the Internet services on which we depend.\nThe field of network security is about how the bad guys can attack\ncomputer networks and about how we, soon-to-be experts in computer\nnetworking, can defend networks against those attacks, or better yet, design\nnew architectures that are immune to such attacks in the first place. Given\nthe frequency and variety of existing attacks as well as the threat of new\nand more destructive future attacks, network security has become a central\ntopic in the field of computer networking. One of the features of this\ntextbook is that it brings network security issues to the forefront.\nSince we don’t yet have expertise in computer networking and Internet\nprotocols, we’ll begin here by surveying some of today’s more prevalent\nsecurity-related problems. This will whet our appetite for more substantial\ndiscussions in the upcoming chapters. So we begin here by simply asking,\nwhat can go wrong? How are computer networks vulnerable? What are\nsome of the more prevalent types of attacks today?\nThe Bad Guys Can Put Malware into Your Host Via the Internet\nWe attach devices to the Internet because we want to receive/send data\nfrom/to the Internet. This includes all kinds of good stuff, including\nInstagram posts, Internet search results, streaming music, video conference\ncalls, streaming movies, and so on. But, unfortunately, along with all that\ngood stuff comes malicious stuff—­collectively known as malware—that\ncan also enter and infect our devices. Once malware infects our device it\ncan do all kinds of devious things, including deleting our files and installing\nspyware that collects our private information, such as  social ­security\nnumbers, passwords, and keystrokes, and then sends this (over the Internet,\nof course!) back to the bad guys. Our compromised host may also\nbe enrolled in a network of thousands of similarly compromised devices,\ncollectively known as a botnet, which the bad guys control and leverage for\nspam e-mail distribution or distributed denial-of-service attacks (soon to be\ndiscussed) against targeted hosts.\nMuch of the malware out there today is self-replicating: once it infects\none host, from that host it seeks entry into other hosts over the Internet, and\nfrom ­the newly infected hosts, it seeks entry into yet more hosts. In this\nmanner, self-­replicating malware can spread exponentially fast.\nThe Bad Guys Can Attack Servers and Network Infrastructure\nAnother broad class of security threats are known as denial-of-service\n(DoS) attacks. As the name suggests, a DoS attack renders a network, host,\nor other piece of infrastructure unusable by legitimate users. Web servers, e-\nmail servers, DNS servers (discussed in Chapter 2), and institutional\nnetworks can all be subject to DoS attacks. The site Digital Attack Map\nallows use to visualize the top daily DoS attacks worldwide [DAM 2020].\nMost Internet DoS attacks fall into one of three categories:\nVulnerability attack. This involves sending a few well-crafted messages\nto a vulnerable application or operating system running on a targeted\nhost. If the right sequence of packets is sent to a vulnerable application\nor operating system, the service can stop or, worse, the host can crash.\nBandwidth flooding. The attacker sends a deluge of packets to the\ntargeted host—so many packets that the target’s access link becomes\nclogged, preventing legitimate packets from reaching the server.\nConnection flooding. The attacker establishes a large number of half-\nopen or fully open TCP connections (TCP connections are discussed in\nChapter 3) at the target host. The host can become so bogged down with\nthese bogus connections that it stops accepting legitimate connections.\nLet’s now explore the bandwidth-flooding attack in more detail. Recalling\nour delay and loss analysis discussion in Section 1.4.2, it’s evident that if\nthe server has an access rate of R bps, then the attacker will need to send\ntraffic at a rate of approximately R bps to cause damage. If R is very large, a\nsingle attack source may not be able to generate enough traffic to harm the\nserver. Furthermore, if all the traffic emanates from a single source, an\nupstream router may be able to detect the attack and block all traffic from\nthat source before the traffic gets near the server. In a distributed DoS\n(DDoS) attack, illustrated in Figure 1.25, the attacker controls multiple\nsources and has each source blast traffic at the target. With this approach,\nthe aggregate traffic rate across all the controlled sources needs to be\napproximately R to cripple the ­service. DDoS attacks leveraging botnets\nwith thousands of comprised hosts are a common occurrence today [DAM\n2020]. DDos attacks are much harder to detect and defend against than a\nDoS attack from a single host.\nFigure 1.25 ♦A distributed denial-of-service attack\nWe encourage you to consider the following question as you work your\nway through this book: What can computer network designers do to defend\nagainst DoS attacks? We will see that different defenses are needed for the\nthree types of DoS attacks.\nThe Bad Guys Can Sniff Packets\nMany users today access the Internet via wireless devices, such as WiFi-\nconnected laptops or handheld devices with cellular Internet connections\n(covered in Chapter 7). While ubiquitous Internet access is extremely\nconvenient and enables marvelous new applications for mobile users, it also\ncreates a major security vulnerability—by placing a passive receiver in the\nvicinity of the wireless transmitter, that receiver can obtain a copy of every\npacket that is transmitted! These packets can contain all kinds of sensitive\ninformation, including passwords, social security numbers, trade secrets,\nand private personal messages. A passive receiver that records a copy of\nevery packet that flies by is called a packet sniffer.\nSniffers can be deployed in wired environments as well. In wired\nbroadcast environments, as in many Ethernet LANs, a packet sniffer can\nobtain copies of broadcast packets sent over the LAN. As described in\nSection 1.2, cable access technologies also broadcast packets and are thus\nvulnerable to sniffing. Furthermore, a bad guy who gains access to an\ninstitution’s access router or access link to the Internet may be able to plant\na sniffer that makes a copy of every packet going to/from the organization.\nSniffed packets can then be analyzed offline for sensitive information.\nPacket-sniffing software is freely available at various Web sites and as\ncommercial products. Professors teaching a networking course have been\nknown to assign lab exercises that involve writing a packet-sniffing and\napplication-layer data reconstruction program. Indeed, the Wireshark\n[Wireshark 2020] labs associated with this text (see the introductory\nWireshark lab at the end of this chapter) use exactly such a packet sniffer!\nBecause packet sniffers are passive—that is, they do not inject packets\ninto the channel—they are difficult to detect. So, when we send packets into\na wireless channel, we must accept the possibility that some bad guy may\nbe recording copies of our packets. As you may have guessed, some of the\nbest defenses against packet sniffing involve cryptography. We will\nexamine cryptography as it applies to network security in Chapter 8.\nThe Bad Guys Can Masquerade as Someone You Trust\nIt is surprisingly easy (you will have the knowledge to do so shortly as you\nproceed through this text!) to create a packet with an arbitrary source\naddress, packet content, and destination address and then transmit this\nhand-crafted packet into the Internet, which will dutifully forward the\npacket to its destination. Imagine the unsuspecting receiver (say an Internet\nrouter) who receives such a packet, takes the (false) source address as being\ntruthful, and then performs some command embedded in the packet’s\nmechanisms for end-point authentication in Chapter 8.\nIn closing this section, it’s worth considering how the Internet got to be\nsuch an insecure place in the first place. The answer, in essence, is that the\nInternet was originally designed to be that way, based on the model of “a\ngroup of mutually trusting users attached to a transparent network”\n[Blumenthal 2001]—a model in which (by definition) there is no need for\nsecurity. Many aspects of the original Internet architecture deeply reflect\nthis notion of mutual trust. For example, the ability for one user to send a\npacket to any other user is the default rather than a requested/granted\ncapability, and user identity is taken at declared face value, rather than\nbeing authenticated by default.\nBut today’s Internet certainly does not involve “mutually trusting\nusers.” Nonetheless, today’s users still need to communicate when they\ndon’t necessarily trust each other, may wish to communicate anonymously,\nmay communicate indirectly through third parties (e.g., Web caches, which\nwe’ll study in Chapter 2, or mobility-assisting agents, which we’ll study in\nChapter 7), and may distrust the hardware, software, and even the air\nthrough which they communicate. We now have many security-related\nchallenges before us as we progress through this book: We should seek\ndefenses against sniffing, end-point masquerading, man-in-the-middle\nattacks, DDoS attacks, malware, and more. We should keep in mind that\ncommunication among mutually trusted users is the exception rather than\nthe rule. Welcome to the world of modern computer networking!\n1.7 History of Computer Networking and the\nSections 1.1 through 1.6 presented an overview of the technology of\ncomputer networking and the Internet. You should know enough now to\nimpress your family and friends! However, if you really want to be a big hit\nat the next cocktail party, you should sprinkle your discourse with tidbits\nabout the fascinating history of the Internet [Segaller 1998].\n1.7.1 The Development of Packet Switching: 1961–\nThe field of computer networking and today’s Internet trace their\nbeginnings back to the early 1960s, when the telephone network was the\nworld’s dominant communication network. Recall from Section 1.3 that the\ntelephone network uses circuit switching to transmit information from a\nsender to a receiver—an appropriate choice given that voice is transmitted\nat a constant rate between sender and receiver. Given the increasing\nimportance of computers in the early 1960s and the advent of timeshared\ncomputers, it was perhaps natural to consider how to hook computers\ntogether so that they could be shared among geographically distributed\nusers. The traffic generated by such users was likely to be bursty—intervals\nof activity, such as the sending of a command to a remote computer,\nfollowed by periods of inactivity while waiting for a reply or while\ncontemplating the received response.\nThree research groups around the world, each unaware of the others’\nwork [Leiner 1998], began inventing packet switching as an efficient and\nrobust alternative to circuit switching. The first published work on packet-\nswitching techniques was that of Leonard Kleinrock [Kleinrock 1961;\nKleinrock 1964], then a graduate student at MIT. Using queuing theory,\nKleinrock’s work elegantly demonstrated the effectiveness of the packet-\nswitching approach for bursty traffic sources. In 1964, Paul Baran [Baran\n1964] at the Rand Institute had begun investigating the use of packet\nswitching for secure voice over military networks, and at the National\nPhysical Laboratory in England, Donald Davies and Roger Scantlebury\nwere also developing their ideas on packet switching.\nThe work at MIT, Rand, and the NPL laid the foundations for today’s\nInternet. But the Internet also has a long history of a let’s-build-it-and-\ndemonstrate-it attitude that also dates back to the 1960s. J. C. R. Licklider\n[DEC 1990] and Lawrence Roberts, both colleagues of Kleinrock’s at MIT,\nwent on to lead the computer science program at the Advanced Research\nProjects Agency (ARPA) in the United States. Roberts published an overall\nplan for the ARPAnet [Roberts 1967], the first packet-switched computer\nnetwork and a direct ancestor of today’s public Internet. On Labor Day in\n1969, the first packet switch was installed at UCLA under Kleinrock’s\nsupervision, and three additional packet switches were installed shortly\nthereafter at the Stanford Research Institute (SRI), UC Santa Barbara, and\nthe University of Utah (Figure 1.26). The fledgling precursor to the Internet\nwas four nodes large by the end of 1969. Kleinrock recalls the very first use\nof the network to perform a remote login from UCLA to SRI, crashing the\nsystem [Kleinrock 2004].\nFigure 1.26 ♦An early packet switch\nMark J. Terrill/AP Photo\nBy 1972, ARPAnet had grown to approximately 15 nodes and was given\nits first public demonstration by Robert Kahn. The first host-to-host\nprotocol between ARPAnet end systems, known as the network-control\nprotocol (NCP), was completed [RFC 001]. With an end-to-end protocol\navailable, applications could now be written. Ray Tomlinson wrote the first\ne-mail program in 1972.\n1.7.2 Proprietary Networks and Internetworking:\nThe initial ARPAnet was a single, closed network. In order to communicate\nwith an ARPAnet host, one had to be actually attached to another ARPAnet\nIMP. In the early to mid-1970s, additional stand-alone packet-switching\nnetworks besides ARPAnet came into being: ALOHANet, a microwave\nnetwork linking universities on the Hawaiian islands [Abramson 1970], as\nwell as DARPA’s packet-satellite [RFC 829] and packet-radio networks\n[Kahn 1978]; Telenet, a BBN commercial packet-switching network based\non ARPAnet technology; Cyclades, a French packet-switching network\npioneered by Louis Pouzin [Think 2012]; Time-sharing networks such as\nTymnet and the GE Information Services network, among others, in the late\n1960s and early 1970s [Schwartz 1977]; IBM’s SNA (1969–1974), which\nparalleled the ARPAnet work [Schwartz 1977].\nThe number of networks was growing. With perfect hindsight we can\nsee that the time was ripe for developing an encompassing architecture for\nconnecting networks together. Pioneering work on interconnecting\nnetworks (under the sponsorship of the Defense Advanced Research\nProjects Agency (DARPA)), in essence creating a network of networks, was\ndone by Vinton Cerf and Robert Kahn [Cerf 1974]; the term internetting\nwas coined to describe this work.\nThese architectural principles were embodied in TCP. The early\nversions of TCP, however, were quite different from today’s TCP. The early\nversions of TCP combined a reliable in-sequence delivery of data via end-\nsystem retransmission (still part of today’s TCP) with forwarding functions\n(which today are performed by IP). Early experimentation with TCP,\ncombined with the recognition of the importance of an unreliable, non-\nflow-controlled, end-to-end transport service for applications such as\npacketized voice, led to the separation of IP out of TCP and the\ndevelopment of the UDP protocol. The three key Internet protocols that we\nsee today—TCP, UDP, and IP—were conceptually in place by the end of\nIn addition to the DARPA Internet-related research, many other\nimportant networking activities were underway. In Hawaii, Norman\nAbramson was developing ALOHAnet, a packet-based radio network that\nallowed multiple remote sites on the Hawaiian Islands to communicate with\neach other. The ALOHA protocol [Abramson 1970] was the first multiple-\naccess protocol, allowing geographically distributed users to share a single\nbroadcast communication medium (a radio ­frequency). Metcalfe and Boggs\nbuilt on Abramson’s multiple-access protocol work when they developed\nthe Ethernet protocol [Metcalfe 1976] for wire-based shared broadcast\nnetworks. Interestingly, Metcalfe and Boggs’ Ethernet protocol was\nmotivated by the need to connect multiple PCs, printers, and shared disks\n[Perkins 1994]. Twenty-five years ago, well before the PC revolution and\nthe explosion of networks, Metcalfe and Boggs were laying the foundation\nfor today’s PC LANs.\n1.7.3 A Proliferation of Networks: 1980–1990\nBy the end of the 1970s, approximately two hundred hosts were connected\nto the ARPAnet. By the end of the 1980s the number of hosts connected to\nthe public ­Internet, a confederation of networks looking much like today’s\nInternet, would reach a hundred thousand. The 1980s would be a time of\ntremendous growth.\nMuch of that growth resulted from several distinct efforts to create\ncomputer networks linking universities together. BITNET provided e-mail\nand file transfers among several universities in the Northeast. CSNET\n(computer science network) was formed to link university researchers who\ndid not have access to ARPAnet. In 1986, NSFNET was created to provide\naccess to NSF-sponsored supercomputing centers. Starting with an initial\nbackbone speed of 56 kbps, NSFNET’s backbone would be running at 1.5\nMbps by the end of the decade and would serve as a primary backbone\nlinking regional networks.\nIn the ARPAnet community, many of the final pieces of today’s Internet\narchitecture were falling into place. January 1, 1983 saw the official\ndeployment of TCP/IP as the new standard host protocol for ARPAnet\n(replacing the NCP pro­tocol). The transition [RFC 801] from NCP to\nTCP/IP was a flag day event—all hosts were required to transfer over to\nTCP/IP as of that day. In the late 1980s, important extensions were made to\nTCP to implement host-based congestion control [Jacobson 1988]. The\nDNS, used to map between a human-readable Internet name (for example,\ngaia.cs.umass.edu) and its 32-bit IP address, was also developed [RFC\nParalleling this development of the ARPAnet (which was for the most\npart a US effort), in the early 1980s the French launched the Minitel project,\nan ambitious plan to bring data networking into everyone’s home.\nSponsored by the French government, the Minitel system consisted of a\npublic packet-switched network (based on the X.25 protocol suite), Minitel\nservers, and inexpensive terminals with built-in low-speed modems. The\nMinitel became a huge success in 1984 when the French government gave\naway a free Minitel terminal to each French household that wanted one.\nMinitel sites included free sites—such as a telephone directory site—as\nwell as private sites, which collected a usage-based fee from each user. At\nits peak in the mid 1990s, it offered more than 20,000 services, ranging\nfrom home banking to specialized research databases. The Minitel was in a\nlarge proportion of French homes 10 years before most Americans had ever\nheard of the Internet.\n1.7.4 The Internet Explosion: The 1990s\nThe 1990s were ushered in with a number of events that symbolized the\ncontinued evolution and the soon-to-arrive commercialization of the\nInternet. ARPAnet, the progenitor of the Internet, ceased to exist. In 1991,\nNSFNET lifted its restrictions on the use of NSFNET for commercial\npurposes. NSFNET itself would be decommissioned in 1995, with Internet\nbackbone traffic being carried by commercial Internet Service Providers.\nThe main event of the 1990s was to be the emergence of the World\nWide Web application, which brought the Internet into the homes and\nbusinesses of millions of people worldwide. The Web served as a platform\nfor enabling and deploying hundreds of new applications that we take for\ngranted today, including search (e.g., Google and Bing) Internet commerce\n(e.g., Amazon and eBay) and social networks (e.g., Facebook).\nThe Web was invented at CERN by Tim Berners-Lee between 1989 and\n1991 [Berners-Lee 1989], based on ideas originating in earlier work on\nhypertext from the 1940s by Vannevar Bush [Bush 1945] and since the\n1960s by Ted Nelson [Xanadu 2012]. Berners-Lee and his associates\ndeveloped initial versions of HTML, HTTP, a Web server, and a browser—\nthe four key components of the Web. Around the end of 1993 there were\nabout two hundred Web servers in operation, this collection of servers being\njust a harbinger of what was about to come. At about this time several\nresearchers were developing Web browsers with GUI interfaces, including\nMarc Andreessen, who along with Jim Clark, formed Mosaic\nCommunications, \nCommunications\nCorporation [Cusumano 1998; Quittner 1998]. By 1995, university students\nwere using Netscape browsers to surf the Web on a daily basis. At about this\ntime companies—big and small—began to operate Web servers and transact\ncommerce over the Web. In 1996, Microsoft started to make browsers,\nwhich started the browser war between Netscape and Microsoft, which\nMicrosoft won a few years later [Cusumano 1998].\nThe second half of the 1990s was a period of tremendous growth and\ninnovation for the Internet, with major corporations and thousands of\nstartups creating Internet products and services. By the end of the\nmillennium the Internet was supporting hundreds of popular applications,\nincluding four killer applications:\nE-mail, including attachments and Web-accessible e-mail\nThe Web, including Web browsing and Internet commerce\nInstant messaging, with contact lists\nPeer-to-peer file sharing of MP3s, pioneered by Napster\nInterestingly, the first two killer applications came from the research\ncommunity, whereas the last two were created by a few young\nentrepreneurs.\nThe period from 1995 to 2001 was a roller-coaster ride for the Internet\nin the financial markets. Before they were even profitable, hundreds of\nInternet startups made initial public offerings and started to be traded in a\nstock market. Many companies were valued in the billions of dollars\nwithout having any significant revenue streams. The Internet stocks\ncollapsed in 2000–2001, and many startups shut down. Nevertheless, a\nnumber of companies emerged as big winners in the Internet space,\nincluding Microsoft, Cisco, Yahoo, eBay, Google, and Amazon.\n1.7.5 The New Millennium\nIn the first two decades of the 21st century, perhaps no other technology has\ntransformed society more than the Internet along with Internet-connected\nsmartphones. And innovation in computer networking continues at a rapid\npace. Advances are being made on all fronts, including deployments of\nfaster routers and higher transmission speeds in both access networks and in\nnetwork backbones. But the following developments merit special attention:\nSince the beginning of the millennium, we have been seeing aggressive\ndeployment of broadband Internet access to homes—not only cable\nmodems and DSL but also fiber to the home, and now 5G fixed wireless\nas discussed in Section 1.2. This high-speed Internet access has set the\nstage for a wealth of video applications, including the distribution of\nuser-generated video (for example, YouTube), on-demand streaming of\nmovies and television shows (e.g., Netflix), and multi-person video\nconference (e.g., Skype, Facetime, and Google Hangouts).\nThe increasing ubiquity of high-speed wireless Internet access is not\nonly making it possible to remain constantly connected while on the\nmove, but also enabling new location-specific applications such as Yelp,\nTinder, and Waz. The number of wireless devices connecting to the\nInternet surpassed the number of wired devices in 2011. This high-\nspeed wireless access has set the stage for the rapid emergence of hand-\nheld computers (iPhones, Androids, iPads, and so on), which enjoy\nconstant and untethered access to the Internet.\nOnline social networks—such as Facebook, Instagram, Twitter, and\nWeChat (hugely popular in China)—have created massive people\nnetworks on top of the Internet. Many of these social networks are\nextensively used for messaging as well as photo sharing. Many Internet\nusers today “live” primarily within one or more social networks.\nThrough their APIs, the online social networks create platforms for new\nnetworked applications, including mobile payments and distributed\nAs discussed in Section 1.3.3, online service providers, such as Google\nand Microsoft, have deployed their own extensive private networks,\nwhich not only connect together their globally distributed data centers,\nbut are used to bypass the Internet as much as possible by peering\ndirectly with lower-tier ISPs. As a result, Google provides search results\nand e-mail access almost instantaneously, as if their data centers were\nrunning within one’s own computer.\nMany Internet commerce companies are now running their applications\nin the “cloud”—such as in Amazon’s EC2, in Microsoft’s Azure, or in\nthe Alibaba Cloud. Many companies and universities have also\nmigrated their Internet applications (e.g., e-mail and Web hosting) to the\ncloud. Cloud companies not only provide applications scalable\ncomputing and storage environments, but also provide the applications\nimplicit access to their high-performance private networks.\n1.8 Summary\nIn this chapter, we’ve covered a tremendous amount of material! We’ve\nlooked at the various pieces of hardware and software that make up the\nInternet in particular and computer networks in general. We started at the\nedge of the network, looking at end systems and applications, and at the\ntransport service provided to the applications running on the end systems.\nWe also looked at the link-layer technologies and physical media typically\nfound in the access network. We then dove deeper inside the network, into\nthe network core, identifying packet switching and circuit switching as the\ntwo basic approaches for transporting data through a telecommunication\nnetwork, and we examined the strengths and weaknesses of each approach.\nWe also examined the structure of the global Internet, learning that the\nInternet is a network of networks. We saw that the Internet’s hierarchical\nstructure, consisting of higher- and lower-tier ISPs, has allowed it to scale\nto include thousands of networks.\nIn the second part of this introductory chapter, we examined several\ntopics central to the field of computer networking. We first examined the\ncauses of delay, throughput and packet loss in a packet-switched network.\nWe developed simple quantitative models for transmission, propagation,\nand queuing delays as well as for throughput; we’ll make extensive use of\nthese delay models in the homework problems throughout this book. Next\nwe examined protocol layering and service models, key architectural\nprinciples in networking that we will also refer back to throughout this\nbook. We also surveyed some of the more prevalent security attacks in the\nInternet day. We finished our introduction to networking with a brief history\nof computer networking. The first chapter in itself constitutes a mini-course\nin computer networking.\nSo, we have indeed covered a tremendous amount of ground in this first\nchapter! If you’re a bit overwhelmed, don’t worry. In the following\nchapters, we’ll revisit all of these ideas, covering them in much more detail\n(that’s a promise, not a threat!). At this point, we hope you leave this\nchapter with a still-developing intuition for the pieces that make up a\nnetwork, a still-developing command of the vocabulary of networking\n(don’t be shy about referring back to this chapter), and an ever-growing\ndesire to learn more about networking. That’s the task ahead of us for the\nrest of this book.\nRoad-Mapping This Book\nBefore starting any trip, you should always glance at a road map in order to\nbecome familiar with the major roads and junctures that lie ahead. For the\ntrip we are about to embark on, the ultimate destination is a deep\nunderstanding of the how, what, and why of computer networks. Our road\nmap is the sequence of chapters of this book:\n1. Computer Networks and the Internet\n2. Application Layer\n3. Transport Layer\n4. Network Layer: Data Plane\n5. Network Layer: Control Plane\n6. The Link Layer and LANs\n7. Wireless and Mobile Networks\n8. Security in Computer Networks\nChapters 2 through 6 are the five core chapters of this book. You should\nnotice that these chapters are organized around the top four layers of the\nfive-layer Internet protocol. Further note that our journey will begin at the\ntop of the Internet protocol stack, namely, the application layer, and will\nwork its way downward. The rationale behind this top-down journey is that\nonce we understand the applications, we can understand the network\nservices needed to support these applications. We can then, in turn, examine\nthe various ways in which such services might be implemented by a\nnetwork architecture. Covering applications early thus provides motivation\nfor the remainder of the text.\nThe second half of the book—Chapters 7 and 8—zooms in on two\nenormously important (and somewhat independent) topics in modern\ncomputer networking. In Chapter 7, we examine wireless and mobile\nnetworks, including wireless LANs (including WiFi and Bluetooth),\nCellular networks (including 4G and 5G), and mobility. Chapter 8, which\naddresses security in computer networks, first looks at the underpinnings of\nencryption and network security, and then we examine how the basic theory\nis being applied in a broad range of Internet contexts.\nHomework Problems and Questions\nChapter 1 Review Questions\nSECTION 1.1\nR1. What is the difference between a host and an end system? List several\ndifferent types of end systems. Is a Web server an end system?\nR2. Describe the protocol that might be used by two people having a\ntelephonic conversation to initiate and end the conversation, i.e., the\nway that they talk.\nR3. Why are standards important for protocols?\nSECTION 1.2\nR4. List four access technologies. Classify each one as home access,\nenterprise access, or wide-area wireless access.\nR5. Is HFC transmission rate dedicated or shared among users? Are\ncollisions possible in a downstream HFC channel? Why or why not?\nR6. What access network technologies would be most suitable for\nproviding internet access in rural areas?\nR7. Dial-up modems and DSL both use the telephone line (a twisted-pair\ncopper cable) as their transmission medium. Why then is DSL much\nfaster than dial-up access?\nR8. What are some of the physical media that Ethernet can run over?\nR9. HFC, DSL, and FTTH are all used for residential access. For each of\nthese access technologies, provide a range of ­transmission rates and\ncomment on whether the transmission rate is shared or dedicated.\nR10. Describe the different wireless technologies you use during the day\nand their characteristics. If you have a choice between multiple\ntechnologies, why do you prefer one over another?\nSECTION 1.3\nR11. Suppose there is exactly one packet switch between a sending host\nand a receiving host. The transmission rates between the sending host\nand the switch and between the switch and the receiving host are R\nand R , respectively. Assuming that the switch uses store-and-forward\npacket switching, what is the total end-to-end delay to send a packet\nof length L? (Ignore queuing, propagation delay, and processing\nR12. What advantage does a circuit-switched network have over a packet-\nswitched network? What advantages does TDM have over FDM in a\ncircuit-switched network?\nR13. Suppose users share a 2 Mbps link. Also suppose each user transmits\ncontinuously at 1 Mbps when transmitting, but each user transmits\nonly 20 percent of the time. (See the discussion of statistical\nmultiplexing in Section 1.3.)\na. When circuit switching is used, how many users can be\nb. For the remainder of this problem, suppose packet switching is\nused. Why will there be essentially no queuing delay before the\nlink if two or fewer users transmit at the same time? Why will\nthere be a queuing delay if three users transmit at the same time?\nc. Find the probability that a given user is transmitting.\nd. Suppose now there are three users. Find the probability that at\nany given time, all three users are transmitting simultaneously.\nFind the fraction of time during which the queue grows.\nR14. Why will two ISPs at the same level of the hierarchy often peer with\neach other? How does an IXP earn money?\nR15. Why is a content provider considered a different Internet entity\ntoday? How does a content provider connect to other ISPs? Why?\nSECTION 1.4\nR16. Consider sending a packet from a source host to a destination host\nover a fixed route. List the delay components in the end-to-end delay.\nWhich of these delays are constant and which are variable?\nR17. Visit the Transmission Versus Propagation Delay interactive\nanimation at the Companion Website. Among the rates, propagation\ndelay, and packet sizes available, find a combination for which the\nsender finishes transmitting before the first bit of the packet reaches\nthe receiver. Find another combination for which the first bit of the\npacket reaches the receiver before the sender finishes transmitting.\nR18. A user can directly connect to a server through either long-range\nwireless or a twisted-pair cable for transmitting a 1500-bytes file. The\ntransmission rates of the wireless and wired media are 2 and 100\nMbps, respectively. Assume that the propagation speed in air is 3 ×\n10  m/s, while the speed in the twisted pair is 2 × 10  m/s. If the user\nis located 1 km away from the server, what is the nodal delay when\nusing each of the two technologies?\nR19. Suppose Host A wants to send a large file to Host B. The path from\nHost A to Host B has three links, of rates R  = 500 kbps, R  = 2 Mbps,\nand R  = 1 Mbps.\na. Assuming no other traffic in the network, what is the throughput\nfor the file transfer?\nb. Suppose the file is 4 million bytes. Dividing the file size by the\nthroughput, roughly how long will it take to transfer the file to\nc. Repeat (a) and (b), but now with R  reduced to 100 kbps.\nR20. Suppose end system A wants to send a large file to end system B. At a\nvery high level, describe how end system A creates packets from the\nfile. When one of these packets arrives to a router, what information\nin the packet does the router use to determine the link onto which the\npacket is forwarded? Why is packet switching in the Internet\nanalogous to driving from one city to another and asking directions\nalong the way?\nR21. Visit the Queuing and Loss interactive animation at the Companion\nWebsite. What is the maximum emission rate and the minimum\ntransmission rate? With those rates, what is the traffic intensity? Run\nthe interactive animation with these rates and determine how long it\ntakes for packet loss to occur. Then repeat the experiment a second\ntime and determine again how long it takes for packet loss to occur.\nAre the values different? Why or why not?\nSECTION 1.5\nR22. If two end-systems are connected through multiple routers and the\ndata-link level between them ensures reliable data delivery, is a\ntransport protocol ­offering reliable data delivery between these two\nend-systems necessary? Why?\nR23. What are the five layers in the Internet protocol stack? What are the\nprincipal responsibilities of each of these layers?\nR24. What do encapsulation and de-encapsulation mean? Why are they\nneeded in a layered protocol stack?\nR25. Which layers in the Internet protocol stack does a router process?\nWhich layers does a link-layer switch process? Which layers does a\nhost process?\nSECTION 1.6\nR26. What is self-replicating malware?\nR27. Describe how a botnet can be created and how it can be used for a\nDDoS attack.\nR28. Suppose Alice and Bob are sending packets to each other over a\ncomputer network. Suppose Trudy positions herself in the network so\nthat she can capture all the packets sent by Alice and send whatever\nshe wants to Bob; she can also capture all the packets sent by Bob\nand send whatever she wants to Alice. List some of the malicious\nthings Trudy can do from this position.\nP1. Design and describe an application-level protocol to be used between\nan automatic teller machine and a bank’s centralized computer. Your\nprotocol should allow a user’s card and password to be verified, the\naccount balance (which is maintained at the centralized computer) to\nbe queried, and an account withdrawal to be made (that is, money\ndisbursed to the user). Your protocol entities should be able to handle\nthe all-too-common case in which there is not enough money in the\naccount to cover the withdrawal. Specify your protocol by listing the\nmessages exchanged and the action taken by the automatic teller\nmachine or the bank’s centralized computer on transmission and\nreceipt of messages. Sketch the operation of your protocol for the\ncase of a simple withdrawal with no errors, using a diagram similar to\nthat in Figure 1.2. Explicitly state the assumptions made by your\nprotocol about the underlying end-to-end transport service.\nP2. Equation 1.1 gives a formula for the end-to-end delay of sending one\npacket of length L over N links of transmission rate R. Generalize this\nformula for sending P such packets back-to-back over the N links.\nP3. Consider an application that transmits data at a steady rate (for\nexample, the sender generates an N-bit unit of data every k time units,\nwhere k is small and fixed). Also, when such an application starts, it\nwill continue running for a relatively long period of time. Answer the\nfollowing questions, briefly justifying your answer:\na. Would a packet-switched network or a circuit-switched network\nbe more appropriate for this application? Why?\nb. Suppose that a packet-switched network is used and the only\ntraffic in this network comes from such applications as described\nabove. Furthermore, assume that the sum of the application data\nrates is less than the capacities of each and every link. Is some\nform of congestion control needed? Why?\nP4. Consider the circuit-switched network in Figure 1.13. Recall that\nthere are four circuits on each link. Label the four switches A, B, C,\nand D, going in the clockwise direction.\na. What is the maximum number of simultaneous connections that\ncan be in progress at any one time in this network?\nb. Suppose that all connections are between switches A and C. What\nis the maximum number of simultaneous connections that can be\nin progress?\nc. Suppose we want to make four connections between switches A\nand C, and another four connections between switches B and D.\nCan we route these calls through the four links to accommodate\nall eight ­connections?\nP5. Review the car-caravan analogy in Section 1.4. Assume a propagation\nspeed of 100 km/hour.\na. Suppose the caravan travels 175 km, beginning in front of one\ntollbooth, passing through a second tollbooth, and finishing just\nafter a third tollbooth. What is the end-to-end delay?\nb. Repeat (a), now assuming that there are eight cars in the caravan\ninstead of ten.\nP6. This elementary problem begins to explore propagation delay and\ntransmission delay, two central concepts in data networking. Consider\ntwo hosts, A and B, connected by a single link of rate R bps. Suppose\nthat the two hosts are separated by m meters, and suppose the\npropagation speed along the link is s meters/sec. Host A is to send a\npacket of size L bits to Host B.\nExploring propagation delay and transmission delay\na. Express the propagation delay, d\n, in terms of m and s.\nb. Determine the transmission time of the packet, d\n, in terms of L\nc. Ignoring processing and queuing delays, obtain an expression for\nthe end-to-end delay.\nd. Suppose Host A begins to transmit the packet at time t = 0. At\n, where is the last bit of the packet?\ne. Suppose d\n is greater than d\n. At time t = d\n, where is the\nfirst bit of the packet?\nf. Suppose d\n is less than d\n. At time t = d\n, where is the first\nbit of the packet?\ng. Suppose s = 2.5 · 10 , L = 1500 bytes, and R = 10 Mbps. Find the\ndistance m so that d\nP7. In this problem, we consider sending real-time voice from Host A to\nHost B over a packet-switched network (VoIP). Host A converts\nanalog voice to a digital 64 kbps bit stream on the fly. Host A then\ngroups the bits into 56-byte packets. There is one link between Hosts\nA and B; its transmission rate is 10 Mbps and its propagation delay is\n10 msec. As soon as Host A gathers a packet, it sends it to Host B. As\nsoon as Host B receives an entire packet, it converts the packet’s bits\nto an analog signal. How much time elapses from the time a bit is\ncreated (from the original analog signal at Host A) until the bit is\ndecoded (as part of the analog signal at Host B)?\nP8. Suppose users share a 10 Mbps link. Also suppose each user requires\n200 kbps when transmitting, but each user transmits only 10 percent\nof the time. (See the discussion of packet switching versus circuit\nswitching in Section 1.3.)\na. When circuit switching is used, how many users can be\nb. For the remainder of this problem, suppose packet switching is\nused. Find the probability that a given user is transmitting.\nc. Suppose there are 120 users. Find the probability that at any\ngiven time, exactly n users are transmitting simultaneously.\n(Hint: Use the binomial distribution.)\nd. Find the probability that there are 51 or more users transmitting ­-\nsimultaneously.\nP9. Consider the discussion in Section 1.3 of packet switching versus\ncircuit switching in which an example is provided with a 1 Mbps link.\nUsers are generating data at a rate of 100 kbps when busy, but are\nbusy generating data only with probability p = 0.1. Suppose that the 1\nMbps link is replaced by a 1 Gbps link.\na. What is N, the maximum number of users that can be supported\nsimultaneously under circuit switching?\nb. Now consider packet switching and a user population of M users.\nGive a formula (in terms of p, M, N) for the probability that more\nthan N users are sending data.\nP10. Consider the network illustrated in Figure 1.16. Assume the two hosts\non the left of the figure start transmitting packets of 1500 bytes at the\nsame time towards Router B. Suppose the link rates between the hosts\nand Router A is 4-Mbps. One link has a 6-ms propagation delay and\nthe other has a 2-ms propagation delay. Will queuing delay occur at\nP11. Consider the scenario in Problem P10 again, but now assume the links\nbetween the hosts and Router A have different rates R  and R  byte/s\nin addition to different propagation delays d  and d . Assume the\npacket lengths for the two hosts are of L bytes. For what values of the\npropagation delay will no queuing delay occur at Router A?\nP12. Consider a client and a server connected through one router. Assume\nthe router can start transmitting an incoming packet after receiving its\nfirst h bytes instead of the whole packet. Suppose that the link rates\nare R byte/s and that the client transmits one packet with a size of L\nbytes to the server. What is the end-to-end delay? Assume the\npropagation, processing, and queuing delays are negligible.\nGeneralize the previous result to a scenario where the client and the\nserver are interconnected by N routers.\nP13. (a) Suppose N packets arrive simultaneously to a link at which no\npackets are currently being transmitted or queued. Each packet is\nof length L and the link has transmission rate R. What is the\naverage queuing delay for the N packets?\n(b) Now suppose that N such packets arrive to the link every LN/R\nseconds. What is the average queuing delay of a packet?\nP14. Consider the queuing delay in a router buffer. Let I denote traffic\nintensity; that is, I = La/R. Suppose that the queuing delay takes the\nform IL/R (1 − I) for I < 1.\na. Provide a formula for the total delay, that is, the queuing delay\nplus the transmission delay.\nb. Plot the total delay as a function of L /R.\nP15. Let a denote the rate of packets arriving at a link in packets/sec, and\nlet µ denote the link’s transmission rate in packets/sec. Based on the\nformula for the total delay (i.e., the queuing delay plus the\ntransmission delay) derived in the previous problem, derive a formula\nfor the total delay in terms of a and µ.\nP16. Consider a router buffer preceding an outbound link. In this problem,\nyou will use Little’s formula, a famous formula from queuing theory.\nLet N denote the average number of packets in the buffer plus the\npacket being transmitted. Let a denote the rate of packets arriving at\nthe link. Let d denote the average total delay (i.e., the queuing delay\nplus the transmission delay) experienced by a packet. Little’s formula\nis N = a · d. Suppose that on average, the buffer contains 100 packets,\nand the average packet queuing delay is 20 msec. The link’s\ntransmission rate is 100 packets/sec. Using Little’s formula, what is\nthe average packet arrival rate, assuming there is no packet loss?\nP17. Consider the network illustrated in Figure 1.12. Would Equation 1.2\nhold in such a scenario? If so, under which conditions? If not, why?\n(Assume N is the number of links between a source and a destination\nin the figure.)\nP18. Perform a Traceroute between source and destination on the same\ncontinent at three different hours of the day.\nUsing Traceroute to discover network paths and measure network delay\na. Find the average and standard deviation of the round-trip delays\nat each of the three hours.\nb. Find the number of routers in the path at each of the three hours.\nDid the paths change during any of the hours?\nc. Try to identify the number of ISP networks that the Traceroute\npackets pass through from source to destination. Routers with\nsimilar names and/or similar IP addresses should be considered\nas part of the same ISP. In your experiments, do the largest delays\noccur at the peering interfaces between adjacent ISPs?\nd. Repeat the above for a source and destination on different\ncontinents. Compare the intra-continent and inter-continent\nP19. Metcalfe’s law states the value of a computer network is proportional\nto the square of the number of connected users of the system. Let n\ndenote the number of users in a computer network. Assuming each\nuser sends one message to each of the other users, how many\nmessages will be sent? Does your answer support Metcalfe’s law?\nP20. Consider the throughput example corresponding to Figure 1.20(b).\nNow suppose that there are M client-server pairs rather than 10.\nDenote R , R , and R for the rates of the server links, client links, and\nnetwork link. Assume all other links have abundant capacity and that\nthere is no other traffic in the network besides the traffic generated by\nthe M client-server pairs. Derive a general expression for throughput\nin terms of R , R , R, and M.\nP21. Assume a client and a server can connect through either network (a)\nor (b) in Figure 1.19. Assume that R  = (R  + R ) / i, for i = 1, 2, ..., N.\nIn what case will network (a) have a higher throughput than network\nP22. Consider Figure 1.19(b). Suppose that each link between the server\nand the client has a packet loss probability p, and the packet loss\nprobabilities for these links are independent. What is the probability\nthat a packet (sent by the server) is successfully received by the\nreceiver? If a packet is lost in the path from the server to the client,\nthen the server will re-transmit the packet. On average, how many\ntimes will the server re-transmit the packet in order for the client to\nsuccessfully receive the packet?\nP23. Consider Figure 1.19(a). Assume that we know the bottleneck link\nalong the path from the server to the client is the first link with rate R\nbits/sec. Suppose we send a pair of packets back to back from the\nserver to the client, and there is no other traffic on this path. Assume\neach packet of size L bits, and both links have the same propagation\na. What is the packet inter-arrival time at the destination? That is,\nhow much time elapses from when the last bit of the first packet\narrives until the last bit of the second packet arrives?\nb. Now assume that the second link is the bottleneck link (i.e., R  <\nR ). Is it possible that the second packet queues at the input queue\nof the second link? Explain. Now suppose that the server sends\nthe second packet T seconds after sending the first packet. How\nlarge must T be to ensure no queuing before the second link?\nP24. Consider a user who needs to transmit 1.5 gigabytes of data to a\nserver. The user lives in a village where only dial-up access is\navailable. As an alternative, a bus collects data from users in rural\nareas and transfer them to the Internet through a 1 Gbps link once it\ngets back to the city. The bus visits the village once a day and stops in\nfront of the user’s house just long enough to receive the data. The bus\nhas a 100 Mbps WiFi connection. Suppose the average speed of the\nbus is 60 km/h and that the distance between the village and the city\nis 150 km. What is the fastest way the user can transfer the data to the\nP25. Suppose two hosts, A and B, are separated by 20,000 kilometers and\nare connected by a direct link of R = 5 Mbps. Suppose the\npropagation speed over the link is 2.5 · 10  meters/sec.\na. Calculate the bandwidth-delay product, R · d\nb. Consider sending a file of 800,000 bits from Host A to Host B.\nSuppose the file is sent continuously as one large message. What\nis the maximum number of bits that will be in the link at any\ngiven time?\nc. Provide an interpretation of the bandwidth-delay product.\nd. What is the width (in meters) of a bit in the link? Is it longer than\na ­football field?\ne. Derive a general expression for the width of a bit in terms of the\npropagation speed s, the transmission rate R, and the length of the\nP26. Consider problem P25 but now with a link of R = 1 Gbps.\na. Calculate the bandwidth-delay product, R · d\nb. Consider sending a file of 800,000 bits from Host A to Host B.\nSuppose the file is sent continuously as one big message. What is\nthe maximum number of bits that will be in the link at any given\nc. What is the width (in meters) of a bit in the link?\nP27. Consider the scenario illustrated in Figure 1.19(a). Assume R  is 20\nMbps, R  is 10 Mbps, and the server is continuously sending traffic to\nthe client. Also assume the router between the server and the client\ncan buffer at most four messages. After how many messages sent by\nthe server will packet loss starts occurring at the router?\nP28. Generalize the result obtained in Problem P27 for the case where the\nrouter can buffer m messages.\nP29. Suppose there is a 10 Mbps microwave link between a geostationary\nsatellite and its base station on Earth. Every minute the satellite takes\na digital photo and sends it to the base station. Assume a propagation\nspeed of 2.4 · 10  meters/sec.\na. What is the propagation delay of the link?\nb. What is the bandwidth-delay product, R · d\nc. Let x denote the size of the photo. What is the minimum value of\nx for the microwave link to be continuously transmitting?\nP30. Consider the airline travel analogy in our discussion of layering in\nSection 1.5, and the addition of headers to protocol data units as they\nflow down the protocol stack. Is there an equivalent notion of header\ninformation that is added to passengers and baggage as they move\ndown the airline protocol stack?\nP31. In modern packet-switched networks, including the Internet, the\nsource host segments long, application-layer messages (for example,\nan image or a music file) into smaller packets and sends the packets\ninto the network. The receiver then reassembles the packets back into\nthe original message. We refer to this process as message\nsegmentation. Figure 1.27 illustrates the end-to-end transport of a\nmessage with and without message segmentation. Consider a message\nthat is 10  bits long that is to be sent from source to destination in\nFigure 1.27. Suppose each link in the figure is 5 Mbps. Ignore\npropagation, queuing, and processing delays.\nFigure 1.27 ♦End-to-end message transport: (a) without\nmessage segmentation; (b) with message\nsegmentation\na. Consider sending the message from source to destination without\nmessage segmentation. How long does it take to move the\nmessage from the source host to the first packet switch? Keeping\nin mind that each switch uses store-and-forward packet\nswitching, what is the total time to move the message from\nsource host to destination host?\nb. Now suppose that the message is segmented into 100 packets,\nwith each packet being 10,000 bits long. How long does it take to\nmove the first packet from source host to the first switch? When\nthe first packet is being sent from the first switch to the second\nswitch, the second packet is being sent from the source host to\nthe first switch. At what time will the second packet be fully\nreceived at the first switch?\nc. How long does it take to move the file from source host to\ndestination host when message segmentation is used? Compare\nthis result with your answer in part (a) and comment.\nd. In addition to reducing delay, what are reasons to use message ­-\nsegmentation?\ne. Discuss the drawbacks of message segmentation.\nP32. Consider Problem P31 and assume that the propagation delay is 250\nms. Recalculate the total time needed to transfer the source data with\nand without segmentation. Is segmentation more beneficial or less if\nthere is propagation delay?\nP33. Consider sending a large file of F bits from Host A to Host B. There\nare three links (and two switches) between A and B, and the links are\nuncongested (that is, no queuing delays). Host A segments the file\ninto segments of S bits each and adds 80 bits of header to each\nsegment, forming packets of L = 80 + S bits. Each link has a\ntransmission rate of R bps. Find the value of S that minimizes the\ndelay of moving the file from Host A to Host B. Disregard\npropagation delay.\nP34. Early versions of TCP combined functions for both forwarding and\nreliable delivery. How are these TCP variants located in the ISO/OSI\nprotocol stack? Why were forwarding functions later separated from\nTCP? What were the consequences?\nWireshark Lab\n“Tell me and I forget. Show me and I remember. Involve me and I\nunderstand.”\nChinese proverb\nOne’s understanding of network protocols can often be greatly deepened by\nseeing them in action and by playing around with them—observing the\nsequence of messages exchanged between two protocol entities, delving\ninto the details of protocol operation, causing protocols to perform certain\nactions, and observing these actions and their consequences. This can be\ndone in simulated scenarios or in a real network environment such as the\nInternet. The interactive animations at the textbook Web site take the first\napproach. In the Wireshark labs, we’ll take the latter approach. You’ll run\nnetwork applications in various scenarios using a computer on your desk, at\nhome, or in a lab. You’ll observe the network protocols in your computer,\ninteracting and exchanging messages with protocol entities executing\nelsewhere in the Internet. Thus, you and your computer will be an integral\npart of these live labs. You’ll observe—and you’ll learn—by doing.\nThe basic tool for observing the messages exchanged between\nexecuting protocol entities is called a packet sniffer. As the name suggests,\na packet sniffer passively copies (sniffs) messages being sent from and\nChapter 1, and as shown earlier in Figure 1.24, network-core devices do not\nfunction at the application layer but instead function at lower layers—\nspecifically at the network layer and below. This basic design—namely,\nconfining application software to the end systems—as shown in Figure 2.1,\nhas facilitated the rapid development and deployment of a vast array of\nnetwork applications.\n2.1.1 Network Application Architectures\nBefore diving into software coding, you should have a broad architectural\nplan for your application. Keep in mind that an application’s architecture is\ndistinctly different from the network architecture (e.g., the five-layer\nInternet architecture discussed in Chapter 1). From the application\ndeveloper’s perspective, the network architecture is fixed and provides a\nspecific set of services to applications. The application architecture, on\nthe other hand, is designed by the application developer and dictates how\nthe application is structured over the various end systems. In choosing the\napplication architecture, an application developer will likely draw on one of\nthe two predominant architectural paradigms used in modern network\napplications: the client-server architecture or the peer-to-peer (P2P)\narchitecture.\nIn a client-server architecture, there is an always-on host, called the\nserver, which services requests from many other hosts, called clients. A\nclassic example is the Web application for which an always-on Web server\nservices requests from browsers running on client hosts. When a Web server\nreceives a request for an object from a client host, it responds by sending\nthe requested object to the client host. Note that with the client-server\narchitecture, clients do not directly communicate with each other; for\nexample, in the Web application, two browsers do not directly\ncommunicate. Another characteristic of the client-server architecture is that\nthe server has a fixed, well-known address, called an IP address (which\nwe’ll discuss soon). Because the server has a fixed, well-known address,\nand because the server is always on, a client can always contact the server\nby sending a packet to the server’s IP address. Some of the better-known\napplications with a client-server architecture include the Web, FTP, Telnet,\nand e-mail. The client-server architecture is shown in Figure 2.2(a).\nFigure 2.2 ♦(a) Client-server architecture; (b) P2P architecture\nOften in a client-server application, a single-server host is incapable of\nkeeping up with all the requests from clients. For example, a popular social-\nnetworking site can quickly become overwhelmed if it has only one server\nhandling all of its requests. For this reason, a data center, housing a large\nnumber of hosts, is often used to create a powerful virtual server. The most\npopular Internet services—such as search engines (e.g., Google, Bing,\nBaidu), Internet commerce (e.g., Amazon, eBay, Alibaba), Web-based e-\nmail (e.g., Gmail and Yahoo Mail), social media (e.g., Facebook, Instagram,\nTwitter, and WeChat)—run in one or more data centers. As discussed in\nSection 1.3.3, Google has 19 data centers distributed around the world,\nwhich collectively handle search, YouTube, Gmail, and other services. A\ndata center can have hundreds of thousands of servers, which must be\npowered and maintained. Additionally, the service providers must pay\nrecurring interconnection and bandwidth costs for sending data from their\ndata centers.\nIn a P2P architecture, there is minimal (or no) reliance on dedicated\nservers in data centers. Instead the application exploits direct\ncommunication between pairs of intermittently connected hosts, called\npeers. The peers are not owned by the service provider, but are instead\ndesktops and laptops controlled by users, with most of the peers residing in\nhomes, universities, and offices. Because the peers communicate without\npassing through a dedicated server, the architecture is called peer-to-peer.\nAn example of a popular P2P application is the file-sharing application\nBitTorrent.\nOne of the most compelling features of P2P architectures is their self-\nscalability. For example, in a P2P file-sharing application, although each\npeer ­generates workload by requesting files, each peer also adds service\ncapacity to the system by distributing files to other peers. P2P architectures\nare also cost effective, since they normally don’t require significant server\ninfrastructure and server bandwidth (in contrast with clients-server designs\nwith datacenters). However, P2P applications face challenges of security,\nperformance, and reliability due to their highly ­decentralized structure.\n2.1.2 Processes Communicating\nBefore building your network application, you also need a basic\nunderstanding of how the programs, running in multiple end systems,\ncommunicate with each other. In the jargon of operating systems, it is not\nactually programs but processes that communicate. A process can be\nthought of as a program that is running within an end system. When\nprocesses are running on the same end system, they can communicate with\neach other with interprocess communication, using rules that are governed\nby the end system’s operating system. But in this book, we are not\nparticularly interested in how processes in the same host communicate, but\ninstead in how processes running on different hosts (with potentially\ndifferent operating systems) communicate.\nProcesses on two different end systems communicate with each other\nby exchanging messages across the computer network. A sending process\ncreates and sends messages into the network; a receiving process receives\nthese messages and possibly responds by sending messages back. Figure\n2.1 illustrates that processes communicating with each other reside in the\napplication layer of the five-layer protocol stack.\nClient and Server Processes\nA network application consists of pairs of processes that send messages to\neach other over a network. For example, in the Web application a client\nbrowser process exchanges messages with a Web server process. In a P2P\nfile-sharing system, a file is transferred from a process in one peer to a\nprocess in another peer. For each pair of communicating processes, we\ntypically label one of the two processes as the client and the other process\nas the server. With the Web, a browser is a client process and a Web server\nis a server process. With P2P file sharing, the peer that is downloading the\nfile is labeled as the client, and the peer that is uploading the file is labeled\nas the server.\nYou may have observed that in some applications, such as in P2P file\nsharing, a process can be both a client and a server. Indeed, a process in a\nP2P file-sharing system can both upload and download files. Nevertheless,\nin the context of any given communication session between a pair of\nprocesses, we can still label one process as the client and the other process\nas the server. We define the client and server processes as follows:\nIn the context of a communication session between a pair of processes,\nthe process that initiates the communication (that is, initially contacts\nthe other process at the beginning of the session) is labeled as the\nclient. The process that waits to be contacted to begin the session is the\nIn the Web, a browser process initializes contact with a Web server\nprocess; hence the browser process is the client and the Web server process\nis the server. In P2P file sharing, when Peer A asks Peer B to send a specific\nfile, Peer A is the client and Peer B is the server in the context of this\nspecific communication session. When there’s no confusion, we’ll\nsometimes also use the terminology “client side and server side of an\napplication.” At the end of this chapter, we’ll step through simple code for\nboth the client and server sides of network applications.\nThe Interface Between the Process and the Computer Network\nAs noted above, most applications consist of pairs of communicating\nprocesses, with the two processes in each pair sending messages to each\nother. Any message sent from one process to another must go through the\nunderlying network. A process sends messages into, and receives messages\nfrom, the network through a software interface called a socket. Let’s\nconsider an analogy to help us understand processes and sockets. A process\nis analogous to a house and its socket is analogous to its door. When a\nprocess wants to send a message to another process on another host, it\nshoves the message out its door (socket). This sending process assumes that\nthere is a transportation infrastructure on the other side of its door that will\ntransport the message to the door of the destination process. Once the\nmessage arrives at the destination host, the message passes through the\nreceiving process’s door (socket), and the receiving process then acts on the\nFigure 2.3 illustrates socket communication between two processes that\ncommunicate over the Internet. (Figure 2.3 assumes that the underlying\ntransport protocol used by the processes is the Internet’s TCP protocol.) As\nshown in this figure, a socket is the interface between the application layer\nand the transport layer within a host. It is also referred to as the Application\nProgramming Interface (API) between the application and the network,\nsince the socket is the programming interface with which network\napplications are built. The application developer has control of everything\non the application-layer side of the socket but has little control of the\ntransport-layer side of the socket. The only control that the application\ndeveloper has on the transport-layer side is (1) the choice of transport\nprotocol and (2) perhaps the ability to fix a few transport-layer parameters\nsuch as maximum buffer and maximum segment sizes (to be covered in\nChapter 3). Once the application developer chooses a transport protocol (if\na choice is available), the application is built using the transport-layer\nservices provided by that protocol. We’ll explore sockets in some detail in\nSection 2.7.\nFigure 2.3 ♦Application processes, sockets, and underlying\ntransport protocol\nAddressing Processes\nIn order to send postal mail to a particular destination, the destination needs\nto have an address. Similarly, in order for a process running on one host to\nsend packets to a process running on another host, the receiving process\nneeds to have an address. To identify the receiving process, two pieces of\ninformation need to be specified: (1)  the address of the host and (2) an\nidentifier that specifies the receiving process in the destination host.\nIn the Internet, the host is identified by its IP address. We’ll discuss IP\naddresses in great detail in Chapter 4. For now, all we need to know is that\nan IP address is a 32-bit quantity that we can think of as uniquely\nidentifying the host. In addition to knowing the address of the host to which\na message is destined, the sending process must also identify the receiving\nprocess (more specifically, the receiving socket) running in the host. This\ninformation is needed because in general a host could be running many\nnetwork applications. A destination port number serves this purpose.\nPopular applications have been assigned specific port numbers. For\nexample, a Web server is identified by port number 80. A mail server\nprocess (using the SMTP protocol) is identified by port number 25. A list of\nwell-known port numbers for all Internet standard protocols can be found at\nwww.iana.org. We’ll examine port numbers in detail in Chapter 3.\n2.1.3 Transport Services Available to Applications\nRecall that a socket is the interface between the application process and the\ntransport-layer protocol. The application at the sending side pushes\nmessages through the socket. At the other side of the socket, the transport-\nlayer protocol has the responsibility of getting the messages to the socket of\nthe receiving process.\nMany networks, including the Internet, provide more than one\ntransport-layer protocol. When you develop an application, you must\nchoose one of the available transport-layer protocols. How do you make\nthis choice? Most likely, you would study the services provided by the\navailable transport-layer protocols, and then pick the protocol with the\nservices that best match your application’s needs. The situation is similar to\nchoosing either train or airplane transport for travel between two cities. You\nhave to choose one or the other, and each transportation mode offers\ndifferent services. (For example, the train offers downtown pickup and\ndrop-off, whereas the plane offers shorter travel time.)\nWhat are the services that a transport-layer protocol can offer to\napplications invoking it? We can broadly classify the possible services\nalong four dimensions: reliable data transfer, throughput, timing, and\nReliable Data Transfer\nAs discussed in Chapter 1, packets can get lost within a computer network.\nFor example, a packet can overflow a buffer in a router, or can be discarded\nby a host or router after having some of its bits corrupted. For many\napplications—such as electronic mail, file transfer, remote host access, Web\ndocument transfers, and financial applications—data loss can have\ndevastating consequences (in the latter case, for either the bank or the\ncustomer!). Thus, to support these applications, something has to be done to\nguarantee that the data sent by one end of the application is delivered\ncorrectly and completely to the other end of the application. If a protocol\nprovides such a guaranteed data delivery service, it is said to provide\nreliable data transfer. One important service that a transport-layer\nprotocol can potentially provide to an application is process-to-process\nreliable data transfer. When a transport protocol provides this service, the\nsending process can just pass its data into the socket and know with\ncomplete confidence that the data will arrive without errors at the receiving\nWhen a transport-layer protocol doesn’t provide reliable data transfer,\nsome of the data sent by the sending process may never arrive at the\nreceiving process. This may be acceptable for loss-tolerant applications,\nmost notably multimedia applications such as conversational audio/video\nthat can tolerate some amount of data loss. In these multimedia\napplications, lost data might result in a small glitch in the audio/video—not\na crucial impairment.\nIn Chapter 1, we introduced the concept of available throughput, which, in\nthe context of a communication session between two processes along a\nnetwork path, is the rate at which the sending process can deliver bits to the\nreceiving process. Because other sessions will be sharing the bandwidth\nalong the network path, and because these other sessions will be coming\nand going, the available throughput can fluctuate with time. These\nobservations lead to another natural service that a transport-layer protocol\ncould provide, namely, guaranteed available throughput at some specified\nrate. With such a service, the application could request a guaranteed\nthroughput of r bits/sec, and the transport protocol would then ensure that\nthe available throughput is always at least r bits/sec. Such a guaranteed\nthroughput service would appeal to many applications. For example, if an\nInternet telephony application encodes voice at 32 kbps, it needs to send\ndata into the network and have data delivered to the receiving application at\nthis rate. If the transport protocol cannot provide this throughput, the\napplication would need to encode at a lower rate (and receive enough\nthroughput to sustain this lower coding rate) or may have to give up, since\nreceiving, say, half of the needed throughput is of little or no use to this\nInternet telephony application. Applications that have throughput\nrequirements are said to be bandwidth-sensitive applications. Many\ncurrent multimedia applications are bandwidth sensitive, although some\nmultimedia applications may use adaptive coding techniques to encode\ndigitized voice or video at a rate that matches the currently available\nthroughput.\nWhile bandwidth-sensitive applications have specific throughput\nrequirements, elastic applications can make use of as much, or as little,\nthroughput as happens to be available. Electronic mail, file transfer, and\nWeb transfers are all elastic applications. Of course, the more throughput,\nthe better. There’s an adage that says that one cannot be too rich, too thin, or\nhave too much throughput!\nA transport-layer protocol can also provide timing guarantees. As with\nthroughput guarantees, timing guarantees can come in many shapes and\nforms. An example guarantee might be that every bit that the sender pumps\ninto the socket arrives at the receiver’s socket no more than 100 msec later.\nSuch a service would be appealing to interactive real-time applications,\nsuch as Internet telephony, virtual environments, teleconferencing, and\nmultiplayer games, all of which require tight timing constraints on data\ndelivery in order to be effective, see [Gauthier 1999; Ramjee 1994]. Long\ndelays in Internet telephony, for example, tend to result in unnatural pauses\nin the conversation; in a multiplayer game or virtual interactive\nenvironment, a long delay between taking an action and seeing the response\nfrom the environment (for example, from another player at the end of an\nend-to-end connection) makes the application feel less realistic. For non-\nreal-time applications, lower delay is always preferable to higher delay, but\nno tight constraint is placed on the end-to-end delays.\nFinally, a transport protocol can provide an application with one or more\nsecurity services. For example, in the sending host, a transport protocol can\nencrypt all data transmitted by the sending process, and in the receiving\nhost, the transport-layer protocol can decrypt the data before delivering the\ndata to the receiving process. Such a service would provide confidentiality\nbetween the two processes, even if the data is somehow observed between\nsending and receiving processes. A transport protocol can also provide other\nsecurity services in addition to confidentiality, including data integrity and\nend-point authentication, topics that we’ll cover in detail in Chapter 8.\n2.1.4 Transport Services Provided by the Internet\nUp until this point, we have been considering transport services that a\ncomputer network could provide in general. Let’s now get more specific\nand examine the type of transport services provided by the Internet. The\nInternet (and, more generally, TCP/IP networks) makes two transport\nprotocols available to applications, UDP and TCP. When you (as an\napplication developer) create a new network application for the Internet,\none of the first decisions you have to make is whether to use UDP or TCP.\nEach of these protocols offers a different set of services to the invoking\napplications. Figure 2.4 shows the service requirements for some selected\napplications.\nFigure 2.4 ♦Requirements of selected network applications\nTCP Services\nThe TCP service model includes a connection-oriented service and a\nreliable data transfer service. When an application invokes TCP as its\ntransport protocol, the application receives both of these services from TCP.\nConnection-oriented service. TCP has the client and server exchange\ntransport-layer control information with each other before the\napplication-level messages begin to flow. This so-called handshaking\nprocedure alerts the client and server, allowing them to prepare for an\nonslaught of packets. After the handshaking phase, a TCP connection\nis said to exist between the sockets of the two processes. The\nconnection is a full-duplex connection in that the two processes can\nsend messages to each other over the connection at the same time.\nWhen the application finishes sending messages, it must tear down the\nconnection. In Chapter 3, we’ll discuss connection-oriented service in\ndetail and examine how it is implemented.\nReliable data transfer service. The communicating processes can rely\non TCP to deliver all data sent without error and in the proper order.\nWhen one side of the application passes a stream of bytes into a socket,\nit can count on TCP to deliver the same stream of bytes to the receiving\nsocket, with no missing or duplicate bytes.\nTCP also includes a congestion-control mechanism, a service for the ­-\ngeneral welfare of the Internet rather than for the direct benefit of the\ncommunicating processes. The TCP congestion-control mechanism throttles\na sending process (client or server) when the network is congested between\nsender and receiver. As we will see in Chapter 3, TCP congestion control\nalso attempts to limit each TCP connection to its fair share of network\nUDP Services\nUDP is a no-frills, lightweight transport protocol, providing minimal\nservices. UDP is connectionless, so there is no handshaking before the two\nprocesses start to communicate. UDP provides an unreliable data transfer\nservice—that is, when a process sends a message into a UDP socket, UDP\nprovides no guarantee that the message will ever reach the receiving\nprocess. Furthermore, messages that do arrive at the receiving process may\narrive out of order.\nNeither TCP nor UDP provides any encryption—the data that the sending process\npasses into its socket is the same data that travels over the network to the destination\nprocess. So, for example, if the sending process sends a password in cleartext (i.e.,\nunencrypted) into its socket, the cleartext password will travel over all the links\nbetween sender and receiver, potentially getting sniffed and discovered at any of the\nintervening links. Because privacy and other security issues have become critical for\nmany applications, the Internet community has developed an enhancement for TCP,\ncalled Transport Layer Security (TLS) [RFC 5246]. TCP-enhanced-with-TLS not only\ndoes everything that traditional TCP does but also provides critical process-to-process\nsecurity services, including encryption, data integrity, and end-point authentication. We\nemphasize that TLS is not a third Internet transport protocol, on the same level as TCP\nand UDP, but instead is an enhancement of TCP, with the enhancements being\nimplemented in the application layer. In particular, if an application wants to use the\nservices of TLS, it needs to include TLS code (existing, highly optimized libraries and\nclasses) in both the client and server sides of the application. TLS has its own socket\nAPI that is similar to the traditional TCP socket API. When an application uses TLS, the\nsending process passes cleartext data to the TLS socket; TLS in the sending host then\nencrypts the data and passes the encrypted data to the TCP socket. The encrypted\ndata travels over the Internet to the TCP socket in the receiving process. The receiving\nsocket passes the encrypted data to TLS, which decrypts the data. Finally, TLS passes\nthe cleartext data through its TLS socket to the receiving process. We’ll cover TLS in\nsome detail in Chapter 8.\nUDP does not include a congestion-control mechanism, so the sending\nside of UDP can pump data into the layer below (the network layer) at any\nrate it pleases. (Note, however, that the actual end-to-end throughput may\nbe less than this rate due to the limited transmission capacity of intervening\nlinks or due to congestion).\nServices Not Provided by Internet Transport Protocols\nWe have organized transport protocol services along four dimensions:\nreliable data transfer, throughput, timing, and security. Which of these\nservices are provided by TCP and UDP? We have already noted that TCP\nprovides reliable end-to-end data transfer. And we also know that TCP can\nbe easily enhanced at the application layer with TLS to provide security\nservices. But in our brief description of TCP and UDP, conspicuously\nmissing was any mention of throughput or timing guarantees—services not\nprovided by today’s Internet transport protocols. Does this mean that time-\nsensitive applications such as Internet telephony cannot run in today’s\nInternet? The answer is clearly no—the Internet has been hosting time-\nsensitive applications for many years. These applications often work fairly\nwell because they have been designed to cope, to the greatest extent\npossible, with this lack of guarantee. Nevertheless, clever design has its\nlimitations when delay is excessive, or the end-to-end throughput is limited.\nIn summary, today’s Internet can often provide satisfactory service to time-\nsensitive applications, but it cannot provide any timing or throughput\nguarantees.\nFigure 2.5 indicates the transport protocols used by some popular\nInternet applications. We see that e-mail, remote terminal access, the Web,\nand file transfer all use TCP. These applications have chosen TCP primarily\nbecause TCP provides reliable data transfer, guaranteeing that all data will\neventually get to its destination. Because Internet telephony applications\n(such as Skype) can often tolerate some loss but require a minimal rate to\nbe effective, developers of Internet telephony applications usually prefer to\nrun their applications over UDP, thereby circumventing TCP’s congestion\ncontrol mechanism and packet overheads. But because many firewalls are\nconfigured to block (most types of) UDP traffic, Internet telephony\napplications often are designed to use TCP as a backup if UDP\ncommunication fails.\nFigure 2.5 ♦Popular Internet applications, their application-layer\nprotocols, and their underlying transport protocols\n2.1.5 Application-Layer Protocols\nWe have just learned that network processes communicate with each other\nby sending messages into sockets. But how are these messages structured?\nWhat are the meanings of the various fields in the messages? When do the\nprocesses send the messages? These questions bring us into the realm of\napplication-layer protocols. An application-layer protocol defines how an\napplication’s processes, running on different end systems, pass messages to\neach other. In particular, an application-layer protocol defines:\nThe types of messages exchanged, for example, request messages and\nresponse messages\nThe syntax of the various message types, such as the fields in the\nmessage and how the fields are delineated\nThe semantics of the fields, that is, the meaning of the information in\nRules for determining when and how a process sends messages and\nresponds to messages\nSome application-layer protocols are specified in RFCs and are therefore in\nthe public domain. For example, the Web’s application-layer protocol,\nHTTP (the HyperText Transfer Protocol [RFC 7230]), is available as an\nRFC. If a browser developer follows the rules of the HTTP RFC, the\nbrowser will be able to retrieve Web pages from any Web server that has\nalso followed the rules of the HTTP RFC. Many other application-layer\nprotocols are proprietary and intentionally not available in the public\ndomain. For example, Skype uses proprietary application-layer protocols.\nIt is important to distinguish between network applications and\napplication-layer protocols. An application-layer protocol is only one piece\nof a network application (albeit, a very important piece of the application\nfrom our point of view!). Let’s look at a couple of examples. The Web is a\nclient-server application that allows users to obtain documents from Web\nservers on demand. The Web application consists of many components,\nincluding a standard for document formats (that is, HTML), Web browsers\n(for example, Chrome and Microsoft Internet Explorer), Web servers\n(for  example, Apache and Microsoft servers), and an application-layer\nprotocol. The Web’s application-layer protocol, HTTP, defines the format\nand sequence of  messages exchanged between browser and Web server.\nThus, HTTP is only one  piece (albeit, an important piece) of the Web\napplication. As another example, we’ll see in Section 2.6 that Netflix’s\nvideo service also has many components, including servers that store and\ntransmit videos, other servers that manage billing and other ­client functions,\nclients (e.g., the Netflix app on your smartphone, tablet, or ­computer), and\nan application-level DASH protocol defines the format and sequence of\nmessages exchanged between a Netflix server and client. Thus, DASH is\nonly one piece (albeit, an important piece) of the Netflix application.\n2.1.6 Network Applications Covered in This Book\nNew applications are being developed every day. Rather than covering a\nlarge number of Internet applications in an encyclopedic manner, we have\nchosen to focus on a small number of applications that are both pervasive\nand important. In this chapter, we discuss five important applications: the\nWeb, electronic mail, directory service, video streaming, and P2P\napplications. We first discuss the Web, not only because it is an enormously\npopular application, but also because its application-layer protocol, HTTP,\nis straightforward and easy to understand. We then discuss electronic mail,\nthe Internet’s first killer application. E-mail is more complex than the Web\nin the sense that it makes use of not one but several application-layer\nprotocols. After e-mail, we cover DNS, which provides a directory service\nfor the Internet. Most users do not interact with DNS directly; instead, users\ninvoke DNS indirectly through other applications (including the Web, file\ntransfer, and electronic mail). DNS illustrates nicely how a piece of core\nnetwork functionality (network-name to network-address translation) can be\nimplemented at the application layer in the Internet. We then discuss P2P\nfile sharing applications, and complete our application study by discussing\nvideo streaming on demand, including distributing stored video over\ncontent distribution networks.\n2.2 The Web and HTTP\nUntil the early 1990s, the Internet was used primarily by researchers,\nacademics, and university students to log in to remote hosts, to transfer files\nfrom local hosts to remote hosts and vice versa, to receive and send news,\nand to receive and send electronic mail. Although these applications were\n(and continue to be) extremely useful, the Internet was essentially unknown\noutside of the academic and research communities. Then, in the early\n1990s, a major new application arrived on the scene—the World Wide Web\n[Berners-Lee 1994]. The Web was the first Internet application that caught\nthe general public’s eye. It dramatically changed how people interact inside\nand outside their work environments. It elevated the Internet from just one\nof many data networks to essentially the one and only data network.\nPerhaps what appeals the most to users is that the Web operates on\ndemand. Users receive what they want, when they want it. This is unlike\ntraditional broadcast radio and television, which force users to tune in when\nthe content provider makes the content available. In addition to being\navailable on demand, the Web has many other wonderful features that\npeople love and cherish. It is enormously easy for any individual to make\ninformation available over the Web—everyone can become a publisher at\nextremely low cost. Hyperlinks and search engines help us navigate through\nan ocean of information. Photos and videos stimulate our senses. Forms,\nJavaScript, video, and many other devices enable us to interact with pages\nand sites. And the Web and its protocols serve as a platform for YouTube,\nWeb-based e-mail (such as Gmail), and most mobile Internet applications,\nincluding Instagram and Google Maps.\n2.2.1 Overview of HTTP\nThe HyperText Transfer Protocol (HTTP), the Web’s application-layer\nprotocol, is at the heart of the Web. It is defined in [RFC 1945], [RFC 7230]\nand [RFC 7540]. HTTP is implemented in two programs: a client program\nand a server program. The client program and server program, executing on\ndifferent end systems, talk to each other by exchanging HTTP messages.\nHTTP defines the structure of these messages and how the client and server\nexchange the messages. Before explaining HTTP in detail, we should\nreview some Web terminology.\nA Web page (also called a document) consists of objects. An object is ­-\nsimply a file—such as an HTML file, a JPEG image, a Javascrpt file, a CCS\nstyle sheet file, or a video clip—that is addressable by a single URL. Most\nWeb pages consist of a base HTML file and several referenced objects. For\nexample, if a Web page contains HTML text and five JPEG images, then\nthe Web page has six objects: the base HTML file plus the five images. The\nTCP congestion control, discussed in detail in Chapter 3, also provides\nbrowsers an unintended incentive to use multiple parallel TCP connections\nrather than a single persistent connection. Very roughly speaking, TCP\ncongestion control aims to give each TCP connection sharing a bottleneck\nlink an equal share of the available bandwidth of that link; so if there are n\nTCP connections operating over a bottleneck link, then each connection\napproximately gets 1/nth of the bandwidth. By opening multiple parallel\nTCP connections to transport a single Web page, the browser can “cheat”\nand grab a larger portion of the link bandwidth. Many HTTP/1.1 browsers\nopen up to six parallel TCP connections not only to circumvent HOL\nblocking but also to obtain more bandwidth.\nOne of the primary goals of HTTP/2 is to get rid of (or at least reduce\nthe number of) parallel TCP connections for transporting a single Web page.\nThis not only reduces the number of sockets that need to be open and\nmaintained at servers, but also allows TCP congestion control to operate as\nintended. But with only one TCP connection to transport a Web page,\nHTTP/2 requires carefully designed mechanisms to avoid HOL blocking.\nHTTP/2 Framing\nThe HTTP/2 solution for HOL blocking is to break each message into small\nframes, and interleave the request and response messages on the same TCP\nconnection. To understand this, consider again the example of a Web page\nconsisting of one large video clip and, say, 8 smaller objects. Thus the\nserver will receive 9 concurrent requests from any browser wanting to see\nthis Web page. For each of these requests, the server needs to send 9\ncompeting HTTP response messages to the browser. ­Suppose all frames are\nof fixed length, the video clip consists of 1000 frames, and each of the\nsmaller objects consists of two frames. With frame interleaving, after\nsending one frame from the video clip, the first frames of each of the small\nobjects are sent. Then after sending the second frame of the video clip, the\nlast frames of each of the small objects are sent. Thus, all of the smaller\nobjects are sent after sending a total of 18 frames. If interleaving were not\nused, the smaller objects would be sent only after sending 1016 frames.\nThus the HTTP/2 framing mechanism can significantly decrease user-\nperceived delay.\nThe ability to break down an HTTP message into independent frames,\ninterleave them, and then reassemble them on the other end is the single\nmost important enhancement of HTTP/2. The framing is done by the\nframing sub-layer of the HTTP/2 protocol. When a server wants to send an\nHTTP response, the response is processed by the framing sub-layer, where\nit is broken down into frames. The header field of the response becomes one\nframe, and the body of the message is broken down into one for more\nadditional frames. The frames of the response are then interleaved by the\nframing sub-layer in the server with the frames of other responses and sent\nover the single persistent TCP connection. As the frames arrive at the client,\nthey are first reassembled into the original response messages at the framing\nsub-layer and then processed by the browser as usual. Similarly, a client’s\nHTTP requests are broken into frames and interleaved.\nIn addition to breaking down each HTTP message into independent\nframes, the framing sublayer also binary encodes the frames. Binary\nprotocols are more efficient to parse, lead to slightly smaller frames, and are\nless error-prone.\nResponse Message Prioritization and Server Pushing\nMessage prioritization allows developers to customize the relative priority\nof requests to better optimize application performance. As we just learned,\nthe framing sub-layer organizes messages into parallel streams of data\ndestined to the same requestor. When a client sends concurrent requests to a\nserver, it can prioritize the responses it is requesting by assigning a weight\nbetween 1 and 256 to each message. The higher number indicates higher\npriority. Using these weights, the server can send first the frames for the\nresponses with the highest priority. In addition to this, the client also states\neach message’s dependency on other messages by specifying the ID of the\nmessage on which it depends.\nAnother feature of HTTP/2 is the ability for a server to send multiple\nresponses for a single client request. That is, in addition to the response to\nthe original request, the server can push additional objects to the client,\nwithout the client having to request each one. This is possible since the\nHTML base page indicates the objects that will be needed to fully render\nthe Web page. So instead of waiting for the HTTP requests for these\nobjects, the server can analyze the HTML page, identify the objects that are\nneeded, and send them to the client before receiving explicit requests for\nthese objects. Server push eliminates the extra latency due to waiting for the\nQUIC, discussed in Chapter 3, is a new “transport” protocol that is\nimplemented in the application layer over the bare-bones UDP protocol.\nQUIC has several features that are desirable for HTTP, such as message\nmultiplexing (interleaving), per-stream flow control, and low-latency\nconnection establishment. HTTP/3 is yet a new HTTP protocol that is\ndesigned to operate over QUIC. As of 2020, HTTP/3 is described in\nInternet drafts and has not yet been fully standardized. Many of the HTTP/2\nfeatures (such as message interleaving) are subsumed by QUIC, allowing\nfor a simpler, streamlined design for HTTP/3.\n2.3 Electronic Mail in the Internet\nElectronic mail has been around since the beginning of the Internet. It was\nthe most popular application when the Internet was in its infancy [Segaller\n1998], and has become more elaborate and powerful over the years. It\nremains one of the Internet’s most important and utilized applications.\nAs with ordinary postal mail, e-mail is an asynchronous communication\nmedium—people send and read messages when it is convenient for them,\nwithout having to coordinate with other people’s schedules. In contrast with\npostal mail, electronic mail is fast, easy to distribute, and inexpensive.\nModern e-mail has many powerful features, including messages with\nattachments, hyperlinks, HTML-formatted text, and embedded photos.\nIn this section, we examine the application-layer protocols that are at\nthe heart of Internet e-mail. But before we jump into an in-depth discussion\nof these protocols, let’s take a high-level view of the Internet mail system\nand its key components.\nFigure 2.14 presents a high-level view of the Internet mail system. We\nsee from this diagram that it has three major components: user agents, mail\nservers, and the Simple Mail Transfer Protocol (SMTP). We now\ndescribe each of these components in the context of a sender, Alice, sending\nan e-mail message to a recipient, Bob. User agents allow users to read,\nreply to, forward, save, and compose ­messages. Examples of user agents for\ne-mail include Microsoft Outlook, Apple Mail, Web-based Gmail, the\nGmail App running in a smartphone, and so on. When Alice is finished\ncomposing her message, her user agent sends the message to her mail\nserver, where the message is placed in the mail server’s outgoing message\nqueue. When Bob wants to read a message, his user agent retrieves the\nmessage from his mailbox in his mail server.\nFigure 2.14 ♦A high-level view of the Internet e-mail system\nMail servers form the core of the e-mail infrastructure. Each recipient,\nsuch as Bob, has a mailbox located in one of the mail servers. Bob’s\nmailbox manages and maintains the messages that have been sent to him. A\ntypical message starts its journey in the sender’s user agent, then travels to\nthe sender’s mail server, and then travels to the recipient’s mail server,\nwhere it is deposited in the recipient’s mailbox. When Bob wants to access\nthe messages in his mailbox, the mail server containing his mailbox\nauthenticates Bob (with his username and password). Alice’s mail server\nmust also deal with failures in Bob’s mail server. If Alice’s server cannot\ndeliver mail to Bob’s server, Alice’s server holds the message in a message\nqueue and attempts to transfer the message later. Reattempts are often done\nevery 30 minutes or so; if there is no success after several days, the server\nremoves the message and notifies the sender (Alice) with an e-mail\nSMTP is the principal application-layer protocol for Internet electronic\nmail. It uses the reliable data transfer service of TCP to transfer mail from\nthe sender’s mail server to the recipient’s mail server. As with most\napplication-layer protocols, SMTP has two sides: a client side, which\nexecutes on the sender’s mail server, and a server side, which executes on\nthe recipient’s mail server. Both the client and server sides of SMTP run on\nevery mail server. When a mail server sends mail to other mail servers, it\nacts as an SMTP client. When a mail server receives mail from other mail\nservers, it acts as an SMTP server.\nSMTP, defined in RFC 5321, is at the heart of Internet electronic mail. As\nmentioned above, SMTP transfers messages from senders’ mail servers to\nthe recipients’ mail servers. SMTP is much older than HTTP. (The original\nSMTP RFC dates back to 1982, and SMTP was around long before that.)\nAlthough SMTP has numerous wonderful qualities, as evidenced by its\nubiquity in the Internet, it is nevertheless a legacy technology that possesses\ncertain archaic characteristics. For example, it restricts the body (not just\nthe headers) of all mail messages to simple 7-bit ASCII. This restriction\nmade sense in the early 1980s when transmission capacity was scarce and\nno one was e-mailing large attachments or large image, audio, or video\nfiles. But today, in the multimedia era, the 7-bit ASCII restriction is a bit of\na pain—it requires binary multimedia data to be encoded to ASCII before\nbeing sent over SMTP; and it requires the corresponding ASCII message to\nbe decoded back to binary after SMTP transport. Recall from Section 2.2\nthat HTTP does not require multimedia data to be ASCII encoded before\nTo illustrate the basic operation of SMTP, let’s walk through a common\nscenario. Suppose Alice wants to send Bob a simple ASCII message.\n1. Alice invokes her user agent for e-mail, provides Bob’s e-mail address\n(for example, bob@someschool.edu), composes a message, and\ninstructs the user agent to send the message.\n2. Alice’s user agent sends the message to her mail server, where it is\nplaced in a message queue.\n3. The client side of SMTP, running on Alice’s mail server, sees the\nmessage in the message queue. It opens a TCP connection to an SMTP\nserver, running on Bob’s mail server.\n4. After some initial SMTP handshaking, the SMTP client sends Alice’s\nmessage into the TCP connection.\n5. At Bob’s mail server, the server side of SMTP receives the message.\nBob’s mail server then places the message in Bob’s mailbox.\n6. Bob invokes his user agent to read the message at his convenience.\nThe scenario is summarized in Figure 2.15.\nFigure 2.15 ♦Alice sends a message to Bob\nIt is important to observe that SMTP does not normally use\nintermediate mail servers for sending mail, even when the two mail servers\nare located at opposite ends of the world. If Alice’s server is in Hong Kong\nand Bob’s server is in St. Louis, the TCP connection is a direct connection\nbetween the Hong Kong and St. Louis servers. In particular, if Bob’s mail\nserver is down, the message remains in Alice’s mail server and waits for a\nnew attempt—the message does not get placed in some intermediate mail\nLet’s now take a closer look at how SMTP transfers a message from a\nsending mail server to a receiving mail server. We will see that the SMTP\nprotocol has many similarities with protocols that are used for face-to-face\nhuman interaction. First, the client SMTP (running on the sending mail\nserver host) has TCP establish a connection to port 25 at the server SMTP\n(running on the receiving mail server host). If the server is down, the client\ntries again later. Once this connection is established, the server and client\nperform some application-layer handshaking—just as humans often\nintroduce themselves before transferring information from one to another,\nSMTP clients and servers introduce themselves before transferring\ninformation. During this SMTP handshaking phase, the SMTP client\nindicates the e-mail address of the sender (the person who generated the\nmessage) and the e-mail address of the recipient. Once the SMTP client and\nserver have introduced themselves to each other, the client sends the\nmessage. SMTP can count on the reliable data transfer service of TCP to get\nthe message to the server without errors. The client then repeats this process\nover the same TCP connection if it has other messages to send to the server;\notherwise, it instructs TCP to close the connection.\nLet’s next take a look at an example transcript of messages exchanged\nbetween an SMTP client (C) and an SMTP server (S). The hostname of the\nclient is crepes.fr and the hostname of the server is hamburger.edu.\nWe discuss IP addresses in some detail in Chapter 4, but it is useful to\nsay a few brief words about them now. An IP address consists of four bytes\nand has a rigid hierarchical structure. An IP address looks like\n121.7.106.83, where each period separates one of the bytes expressed\nin decimal notation from 0 to 255. An IP address is hierarchical because as\nwe scan the address from left to right, we obtain more and more specific\ninformation about where the host is located in the Internet (that is, within\nwhich network, in the network of networks). Similarly, when we scan a\npostal address from bottom to top, we obtain more and more specific\ninformation about where the addressee is located.\n2.4.1 Services Provided by DNS\nWe have just seen that there are two ways to identify a host—by a hostname\nand by an IP address. People prefer the more mnemonic hostname\nidentifier, while routers prefer fixed-length, hierarchically structured IP\nDHCP, which is discussed in Chapter 4). You can easily determine the IP\naddress of your local DNS server by accessing network status windows in\nWindows or UNIX. A host’s local DNS server is typically “close to” the\nhost. For an institutional ISP, the local DNS server may be on the same\nLAN as the host; for a residential ISP, it is typically separated from the host\nby no more than a few routers. When a host makes a DNS query, the query\nis sent to the local DNS server, which acts a proxy, ­forwarding the query\ninto the DNS server hierarchy, as we’ll discuss in more detail below.\nLet’s take a look at a simple example. Suppose the host\ncse.nyu.edu desires the IP address of gaia.cs.umass.edu. Also\nsuppose that NYU’s local DNS server for cse.nyu.edu is called\ndns.nyu.edu \nauthoritative \ngaia.cs.umass.edu is called dns.umass.edu. As shown in ­Figure\n2.19, the host cse.nyu.edu first sends a DNS query message to its local\nDNS server, dns.nyu.edu. The query message contains the hostname to\nbe translated, namely, gaia.cs.umass.edu. The local DNS server\nforwards the query message to a root DNS server. The root DNS server\ntakes note of the edu suffix and returns to the local DNS server a list of IP\naddresses for TLD servers responsible for edu. The local DNS server then\nresends the query message to one of these TLD servers. The TLD server\ntakes note of the umass.edu suffix and responds with the IP address of\nthe authoritative DNS server for the University of Massachusetts, namely,\ndns.umass.edu. Finally, the local DNS server resends the query\nmessage directly to dns.umass.edu, which responds with the IP address\nof gaia.cs.umass.edu. Note that in this example, in order to obtain\nthe mapping for one hostname, eight DNS messages were sent: four query\nmessages and four reply messages! We’ll soon see how DNS caching\nreduces this query traffic.\nFigure 2.19 ♦Interaction of the various DNS servers\nOur previous example assumed that the TLD server knows the\nauthoritative DNS server for the hostname. In general, this is not always\ntrue. Instead, the TLD server may know only of an intermediate DNS\nserver, which in turn knows the authoritative DNS server for the hostname.\nFor example, suppose again that the University of Massachusetts has a DNS\nserver for the university, called dns.umass.edu. Also suppose that each\nof the departments at the University of Massachusetts has its own DNS\nserver, and that each departmental DNS server is authoritative for all hosts\nin the department. In this case, when the intermediate DNS server,\ndns.umass.edu, receives a query for a host with a hostname ending\nwith cs.umass.edu, it returns to dns.nyu.edu the IP address of\ndns.cs.umass.edu, which is authoritative for all hostnames ending\nwith cs.umass.edu. The local DNS server dns.nyu.edu then sends\nthe query to the authoritative DNS server, which returns the desired\nmapping to the local DNS server, which in turn returns the mapping to the\nrequesting host. In this case, a total of 10 DNS messages are sent!\nThe example shown in Figure 2.19 makes use of both recursive\nqueries and iterative queries. The query sent from cse.nyu.edu to\ndns.nyu.edu is a recursive query, since the query asks dns.nyu.edu\nto obtain the mapping on its behalf. However, the subsequent three queries\nare iterative since all of the replies are directly returned to dns.nyu.edu.\nIn theory, any DNS query can be iterative or recursive. For example, Figure\n2.20 shows a DNS query chain for which all of the queries are recursive. In\npractice, the queries typically follow the pattern in Figure 2.19: The query\nfrom the requesting host to the local DNS server is recursive, and the\nremaining queries are iterative.\nFigure 2.20 ♦Recursive queries in DNS\nDNS Caching\nOur discussion thus far has ignored DNS caching, a critically important\nfeature of the DNS system. In truth, DNS extensively exploits DNS caching\nin order to improve the delay performance and to reduce the number of\nDNS messages ricocheting around the Internet. The idea behind DNS\ncaching is very simple. In a query chain, when a DNS server receives a\nDNS reply (containing, for example, a mapping from a hostname to an IP\naddress), it can cache the mapping in its local memory. For example, in\nFigure 2.19, each time the local DNS server dns.nyu.edu receives a\nreply from some DNS server, it can cache any of the information contained\nin the reply. If a hostname/IP address pair is cached in a DNS server and\nanother query arrives to the DNS server for the same hostname, the DNS\nserver can provide the desired IP address, even if it is not authoritative for\nthe hostname. Because hosts and mappings between hostnames and IP\naddresses are by no means permanent, DNS servers discard cached\ninformation after a period of time (often set to two days).\nAs an example, suppose that a host apricot.nyu.edu queries\ndns.nyu.edu for the IP address for the hostname cnn.com.\nFurthermore, ­suppose that a few hours later, another NYU host, say,\nkiwi.nyu.edu, also queries dns.nyu.edu with the same hostname.\nBecause of caching, the local DNS server will be able to immediately return\nthe IP address of cnn.com to this second requesting host without having to\nquery any other DNS servers. A local DNS server can also cache the IP\naddresses of TLD servers, thereby allowing the local DNS server to bypass\nthe root DNS servers in a query chain. In fact, because of caching, root\nservers are bypassed for all but a very small fraction of DNS queries.\n2.4.3 DNS Records and Messages\nThe DNS servers that together implement the DNS distributed database\nstore resource records (RRs), including RRs that provide hostname-to-IP\naddress mappings. Each DNS reply message carries one or more resource\nrecords. In this and the following subsection, we provide a brief overview\nof DNS resource records and messages; more details can be found in\n[Albitz 1993] or in the DNS RFCs [RFC 1034; RFC 1035].\nA resource record is a four-tuple that contains the following fields:\n(Name, Value, Type, TTL)\nTTL is the time to live of the resource record; it determines when a resource\nshould be removed from a cache. In the example records given below, we\nignore the TTL field. The meaning of Name and Value depend on Type:\nIf Type=A, then Name is a hostname and Value is the IP address for\nthe hostname. Thus, a Type A record provides the standard hostname-to-\nIP address mapping. As an example, (relay1.bar.foo.com,\n145.37.93.126, A) is a Type A record.\nIf Type=NS, then Name is a domain (such as foo.com) and Value\nis the hostname of an authoritative DNS server that knows how to\nobtain the IP addresses for hosts in the domain. This record is used to\nroute DNS queries further along in the query chain. As an example,\n(foo.com, dns.foo.com, NS) is a Type NS record.\nIf Type=CNAME, then Value is a canonical hostname for the alias\nhostname Name. This record can provide querying hosts the canonical\nrelay1.bar.foo.com, CNAME) is a CNAME record.\nIf Type=MX, then Value is the canonical name of a mail server that\nhas an alias hostname Name. As an example, (foo.com,\nmail.bar.foo.com, MX) is an MX record. MX records allow the\nhostnames of mail servers to have simple aliases. Note that by using the\nMX record, a company can have the same aliased name for its mail\nserver and for one of its other servers (such as its Web server). To obtain\nthe canonical name for the mail server, a DNS client would query for an\nMX record; to obtain the canonical name for the other server, the DNS\nclient would query for the CNAME record.\nIf a DNS server is authoritative for a particular hostname, then the DNS\nserver will contain a Type A record for the hostname. (Even if the DNS\nserver is not authoritative, it may contain a Type A record in its cache.) If a\nserver is not authoritative for a hostname, then the server will contain a\nType NS record for the domain that includes the hostname; it will also\ncontain a Type A record that provides the IP address of the DNS server in\nthe Value field of the NS record. As an example, suppose an edu TLD\nserver is not authoritative for the host gaia.cs.umass.edu. Then this\nserver will contain a record for a domain that includes the host\ngaia.cs.umass.edu, \n(umass.edu,\ndns.umass.edu, NS). The edu TLD server would also contain a Type\nA record, which maps the DNS server dns.umass.edu to an IP address,\nfor example, (dns.umass.edu, 128.119.40.111, A).\nDNS Messages\nEarlier in this section, we referred to DNS query and reply messages. These\nare the only two kinds of DNS messages. Furthermore, both query and\nreply messages have the same format, as shown in Figure 2.21.The\nsemantics of the various fields in a DNS message are as follows:\nThe first 12 bytes is the header section, which has a number of fields.\nThe first field is a 16-bit number that identifies the query. This identifier\nis copied into the reply message to a query, allowing the client to match\nreceived replies with sent queries. There are a number of flags in the\nflag field. A 1-bit query/reply flag indicates whether the message is a\nquery (0) or a reply (1). A 1-bit authoritative flag is set in a reply\nmessage when a DNS server is an authoritative server for a queried\nname. A 1-bit recursion-desired flag is set when a client (host or DNS\nserver) desires that the DNS server perform recursion when it doesn’t\nhave the record. A 1-bit recursion-available field is set in a reply if the\nDNS server supports recursion. In the header, there are also four\nnumber-of fields. These fields indicate the number of occurrences of the\nfour types of data sections that follow the header.\nThe question section contains information about the query that is being\nmade. This section includes (1) a name field that contains the name that\nis being queried, and (2) a type field that indicates the type of question\nbeing asked about the name—for example, a host address associated\nwith a name (Type A) or the mail server for a name (Type MX).\nFigure 2.21 ♦DNS message format\nIn a reply from a DNS server, the answer section contains the resource\nrecords for the name that was originally queried. Recall that in each\nresource record there is the Type (for example, A, NS, CNAME, and\nMX), the Value, and the TTL. A reply can return multiple RRs in the\nanswer, since a hostname can have multiple IP addresses (for example,\nfor replicated Web servers, as discussed earlier in this section).\nThe authority section contains records of other authoritative servers.\nThe additional section contains other helpful records. For example, the\nanswer field in a reply to an MX query contains a resource record\nproviding the canonical hostname of a mail server. The additional\nsection contains a Type A record providing the IP address for the\ncanonical hostname of the mail server.\nHow would you like to send a DNS query message directly from the\nhost you’re working on to some DNS server? This can easily be done with\nthe nslookup program, which is available from most Windows and UNIX\nplatforms. For example, from a Windows host, open the Command Prompt\nand invoke the nslookup program by simply typing “nslookup.” After\ninvoking nslookup, you can send a DNS query to any DNS server (root,\nTLD, or authoritative). After receiving the reply message from the DNS\nserver, nslookup will display the records included in the reply (in a human-\nreadable format). As an alternative to running nslookup from your own\nhost, you can visit one of many Web sites that allow you to remotely\nemploy nslookup. (Just type “nslookup” into a search engine and you’ll be\nbrought to one of these sites.) The DNS Wireshark lab at the end of this\nchapter will allow you to explore the DNS in much more detail.\nInserting Records into the DNS Database\nThe discussion above focused on how records are retrieved from the DNS\ndatabase. You might be wondering how records get into the database in the\nfirst place. Let’s look at how this is done in the context of a specific\nexample. Suppose you have just created an exciting new startup company\ncalled Network Utopia. The first thing you’ll surely want to do is register\nthe domain name networkutopia.com at a registrar. A registrar is a\ncommercial entity that verifies the uniqueness of the domain name, enters\nthe domain name into the DNS database (as discussed below), and collects\na small fee from you for its services. Prior to 1999, a single registrar,\nNetwork Solutions, had a monopoly on domain name registration for com,\nnet, and org domains. But now there are many registrars competing for\ncustomers, and the Internet Corporation for Assigned Names and Numbers\n(ICANN) accredits the various registrars. A complete list of accredited\nregistrars is available at http://www.internic.net.\nWhen you register the domain name networkutopia.com with\nsome registrar, you also need to provide the registrar with the names and IP\naddresses of your primary and secondary authoritative DNS servers.\nSuppose the names and IP addresses are dns1.networkutopia.com,\ndns2.networkutopia.com, \n212.2.212.1, \n212.212.212.2. For each of these two authoritative DNS servers, the\nregistrar would then make sure that a Type NS and a Type A record are\nentered into the TLD com servers. Specifically, for the primary authoritative\nserver for networkutopia.com, the registrar would insert the following\ntwo resource records into the DNS system:\n(networkutopia.com, dns1.networkutopia.com, NS)\n(dns1.networkutopia.com, 212.212.212.1, A)\nWe have seen that DNS is a critical component of the Internet infrastructure, with many\nimportant services—including the Web and e-mail—simply incapable of functioning\nwithout it. We therefore naturally ask, how can DNS be attacked? Is DNS a sitting\nduck, waiting to be knocked out of service, while taking most Internet applications\ndown with it?\nThe first type of attack that comes to mind is a DDoS bandwidth-flooding attack (see\nSection 1.6) against DNS servers. For example, an attacker could attempt to send to\neach DNS root server a deluge of packets, so many that the majority of legitimate DNS\nqueries never get answered. Such a large-scale DDoS attack against DNS root servers\nactually took place on October 21, 2002. In this attack, the attackers leveraged a\nbotnet to send truck loads of ICMP ping messages to each of the 13 DNS root IP\naddresses. (ICMP messages are discussed in Section 5.6. For now, it suffices to know\nthat ICMP packets are special types of IP datagrams.) Fortunately, this large-scale\nattack caused minimal damage, having little or no impact on users’ Internet experience.\nThe attackers did succeed at directing a deluge of packets at the root servers. But\nmany of the DNS root servers were protected by packet filters, configured to always\nblock all ICMP ping messages directed at the root servers. These protected servers\nwere thus spared and functioned as normal. Furthermore, most local DNS servers\ncache the IP addresses of top-level-domain servers, allowing the query process to\noften bypass the DNS root servers.\nA potentially more effective DDoS attack against DNS is send a deluge of DNS\nqueries to top-level-domain servers, for example, to top-level-domain servers that\nhandle the .com domain. It is harder to filter DNS queries directed to DNS servers; and\ntop-level-domain servers are not as easily bypassed as are root servers. Such an\nattack took place against the top-level-domain service provider Dyn on October 21,\n2016. This DDoS attack was accomplished through a large number of DNS lookup\nrequests from a botnet consisting of about one hundred thousand IoT devices such as\nprinters, IP cameras, residential gateways and baby monitors that had been infected\nwith Mirai malware. For almost a full day, Amazon, Twitter, Netflix, Github and Spotify\nwere disturbed.\nDNS could potentially be attacked in other ways. In a man-in-the-middle attack, the\nattacker intercepts queries from hosts and returns bogus replies. In the DNS poisoning\nattack, the attacker sends bogus replies to a DNS server, tricking the server into\naccepting bogus records into its cache. Either of these attacks could be used, for\nexample, to redirect an unsuspecting Web user to the attacker’s Web site. The DNS\nSecurity Extensions (DNSSEC [Gieben 2004; RFC 4033] have been designed and\ndeployed to protect against such exploits. DNSSEC, a secured version of DNS,\naddresses many of these possible attacks and is gaining popularity in the Internet.\nYou’ll also have to make sure that the Type A resource record for your Web\nserver www.networkutopia.com and the Type MX resource record for\nyour mail server mail.networkutopia.com are entered into your\nuser. (Recall from Chapter 1 that the end-to-end throughput of a stream is\ngoverned by the throughput at the bottleneck link.) The likelihood of this\nhappening increases as the number of links in the end-to-end path increases.\nA second drawback is that a popular video will likely be sent many times\nover the same communication links. Not only does this waste network\nbandwidth, but the Internet video company itself will be paying its provider\nISP (connected to the data center) for sending the same bytes into the\nInternet over and over again. A third problem with this solution is that a\nsingle data center represents a single point of failure—if the data center or\nits links to the Internet goes down, it would not be able to distribute any\nvideo streams.\nIn order to meet the challenge of distributing massive amounts of video\ndata to users distributed around the world, almost all major video-streaming\ncompanies make use of Content Distribution Networks (CDNs). A CDN\nmanages servers in multiple geographically distributed locations, stores\ncopies of the videos (and other types of Web content, including documents,\nimages, and audio) in its servers, and attempts to direct each user request to\na CDN location that will provide the best user experience. The CDN may be\na private CDN, that is, owned by the content provider itself; for example,\nGoogle’s CDN distributes YouTube videos and other types of content. The\nCDN may alternatively be a third-party CDN that distributes content on\nbehalf of multiple content providers; Akamai, Limelight and Level-3 all\noperate third-party CDNs. A very readable overview of modern CDNs is\n[Leighton 2009; Nygren 2010].\nCDNs typically adopt one of two different server placement\nphilosophies [Huang 2008]:\nEnter Deep. One philosophy, pioneered by Akamai, is to enter deep\ninto the access networks of Internet Service Providers, by deploying\nserver clusters in access ISPs all over the world. (Access networks are\ndescribed in Section 1.3.) Akamai takes this approach with clusters in\nthousands of locations. The goal is to get close to end users, thereby\nimproving user-perceived delay and throughput by decreasing the\nnumber of links and routers between the end user and the CDN server\nfrom which it receives content. Because of this highly distributed\ndesign, the task of maintaining and managing the clusters becomes\nchallenging.\nBring Home. A second design philosophy, taken by Limelight and\nmany other CDN companies, is to bring the ISPs home by building\nlarge clusters at a smaller number (for example, tens) of sites. Instead of\ngetting inside the access ISPs, these CDNs typically place their clusters\nin Internet Exchange Points (IXPs) (see Section 1.3). Compared with\nthe enter-deep design philosophy, the bring-home design typically\nresults in lower maintenance and management overhead, possibly at the\nexpense of higher delay and lower throughput to end users.\nOnce its clusters are in place, the CDN replicates content across its clusters.\nThe CDN may not want to place a copy of every video in each cluster, since\nsome videos are rarely viewed or are only popular in some countries. In\nfact, many CDNs do not push videos to their clusters but instead use a\nsimple pull strategy: If a client requests a video from a cluster that is not\nstoring the video, then the cluster retrieves the video (from a central\nrepository or from another cluster) and stores a copy locally while\nstreaming the video to the client at the same time. Similar Web caching (see\nSection 2.2.5), when a cluster’s storage becomes full, it removes videos that\nare not frequently requested.\nCDN Operation\nHaving identified the two major approaches toward deploying a CDN, let’s\nnow dive down into the nuts and bolts of how a CDN operates. When a\nbrowser in a user’s host is instructed to retrieve a specific video (identified\nby a URL), the CDN must intercept the request so that it can (1) determine\na suitable CDN server cluster for that client at that time, and (2) redirect the\nclient’s request to a server in that cluster. We’ll shortly discuss how a CDN\ncan determine a suitable cluster. But first let’s examine the mechanics\nbehind intercepting and redirecting a request.\nGOOGLE’S NETWORK INFRASTRUCTURE\nTo support its vast array of services—including search, Gmail, calendar, YouTube\nvideo, maps, documents, and social networks—Google has deployed an extensive\nprivate network and CDN infrastructure. Google’s CDN infrastructure has three tiers of\nserver clusters:\nNineteen “mega data centers” in North America, Europe, and Asia [Google\nLocations 2020], with each data center having on the order of 100,000 servers.\nThese mega data centers are responsible for serving dynamic (and often\npersonalized) content, including search results and Gmail messages.\nWith about 90 clusters in IXPs scattered throughout the world, with each cluster\nconsisting of hundreds of servers servers [Adhikari 2011a] [Google CDN 2020].\nThese clusters are responsible for serving static content, including YouTube videos.\nMany hundreds of “enter-deep” clusters located within an access ISP. Here a\ncluster typically consists of tens of servers within a single rack. These enter-deep ­-\nservers perform TCP splitting (see Section 3.7) and serve static content [Chen\n2011], including the static portions of Web pages that embody search results.\nAll of these data centers and cluster locations are networked together with Google’s\nown private network. When a user makes a search query, often the query is first sent\nover the local ISP to a nearby enter-deep cache, from where the static content is\nretrieved; while providing the static content to the client, the nearby cache also\nforwards the query over Google’s private network to one of the mega data centers,\nfrom where the personalized search results are retrieved. For a YouTube video, the\nvideo itself may come from one of the bring-home caches, whereas portions of the\nWeb page surrounding the video may come from the nearby enter-deep cache, and the\nadvertisements surrounding the video come from the data centers. In summary, except\nfor the local ISPs, the Google cloud services are largely provided by a network\ninfrastructure that is independent of the public Internet.\nMost CDNs take advantage of DNS to intercept and redirect requests;\nan interesting discussion of such a use of the DNS is [Vixie 2009]. Let’s\nconsider a simple example to illustrate how the DNS is typically involved.\nSuppose a content provider, NetCinema, employs the third-party CDN\ncompany, KingCDN, to distribute its videos to its customers. On the\nNetCinema Web pages, each of its videos is assigned a URL that includes\nthe string “video” and a unique identifier for the video itself; for example,\nTransformers 7 might be assigned http://video.netcinema.com/6Y7B23V.\nSix steps then occur, as shown in Figure 2.25:\nFigure 2.25 ♦DNS redirects a user’s request to a CDN server\n1. The user visits the Web page at NetCinema.\n2. When the user clicks on the link http://video.netcinema.com/6Y7B23V,\nthe user’s host sends a DNS query for video.netcinema.com.\n3. The user’s Local DNS Server (LDNS) relays the DNS query to an\nauthoritative DNS server for NetCinema, which observes the string\n“video” in the hostname video.netcinema.com. To “hand over” the DNS\nquery to KingCDN, instead of returning an IP address, the NetCinema\nauthoritative DNS server returns to the LDNS a hostname in the\nKingCDN’s domain, for example, a1105.kingcdn.com.\n4. From this point on, the DNS query enters into KingCDN’s private DNS\ninfrastructure. The user’s LDNS then sends a second query, now for\na1105.kingcdn.com, and KingCDN’s DNS system eventually returns the\nIP addresses of a KingCDN content server to the LDNS. It is thus here,\nwithin the KingCDN’s DNS system, that the CDN server from which\nthe client will receive its content is specified.\n5. The LDNS forwards the IP address of the content-serving CDN node to\nthe user’s host.\n6. Once the client receives the IP address for a KingCDN content server, it\nestablishes a direct TCP connection with the server at that IP address\nand issues an HTTP GET request for the video. If DASH is used, the\nserver will first send to the client a manifest file with a list of URLs, one\nfor each version of the video, and the client will dynamically select\nchunks from the different versions.\nCluster Selection Strategies\nAt the core of any CDN deployment is a cluster selection strategy, that is,\na mechanism for dynamically directing clients to a server cluster or a data\ncenter within the CDN. As we just saw, the CDN learns the IP address of\nthe client’s LDNS server via the client’s DNS lookup. After learning this IP\naddress, the CDN needs to select an appropriate cluster based on this IP\naddress. CDNs generally employ proprietary cluster selection strategies. We\nnow briefly survey a few approaches, each of which has its own advantages\nand disadvantages.\nOne simple strategy is to assign the client to the cluster that is\ngeographically closest. Using commercial geo-location databases (such as\nQuova [Quova 2020] and Max-Mind [MaxMind 2020]), each LDNS IP\naddress is mapped to a geographic location. When a DNS request is\nreceived from a particular LDNS, the CDN chooses the geographically\nclosest cluster, that is, the cluster that is the fewest kilometers from the\nLDNS “as the bird flies.” Such a solution can work reasonably well for a\nlarge fraction of the clients [Agarwal 2009]. However, for some clients, the\nsolution may perform poorly, since the geographically closest cluster may\nnot be the closest cluster in terms of the length or number of hops of the\nnetwork path. Furthermore, a problem inherent with all DNS-based\napproaches is that some end-users are configured to use remotely located\nLDNSs [Shaikh 2001; Mao 2002], in which case the LDNS location may be\nfar from the client’s location. Moreover, this simple strategy ignores the\nvariation in delay and available bandwidth over time of Internet paths,\nalways assigning the same cluster to a particular client.\nIn order to determine the best cluster for a client based on the current\ntraffic conditions, CDNs can instead perform periodic real-time\nmeasurements of delay and loss performance between their clusters and\nclients. For instance, a CDN can have each of its clusters periodically send\nprobes (for example, ping messages or DNS queries) to all of the LDNSs\naround the world. One drawback of this approach is that many LDNSs are\nconfigured to not respond to such probes.\n2.6.4 Case Studies: Netflix and YouTube\nWe conclude our discussion of streaming stored video by taking a look at\ntwo highly successful large-scale deployments: Netflix and YouTube. We’ll\nsee that each of these systems take a very different approach, yet employ\nmany of the underlying principles discussed in this section.\nAs of 2020, Netflix is the leading service provider for online movies and\nTV series in North America. As we discuss below, Netflix video distribution\nhas two major components: the Amazon cloud and its own private CDN\ninfrastructure.\nNetflix has a Web site that handles numerous functions, including user\nregistration and login, billing, movie catalogue for browsing and searching,\nand a movie recommendation system. As shown in Figure 2.26, this Web\nsite (and its associated backend databases) run entirely on Amazon servers\nin the Amazon cloud. Additionally, the Amazon cloud handles the following\ncritical functions:\nContent ingestion. Before Netflix can distribute a movie to its\ncustomers, it must first ingest and process the movie. Netflix receives\nstudio master versions of movies and uploads them to hosts in the\nAmazon cloud.\nContent processing. The machines in the Amazon cloud create many\ndifferent formats for each movie, suitable for a diverse array of client\nvideo players running on desktop computers, smartphones, and game\nconsoles connected to televisions. A different version is created for each\nof these formats and at multiple bit rates, allowing for adaptive\nstreaming over HTTP using DASH.\nUploading versions to its CDN. Once all of the versions of a movie\nhave been created, the hosts in the Amazon cloud upload the versions to\nFigure 2.26 ♦Netflix video streaming platform\nWhen Netflix first rolled out its video streaming service in 2007, it\nemployed three third-party CDN companies to distribute its video content.\nNetflix has since created its own private CDN, from which it now streams\nall of its videos. To create its own CDN, Netflix has installed server racks\nboth in IXPs and within residential ISPs themselves. Netflix currently has\nserver racks in over 200 IXP locations; see [Bottger 2018] [Netflix Open\nConnect 2020] for a current list of IXPs housing Netflix racks. There are\nalso hundreds of ISP locations housing Netflix racks; also see [Netflix Open\nConnect 2020], where Netflix provides to potential ISP partners instructions\nabout installing a (free) Netflix rack for their networks. Each server in the\nrack has several 10 Gbps Ethernet ports and over 100 terabytes of storage.\nThe number of servers in a rack varies: IXP installations often have tens of\nservers and contain the entire Netflix streaming video library, including\nmultiple versions of the videos to support DASH. Netflix does not use pull-\ncaching (Section 2.2.5) to populate its CDN servers in the IXPs and ISPs.\nInstead, Netflix distributes by pushing the videos to its CDN servers during\noff-peak hours. For those locations that cannot hold the entire library,\nNetflix pushes only the most popular videos, which are determined on a\nday-to-day basis. The Netflix CDN design is described in some detail in the\nYouTube videos [Netflix Video 1] and [Netflix Video 2]; see also [Bottger\nHaving described the components of the Netflix architecture, let’s take\na closer look at the interaction between the client and the various servers\nthat are involved in movie delivery. As indicated earlier, the Web pages for\nbrowsing the Netflix video library are served from servers in the Amazon\ncloud. When a user selects a movie to play, the Netflix software, running in\nthe Amazon cloud, first determines which of its CDN servers have copies of\nthe movie. Among the servers that have the movie, the software then\ndetermines the “best” server for that client request. If the client is using a\nresidential ISP that has a Netflix CDN server rack installed in that ISP, and\nthis rack has a copy of the requested movie, then a server in this rack is\ntypically selected. If not, a server at a nearby IXP is typically selected.\nOnce Netflix determines the CDN server that is to deliver the content, it\nsends the client the IP address of the specific server as well as a manifest\nfile, which has the URLs for the different versions of the requested movie.\nThe client and that CDN server then directly interact using a proprietary\nversion of DASH. Specifically, as described in Section 2.6.2, the client uses\nthe byte-range header in HTTP GET request messages, to request chunks\nfrom the different versions of the movie. Netflix uses chunks that are\napproximately four-seconds long [Adhikari 2012]. While the chunks are\nbeing downloaded, the client measures the received throughput and runs a\nrate-determination algorithm to determine the quality of the next chunk to\nNetflix embodies many of the key principles discussed earlier in this\nsection, including adaptive streaming and CDN distribution. However,\nbecause Netflix uses its own private CDN, which distributes only video\n(and not Web pages), Netflix has been able to simplify and tailor its CDN\ndesign. In particular, Netflix does not need to employ DNS redirect, as\ndiscussed in Section 2.6.3, to connect a particular client to a CDN server;\ninstead, the Netflix software (running in the Amazon cloud) directly tells\nthe client to use a particular CDN server. Furthermore, the Netflix CDN\nuses push caching rather than pull caching (Section 2.2.5): content is\npushed into the servers at scheduled times at off-peak hours, rather than\ndynamically during cache misses.\nWith hundreds of hours of video uploaded to YouTube every minute and\nseveral billion video views per day, YouTube is indisputably the world’s\nlargest video-sharing site. YouTube began its service in April 2005 and was\nacquired by Google in November 2006. Although the Google/YouTube\ndesign and protocols are proprietary, through several independent\nmeasurement efforts we can gain a basic understanding about how YouTube\noperates [Zink 2009; Torres 2011; Adhikari 2011a]. As with Netflix,\nYouTube makes extensive use of CDN technology to distribute its videos\n[Torres 2011]. Similar to Netflix, Google uses its own private CDN to\ndistribute YouTube videos, and has installed server clusters in many\nhundreds of different IXP and ISP locations. From these locations and\ndirectly from its huge data centers, Google distributes YouTube videos\n[Adhikari 2011a]. Unlike Netflix, however, Google uses pull caching, as\ndescribed in Section 2.2.5, and DNS redirect, as described in Section 2.6.3.\nMost of the time, Google’s cluster-selection strategy directs the client to the\ncluster for which the RTT between client and cluster is the lowest; however,\nin order to balance the load across clusters, sometimes the client is directed\n(via DNS) to a more distant cluster [Torres 2011].\nYouTube employs HTTP streaming, often making a small number of\ndifferent versions available for a video, each with a different bit rate and\ncorresponding quality level. YouTube does not employ adaptive streaming\n(such as DASH), but instead requires the user to manually select a version.\nIn order to save bandwidth and server resources that would be wasted by\nrepositioning or early termination, YouTube uses the HTTP byte range\nrequest to limit the flow of transmitted data after a target amount of video is\nprefetched.\nSeveral million videos are uploaded to YouTube every day. Not only\nare YouTube videos streamed from server to client over HTTP, but YouTube\nuploaders also upload their videos from client to server over HTTP.\nYouTube processes each video it receives, converting it to a YouTube video\nformat and creating multiple versions at different bit rates. This processing\ntakes place entirely within Google data centers. (See the case study on\nGoogle’s network infrastructure in Section 2.6.3.)\n2.7 Socket Programming: Creating Network\nApplications\nNow that we’ve looked at a number of important network applications, let’s\nexplore how network application programs are actually created. Recall from\nSection 2.1 that a typical network application consists of a pair of programs\n—a client program and a server program—residing in two different end\nsystems. When these two programs are executed, a client process and a\nserver process are created, and these processes communicate with each\nother by reading from, and writing to, sockets. When creating a network\napplication, the developer’s main task is therefore to write the code for both\nthe client and server programs.\nThere are two types of network applications. One type is an\nimplementation whose operation is specified in a protocol standard, such as\nan RFC or some other standards document; such an application is\nsometimes referred to as “open,” since the rules specifying its operation are\nknown to all. For such an implementation, the client and server programs\nmust conform to the rules dictated by the RFC. For example, the client\nprogram could be an implementation of the client side of the HTTP\nprotocol, described in Section 2.2 and precisely defined in RFC 2616;\nsimilarly, the server program could be an implementation of the HTTP\nserver protocol, also precisely defined in RFC 2616. If one developer writes\ncode for the client program and another developer writes code for the server\nprogram, and both developers carefully follow the rules of the RFC, then\nthe two programs will be able to interoperate. Indeed, many of today’s\nnetwork applications involve communication between client and server\nprograms that have been created by independent developers—for example,\na Google Chrome browser communicating with an Apache Web server, or a\nBitTorrent client communicating with BitTorrent tracker.\nThe other type of network application is a proprietary network\napplication. In this case, the client and server programs employ an\napplication-layer protocol that has not been openly published in an RFC or\nelsewhere. A single developer (or development team) creates both the client\nand server programs, and the developer has complete control over what\ngoes in the code. But because the code does not implement an open\nprotocol, other independent developers will not be able to develop code that\ninteroperates with the application.\nIn this section, we’ll examine the key issues in developing a client-\nserver application, and we’ll “get our hands dirty” by looking at code that\nimplements a very simple client-server application. During the development\nphase, one of the first decisions the developer must make is whether the\napplication is to run over TCP or over UDP. Recall that TCP is connection\noriented and provides a reliable byte-stream channel through which data\nflows between two end systems. UDP is connectionless and sends\nindependent packets of data from one end system to the other, without any\nguarantees about delivery. Recall also that when a client or server program\nimplements a protocol defined by an RFC, it should use the well-known\nport number associated with the protocol; conversely, when developing a\nproprietary application, the developer must be careful to avoid using such\nwell-known port numbers. (Port numbers were briefly discussed in Section\n2.1. They are covered in more detail in Chapter 3.)\nWe introduce UDP and TCP socket programming by way of a simple\nUDP application and a simple TCP application. We present the simple UDP\nand TCP applications in Python 3. We could have written the code in Java,\nC, or C++, but we chose Python mostly because Python clearly exposes the\nkey socket concepts. With Python there are fewer lines of code, and each\nline can be explained to the novice programmer without difficulty. But\nthere’s no need to be frightened if you are not familiar with Python. You\nshould be able to easily follow the code if you have experience\nprogramming in Java, C, or C++.\nIf you are interested in client-server programming with Java, you are\nencouraged to see the Companion Website for this textbook; in fact, you can\nfind there all the examples in this section (and associated labs) in Java. For\nreaders who are interested in client-server programming in C, there are\nwe will discuss IPv4 in Chapter 4.) The second parameter indicates that the\nsocket is of type SOCK_DGRAM, which means it is a UDP socket (rather\nthan a TCP socket). Note that we are not specifying the port number of the\nclient socket when we create it; we are instead letting the operating system\ndo this for us. Now that the client process’s door has been created, we will\nwant to create a message to send through the door.\nmessage = input(’Input lowercase sentence:’)\ninput() is a built-in function in Python. When this command is executed,\nthe user at the client is prompted with the words “Input lowercase\nsentence:” The user then uses her keyboard to input a line, which is put into\nthe variable message. Now that we have a socket and a message, we will\nwant to send the message through the socket to the destination host.\nclientSocket.sendto(message.encode(), (serverName,\n serverPort))\nIn the above line, we first convert the message from string type to byte type,\nas we need to send bytes into a socket; this is done with the encode()\nmethod. The method sendto() attaches the destination address\n(serverName, serverPort) to the message and sends the resulting\npacket into the process’s socket, clientSocket. (As mentioned earlier,\nthe source address is also attached to the packet, although this is done\nautomatically rather than explicitly by the code.) Sending a client-to-server\nmessage via a UDP socket is that simple! After sending the packet, the\nclient waits to receive data from the server.\nmodifiedMessage, serverAddress =\nclientSocket.recvfrom(2048)\nWith the above line, when a packet arrives from the Internet at the client’s\nsocket, the packet’s data is put into the variable modifiedMessage and\nthe packet’s source address is put into the variable serverAddress. The\nvariable serverAddress contains both the server’s IP address and the\nserver’s port number. The program UDPClient doesn’t actually need this\nserver address information, since it already knows the server address from\nthe outset; but this line of Python provides the server address nevertheless.\nThe method recvfrom also takes the buffer size 2048 as input. (This\nbuffer size works for most purposes.)\nprint(modifiedMessage.decode())\nThis line prints out modifiedMessage on the user’s display, after converting\nthe message from bytes to string. It should be the original line that the user\ntyped, but now capitalized.\nclientSocket.close()\nThis line closes the socket. The process then terminates.\nUDPServer.py\nLet’s now take a look at the server side of the application:\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET, SOCK_DGRAM)\nserverSocket.bind((’’, serverPort))\nprint(”The server is ready to receive”)\nwhile True:\n    message, clientAddress =\nserverSocket.recvfrom(2048)\n    modifiedMessage = message.decode().upper()\n    serverSocket.sendto(modifiedMessage.encode(),\nclientAddress)\nNote that the beginning of UDPServer is similar to UDPClient. It also\nimports the socket module, also sets the integer variable serverPort to\n12000, and also creates a socket of type SOCK_DGRAM (a UDP socket).\nThe first line of code that is significantly different from UDPClient is:\nserverSocket.bind((’’, serverPort))\nThe above line binds (that is, assigns) the port number 12000 to the server’s\nsocket. Thus, in UDPServer, the code (written by the application developer)\nis explicitly assigning a port number to the socket. In this manner, when\nanyone sends a packet to port 12000 at the IP address of the server, that\npacket will be directed to this socket. UDPServer then enters a while loop;\nthe while loop will allow UDPServer to receive and process packets from\nclients indefinitely. In the while loop, UDPServer waits for a packet to\nmessage, clientAddress =\nserverSocket.recvfrom(2048)\nThis line of code is similar to what we saw in UDPClient. When a packet\narrives at the server’s socket, the packet’s data is put into the variable\nmessage and the packet’s source address is put into the variable\nclientAddress. The variable ­clientAddress contains both the client’s IP\naddress and the client’s port number. Here, UDPServer will make use of this\naddress information, as it provides a return address, similar to the return\naddress with ordinary postal mail. With this source address information, the\nserver now knows to where it should direct its reply.\nmodifiedMessage = message.decode().upper()\nThis line is the heart of our simple application. It takes the line sent by the\nclient and, after converting the message to a string, uses the method\nupper() to capitalize it.\nserverSocket.sendto(modifiedMessage.encode(),\nclientAddress)\nThis last line attaches the client’s address (IP address and port number) to\nthe capitalized message (after converting the string to bytes), and sends the\nresulting packet into the server’s socket. (As mentioned earlier, the server\naddress is also attached to the packet, although this is done automatically\nrather than explicitly by the code.) The Internet will then deliver the packet\nto this client address. After the server sends the packet, it remains in the\nwhile loop, waiting for another UDP packet to arrive (from any client\nrunning on any host).\nTo test the pair of programs, you run UDPClient.py on one host and\nUDPServer.py on another host. Be sure to include the proper hostname or\nIP address of the server in UDPClient.py. Next, you execute UDPServer.py,\nthe compiled server program, in the server host. This creates a process in\nthe server that idles until it is contacted by some client. Then you execute\nUDPClient.py, the compiled client program, in the client. This creates a\nprocess in the client. Finally, to use the application at the client, you type a\nsentence followed by a carriage return.\nTo develop your own UDP client-server application, you can begin by\nslightly modifying the client or server programs. For example, instead of\nconverting all the letters to uppercase, the server could count the number of\ntimes the letter s appears and return this number. Or you can modify the\nclient so that after receiving a capitalized sentence, the user can continue to\nsend more sentences to the server.\n2.7.2 Socket Programming with TCP\nUnlike UDP, TCP is a connection-oriented protocol. This means that before\nthe client and server can start to send data to each other, they first need to\nhandshake and establish a TCP connection. One end of the TCP connection\nis attached to the client socket and the other end is attached to a server\nsocket. When creating the TCP connection, we associate with it the client\nsocket address (IP address and port number) and the server socket address\n(IP address and port number). With the TCP connection established, when\none side wants to send data to the other side, it just drops the data into the\nTCP connection via its socket. This is different from UDP, for which the\nserver must attach a destination address to the packet before dropping it into\nthe socket.\nNow let’s take a closer look at the interaction of client and server\nprograms in TCP. The client has the job of initiating contact with the server.\nIn order for the server to be able to react to the client’s initial contact, the\nserver has to be ready. This implies two things. First, as in the case of UDP,\nthe TCP server must be running as a process before the client attempts to\ninitiate contact. Second, the server program must have a special door—\nmore precisely, a special socket—that welcomes some initial contact from a\nclient process running on an arbitrary host. Using our house/door analogy\nfor a process/socket, we will sometimes refer to the client’s initial contact as\n“knocking on the welcoming door.”\nWith the server process running, the client process can initiate a TCP\nconnection to the server. This is done in the client program by creating a\nTCP socket. When the client creates its TCP socket, it specifies the address\nof the welcoming socket in the server, namely, the IP address of the server\nhost and the port number of the socket. After creating its socket, the client\ninitiates a three-way handshake and establishes a TCP connection with the\nserver. The three-way handshake, which takes place within the transport\nlayer, is completely invisible to the client and server programs.\nDuring the three-way handshake, the client process knocks on the\nwelcoming door of the server process. When the server “hears” the\nknocking, it creates a new door—more precisely, a new socket that is\ndedicated to that particular ­client. In our example below, the welcoming\ndoor is a TCP socket object that we call ­serverSocket; the newly\ncreated socket dedicated to the client making the connection is called\nconnectionSocket. Students who are encountering TCP sockets for\nthe first time sometimes confuse the welcoming socket (which is the initial\npoint of contact for all clients wanting to communicate with the server), and\neach newly created server-side connection socket that is subsequently\ncreated for communicating with each client.\nFrom the application’s perspective, the client’s socket and the server’s\nconnection socket are directly connected by a pipe. As shown in Figure\n2.28, the client process can send arbitrary bytes into its socket, and TCP\nguarantees that the server process will receive (through the connection\nsocket) each byte in the order sent. TCP thus provides a reliable service\nbetween the client and server processes. Furthermore, just as people can go\nin and out the same door, the client process not only sends bytes into but\nalso receives bytes from its socket; similarly, the server process not only\nreceives bytes from but also sends bytes into its connection socket.\nFigure 2.28 ♦The TCPServer process has two sockets\nWe use the same simple client-server application to demonstrate socket\nprogramming with TCP: The client sends one line of data to the server, the\nserver capitalizes the line and sends it back to the client. Figure 2.29\nhighlights the main socket-related activity of the client and server that\ncommunicate over the TCP transport service.\nFigure 2.29 ♦The client-server application using TCP\nTCPClient.py\nHere is the code for the client side of the application:\nfrom socket import *\nserverName = ’servername’\nserverPort = 12000\nclientSocket = socket(AF_INET, SOCK_STREAM)\nclientSocket.connect((serverName,serverPort))\nsentence = input(’Input lowercase sentence:’)\nclientSocket.send(sentence.encode())\nmodifiedSentence = clientSocket.recv(1024)\nprint(’From Server: ’, modifiedSentence.decode()) \nclientSocket.close()\nLet’s now take a look at the various lines in the code that differ significantly\nfrom the UDP implementation. The first such line is the creation of the\nclient socket.\nclientSocket = socket(AF_INET, SOCK_STREAM)\nThis line creates the client’s socket, called clientSocket. The first\nparameter again indicates that the underlying network is using IPv4. The\nsecond parameter indicates that the socket is of type SOCK_STREAM,\nwhich means it is a TCP socket (rather than a UDP socket). Note that we are\nagain not specifying the port number of the client socket when we create it;\nwe are instead letting the operating system do this for us. Now the next line\nof code is very different from what we saw in UDPClient:\nclientSocket.connect((serverName,serverPort))\nRecall that before the client can send data to the server (or vice versa) using\na TCP socket, a TCP connection must first be established between the client\nand server. The above line initiates the TCP connection between the client\nand server. The parameter of the connect() method is the address of the\nserver side of the connection. After this line of code is executed, the three-\nway handshake is performed and a TCP connection is established between\nthe client and server.\nsentence = input(’Input lowercase sentence:’)\nAs with UDPClient, the above obtains a sentence from the user. The string\nsentence continues to gather characters until the user ends the line by\ntyping a carriage return. The next line of code is also very different from\nclientSocket.send(sentence.encode())\nThe above line sends the sentence through the client’s socket and into\nthe TCP connection. Note that the program does not explicitly create a\npacket and attach the destination address to the packet, as was the case with\nUDP sockets. Instead the client program simply drops the bytes in the string\nsentence into the TCP connection. The client then waits to receive bytes\nfrom the server.\nmodifiedSentence = clientSocket.recv(2048)\nWhen characters arrive from the server, they get placed into the string\nmodifiedSentence. \nmodifiedSentence until the line ends with a carriage return character.\nAfter printing the capitalized sentence, we close the client’s socket:\nclientSocket.close()\nThis last line closes the socket and, hence, closes the TCP connection\nbetween the client and the server. It causes TCP in the client to send a TCP\nmessage to TCP in the server (see Section 3.5).\nTCPServer.py\nNow let’s take a look at the server program.\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET,SOCK_STREAM)\nserverSocket.bind((’’,serverPort))\nserverSocket.listen(1)\nprint(’The server is ready to receive’)\nwhile True:\n    connectionSocket, addr = serverSocket.accept()\nconnectionSocket.recv(1024).decode()\n    capitalizedSentence = sentence.upper()\n    connectionSocket.send(capitalizedSentence.enco\n    connectionSocket.close()\nLet’s now take a look at the lines that differ significantly from UDPServer\nand TCPClient. As with TCPClient, the server creates a TCP socket with:\nserverSocket=socket(AF_INET,SOCK_STREAM)\nSimilar to UDPServer, we associate the server port number, serverPort,\nwith this socket:\nserverSocket.bind((’’,serverPort))\nBut with TCP, serverSocket will be our welcoming socket. After\nestablishing this welcoming door, we will wait and listen for some client to\nknock on the door:\nserverSocket.listen(1)\nThis line has the server listen for TCP connection requests from the client.\nThe parameter specifies the maximum number of queued connections (at\nconnectionSocket, addr = serverSocket.accept()\nWhen a client knocks on this door, the program invokes the accept()\nmethod for serverSocket, which creates a new socket in the server, called ­-\nconnectionSocket, dedicated to this particular client. The client and\nserver then complete the handshaking, creating a TCP connection between\nthe client’s clientSocket and the server’s connectionSocket.\nWith the TCP connection established, the client and server can now send\nbytes to each other over the connection. With TCP, all bytes sent from one\nside are only guaranteed to arrive at the other side but also guaranteed to\narrive in order.\nconnectionSocket.close()\nIn this program, after sending the modified sentence to the client, we close\nthe connection socket. But since serverSocket remains open, another\nclient can now knock on the door and send the server a sentence to modify.\nThis completes our discussion of socket programming in TCP. You are\nencouraged to run the two programs in two separate hosts, and also to\nmodify them to achieve slightly different goals. You should compare the\nUDP program pair with the TCP program pair and see how they differ. You\nshould also do many of the socket programming assignments described at\nthe ends of Chapter 2 and 5. Finally, we hope someday, after mastering\nthese and more advanced socket programs, you will write your own popular\nnetwork application, become very rich and famous, and remember the\nauthors of this textbook!\n2.8 Summary\nIn this chapter, we’ve studied the conceptual and the implementation\naspects of network applications. We’ve learned about the ubiquitous client-\nserver architecture adopted by many Internet applications and seen its use in\nthe HTTP, SMTP, and DNS protocols. We’ve studied these important\napplication-level protocols, and their corresponding associated applications\n(the Web, file transfer, e-mail, and DNS) in some detail. We’ve learned\nabout the P2P architecture and contrasted it with the client-server\narchitecture. We’ve also learned about streaming video, and how modern\nvideo distribution systems leverage CDNs. We’ve examined how the socket\nAPI can be used to build network applications. We’ve walked through the\nuse of sockets for connection-oriented (TCP) and connectionless (UDP)\nend-to-end transport services. The first step in our journey down the layered\nnetwork architecture is now complete!\nAt the very beginning of this book, in Section 1.1, we gave a rather\nvague, bare-bones definition of a protocol: “the format and the order of\nmessages exchanged between two or more communicating entities, as well\nas the actions taken on the transmission and/or receipt of a message or other\nevent.” The material in this chapter, and in particular our detailed study of\nthe HTTP, SMTP, and DNS protocols, has now added considerable\nsubstance to this definition. Protocols are a key concept in networking; our\nstudy of application protocols has now given us the opportunity to develop\na more intuitive feel for what protocols are all about.\nIn Section 2.1, we described the service models that TCP and UDP\noffer to applications that invoke them. We took an even closer look at these\nservice models when we developed simple applications that run over TCP\nand UDP in Section 2.7. However, we have said little about how TCP and\nUDP provide these service models. For example, we know that TCP\nprovides a reliable data service, but we haven’t said yet how it does so. In\nthe next chapter, we’ll take a careful look at not only the what, but also the\nhow and why of transport protocols.\nEquipped with knowledge about Internet application structure and\napplication-level protocols, we’re now ready to head further down the\nprotocol stack and examine the transport layer in Chapter 3.\nHomework Problems and Questions\nChapter 2 Review Questions\nSECTION 2.1\nR1. List five nonproprietary Internet applications and the application-\nlayer protocols that they use.\nR2. What is the difference between network architecture and application\narchitecture?\nR3. For a communication session between a pair of processes, which\nprocess is the client and which is the server?\nR4. Why are the terms client and server still used in peer-to-peer\napplications?\nR5. What information is used by a process running on one host to identify\na process running on another host?\nR6. What is the role of HTTP in a network application? What other\ncomponents are needed to complete a Web application?\nR7. Referring to Figure 2.4, we see that none of the applications listed in\nFigure 2.4 requires both no data loss and timing. Can you conceive of\nan application that requires no data loss and that is also highly time-\nR8. List the four broad classes of services that a transport protocol can\nprovide. For each of the service classes, indicate if either UDP or\nTCP (or both) provides such a service.\nR9. Recall that TCP can be enhanced with TLS to provide process-to-\nprocess security services, including encryption. Does TLS operate at\nthe transport layer or the application layer? If the application\ndeveloper wants TCP to be enhanced with TLS, what does the\ndeveloper have to do?\nSECTIONS 2.2-2.5\nR10. What is meant by a handshaking protocol?\nR11. What does a stateless protocol mean? Is IMAP stateless? What about\nR12. How can websites keep track of users? Do they always need to use\nR13. Describe how Web caching can reduce the delay in receiving a\nrequested object. Will Web caching reduce the delay for all objects\nrequested by a user or for only some of the objects? Why?\nR14. Telnet into a Web server and send a multiline request message.\nInclude in the request message the If-modified-since: header\nline to force a response message with the 304 Not Modified\nstatus code.\nR15. Are there any constraints on the format of the HTTP body? What\nabout the email message body sent with SMTP? How can arbitrary\ndata be transmitted over SMTP?\nR16. Suppose Alice, with a Web-based e-mail account (such as Hotmail or\nGmail), sends a message to Bob, who accesses his mail from his mail\nserver using IMAP. Discuss how the message gets from Alice’s host\nto Bob’s host. Be sure to list the series of application-layer protocols\nthat are used to move the message between the two hosts.\nR17. Print out the header of an e-mail message you have recently received.\nHow many Received: header lines are there? Analyze each of the\nheader lines in the message.\nR18. What is the HOL blocking issue in HTTP/1.1? How does HTTP/2\nattempt to solve it?\nR19. Why are MX records needed? Would it not be enough to use a\nCNAME record? (Assume the email client looks up email addresses\nthrough a Type A query and that the target host only runs an email\nR20. What is the difference between recursive and iterative DNS queries?\nSECTION 2.5\nR21. Under what circumstances is file downloading through P2P much\nfaster than through a centralized client-server approach? Justify your\nanswer using Equation 2.2.\nR22. Consider a new peer Alice that joins BitTorrent without possessing\nany chunks. Without any chunks, she cannot become a top-four\nuploader for any of the other peers, since she has nothing to upload.\nHow then will Alice get her first chunk?\nR23. Assume a BitTorrent tracker suddenly becomes unavailable. What are\nits consequences? Can files still be downloaded?\nSECTION 2.6\nR24. CDNs typically adopt one of two different server placement\nphilosophies. Name and briefly describe them.\nR25. Besides network-related considerations such as delay, loss, and\nbandwidth performance, there are other important factors that go into\ndesigning a CDN server selection strategy. What are they?\nSECTION 2.7\nR26. In Section 2.7, the UDP server described needed only one socket,\nwhereas the TCP server needed two sockets. Why? If the TCP server\nwere to support n simultaneous connections, each from a different\nclient host, how many sockets would the TCP server need?\nR27. For the client-server application over TCP described in Section 2.7,\nwhy must the server program be executed before the client program?\nFor the client-server application over UDP, why may the client\nprogram be executed before the server program?\nP1. True or false?\na. A user requests a Web page that consists of some text and three\nimages. For this page, the client will send one request message\nand receive four response messages.\nb. Two distinct Web pages (for example,\nwww.mit.edu/research.html and\nwww.mit.edu/students.html) can be sent over the same\npersistent connection.\nc. With nonpersistent connections between browser and origin\nserver, it is possible for a single TCP segment to carry two\ndistinct HTTP request messages.\nd. The Date: header in the HTTP response message indicates\nwhen the object in the response was last modified.\ne. HTTP response messages never have an empty message body.\nP2. SMS, iMessage, Wechat, and WhatsApp are all smartphone real-time\nmessaging systems. After doing some research on the Internet, for\neach of these systems write one paragraph about the protocols they\nuse. Then write a paragraph explaining how they differ.\nP3. Assume you open a browser and enter\nhttp://yourbusiness.com/about.html in the address bar.\nWhat happens until the webpage is ­displayed? Provide details about\nthe protocol(s) used and a high-level description of the messages\nP4. Consider the following string of ASCII characters that were captured\nby Wireshark when the browser sent an HTTP GET message (i.e., this\nis the actual content of an HTTP GET message). The characters <cr>\n<lf> are carriage return and line-feed characters (that is, the italized\ncharacter string <cr> in the text below represents the single carriage-\nreturn character that was contained at that point in the HTTP header).\nAnswer the following questions, indicating where in the HTTP GET\nmessage below you find the answer.\nmakes use of the ICMP protocol and is summarized at the end of Chapter 5.\nIt is highly recommended that students complete several, if not all, of these\nassignments. Students can find full details of these assignments, as well as\nwww.pearsonglobaleditions.com.\nAssignment 1: Web Server\nIn this assignment, you will develop a simple Web server in Python that is\ncapable of processing only one request. Specifically, your Web server will\n(i) create a connection socket when contacted by a client (browser); (ii)\nreceive the HTTP request from this connection; (iii) parse the request to\ndetermine the specific file being requested; (iv) get the requested file from\nthe server’s file system; (v) create an HTTP response message consisting of\nthe requested file preceded by header lines; and (vi) send the response over\nthe TCP connection to the requesting browser. If a browser requests a file\nthat is not present in your server, your server should return a “404 Not\nFound” error message.\nIn the Companion Website, we provide the skeleton code for your\nserver. Your job is to complete the code, run your server, and then test your\nserver by sending requests from browsers running on different hosts. If you\nrun your server on a host that already has a Web server running on it, then\nyou should use a different port than port 80 for your Web server.\nAssignment 2: UDP Pinger\nIn this programming assignment, you will write a client ping program in\nPython. Your client will send a simple ping message to a server, receive a\ncorresponding pong message back from the server, and determine the delay\nbetween when the client sent the ping message and received the pong\nmessage. This delay is called the Round Trip Time (RTT). The functionality\nprovided by the client and server is similar to the functionality provided by\nstandard ping program available in modern operating systems. However,\nstandard ping programs use the Internet Control Message Protocol (ICMP)\n(which we will study in Chapter 5). Here we will create a nonstandard (but\nsimple!) UDP-based ping program.\nYour ping program is to send 10 ping messages to the target server over\nUDP. For each message, your client is to determine and print the RTT when\nthe corresponding pong message is returned. Because UDP is an unreliable\nprotocol, a packet sent by the client or server may be lost. For this reason,\nthe client cannot wait indefinitely for a reply to a ping message. You should\nhave the client wait up to one second for a reply from the server; if no reply\nis received, the client should assume that the packet was lost and print a\nmessage accordingly.\nIn this assignment, you will be given the complete code for the server\n(available in the Companion Website). Your job is to write the client code,\nwhich will be very similar to the server code. It is recommended that you\nfirst study carefully the server code. You can then write your client code,\nliberally cutting and pasting lines from the server code.\nAssignment 3: Mail Client\nThe goal of this programming assignment is to create a simple mail client\nthat sends e-mail to any recipient. Your client will need to establish a TCP\nconnection with a mail server (e.g., a Google mail server), dialogue with the\nmail server using the SMTP protocol, send an e-mail message to a recipient\n(e.g., your friend) via the mail server, and finally close the TCP connection\nwith the mail server.\nFor this assignment, the Companion Website provides the skeleton code\nfor your client. Your job is to complete the code and test your client by\nsending e-mail to different user accounts. You may also try sending through\ndifferent servers (for example, through a Google mail server and through\nyour university mail server).\nAssignment 4: Web Proxy\nIn this assignment, you will develop a Web proxy. When your proxy\nreceives an HTTP request for an object from a browser, it generates a new\nHTTP request for the same object and sends it to the origin server. When\nthe proxy receives the corresponding HTTP response with the object from\nthe origin server, it creates a new HTTP response, including the object, and\nsends it to the client.\nFor this assignment, the Companion Website provides the skeleton code\nfor the proxy server. Your job is to complete the code, and then test it by\nhaving different browsers request Web objects via your proxy.\nWireshark Lab: HTTP\nHaving gotten our feet wet with the Wireshark packet sniffer in Lab 1,\nwe’re now ready to use Wireshark to investigate protocols in operation. In\nthis lab, we’ll explore several aspects of the HTTP protocol: the basic\nGET/reply interaction, HTTP message formats, retrieving large HTML\nfiles, retrieving HTML files with embedded URLs, persistent and non-\npersistent connections, and HTTP authentication and security.\nAs is the case with all Wireshark labs, the full description of this lab is\navailable at this book’s Web site, www.pearsonglobaleditions.com.\nWireshark Lab: DNS\nIn this lab, we take a closer look at the client side of the DNS, the protocol\nthat translates Internet hostnames to IP addresses. Recall from Section 2.5\nthat the client’s role in the DNS is relatively simple—a client sends a query\nto its local DNS server and receives a response back. Much can go on under\nthe covers, invisible to the DNS clients, as the hierarchical DNS servers\ncommunicate with each other to either recursively or iteratively resolve the\nclient’s DNS query. From the DNS client’s standpoint, however, the\nprotocol is quite simple—a query is formulated to the local DNS server and\na response is received from that server. We observe DNS in action in this\nAs is the case with all Wireshark labs, the full description of this lab is\navailable at this book’s Web site, www.pearsonglobaleditions.com.\nAN INTERVIEW WITH…\nTim Berners-Lee\nSir Tim Berners-Lee is known as the inventor of the World\nWide Web. In 1989, while working as a fellow at CERN, he\nproposed an Internet-based distributed information\nmanagement system including the original version of the\nHTTP protocol. In the same year he successfully\nimplemented his design on a client and server. He received\nthe 2016 Turing award for “inventing the World Wide Web, the\nfirst Web browser, and the fundamental protocols and\nalgorithms allowing the Web to scale.” He is the Co-Founder\nof the World Wide Web Foundation, and currently is a\nProfessorial Fellow of Computer Science at the University of\nOxford and a professor at CSAIL at MIT.\nCourtesy of Tim Berners-Lee\nYou originally studied physics. How is\nnetworking similar to physics?\nWhen you study physics, you imagine what rules\nof behavior on the very small scale could possibly\ngive rise to the large-scale world as we see it.\nWhen you design a global system like the Web,\nyou try to invent rules of behavior of Web pages\nand links and things that could in the large create a\nlarge-scale world as we would like it. One is\nanalysis and the other synthesis, but they are very\nWhat influenced you to specialize in\nnetworking?\nAfter my physics degree, the telecommunications\nresearch companies seemed to be the most\ninteresting places. The microprocessor had just\ncome out, and telecommunications was switching\nvery fast from hardwired logic to microprocessor-\nbased systems. It was very exciting.\nWhat is the most challenging part of your\nWhen two groups disagree strongly about\nsomething, but want in the end to achieve a\ncommon goal, finding exactly what they each\nmean and where the misunderstandings are can be\nvery demanding. The chair of any working group\nknows that. However, this is what it takes to make\nprogress toward consensus on a large scale.\nWhat people have inspired you\nprofessionally?\nMy parents, who were involved in the early days\nof computing, gave me a fascination with the\nwhole subject. Mike Sendall and Peggie Rimmer,\nfor whom I worked at various times at CERN are\namong the people who taught me and encouraged\nme. I later learned to admire the people, including\nVanevar Bush, Doug Englebart, and Ted Nelson,\nwho had had similar dreams in their time but had\nnot had the benefit of the existence for PCs and the\nInternet to be able to realize it.\nTransport Layer\nResiding between the application and network\nlayers, the transport layer is a central piece of\nthe layered network architecture. It has the\ncritical role of providing communication\nservices directly to the application processes\nrunning on different hosts. The pedagogic\napproach we take in this chapter is to\nalternate between discussions of transport-\nlayer principles and discussions of how these\nimplemented in \nprotocols; as usual, particular emphasis will\nbe given to Internet protocols, in particular\nthe TCP and UDP transport-layer protocols.\nWe’ll begin by discussing the relationship\nbetween the transport and network layers.\nThis sets the stage for examining the first\ncritical function of the transport layer—\nextending the network layer’s delivery service\nbetween two end systems to a delivery service\nbetween two application-layer processes\nrunning on the end systems. We’ll illustrate\nthis function in our coverage of the Internet’s\nconnectionless transport protocol, UDP.\nWe’ll then return to principles and\nconfront one of the most fundamental\nproblems in computer networking—how two\nentities can communicate reliably over a\nmedium that may lose and corrupt data.\nThrough a series of increasingly complicated\n(and realistic!) scenarios, we’ll build up an\narray of techniques that transport protocols\nuse to solve this problem. We’ll then show\nhow these principles are embodied in TCP,\nthe Internet’s connection-oriented transport\nWe’ll next move on to a second\nfundamentally \nnetworking—controlling the transmission rate\nof transport-layer entities in order to avoid, or\nrecover from, congestion within the network.\nWe’ll consider the causes and consequences\nof congestion, as well as commonly used\ncongestion-control \ntechniques. \nobtaining a solid understanding of the issues\nbehind congestion control, we’ll study TCP’s\napproach to congestion control.\n3.1 Introduction and Transport-Layer Services\nIn the previous two chapters, we touched on the role of the transport layer\nand the services that it provides. Let’s quickly review what we have already\nlearned about the transport layer.\nA transport-layer protocol provides for logical communication\nbetween application processes running on different hosts. By logical\ncommunication, we mean that from an application’s perspective, it is as if\nthe hosts running the processes were directly connected; in reality, the hosts\nmay be on opposite sides of the planet, connected via numerous routers and\na wide range of link types. Application processes use the logical\ncommunication provided by the transport layer to send messages to each\nother, free from the worry of the details of the physical infrastructure used\nto carry these messages. Figure 3.1 illustrates the notion of logical\ncommunication.\nFigure 3.1 ♦The transport layer provides logical rather than\nphysical communication between application\nAs shown in Figure 3.1, transport-layer protocols are implemented in\nthe end systems but not in network routers. On the sending side, the\ntransport layer converts the application-layer messages it receives from a\nsending application process into transport-layer packets, known as\ntransport-layer segments in Internet terminology. This is done by (possibly)\nbreaking the application messages into smaller chunks and adding a\ntransport-layer header to each chunk to create the transport-layer segment.\nThe transport layer then passes the segment to the network layer at the\nsending end system, where the segment is encapsulated within a network-\nlayer packet (a datagram) and sent to the destination. It’s important to note\nthat network routers act only on the network-layer fields of the datagram;\nthat is, they do not examine the fields of the transport-layer segment\nencapsulated with the datagram. On the receiving side, the network layer\nextracts the transport-layer segment from the datagram and passes the\nsegment up to the transport layer. The transport layer then processes the\nreceived segment, making the data in the segment available to the receiving\napplication.\nMore than one transport-layer protocol may be available to network\napplications. For example, the Internet has two protocols—TCP and UDP.\nEach of these protocols provides a different set of transport-layer services to\nthe invoking application.\n3.1.1 Relationship Between Transport and Network\nRecall that the transport layer lies just above the network layer in the\nprotocol stack. Whereas a transport-layer protocol provides logical\ncommunication between processes running on different hosts, a network-\nlayer protocol provides logical-communication between hosts. This\ndistinction is subtle but important. Let’s examine this distinction with the\naid of a household analogy.\nConsider two houses, one on the East Coast and the other on the West\nCoast, with each house being home to a dozen kids. The kids in the East\nCoast household are cousins of the kids in the West Coast household. The\nkids in the two households love to write to each other—each kid writes each\ncousin every week, with each letter delivered by the traditional postal\nservice in a separate envelope. Thus, each household sends 144 letters to the\nother household every week. (These kids would save a lot of money if they\nhad e-mail!) In each of the households, there is one kid—Ann in the West\nCoast house and Bill in the East Coast house—responsible for mail\ncollection and mail distribution. Each week Ann visits all her brothers and\nsisters, collects the mail, and gives the mail to a postal-service mail carrier,\nwho makes daily visits to the house. When letters arrive at the West Coast\nhouse, Ann also has the job of distributing the mail to her brothers and\nsisters. Bill has a similar job on the East Coast.\nIn this example, the postal service provides logical communication\nbetween the two houses—the postal service moves mail from house to\nhouse, not from person to person. On the other hand, Ann and Bill provide\nlogical communication among the cousins—Ann and Bill pick up mail\nfrom, and deliver mail to, their brothers and sisters. Note that from the\ncousins’ perspective, Ann and Bill are the mail service, even though Ann\nand Bill are only a part (the end-system part) of the end-to-end delivery\nprocess. This household example serves as a nice analogy for explaining\nhow the transport layer relates to the network layer:\napplication messages = letters in envelopes\nprocesses = cousins\nhosts (also called end systems) = houses\ntransport-layer protocol = Ann and Bill\nnetwork-layer protocol = postal service (including mail carriers)\nContinuing with this analogy, note that Ann and Bill do all their work\nwithin their respective homes; they are not involved, for example, in sorting\nmail in any intermediate mail center or in moving mail from one mail center\nto another. Similarly, transport-layer protocols live in the end systems.\nWithin an end system, a transport protocol moves messages from\napplication processes to the network edge (that is, the network layer) and\nvice versa, but it doesn’t have any say about how the messages are moved\nwithin the network core. In fact, as illustrated in Figure 3.1, intermediate\nrouters neither act on, nor recognize, any information that the transport\nlayer may have added to the application messages.\nContinuing with our family saga, suppose now that when Ann and Bill\ngo on vacation, another cousin pair—say, Susan and Harvey—substitute for\nthem and provide the household-internal collection and delivery of mail.\nUnfortunately for the two families, Susan and Harvey do not do the\ncollection and delivery in exactly the same way as Ann and Bill. Being\nyounger kids, Susan and Harvey pick up and drop off the mail less\nfrequently and occasionally lose letters (which are sometimes chewed up by\nthe family dog). Thus, the cousin-pair Susan and Harvey do not provide the\nsame set of services (that is, the same service model) as Ann and Bill. In an\nanalogous manner, a computer network may make available multiple\ntransport protocols, with each protocol offering a different service model to\napplications.\nThe possible services that Ann and Bill can provide are clearly\nconstrained by the possible services that the postal service provides. For\nexample, if the postal service doesn’t provide a maximum bound on how\nlong it can take to deliver mail between the two houses (for example, three\ndays), then there is no way that Ann and Bill can guarantee a maximum\ndelay for mail delivery between any of the cousin pairs. In a similar manner,\nthe services that a transport protocol can provide are often constrained by\nthe service model of the underlying network-layer protocol. If the network-\nlayer protocol cannot provide delay or bandwidth guarantees for transport-\nlayer segments sent between hosts, then the transport-layer protocol cannot\nprovide delay or bandwidth guarantees for application messages sent\nbetween processes.\nNevertheless, certain services can be offered by a transport protocol\neven when the underlying network protocol doesn’t offer the corresponding\nservice at the network layer. For example, as we’ll see in this chapter, a\ntransport protocol can offer reliable data transfer service to an application\neven when the underlying network protocol is unreliable, that is, even when\nthe network protocol loses, garbles, or duplicates packets. As another\nexample (which we’ll explore in Chapter 8 when we discuss network\nsecurity), a transport protocol can use encryption to guarantee that\napplication messages are not read by intruders, even when the network\nlayer cannot guarantee the confidentiality of transport-layer segments.\n3.1.2 Overview of the Transport Layer in the\nRecall that the Internet makes two distinct transport-layer protocols\navailable to the application layer. One of these protocols is UDP (User\nDatagram Protocol), which provides an unreliable, connectionless service to\nthe invoking application. The second of these protocols is TCP\n(Transmission Control Protocol), which provides a reliable, connection-\noriented service to the invoking application. When designing a network\napplication, the application developer must specify one of these two\ntransport protocols. As we saw in Section 2.7, the application developer\nselects between UDP and TCP when creating sockets.\nTo simplify terminology, we refer to the transport-layer packet as a\nsegment. We mention, however, that the Internet literature (for example, the\nRFCs) also refers to the transport-layer packet for TCP as a segment but\noften refers to the packet for UDP as a datagram. However, this same\nInternet literature also uses the term datagram for the network-layer packet!\nFor an introductory book on computer networking such as this, we believe\nthat it is less confusing to refer to both TCP and UDP packets as segments,\nand reserve the term datagram for the network-layer packet.\nBefore proceeding with our brief introduction of UDP and TCP, it will\nbe useful to say a few words about the Internet’s network layer. (We’ll learn\nabout the network layer in detail in Chapters 4 and 5.) The Internet’s\nnetwork-layer protocol has a name—IP, for Internet Protocol. IP provides\nlogical communication between hosts. The IP service model is a best-effort\ndelivery service. This means that IP makes its “best effort” to deliver\nsegments between communicating hosts, but it makes no guarantees. In\nparticular, it does not guarantee segment delivery, it does not guarantee\norderly delivery of segments, and it does not guarantee the integrity of the\ndata in the segments. For these reasons, IP is said to be an unreliable\nservice. We also mention here that every host has at least one network-layer\naddress, a so-called IP address. We’ll examine IP addressing in detail in\nChapter 4; for this chapter we need only keep in mind that each host has an\nIP address.\nHaving taken a glimpse at the IP service model, let’s now summarize\nthe service models provided by UDP and TCP. The most fundamental\nresponsibility of UDP and TCP is to extend IP’s delivery service between\ntwo end systems to a delivery service between two processes running on the\nend systems. Extending host-to-host delivery to process-to-process delivery\nis called transport-layer multiplexing and demultiplexing. We’ll discuss\ntransport-layer multiplexing and demultiplexing in the next section. UDP\nand TCP also provide integrity checking by including error-detection fields\nin their segments’ headers. These two minimal transport-layer services—\nprocess-to-process data delivery and error checking—are the only two\nservices that UDP provides! In particular, like IP, UDP is an unreliable\nservice—it does not guarantee that data sent by one process will arrive\nintact (or at all!) to the destination process. UDP is discussed in detail in\nSection 3.3.\nTCP, on the other hand, offers several additional services to\napplications. First and foremost, it provides reliable data transfer. Using\nprogramming assignment in Chapter 2, you built a Web server that does just\nthis. For such a server, at any given time there may be many connection\nsockets (with different identifiers) attached to the same process.\nIf the client and server are using persistent HTTP, then throughout the\nduration of the persistent connection the client and server exchange HTTP\nmessages via the same server socket. However, if the client and server use\nnon-persistent HTTP, then a new TCP connection is created and closed for\nevery request/response, and hence a new socket is created and later closed\nfor every request/response. This frequent creating and closing of sockets\ncan severely impact the performance of a busy Web server (although a\nnumber of operating system tricks can be used to mitigate the problem).\nReaders interested in the operating system issues surrounding persistent and\nnon-persistent HTTP are encouraged to see [Nielsen 1997; Nahum 2002].\ntransport-layer \nmultiplexing \ndemultiplexing, let’s move on and discuss one of the Internet’s transport\nprotocols, UDP. In the next section, we’ll see that UDP adds little more to\nthe network-layer protocol than a multiplexing/demultiplexing service.\n3.3 Connectionless Transport: UDP\nIn this section, we’ll take a close look at UDP, how it works, and what it\ndoes. We encourage you to refer back to Section 2.1, which includes an\noverview of the UDP service model, and to Section 2.7.1, which discusses\nsocket programming using UDP.\nTo motivate our discussion about UDP, suppose you were interested in\ndesigning a no-frills, bare-bones transport protocol. How might you go\nabout doing this? You might first consider using a vacuous transport\nprotocol. In particular, on the sending side, you might consider taking the\nmessages from the application process and passing them directly to the\nnetwork layer; and on the receiving side, you might consider taking the\nmessages arriving from the network layer and passing them directly to the\napplication process. But as we learned in the previous section, we have to\ndo a little more than nothing! At the very least, the transport layer has to\nprovide a multiplexing/demultiplexing service in order to pass data between\nthe network layer and the correct application-level process.\nUDP, defined in [RFC 768], does just about as little as a transport\nprotocol can do. Aside from the multiplexing/demultiplexing function and\nsome light error checking, it adds nothing to IP. In fact, if the application\ndeveloper chooses UDP instead of TCP, then the application is almost\ndirectly talking with IP. UDP takes messages from the application process,\ndestination \nmultiplexing/demultiplexing service, adds two other small fields, and\npasses the resulting segment to the network layer. The network layer\nencapsulates the transport-layer segment into an IP datagram and then\nmakes a best-effort attempt to deliver the segment to the receiving host. If\nthe segment arrives at the receiving host, UDP uses the destination port\nnumber to deliver the segment’s data to the correct application process.\nNote that with UDP there is no handshaking between sending and receiving\ntransport-layer entities before sending a segment. For this reason, UDP is\nsaid to be connectionless.\nDNS is an example of an application-layer protocol that typically uses\nUDP. When the DNS application in a host wants to make a query, it\nconstructs a DNS query message and passes the message to UDP. Without\nperforming any handshaking with the UDP entity running on the destination\nend system, the host-side UDP adds header fields to the message and passes\nthe resulting segment to the network layer. The network layer encapsulates\nthe UDP segment into a datagram and sends the datagram to a name server.\nThe DNS application at the querying host then waits for a reply to its query.\nIf it doesn’t receive a reply (possibly because the underlying network lost\nthe query or the reply), it might try resending the query, try sending the\nquery to another name server, or inform the invoking application that it\ncan’t get a reply.\nNow you might be wondering why an application developer would ever\nchoose to build an application over UDP rather than over TCP. Isn’t TCP\nalways preferable, since TCP provides a reliable data transfer service, while\nUDP does not? The answer is no, as some applications are better suited for\nUDP for the following reasons:\nFiner application-level control over what data is sent, and when. Under\nUDP, as soon as an application process passes data to UDP, UDP will\npackage the data inside a UDP segment and immediately pass the\nsegment to the network layer. TCP, on the other hand, has a congestion-\ncontrol mechanism that throttles the transport-layer TCP sender when\none or more links between the source and destination hosts become\nexcessively congested. TCP will also continue to resend a segment until\nthe receipt of the segment has been acknowledged by the destination,\nregardless of how long reliable delivery takes. Since real-time\napplications often require a minimum sending rate, do not want to\noverly delay segment transmission, and can tolerate some data loss,\nTCP’s service model is not particularly well matched to these\napplications’ needs. As discussed below, these applications can use\nUDP and implement, as part of the application, any additional\nfunctionality that is needed beyond UDP’s no-frills segment-delivery\nNo connection establishment. As we’ll discuss later, TCP uses a three-\nway handshake before it starts to transfer data. UDP just blasts away\nwithout any formal preliminaries. Thus UDP does not introduce any\ndelay to establish a connection. This is probably the principal reason\nwhy DNS runs over UDP rather than TCP—DNS would be much\nslower if it ran over TCP. HTTP uses TCP rather than UDP, since\nreliability is critical for Web pages with text. But, as we briefly\ndiscussed in Section 2.2, the TCP connection-establishment delay in\nHTTP is an important contributor to the delays associated with\ndownloading Web documents. Indeed, the QUIC protocol (Quick UDP\nInternet Connection, [IETF QUIC 2020]), used in Google’s Chrome\nbrowser, uses UDP as its underlying transport protocol and implements\nreliability in an application-layer protocol on top of UDP. We’ll take a\ncloser look at QUIC in Section 3.8.\nNo connection state. TCP maintains connection state in the end systems.\nThis connection state includes receive and send buffers, congestion-\nof TCP. We learned in Chapter 2 that early versions of HTTP ran over TCP\nbut that more recent versions of HTTP run over UDP, providing their own\nerror control and congestion control (among other services) at the\napplication layer. Nevertheless, many important applications run over UDP\nrather than TCP. For example, UDP is used to carry network management\n(SNMP; see Section 5.7) data. UDP is preferred to TCP in this case, since\nnetwork management applications must often run when the network is in a\nstressed state—precisely when reliable, congestion-controlled data transfer\nis difficult to achieve. Also, as we mentioned earlier, DNS runs over UDP,\nthereby avoiding TCP’s connection-establishment delays.\nFigure 3.6 ♦Popular Internet applications and their underlying\ntransport protocols\nAs shown in Figure 3.6, both UDP and TCP are sometimes used today\nwith multimedia applications, such as Internet phone, real-time video\nconferencing, and streaming of stored audio and video. We just mention\nnow that all of these applications can tolerate a small amount of packet loss,\nso that reliable data transfer is not absolutely critical for the application’s\nsuccess. Furthermore, real-time applications, like Internet phone and video\nconferencing, react very poorly to TCP’s congestion control. For these\nreasons, developers of multimedia applications may choose to run their\napplications over UDP instead of TCP. When packet loss rates are low, and\nwith some organizations blocking UDP traffic for security reasons (see\nChapter 8), TCP becomes an increasingly attractive protocol for streaming\nmedia transport.\nAlthough commonly done today, running multimedia applications over\nUDP needs to be done with care. As we mentioned above, UDP has no\ncongestion control. But congestion control is needed to prevent the network\nfrom entering a congested state in which very little useful work is done. If\neveryone were to start streaming high-bit-rate video without using any\ncongestion control, there would be so much packet overflow at routers that\nvery few UDP packets would successfully traverse the source-to-destination\npath. Moreover, the high loss rates induced by the uncontrolled UDP\nsenders would cause the TCP senders (which, as we’ll see, do decrease their\nsending rates in the face of congestion) to dramatically decrease their rates.\nThus, the lack of congestion control in UDP can result in high loss rates\nbetween a UDP sender and receiver, and the crowding out of TCP sessions.\nMany researchers have proposed new mechanisms to force all sources,\nincluding UDP sources, to perform adaptive congestion control [Mahdavi\n1997; Floyd 2000; Kohler 2006: RFC 4340].\nBefore discussing the UDP segment structure, we mention that it is ­-\npossible for an application to have reliable data transfer when using UDP.\nThis can be done if reliability is built into the application itself (for\nChapter 6, we’ll examine error-detection and -correction techniques in\ngreater detail; these techniques allow the receiver to detect and possibly\ncorrect packet bit errors. For now, we need only know that these\ntechniques require that extra bits (beyond the bits of original data to be\ntransferred) be sent from the sender to the receiver; these bits will be\ngathered into the packet checksum field of the rdt2.0 data packet.\nReceiver feedback. Since the sender and receiver are typically executing\non different end systems, possibly separated by thousands of miles, the\nonly way for the sender to learn of the receiver’s view of the world (in\nthis case, whether or not a packet was received correctly) is for the\nreceiver to provide explicit feedback to the sender. The positive (ACK)\nhosts. Unlike the bulk data transfer applications discussed in Chapter 2,\nTelnet is an interactive application. We discuss a Telnet example here, as it\nstudy CSMA/CD in Chapter 6.\nFast Retransmit\nOne of the problems with timeout-triggered retransmissions is that the\ntimeout period can be relatively long. When a segment is lost, this long\ntimeout period forces the sender to delay resending the lost packet, thereby\nincreasing the end-to-end delay. Fortunately, the sender can often detect\npacket loss well before the timeout event occurs by noting so-called\nduplicate ACKs. A duplicate ACK is an ACK that reacknowledges a\nsegment for which the sender has already received an earlier\nthe client TCP. (We’ll see in Chapter 8 that the allocation of these\nbuffers and variables before completing the third step of the three-way\nhandshake makes TCP vulnerable to a denial-of-service attack known as\nSYN flooding.) This connection-granted segment also contains no\napplication-layer data. However, it does contain three important pieces\nof information in the segment header. First, the SYN bit is set to 1.\nexamples from Chapter 2). This causes TCP in the client to send a SYN\nsegment to TCP in the server. After having sent the SYN segment, the client\nTCP enters the SYN_SENT state. While in the SYN_SENT state, the client\nTCP waits for a segment from the server TCP that includes an\ndiscussed in Chapter 5.\nNow that we have a good understanding of TCP connection\nmanagement, let’s revisit the nmap port-scanning tool and examine more\nclosely how it works. To explore a specific TCP port, say port 6789, on a\ntarget host, nmap will send a TCP SYN segment with destination port 6789\nto that host. There are three possible outcomes:\nThe source host receives a TCP SYNACK segment from the target host.\nSince this means that an application is running with TCP port 6789 on\nthe target post, nmap returns “open.”\nWe’ve seen in our discussion of TCP’s three-way handshake that a server allocates\nand initializes connection variables and buffers in response to a received SYN. The\nserver then sends a SYNACK in response, and awaits an ACK segment from the client.\nIf the client does not send an ACK to complete the third step of this 3-way handshake,\neventually (often after a minute or more) the server will terminate the half-open\nconnection and reclaim the allocated resources.\nThis TCP connection management protocol sets the stage for a classic Denial of\nService (DoS) attack known as the SYN flood attack. In this attack, the attacker(s)\nsend a large number of TCP SYN segments, without completing the third handshake\nstep. With this deluge of SYN segments, the server’s connection resources become\nexhausted as they are allocated (but never used!) for half-open connections; legitimate\nclients are then denied service. Such SYN flooding attacks were among the first\ndocumented DoS attacks [CERT SYN 1996]. Fortunately, an effective defense known\nas SYN cookies [RFC 4987] are now deployed in most major operating systems. SYN\ncookies work as follows:\nWhen the server receives a SYN segment, it does not know if the segment is\ncoming from a legitimate user or is part of a SYN flood attack. So, instead of\ncreating a half-open TCP connection for this SYN, the server creates an initial TCP\nsequence number that is a complicated function (hash function) of source and\ndestination IP addresses and port numbers of the SYN segment, as well as a\nsecret number only known to the server. This carefully crafted initial sequence\nnumber is the so-called “cookie.” The server then sends the client a SYNACK\npacket with this special initial sequence number. Importantly, the server does not\nremember the cookie or any other state information corresponding to the SYN.\nA legitimate client will return an ACK segment. When the server receives this ACK,\nit must verify that the ACK corresponds to some SYN sent earlier. But how is this\ndone if the server maintains no memory about SYN segments? As you may have\nguessed, it is done with the cookie. Recall that for a legitimate ACK, the value in the\n(Firewalls are discussed in Chapter 8.)\nThe source receives nothing. This likely means that the SYN segment\nwas blocked by an intervening firewall and never reached the target\nNmap is a powerful tool that can “case the joint” not only for open TCP\nports, but also for open UDP ports, for firewalls and their configurations,\nand even for the versions of applications and operating systems. Most of\nthis is done by manipulating TCP connection-management segments. You\ncan download nmap from www.nmap.org.\nThis completes our introduction to error control and flow control in\nTCP. In Section 3.7, we’ll return to TCP and look at TCP congestion control\nin some depth. Before doing so, however, we first step back and examine\ncongestion-control issues in a broader context.\n3.6 Principles of Congestion Control\nIn the previous sections, we examined both the general principles and\nspecific TCP mechanisms used to provide for a reliable data transfer service\nin the face of packet loss. We mentioned earlier that, in practice, such loss\ntypically results from the overflowing of router buffers as the network\nbecomes congested. Packet retransmission thus treats a symptom of\nnetwork congestion (the loss of a specific transport-layer segment) but does\nnot treat the cause of network congestion—too many sources attempting to\nsend data at too high a rate. To treat the cause of network congestion,\nmechanisms are needed to throttle senders in the face of network\ncongestion.\nIn this section, we consider the problem of congestion control in a\ngeneral context, seeking to understand why congestion is a bad thing, how\nnetwork congestion is manifested in the performance received by upper-\nlayer applications, and various approaches that can be taken to avoid, or\nreact to, network congestion. This more general study of congestion control\nis appropriate since, as with reliable data transfer, it is high on our “top-ten”\nlist of fundamentally important problems in networking. The following\nsection contains a detailed study of TCP’s congestion-control algorithm.\n3.6.1 The Causes and the Costs of Congestion\nLet’s begin our general study of congestion control by examining three\nincreasingly complex scenarios in which congestion occurs. In each case,\nwe’ll look at why congestion occurs in the first place and at the cost of\ncongestion (in terms of resources not fully utilized and poor performance\nreceived by the end systems). We’ll not (yet) focus on how to react to, or\navoid, congestion but rather focus on the simpler issue of understanding\nwhat happens as hosts increase their transmission rate and the network\nbecomes congested.\nScenario 1: Two Senders, a Router with Infinite Buffers\nWe begin by considering perhaps the simplest congestion scenario possible:\nTwo hosts (A and B) each have a connection that shares a single hop\nbetween source and destination, as shown in Figure 3.43.\nFigure 3.43 ♦Congestion scenario 1: Two connections sharing a\nsingle hop with infinite buffers\nLet’s assume that the application in Host A is sending data into the\nconnection (for example, passing data to the transport-level protocol via a\nsocket) at an average rate of λ  bytes/sec. These data are original in the\nsense that each unit of data is sent into the socket only once. The underlying\ntransport-level protocol is a simple one. Data is encapsulated and sent; no\nerror recovery (e.g., retransmission), flow control, or congestion control is\nperformed. Ignoring the additional overhead due to adding transport- and\nlower-layer header information, the rate at which Host A offers traffic to the\nrouter in this first scenario is thus λ  bytes/sec. Host B operates in a similar\nmanner, and we assume for simplicity that it too is sending at a rate of λ\nbytes/sec. Packets from Hosts A and B pass through a router and over a\nshared outgoing link of capacity R. The router has buffers that allow it to\nstore incoming packets when the packet-arrival rate exceeds the outgoing\nlink’s capacity. In this first scenario, we assume that the router has an\ninfinite amount of buffer space.\nFigure 3.44 plots the performance of Host A’s connection under this\nfirst scenario. The left graph plots the per-connection throughput (number\nof bytes per second at the receiver) as a function of the connection-sending\nrate. For a sending rate between 0 and R/2, the throughput at the receiver\nequals the sender’s sending rate—everything sent by the sender is received\nat the receiver with a finite delay. When the sending rate is above R/2,\nhowever, the throughput is only R/2. This upper limit on throughput is a\nconsequence of the sharing of link capacity between two connections. The\nlink simply cannot deliver packets to a receiver at a steady-state rate that\nexceeds R/2. No matter how high Hosts A and B set their sending rates, they\nwill each never see a throughput higher than R/2.\nFigure 3.44 ♦Congestion scenario 1: Throughput and delay as a\nfunction of host sending rate\nAchieving a per-connection throughput of R/2 might actually appear to\nbe a good thing, because the link is fully utilized in delivering packets to\ntheir destinations. The right-hand graph in Figure 3.44, however, shows the\nconsequence of operating near link capacity. As the sending rate approaches\nR/2 (from the left), the average delay becomes larger and larger. When the\nsending rate exceeds R/2, the average number of queued packets in the\nrouter is unbounded, and the average delay between source and destination\nbecomes infinite (assuming that the connections operate at these sending\nrates for an infinite period of time and there is an infinite amount of\nbuffering available). Thus, while operating at an aggregate throughput of\nnear R may be ideal from a throughput standpoint, it is far from ideal from a\ndelay standpoint. Even in this (extremely) idealized scenario, we’ve already\nfound one cost of a congested network—large queuing delays are\nexperienced as the packet-arrival rate nears the link capacity.\nScenario 2: Two Senders and a Router with Finite Buffers\nLet’s now slightly modify scenario 1 in the following two ways (see Figure\n3.45). First, the amount of router buffering is assumed to be finite. A\nconsequence of this real-world assumption is that packets will be dropped\nwhen arriving to an already-full buffer. Second, we assume that each\nconnection is reliable. If a packet containing a transport-level segment is\ndropped at the router, the sender will eventually retransmit it. Because\npackets can be retransmitted, we must now be more careful with our use of\nthe term sending rate. Specifically, let us again denote the rate at which the\napplication sends original data into the socket by λ  bytes/sec. The rate at\nwhich the transport layer sends segments (containing original data and\nretransmitted data) into the network will be denoted λ'  bytes/sec. λ'  is\nsometimes referred to as the offered load to the network.\nFigure 3.45 ♦Scenario 2: Two hosts (with retransmissions) and a\nrouter with finite buffers\nThe performance realized under scenario 2 will now depend strongly on\nhow retransmission is performed. First, consider the unrealistic case that\nHost A is able to somehow (magically!) determine whether or not a buffer is\nfree in the router and thus sends a packet only when a buffer is free. In this\ncase, no loss would occur, λ  would be equal to λ' , and the throughput of\nthe connection would be equal to λ . This case is shown in Figure 3.46(a).\nFrom a throughput standpoint, performance is ideal—everything that is sent\nis received. Note that the average host sending rate cannot exceed R/2 under\nthis scenario, since packet loss is assumed never to occur.\nFigure 3.46 ♦Scenario 2 performance with finite buffers\nConsider next the slightly more realistic case that the sender retransmits\nonly when a packet is known for certain to be lost. (Again, this assumption\nis a bit of a stretch. However, it is possible that the sending host might set\nits timeout large enough to be virtually assured that a packet that has not\nbeen acknowledged has been lost.) In this case, the performance might look\nsomething like that shown in Figure 3.46(b). To appreciate what is\nhappening here, consider the case that the offered load, λ'  (the rate of\noriginal data transmission plus retransmissions), equals R/2. According to\nFigure 3.46(b), at this value of the offered load, the rate at which data are\ndelivered to the receiver application is R/3. Thus, out of the 0.5R units of\ndata transmitted, 0.333R bytes/sec (on average) are original data and 0.166R\nbytes/sec (on average) are retransmitted data. We see here another cost of a\ncongested network—the sender must perform retransmissions in order to\ncompensate for dropped (lost) packets due to buffer overflow.\nFinally, let us consider the case that the sender may time out\nprematurely and retransmit a packet that has been delayed in the queue but\nnot yet lost. In this case, both the original data packet and the\nretransmission may reach the receiver. Of course, the receiver needs but one\ncopy of this packet and will discard the retransmission. In this case, the\nwork done by the router in forwarding the retransmitted copy of the original\npacket was wasted, as the receiver will have already received the original\ncopy of this packet. The router would have better used the link transmission\ncapacity to send a different packet instead. Here then is yet another cost of a\ncongested network—unneeded retransmissions by the sender in the face of\nlarge delays may cause a router to use its link bandwidth to forward\nunneeded copies of a packet. Figure 3.46 (c) shows the throughput versus\noffered load when each packet is assumed to be forwarded (on average)\ntwice by the router. Since each packet is forwarded twice, the throughput\nwill have an asymptotic value of R/4 as the offered load approaches R/2.\nScenario 3: Four Senders, Routers with Finite Buffers, and\nMultihop Paths\nIn our final congestion scenario, four hosts transmit packets, each over\noverlapping two-hop paths, as shown in Figure 3.47. We again assume that\neach host uses a timeout/retransmission mechanism to implement a reliable\ndata transfer service, that all hosts have the same value of λ , and that all\nrouter links have capacity R bytes/sec.\nFigure 3.47 ♦Four senders, routers with finite buffers, and multihop\nLet’s consider the connection from Host A to Host C, passing through\nrouters R1 and R2. The A–C connection shares router R1 with the D–B\nconnection and shares router R2 with the B–D connection. For extremely\nsmall values of λ , buffer overflows are rare (as in congestion scenarios 1\nand 2), and the throughput approximately equals the offered load. For\nslightly larger values of λ , the corresponding throughput is also larger,\nsince more original data is being transmitted into the network and delivered\nto the destination, and overflows are still rare. Thus, for small values of λ ,\nan increase in λ  results in an increase in λ .\nHaving considered the case of extremely low traffic, let’s next examine\nthe case that λ  (and hence λ' ) is extremely large. Consider router R2. The\nA–C traffic arriving to router R2 (which arrives at R2 after being forwarded\nfrom R1) can have an arrival rate at R2 that is at most R, the capacity of the\nlink from R1 to R2, regardless of the value of λ . If λ'  is extremely large\nfor all connections (including the B–D connection), then the arrival rate of\nB–D traffic at R2 can be much larger than that of the A–C traffic. Because\nthe A–C and B–D traffic must compete at router R2 for the limited amount\nof buffer space, the amount of A–C traffic that successfully gets through R2\n(that is, is not lost due to buffer overflow) becomes smaller and smaller as\nthe offered load from B–D gets larger and larger. In the limit, as the offered\nload approaches infinity, an empty buffer at R2 is immediately filled by a\nB–D packet, and the throughput of the A–C connection at R2 goes to zero.\nThis, in turn, implies that the A–C end-to-end throughput goes to zero in the\nlimit of heavy traffic. These considerations give rise to the offered load\nversus throughput tradeoff shown in Figure 3.48.\nFigure 3.48 ♦Scenario 3 performance with finite buffers and\nmultihop paths\nThe reason for the eventual decrease in throughput with increasing\noffered load is evident when one considers the amount of wasted work done\nby the network. In the high-traffic scenario outlined above, whenever a\npacket is dropped at a second-hop router, the work done by the first-hop\nrouter in forwarding a packet to the second-hop router ends up being\n“wasted.” The network would have been equally well off (more accurately,\nequally bad off) if the first router had simply discarded that packet and\nremained idle. More to the point, the transmission capacity used at the first\nrouter to forward the packet to the second router could have been much\nmore profitably used to transmit a different packet. (For example, when\nselecting a packet for transmission, it might be better for a router to give\npriority to packets that have already traversed some number of upstream\nrouters.) So here we see yet another cost of dropping a packet due to\ncongestion—when a packet is dropped along a path, the transmission\ncapacity that was used at each of the upstream links to forward that packet\nto the point at which it is dropped ends up having been wasted.\n3.6.2 Approaches to Congestion Control\nIn Section 3.7, we’ll examine TCP’s specific approach to congestion control\nin great detail. Here, we identify the two broad approaches to congestion\ncontrol that are taken in practice and discuss specific network architectures\nand congestion-control protocols embodying these approaches.\nAt the highest level, we can distinguish among congestion-control\napproaches by whether the network layer provides explicit assistance to the\ntransport layer for congestion-control purposes:\nEnd-to-end congestion control. In an end-to-end approach to congestion\ncontrol, the network layer provides no explicit support to the transport\nlayer for congestion-control purposes. Even the presence of network\ncongestion must be inferred by the end systems based only on observed\nnetwork behavior (for example, packet loss and delay). We’ll see\nshortly in Section 3.7.1 that TCP takes this end-to-end approach toward\ncongestion control, since the IP layer is not required to provide\nfeedback to hosts regarding network congestion. TCP segment loss (as\nwe’ll study in Chapter 8), thus providing faster establishment than the\nprotocol stack in Figure 3.58(a), where multiple RTTs are required to\nfirst establish a TCP connection, and then establish a TLS connection\nover the TCP connection.\nFigure 3.58 ♦(a) traditional secure HTTP protocol stack, and\nthe (b) secure QUIC-based HTTP/3 protocol stack\nStreams. QUIC allows several different application-level “streams” to\nbe multiplexed through a single QUIC connection, and once a QUIC\nconnection is established, new streams can be quickly added. A stream\nis an abstraction for the reliable, in-order bi-directional delivery of data\nbetween two QUIC endpoints. In the context of HTTP/3, there would be\na different stream for each object in a Web page. Each connection has a\nconnection ID, and each stream within a connection has a stream ID;\nboth of these IDs are contained in a QUIC packet header (along with\nother header information). Data from multiple streams may be\ncontained within a single QUIC segment, which is carried over UDP.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC\n3286] is an earlier reliable, message-oriented protocol that pioneered\nthe notion of multiplexing multiple application-level “streams” through\na single SCTP connection. We’ll see in Chapter 7 that SCTP is used in\ncontrol plane protocols in 4G/5G cellular wireless networks.\nReliable, TCP-friendly congestion-controlled data transfer. As\nillustrated in Figure 3.59(b), QUIC provides reliable data transfer to\neach QUIC stream separately. Figure 3.59(a) shows the case of\nHTTP/1.1 sending multiple HTTP requests, all over a single TCP\nconnection. Since TCP provides reliable, in-order byte delivery, this\nmeans that the multiple HTTP requests must be delivered in-order at the\ndestination HTTP server. Thus, if bytes from one HTTP request are lost,\nthe remaining HTTP requests can not be delivered until those lost bytes\nare retransmitted and correctly received by TCP at the HTTP server—\nthe so-called HOL blocking problem that we encountered earlier in\nSection 2.2.5. Since QUIC provides a reliable in-order delivery on a\nper-stream basis, a lost UDP segment only impacts those streams whose\ndata was carried in that segment; HTTP messages in other streams can\ncontinue to be received and delivered to the application. QUIC provides\nIn Chapter 1, we said that a computer network can be partitioned into\nthe ­“network edge” and the “network core.” The network edge covers\neverything that happens in the end systems. Having now covered the\napplication layer and the t­ransport layer, our discussion of the network edge\nis complete. It is time to explore the network core! This journey begins in\nthe next two chapters, where we’ll study the network layer, and continues\ninto Chapter 6, where we’ll study the link layer.\nHomework Problems and Questions\nChapter 3 Review Questions\nSECTIONS 3.1–3.3\nR1. Suppose the network layer provides the following service. The\nnetwork layer in the source host accepts a segment of maximum size\n1,200 bytes and a destination host address from the transport layer.\nThe network layer then guarantees to deliver the segment to the\ntransport layer at the destination host. Suppose many network\napplication processes can be running at the destination host.\na. Design the simplest possible transport-layer protocol that will\nget application data to the desired process at the destination host.\nAssume the operating system in the destination host has\nassigned a 4-byte port number to each running application\nb. Modify this protocol so that it provides a “return address” to the\ndestination process.\nc. In your protocols, does the transport layer “have to do anything”\nin the core of the computer network?\nR2. Consider a planet where everyone belongs to a family of six, every\nfamily lives in its own house, each house has a unique address, and\neach person in a given house has a unique name. Suppose this planet\nhas a mail service that delivers letters from source house to\ndestination house. The mail service requires that (1) the letter be in an\nenvelope, and that (2) the address of the destination house (and\nnothing more) be clearly written on the envelope. Suppose each\nfamily has a delegate family member who collects and distributes\nletters for the other family members. The letters do not necessarily\nprovide any indication of the recipients of the letters.\na. Using the solution to Problem R1 above as inspiration, describe\na protocol that the delegates can use to deliver letters from a\nsending family member to a receiving family member.\nb. In your protocol, does the mail service ever have to open the\nenvelope and examine the letter in order to provide its service?\nR3. How is a UDP socket fully identified? What about a TCP socket?\nWhat is the difference between the full identification of both sockets?\nR4. Describe why an application developer might choose to run an\napplication over UDP rather than TCP.\nR5. Why is it that voice and video traffic is often sent over TCP rather\nthan UDP in today’s Internet? (Hint: The answer we are looking for\nhas nothing to do with TCP’s congestion-control mechanism.)\nR6. Is it possible for an application to enjoy reliable data transfer even\nwhen the application runs over UDP? If so, how?\nR7. Suppose a process in Host C has a UDP socket with port number\n6789. Suppose both Host A and Host B each send a UDP segment to\nHost C with destination port number 6789. Will both of these\nsegments be directed to the same socket at Host C? If so, how will the\nprocess at Host C know that these two segments originated from two\ndifferent hosts?\nR8. Suppose that a Web server runs in Host C on port 80. Suppose this\nWeb server uses persistent connections, and is currently receiving\nrequests from two different Hosts, A and B. Are all of the requests\nbeing sent through the same socket at Host C? If they are being\npassed through different sockets, do both of the sockets have port 80?\nDiscuss and explain.\nSECTION 3.4\nR9. In our rdt protocols, why did we need to introduce sequence\nR10. In our rdt protocols, why did we need to introduce timers?\nR11. Suppose that the roundtrip delay between sender and receiver is\nconstant and known to the sender. Would a timer still be necessary in\nprotocol rdt 3.0, assuming that packets can be lost? Explain.\nR12. Visit the Go-Back-N interactive animation at the Companion\na. Have the source send five packets, and then pause the animation\nbefore any of the five packets reach the destination. Then kill the\nfirst packet and resume the animation. Describe what happens.\nb. Repeat the experiment, but now let the first packet reach the\ninteracting parts, the data plane and the control plane. In Chapter 4,\nwe’ll first cover the data plane functions of the network layer—the\nper-router functions in the network layer that determine how a\ndatagram (that is, a network-layer packet) arriving on one of a router’s\ninput links is forwarded to one of that router’s output links. We’ll\ncover both traditional IP forwarding (where forwarding is based on a\ndatagram’s destination address) and generalized forwarding (where\nforwarding and other functions may be performed using values in\nseveral different fields in the datagram’s header). We’ll study the IPv4\nand IPv6 protocols and addressing in detail. In Chapter 5, we’ll cover\nthe control plane functions of the network layer—the network-wide\nlogic that controls how a datagram is routed among routers along an\nend-to-end path from the source host to the destination host. We’ll\ncover routing algorithms, as well as routing protocols, such as OSPF\nand BGP, that are in widespread use in today’s Internet. Traditionally,\nthese control-plane routing protocols and data-plane forwarding\nfunctions have been implemented together, monolithically, within a\nrouter. Software-defined networking (SDN) explicitly separates the\ndata plane and control plane by implementing these control plane\nfunctions as a separate service, typically in a remote “controller.” We’ll\nalso cover SDN controllers in Chapter 5.\nThis distinction between data-plane and control-plane functions in the\nnetwork layer is an important concept to keep in mind as you learn about\nthe network layer —it will help structure your thinking about the network\nlayer and reflects a modern view of the network layer’s role in computer\nnetworking.\n4.1 Overview of Network Layer\nFigure 4.1 shows a simple network with two hosts, H1 and H2, and several\nrouters on the path between H1 and H2. Let’s suppose that H1 is sending\ninformation to H2, and consider the role of the network layer in these hosts\nand in the intervening routers. The network layer in H1 takes segments\nfrom the transport layer in H1, encapsulates each segment into a datagram,\nand then sends the datagrams to its nearby router, R1. At the receiving host,\nH2, the network layer receives the datagrams from its nearby router R2,\nextracts the transport-layer segments, and delivers the segments up to the\ntransport layer at H2. The primary data-plane role of each router is to\nforward datagrams from its input links to its output links; the primary role\nof the network control plane is to coordinate these local, per-router\nforwarding actions so that datagrams are ultimately transferred end-to-end,\nalong paths of routers between source and destination hosts. Note that the\nrouters in Figure 4.1 are shown with a truncated protocol stack, that is, with\nno upper layers above the network layer, because routers do not run\napplication- and transport-layer protocols such as those we examined in\nChapters 2 and 3.\n4.1.1 Forwarding and Routing: The Data and\nControl Planes\nThe primary role of the network layer is deceptively simple—to move\npackets from a sending host to a receiving host. To do so, two important\nnetwork-layer functions can be identified:\nForwarding. When a packet arrives at a router’s input link, the router\nmust move the packet to the appropriate output link. For example, a\npacket arriving from Host H1 to Router R1 in Figure 4.1 must be\nforwarded to the next router on a path to H2. As we will see, forwarding\nis but one function (albeit the most common and important one!)\nimplemented in the data plane. In the more general case, which we’ll\ncover in Section 4.4, a packet might also be blocked from exiting a\nrouter (for example, if the packet originated at a known malicious\nsending host, or if the packet were destined to a forbidden destination\nhost), or might be duplicated and sent over multiple outgoing links.\nRouting. The network layer must determine the route or path taken by\npackets as they flow from a sender to a receiver. The algorithms that\ncalculate these paths are referred to as routing algorithms. A routing\nalgorithm would determine, for example, the path along which packets\nflow from H1 to H2 in Figure 4.1. Routing is implemented in the\ncontrol plane of the network layer.\nFigure 4.1 ♦The network layer\nThe terms forwarding and routing are often used interchangeably by\nauthors discussing the network layer. We’ll use these terms much more\nprecisely in this book. Forwarding refers to the router-local action of\ntransferring a packet from an input link interface to the appropriate output\nlink interface. Forwarding takes place at very short timescales (typically a\nfew nanoseconds), and thus is typically implemented in hardware. Routing\nrefers to the network-wide process that determines the end-to-end paths that\npackets take from source to destination. Routing takes place on much\nlonger timescales (typically seconds), and as we will see is often\nimplemented in software. Using our driving analogy, consider the trip from\nPennsylvania to Florida undertaken by our traveler back in Section 1.3.1.\nDuring this trip, our driver passes through many interchanges en route to\nFlorida. We can think of forwarding as the process of getting through a\nsingle interchange: A car enters the interchange from one road and\ndetermines which road it should take to leave the interchange. We can think\nof routing as the process of planning the trip from Pennsylvania to Florida:\nBefore embarking on the trip, the driver has consulted a map and chosen\none of many paths possible, with each path consisting of a series of road\nsegments connected at interchanges.\nA key element in every network router is its forwarding table. A router\nforwards a packet by examining the value of one or more fields in the\nAn Overview of Chapter 4\nHaving now provided an overview of the network layer, we’ll cover the\ndata-plane component of the network layer in the following sections in this\nchapter. In Section 4.2, we’ll dive down into the internal hardware\noperations of a router, including input and output packet processing, the\nrouter’s internal switching mechanism, and packet queuing and scheduling.\nIn Section 4.3, we’ll take a look at traditional IP forwarding, in which\npackets are forwarded to output ports based on their destination IP\naddresses. We’ll encounter IP addressing, the celebrated IPv4 and IPv6\nprotocols and more. In Section 4.4, we’ll cover more generalized\nforwarding, where packets may be forwarded to output ports based on a\nlarge number of header values (i.e., not only based on destination IP\naddress). Packets may be blocked or duplicated at the router, or may have\ncertain header field values rewritten—all under software control. This more\ngeneralized form of packet forwarding is a key component of a modern\nnetwork data plane, including the data plane in software-defined networks\n(SDN). In Section 4.5, we’ll learn about “middleboxes” that can perform\nfunctions in addition to forwarding.\nWe mention here in passing that the terms forwarding and switching are\noften used interchangeably by computer-networking researchers and\npractitioners; we’ll use both terms interchangeably in this textbook as well.\nWhile we’re on the topic of terminology, it’s also worth mentioning two\nother terms that are often used interchangeably, but that we will use more\ncarefully. We’ll reserve the term packet switch to mean a general packet-\nswitching device that transfers a packet from input link interface to output\nlink interface, according to values in a packet’s header fields. Some packet\nswitches, called link-layer switches (examined in Chapter 6), base their\nforwarding decision on values in the fields of the link-layer frame; switches\nare thus referred to as link-layer (layer 2) devices. Other packet switches,\ncalled routers, base their forwarding decision on header field values in the\nnetwork-layer datagram. Routers are thus network-layer (layer 3) devices.\n(To fully appreciate this important distinction, you might want to review\nSection 1.5.2, where we discuss network-layer datagrams and link-layer\nframes and their relationship.) Since our focus in this chapter is on the\nnetwork layer, we’ll mostly use the term router in place of packet switch.\n4.2 What’s Inside a Router?\nNow that we’ve overviewed the data and control planes within the network\nlayer, the important distinction between forwarding and routing, and the\nservices and functions of the network layer, let’s turn our attention to its\nforwarding function—the actual transfer of packets from a router’s\nincoming links to the appropriate outgoing links at that router.\nA high-level view of a generic router architecture is shown in Figure\n4.4. Four router components can be identified:\nFigure 4.4 ♦Router architecture\nInput ports. An input port performs several key functions. It performs\nthe physical layer function of terminating an incoming physical link at a\nrouter; this is shown in the leftmost box of an input port and the\nrightmost box of an output port in Figure 4.4. An input port also\nperforms link-layer functions needed to interoperate with the link layer\nat the other side of the incoming link; this is represented by the middle\nboxes in the input and output ports. Perhaps most crucially, a lookup\nfunction is also performed at the input port; this will occur in the\nrightmost box of the input port. It is here that the forwarding table is\nconsulted to determine the router output port to which an arriving\npacket will be forwarded via the switching fabric. Control packets (for\nexample, packets carrying routing protocol information) are forwarded\nfrom an input port to the routing processor. Note that the term “port”\nhere—referring to the physical input and output router interfaces—is\ndistinctly different from the software ports associated with network\napplications and sockets discussed in Chapters 2 and 3. In practice, the\nnumber of ports supported by a router can range from a relatively small\nnumber in enterprise routers, to hundreds of 10 Gbps ports in a router at\nan ISP’s edge, where the number of incoming lines tends to be the\ngreatest. The Juniper MX2020, edge router, for example, supports up to\n800 100 Gbps Ethernet ports, with an overall router system capacity of\n800 Tbps [Juniper MX 2020 2020].\nSwitching fabric. The switching fabric connects the router’s input ports\nto its output ports. This switching fabric is completely contained within\nthe router—a network inside of a network router!\nOutput ports. An output port stores packets received from the\nswitching fabric and transmits these packets on the outgoing link by\nperforming the necessary link-layer and physical-layer functions. When\na link is bidirectional (that is, carries traffic in both directions), an\noutput port will typically be paired with the input port for that link on\nthe same line card.\nRouting processor. The routing processor performs control-plane\nfunctions. In traditional routers, it executes the routing protocols (which\nwe’ll study in Sections 5.3 and 5.4), maintains routing tables and\nattached link state information, and computes the forwarding table for\nthe router. In SDN routers, the routing processor is responsible for\ncommunicating with the remote controller in order to (among other\nactivities) receive forwarding table entries computed by the remote\ncontroller, and install these entries in the router’s input ports. The\nrouting processor also performs the network management functions that\nwe’ll study in Section 5.7.\nA router’s input ports, output ports, and switching fabric are almost\nalways implemented in hardware, as shown in Figure 4.4. To appreciate\nwhy a hardware implementation is needed, consider that with a 100 Gbps\ninput link and a 64-byte IP datagram, the input port has only 5.12 ns to\nprocess the datagram before another datagram may arrive. If N ports are\ncombined on a line card (as is often done in practice), the datagram-\nprocessing pipeline must operate N times faster—far too fast for software\nimplementation. Forwarding hardware can be implemented either using a\nrouter vendor’s own hardware designs, or constructed using purchased\nmerchant-silicon chips (for example, as sold by companies such as Intel and\nWhile the data plane operates at the nanosecond time scale, a router’s\ncontrol functions—executing the routing protocols, responding to attached\nlinks that go up or down, communicating with the remote controller (in the\nSDN case) and performing management functions—operate at the\nmillisecond or second timescale. These control plane functions are thus\nusually implemented in software and execute on the routing processor\n(typically a traditional CPU).\nBefore delving into the details of router internals, let’s return to our\nanalogy from the beginning of this chapter, where packet forwarding was\ncompared to cars entering and leaving an interchange. Let’s suppose that\nthe interchange is a roundabout, and that as a car enters the roundabout, a\nbit of processing is required. Let’s consider what information is required for\nthis processing:\nDestination-based forwarding. Suppose the car stops at an entry station\nand indicates its final destination (not at the local roundabout, but the\nultimate destination of its journey). An attendant at the entry station\nlooks up the final destination, determines the roundabout exit that leads\nto that final destination, and tells the driver which roundabout exit to\nGeneralized forwarding. The attendant could also determine the car’s\nexit ramp on the basis of many other factors besides the destination. For\nexample, the selected exit ramp might depend on the car’s origin, for\nexample the state that issued the car’s license plate. Cars from a certain\nset of states might be directed to use one exit ramp (that leads to the\ndestination via a slow road), while cars from other states might be\ndirected to use a different exit ramp (that leads to the destination via\nsuperhighway). The same decision might be made based on the model,\nmake and year of the car. Or a car not deemed roadworthy might be\nblocked and not be allowed to pass through the roundabout. In the case\nof generalized forwarding, any number of factors may contribute to the\nattendant’s choice of the exit ramp for a given car.\nOnce the car enters the roundabout (which may be filled with other cars\nentering from other input roads and heading to other roundabout exits), it\neventually leaves at the prescribed roundabout exit ramp, where it may\nencounter other cars leaving the roundabout at that exit.\nWe can easily recognize the principal router components in Figure 4.4\nin this analogy—the entry road and entry station correspond to the input\nport (with a lookup function to determine to local outgoing port); the\nroundabout corresponds to the switch fabric; and the roundabout exit road\ncorresponds to the output port. With this analogy, it’s instructive to consider\nwhere bottlenecks might occur. What happens if cars arrive blazingly fast\n(for example, the roundabout is in Germany or Italy!) but the station\nattendant is slow? How fast must the attendant work to ensure there’s no\nbackup on an entry road? Even with a blazingly fast attendant, what\nhappens if cars traverse the roundabout slowly—can backups still occur?\nAnd what happens if most of the cars entering at all of the roundabout’s\nentrance ramps all want to leave the roundabout at the same exit ramp—can\nbackups occur at the exit ramp or elsewhere? How should the roundabout\noperate if we want to assign priorities to different cars, or block certain cars\nfrom entering the roundabout in the first place? These are all analogous to\ncritical questions faced by router and switch designers.\nIn the following subsections, we’ll look at router functions in more\ndetail. [Turner 1988; McKeown 1997a; Partridge 1998; Iyer 2008; Serpanos\n2011; Zilberman 2019] provide a discussion of specific router architectures.\nFor concreteness and simplicity, we’ll initially assume in this section that\nforwarding decisions are based only on the packet’s destination address,\nrather than on a generalized set of packet header fields. We will cover the\ncase of more generalized packet forwarding in Section 4.4.\n4.2.1 Input Port Processing and Destination-Based\nA more detailed view of input processing is shown in Figure 4.5. As just\ndiscussed, the input port’s line-termination function and link-layer\nprocessing implement the physical and link layers for that individual input\nlink. The lookup performed in the input port is central to the router’s\noperation—it is here that the router uses the forwarding table to look up the\noutput port to which an arriving packet will be forwarded via the switching\nfabric. The forwarding table is either computed and updated by the routing\nprocessor (using a routing protocol to interact with the routing processors in\nother network routers) or is received from a remote SDN controller. The\nforwarding table is copied from the routing processor to the line cards over\na separate bus (e.g., a PCI bus) indicated by the dashed line from the\nrouting processor to the input line cards in Figure 4.4. With such a shadow\ncopy at each line card, forwarding decisions can be made locally, at each\ninput port, without invoking the centralized routing processor on a per-\npacket basis and thus avoiding a centralized processing bottleneck.\nFigure 4.5 ♦Input port processing\nLet’s now consider the “simplest” case that the output port to which an\nincoming packet is to be switched is based on the packet’s destination\naddress. In the case of 32-bit IP addresses, a brute-force implementation of\nthe forwarding table would have one entry for every possible destination\naddress. Since there are more than 4 billion possible addresses, this option\nis totally out of the question.\nAs an example of how this issue of scale can be handled, let’s suppose\nthat our router has four links, numbered 0 through 3, and that packets are to\nbe forwarded to the link interfaces as follows:\nClearly, for this example, it is not necessary to have 4 billion entries in the\nrouter’s forwarding table. We could, for example, have the following\nforwarding table with just four entries:\nWith this style of forwarding table, the router matches a prefix of the\npacket’s destination address with the entries in the table; if there’s a match,\nthe router forwards the packet to a link associated with the match. For\nexample, suppose the packet’s destination address is 11001000\n00010111 00010110 10100001; because the 21-bit prefix of this\naddress matches the first entry in the table, the router forwards the packet to\nlink interface 0. If a prefix doesn’t match any of the first three entries, then\nthe router forwards the packet to the default interface 3. Although this\nsounds simple enough, there’s a very important subtlety here. You may have\nnoticed that it is possible for a destination address to match more than one\nentry. For example, the first 24 bits of the address 11001000 00010111\n00011000 10101010 match the second entry in the table, and the first\n21 bits of the address match the third entry in the table. When there are\nmultiple matches, the router uses the longest prefix matching rule; that is,\nit finds the longest matching entry in the table and forwards the packet to\nthe link interface associated with the longest prefix match. We’ll see exactly\nwhy this longest prefix-matching rule is used when we study Internet\naddressing in more detail in Section 4.3.\nGiven the existence of a forwarding table, lookup is conceptually\nsimple—­hardware logic just searches through the forwarding table looking\nfor the longest prefix match. But at Gigabit transmission rates, this lookup\nmust be performed in nanoseconds (recall our earlier example of a 10 Gbps\nlink and a 64-byte IP datagram). Thus, not only must lookup be performed\nin hardware, but techniques beyond a simple linear search through a large\ntable are needed; surveys of fast lookup algorithms can be found in [Gupta\n2001, Ruiz-Sanchez 2001]. Special attention must also be paid to memory\naccess times, resulting in designs with embedded on-chip DRAM and faster\nSRAM (used as a DRAM cache) memories. In practice, Ternary Content\nAddressable Memories (TCAMs) are also often used for lookup [Yu 2004].\nWith a TCAM, a 32-bit IP address is presented to the memory, which\nreturns the content of the forwarding table entry for that address in\nessentially constant time. The Cisco Catalyst 6500 and 7600 Series routers\nand switches can hold upwards of a million TCAM forwarding table entries\n[Cisco TCAM 2014].\nOnce a packet’s output port has been determined via the lookup, the\npacket can be sent into the switching fabric. In some designs, a packet may\nbe temporarily blocked from entering the switching fabric if packets from\nother input ports are currently using the fabric. A blocked packet will be\nqueued at the input port and then scheduled to cross the fabric at a later\npoint in time. We’ll take a closer look at the blocking, queuing, and\nscheduling of packets (at both input ports and output ports) shortly.\nAlthough “lookup” is arguably the most important action in input port\nprocessing, many other actions must be taken: (1) physical- and link-layer\nprocessing must occur, as discussed previously; (2) the packet’s version\nnumber, checksum and time-to-live field—all of which we’ll study in\nSection 4.3—must be checked and the latter two fields rewritten; and (3)\ncounters used for network management (such as the number of IP\ndatagrams received) must be updated.\nLet’s close our discussion of input port processing by noting that the\ninput port steps of looking up a destination IP address (“match”) and then\nsending the packet into the switching fabric to the specified output port\n(“action”) is a specific case of a more general “match plus action”\nabstraction that is performed in many networked devices, not just routers. In\nlink-layer switches (covered in Chapter 6), link-layer destination addresses\nare looked up and several actions may be taken in addition to sending the\nframe into the switching fabric towards the output port. In firewalls\n(covered in Chapter 8)—devices that filter out selected incoming packets—\nan incoming packet whose header matches a given criteria (e.g., a\ncombination of source/destination IP addresses and transport-layer port\nnumbers) may be dropped (action). In a network address translator (NAT,\ncovered in Section 4.3), an incoming packet whose transport-layer port\nnumber matches a given value will have its port number rewritten before\nforwarding (action). Indeed, the “match plus action” abstraction [Bosshart\n2013] is both powerful and prevalent in network devices today, and is\ncentral to the notion of generalized forwarding that we’ll study in Section\n4.2.2 Switching\nThe switching fabric is at the very heart of a router, as it is through this\nfabric that the packets are actually switched (that is, forwarded) from an\ninput port to an output port. Switching can be accomplished in a number of\nways, as shown in Figure 4.6:\nSwitching via memory. The simplest, earliest routers were traditional\ncomputers, with switching between input and output ports being done\nunder direct control of the CPU (routing processor). Input and output\nports functioned as traditional I/O devices in a traditional operating\nsystem. An input port with an arriving packet first signaled the routing\nprocessor via an interrupt. The packet was then copied from the input\nport into processor memory. The routing processor then extracted the\ndestination address from the header, looked up the appropriate output\nport in the forwarding table, and copied the packet to the output port’s\nbuffers. In this scenario, if the memory bandwidth is such that a\nmaximum of B packets per second can be written into, or read from,\nmemory, then the overall forwarding throughput (the total rate at which\npackets are transferred from input ports to output ports) must be less\nthan B/2. Note also that two packets cannot be forwarded at the same\ntime, even if they have different destination ports, since only one\nmemory read/write can be done at a time over the shared system bus.\nSome modern routers switch via memory. A major difference from early\nrouters, however, is that the lookup of the destination address and the\nstoring of the packet into the appropriate memory location are\nperformed by processing on the input line cards. In some ways, routers\nthat switch via memory look very much like shared-memory\nmultiprocessors, with the processing on a line card switching (writing)\npackets into the memory of the appropriate output port. Cisco’s Catalyst\n8500 series switches [Cisco 8500 2020] internally switches packets via\na shared memory.\nSwitching via a bus. In this approach, an input port transfers a packet\ndirectly to the output port over a shared bus, without intervention by the\nrouting processor. This is typically done by having the input port pre-\npend a switch-internal label (header) to the packet indicating the local\noutput port to which this packet is being transferred and transmitting the\npacket onto the bus. All output ports receive the packet, but only the\nport that matches the label will keep the packet. The label is then\nremoved at the output port, as this label is only used within the switch\nto cross the bus. If multiple packets arrive to the router at the same time,\neach at a different input port, all but one must wait since only one\npacket can cross the bus at a time. Because every packet must cross the\nsingle bus, the switching speed of the router is limited to the bus speed;\nin our roundabout analogy, this is as if the roundabout could only\ncontain one car at a time. Nonetheless, switching via a bus is often\nsufficient for routers that operate in small local area and enterprise\nnetworks. The Cisco 6500 router [Cisco 6500 2020] internally switches\npackets over a 32-Gbps-backplane bus.\nSwitching via an interconnection network. One way to overcome the\nbandwidth limitation of a single, shared bus is to use a more\nsophisticated interconnection network, such as those that have been\nused in the past to interconnect processors in a multiprocessor computer\narchitecture. A crossbar switch is an interconnection network consisting\nof 2N buses that connect N input ports to N output ports, as shown in\nFigure 4.6. Each vertical bus intersects each horizontal bus at a\ncrosspoint, which can be opened or closed at any time by the switch\nfabric controller (whose logic is part of the switching fabric itself).\nWhen a packet arrives from port A and needs to be forwarded to port Y,\nthe switch controller closes the crosspoint at the intersection of busses A\nand Y, and port A then sends the packet onto its bus, which is picked up\n(only) by bus Y. Note that a packet from port B can be forwarded to port\nX at the same time, since the A-to-Y and B-to-X packets use different\ninput and output busses. Thus, unlike the previous two switching\napproaches, crossbar switches are capable of forwarding multiple\npackets in parallel. A crossbar switch is non-blocking—a packet being\nforwarded to an output port will not be blocked from reaching that\noutput port as long as no other packet is currently being forwarded to\nthat output port. However, if two packets from two different input ports\nare destined to that same output port, then one will have to wait at the\ninput, since only one packet can be sent over any given bus at a time.\nCisco 12000 series switches [Cisco 12000 2020] use a crossbar\nswitching network; the Cisco 7600 series can be configured to use\neither a bus or crossbar switch [Cisco 7600 2020].\nMore sophisticated interconnection networks use multiple stages of\nswitching elements to allow packets from different input ports to\nproceed towards the same output port at the same time through the\nmulti-stage switching fabric. See [Tobagi 1990] for a survey of switch\narchitectures. The Cisco CRS employs a three-stage non-blocking\nswitching strategy. A router’s switching capacity can also be scaled by\nrunning multiple switching fabrics in parallel. In this approach, input\nports and output ports are connected to N switching fabrics that operate\nin parallel. An input port breaks a packet into K smaller chunks, and\nsends (“sprays”) the chunks through K of these N switching fabrics to\nthe selected output port, which reassembles the K chunks back into the\noriginal packet.\nFigure 4.6 ♦Three switching techniques\n4.2.3 Output Port Processing\nOutput port processing, shown in Figure 4.7, takes packets that have been\nstored in the output port’s memory and transmits them over the output link.\nThis includes selecting (i.e., scheduling) and de-queuing packets for\ntransmission, and performing the needed link-layer and physical-layer\ntransmission functions.\nFigure 4.7 ♦Output port processing\n4.2.4 Where Does Queuing Occur?\nIf we consider input and output port functionality and the configurations\nshown in Figure 4.6, it’s clear that packet queues may form at both the input\nports and the output ports, just as we identified cases where cars may wait\nat the inputs and outputs of the traffic intersection in our roundabout\nanalogy. The location and extent of ­queuing (either at the input port queues\nor the output port queues) will depend on the traffic load, the relative speed\nof the switching fabric, and the line speed. Let’s now consider these queues\nin a bit more detail, since as these queues grow large, the router’s memory\ncan eventually be exhausted and packet loss will occur when no memory is\navailable to store arriving packets. Recall that in our earlier ­discussions, we\nsaid that packets were “lost within the network” or “dropped at a router.” It\nis here, at these queues within a router, where such packets are actually\ndropped and lost.\nSuppose that the input and output line speeds (transmission rates) all\nhave an identical transmission rate of R\n packets per second, and that there\nare N input ports and N output ports. To further simplify the discussion, let’s\nassume that all packets have the same fixed length, and that packets arrive\nto input ports in a synchronous manner. That is, the time to send a packet on\nany link is equal to the time to receive a packet on any link, and during such\nan interval of time, either zero or one packets can arrive on an input link.\nDefine the switching fabric transfer rate R\n as the rate at which packets\ncan be moved from input port to output port. If R\n is N times faster than\n, then only negligible queuing will occur at the input ports. This is\nbecause even in the worst case, where all N input lines are receiving\npackets, and all packets are to be forwarded to the same output port, each\nbatch of N packets (one packet per input port) can be cleared through the\nswitch fabric before the next batch arrives.\nInput Queuing\nBut what happens if the switch fabric is not fast enough (relative to the\ninput line speeds) to transfer all arriving packets through the fabric without\ndelay? In this case, packet queuing can also occur at the input ports, as\npackets must join input port queues to wait their turn to be transferred\nthrough the switching fabric to the output port. To illustrate an important\nconsequence of this queuing, consider a crossbar switching fabric and\nsuppose that (1) all link speeds are identical, (2) that one packet can be\ntransferred from any one input port to a given output port in the same\namount of time it takes for a packet to be received on an input link, and (3)\npackets are moved from a given input queue to their desired output queue in\nan FCFS manner. Multiple packets can be transferred in parallel, as long as\ntheir output ports are different. However, if two packets at the front of two\ninput queues are destined for the same output queue, then one of the packets\nwill be blocked and must wait at the input queue—the switching fabric can\ntransfer only one packet to a given output port at a time.\nFigure 4.8 shows an example in which two packets (darkly shaded) at\nthe front of their input queues are destined for the same upper-right output\nport. Suppose that the switch fabric chooses to transfer the packet from the\nfront of the upper-left queue. In this case, the darkly shaded packet in the\nlower-left queue must wait. But not only must this darkly shaded packet\nwait, so too must the lightly shaded packet that is queued behind that packet\nin the lower-left queue, even though there is no contention for the middle-\nright output port (the destination for the lightly shaded packet). This\nphenomenon is known as head-of-the-line (HOL) blocking in an input-\nqueued switch—a queued packet in an input queue must wait for transfer\nthrough the fabric (even though its output port is free) because it is blocked\nby another packet at the head of the line. [Karol 1987] shows that due to\nHOL blocking, the input queue will grow to unbounded length (informally,\nthis is equivalent to saying that significant packet loss will occur) under\ncertain assumptions as soon as the packet arrival rate on the input links\nreaches only 58 percent of their capacity. A number of solutions to HOL\nblocking are discussed in [McKeown 1997].\nFigure 4.8 ♦HOL blocking at and input-queued switch\nOutput Queuing\nLet’s next consider whether queuing can occur at a switch’s output ports.\nSuppose that R\n is again N times faster than R\n and that packets arriving\nat each of the N input ports are destined to the same output port. In this\ncase, in the time it takes to send a single packet onto the outgoing link, N\nnew packets will arrive at this output port (one from each of the N input\nports). Since the output port can transmit only a single packet in a unit of\ntime (the packet transmission time), the N arriving packets will have to\nqueue (wait) for transmission over the outgoing link. Then N more packets\ncan possibly arrive in the time it takes to transmit just one of the N packets\nthat had just previously been queued. And so on. Thus, packet queues can\nform at the output ports even when the switching fabric is N times faster\nthan the port line speeds. Eventually, the number of queued packets can\ngrow large enough to exhaust available memory at the output port.\nWhen there is not enough memory to buffer an incoming packet, a\ndecision must be made to either drop the arriving packet (a policy known as\ndrop-tail) or remove one or more already-queued packets to make room for\nthe newly arrived packet. In some cases, it may be advantageous to drop (or\nmark the header of) a packet before the buffer is full in order to provide a\ncongestion signal to the sender. This marking could be done using the\nExplicit Congestion Notification bits that we studied in Section 3.7.2. A\nnumber of proactive packet-dropping and -marking policies (which\ncollectively have become known as active queue management (AQM)\nalgorithms) have been proposed and analyzed [Labrador 1999, Hollot\n2002]. One of the most widely studied and implemented AQM algorithms is\nthe Random Early Detection (RED) algorithm [Christiansen 2001]. More\nrecent AQM policies include PIE (the Proportional Integral controller\nEnhanced [RFC 8033]), and CoDel [Nichols 2012].\nOutput port queuing is illustrated in Figure 4.9. At time t, a packet has\narrived at each of the incoming input ports, each destined for the uppermost\noutgoing port. Assuming identical line speeds and a switch operating at\nthree times the line speed, one time unit later (that is, in the time needed to\nreceive or send a packet), all three original packets have been transferred to\nthe outgoing port and are queued awaiting transmission. In the next time\nunit, one of these three packets will have been transmitted over the outgoing\nlink. In our example, two new packets have arrived at the incoming side of\nthe switch; one of these packets is destined for this uppermost output port.\nA consequence of such queuing is that a packet scheduler at the output\nport must choose one packet, among those queued, for transmission—a\ntopic we’ll cover in the following section.\nFigure 4.9 ♦Output port queuing\nHow Much Buffering Is “Enough?”\nOur study above has shown how a packet queue forms when bursts of\npackets arrive at a router’s input or (more likely) output port, and the packet\narrival rate temporarily exceeds the rate at which packets can be forwarded.\nThe longer the amount of time that this mismatch persists, the longer the\nqueue will grow, until eventually a port’s buffers become full and packets\nare dropped. One natural question is how much buffering should be\nprovisioned at a port. It turns out the answer to this question is much more\ncomplicated than one might imagine and can teach us quite a bit about the\nsubtle interaction among congestion-aware senders at the network’s edge\nand the network core!\nFor many years, the rule of thumb [RFC 3439] for buffer sizing was\nthat the amount of buffering (B) should be equal to an average round-trip\ntime (RTT, say 250 msec) times the link capacity (C). Thus, a 10-Gbps link\nwith an RTT of 250 msec would need an amount of buffering equal to B =\nRTT · C = 2.5 Gbits of buffers. This result was based on an analysis of the\nqueuing dynamics of a relatively small number of TCP flows [Villamizar\n1994]. More recent theoretical and experimental efforts [Appenzeller 2004],\nhowever, suggest that when a large number of independent TCP flows (N)\npass through a link, the amount of buffering needed is B = RTT · C/√N. In\ncore networks, where a large number of TCP flows typically pass through\nlarge backbone router links, the value of N can be large, with the decrease\nin needed buffer size becoming quite significant. [Appenzeller 2004;\nWischik 2005; Beheshti 2008] provide very readable discussions of the\nbuffer-sizing problem from a theoretical, implementation, and operational\nstandpoint.\nIt’s temping to think that more buffering must be better—larger buffers\nwould allow a router to absorb larger fluctuations in the packet arrival rate,\nthereby decreasing the router’s packet loss rate. But larger buffers also\nmean potentially longer queuing delays. For gamers and for interactive\nteleconferencing users, tens of milliseconds count. Increasing the amount of\nper-hop buffer by a factor of 10 to decrease packet loss could increase the\nend-end delay by a factor of 10! Increased RTTs also make TCP senders\nless responsive and slower to respond to incipient congestion and/or packet\nloss. These delay-based considerations show that buffering is a double-\nedged sword—buffering can be used to absorb short-term statistical\nfluctuations in traffic but can also lead to increased delay and the attendant\nconcerns. Buffering is a bit like salt—just the right amount of salt makes\nfood better, but too much makes it inedible!\nIn the discussion above, we’ve implicitly assumed that many\nindependent senders are competing for bandwidth and buffers at a\ncongested link. While this is probably an excellent assumption for routers\nwithin the network core, at the network edge this may not hold. Figure\n4.10(a) shows a home router sending TCP segments to a remote game\nserver. Following [Nichols 2012], suppose that it takes 20 ms to transmit a\npacket (containing a gamer’s TCP segment), that there are negligible\nqueuing delays elsewhere on the path to the game server, and that the RTT\nis 200 ms. As shown in Figure 4.10(b), suppose that at time t = 0, a burst of\n25 packets arrives to the queue. One of these queued packets is then\ntransmitted once every 20 ms, so that at t = 200 msec, the first ACK arrives,\njust as the 21st packet is being transmitted. This ACK arrival causes the\nTCP sender to send another packet, which is queued at the outgoing link of\nthe home router. At t = 220, the next ACK arrives, and another TCP\nsegment is released by the gamer and is queued, as the 22nd packet is being\ntransmitted, and so on. You should convince yourself that in this scenario,\nACK clocking results in a new packet arriving at the queue every time a\nqueued packet is sent, resulting in queue size at the home router’s outgoing\nlink that is always five packets! That is, the end-end-pipe is full (delivering\npackets to the destination at the path bottleneck rate of one packet every 20\nms), but the amount of queuing delay is constant and persistent. As a result,\nthe gamer is unhappy with the delay, and the parent (who even knows\nwireshark!) is confused because he or she doesn’t understand why delays\nare persistent and excessively long, even when there is no other traffic on\nthe home network.\nFigure 4.10 ♦Bufferbloat: persistent queues\nThis scenario above of long delay due to persistent buffering is known\nas bufferbloat and illustrates that not only is throughput important, but also\nminimal delay is important as well [Kleinrock 2018], and that the\ninteraction among senders at the network edge and queues within the\nnetwork can indeed be complex and subtle. The DOCSIS 3.1 standard for\ncable networks that we will study in Chapter 6, recently added a specific\nAQM mechanism [RFC 8033, RFC 8034] to combat bufferbloat, while\npreserving bulk throughput performance.\n4.2.5 Packet Scheduling\nLet’s now return to the question of determining the order in which queued\npackets are transmitted over an outgoing link. Since you yourself have\nundoubtedly had to wait in long lines on many occasions and observed how\nwaiting customers are served, you’re no doubt familiar with many of the\nqueuing disciplines commonly used in routers. There is first-come-first-\nserved (FCFS, also known as first-in-first-out, FIFO). The British are\nfamous for patient and orderly FCFS queuing at bus stops and in the\nmarketplace (“Oh, are you queuing?”). Other countries operate on a priority\nbasis, with one class of waiting customers given priority service over other\nwaiting customers. There is also round-robin queuing, where customers are\nagain divided into classes (as in priority queuing) but each class of\ncustomer is given service in turn.\nFirst-in-First-Out (FIFO)\nFigure 4.11 shows the queuing model abstraction for the FIFO link-\nscheduling discipline. Packets arriving at the link output queue wait for\ntransmission if the link is currently busy transmitting another packet. If\nthere is not sufficient buffering space to hold the arriving packet, the\nqueue’s packet-discarding policy then determines whether the packet will\nbe dropped (lost) or whether other packets will be removed from the queue\nto make space for the arriving packet, as discussed above. In our discussion\nbelow, we’ll ignore packet discard. When a packet is completely transmitted\nover the outgoing link (that is, receives service) it is removed from the\nFigure 4.11 ♦FIFO queuing abstraction\nThe FIFO (also known as first-come-first-served, or FCFS) scheduling\ndiscipline selects packets for link transmission in the same order in which\nthey arrived at the output link queue. We’re all familiar with FIFO queuing\nfrom service centers, where arriving customers join the back of the single\nwaiting line, remain in order, and are then served when they reach the front\nof the line. Figure 4.12 shows the FIFO queue in operation. Packet arrivals\nare indicated by numbered arrows above the upper timeline, with the\nnumber indicating the order in which the packet arrived. Individual packet\ndepartures are shown below the lower timeline. The time that a packet\nspends in service (being transmitted) is indicated by the shaded rectangle\nbetween the two timelines. In our examples here, let’s assume that each\npacket takes three units of time to be transmitted. Under the FIFO\ndiscipline, packets leave in the same order in which they arrived. Note that\nafter the departure of packet 4, the link remains idle (since packets 1\nthrough 4 have been transmitted and removed from the queue) until the\narrival of packet 5.\nFigure 4.12 ♦The FIFO queue in operation\nPriority Queuing\nUnder priority queuing, packets arriving at the output link are classified into\npriority classes upon arrival at the queue, as shown in Figure 4.13. In\npractice, a network operator may configure a queue so that packets carrying\nnetwork management information (for example, as indicated by the source\nor destination TCP/UDP port number) receive priority over user traffic;\nadditionally, real-time voice-over-IP packets might receive priority over\nnon-real-time traffic such e-mail packets. Each priority class typically has\nits own queue. When choosing a packet to transmit, the priority queuing\ndiscipline will transmit a packet from the highest priority class that has a\nnonempty queue (that is, has packets waiting for transmission). The choice\namong packets in the same priority class is typically done in a FIFO\nFigure 4.13 ♦The priority queuing model\nFigure 4.14 illustrates the operation of a priority queue with two\npriority classes. Packets 1, 3, and 4 belong to the high-priority class, and\npackets 2 and 5 belong to the low-priority class. Packet 1 arrives and,\nfinding the link idle, begins transmission. During the transmission of packet\n1, packets 2 and 3 arrive and are queued in the low- and high-priority\nqueues, respectively. After the transmission of packet 1, packet 3 (a high-\npriority packet) is selected for transmission over packet 2 (which, even\nthough it arrived earlier, is a low-priority packet). At the end of the\ntransmission of packet 3, packet 2 then begins transmission. Packet 4 (a\nhigh-priority packet) arrives during the transmission of packet 2 (a low-\npriority packet). Under a non-preemptive priority queuing discipline, the\ntransmission of a packet is not interrupted once it has begun. In this case,\npacket 4 queues for transmission and begins being transmitted after the\ntransmission of packet 2 is completed.\nFigure 4.14 ♦The priority queue in operation\nWe’ve seen that packet scheduling mechanisms (e.g., priority traffic scheduling disciplines\nsuch a strict priority, and WFQ) can be used to provide different levels of service to different\n“classes” of traffic. The definition of what precisely constitutes a “class” of traffic is up to an\nISP to decide, but could be potentially based on any set of fields in the IP datagram header.\nFor example, the port field in the IP datagram header could be used to classify datagrams\naccording to the “well-know service” associated with that port: SNMP network management\ndatagram (port 161) might be assigned to a higher priority class than an IMAP e-mail\nprotocol (ports 143, or 993) datagram and therefore receive better service. An ISP could\nalso potentially use a datagram’s source IP address to provide priority to datagrams being\nsent by certain companies (who have presumably paid the ISP for this privilege) over\ndatagrams being sent from other companies (who have not paid); an ISP could even block\ntraffic with a source IP address in a given company, or country. There are many\nmechanisms that would allow an ISP to provide different levels of service to different\nclasses of traffic. The real question is what policies and laws determine what an ISP can\nactually do. Of course, these laws will vary by country; see [Smithsonian 2017] for a brief\nsurvey. Here, we’ll briefly consider US policy on what has come to be known as “net\nneutrality.”\nThe term “net neutrality” doesn’t have a precise decision, but the March 2015 Order on\nProtecting and Promoting an Open Internet [FCC 2015] by the US Federal Communications\nCommission provides three “clear, bright line” rules that are now often associated with net\nneutrality:\n“No Blocking. . . . A person engaged in the provision of broadband Internet access\nservice, . . . shall not block lawful content, applications, services, or non-harmful\ndevices, subject to reasonable network management.”\n“No Throttling. . . . A person engaged in the provision of broadband Internet access\nservice, . . . shall not impair or degrade lawful Internet traffic on the basis of Internet\ncontent, application, or service, or use of a non-harmful device, subject to reasonable\nnetwork management.”\n“No Paid Prioritization. . . . A person engaged in the provision of broadband Internet\naccess service, . . . shall not engage in paid prioritization. “Paid prioritization” refers to\nthe management of a broadband provider’s network to directly or indirectly favor some\ntraffic over other traffic, including through use of techniques such as traffic shaping,\nprioritization, resource reservation, or other forms of preferential traffic\nmanagement, . . .”\nQuite interestingly, before the Order, ISP behaviors violating the first two of these rules had\nbeen observed [Faulhaber 2012]. In 2005, an ISP in North Carolina agreed to stop its\npractice of blocking its customers from using Vonage, a voice-over-IP service that competed\nwith its own telephone service. In 2007, Comcast was judged to be interfering with\nBitTorrent P2P traffic by internally creating and sending TCP RST packets to BitTorrent\nsenders and receivers, which caused them to close their BitTorrent connection [FCC 2008].\nBoth sides of the net neutrality debate have been argued strenuously, mostly focused on\nthe extent to which net neutrality provides benefits to customers, while at the same time\npromoting innovation. See [Peha 2006, Faulhaber 2012, Economides 2017, Madhyastha\nThe 2015 FCC Order on Protecting and Promoting an Open Internet, which banned ISPs\nfrom blocking, throttling, or providing paid prioritizing, was superseded by the 2017 FCC\nRestoring Internet Freedom Order, [FCC 2017] which rolled back these prohibitions and\nfocused instead on ISP transparency. With so much interest and so many changes, it’s\nprobably safe to say we aren’t close to having seen the final chapter written on net neutrality\nin the United States, or elsewhere.\nRound Robin and Weighted Fair Queuing (WFQ)\nUnder the round robin queuing discipline, packets are sorted into classes as\nwith priority queuing. However, rather than there being a strict service\npriority among classes, a round robin scheduler alternates service among\nthe classes. In the simplest form of round robin scheduling, a class 1 packet\nis transmitted, followed by a class 2 packet, followed by a class 1 packet,\nfollowed by a class 2 packet, and so on. A so-called work-conserving\nqueuing discipline will never allow the link to remain idle whenever there\nare packets (of any class) queued for transmission. A work-conserving\nround robin discipline that looks for a packet of a given class but finds none\nwill immediately check the next class in the round robin sequence.\nFigure 4.15 illustrates the operation of a two-class round robin queue.\nIn this example, packets 1, 2, and 4 belong to class 1, and packets 3 and 5\nbelong to the second class. Packet 1 begins transmission immediately upon\narrival at the output queue. Packets 2 and 3 arrive during the transmission\nof packet 1 and thus queue for transmission. After the transmission of\npacket 1, the link scheduler looks for a class 2 packet and thus transmits\npacket 3. After the transmission of packet 3, the scheduler looks for a class\n1 packet and thus transmits packet 2. After the transmission of packet 2,\npacket 4 is the only queued packet; it is thus transmitted immediately after\nFigure 4.15 ♦The two-class robin queue in operation\nA generalized form of round robin queuing that has been widely\nimplemented in routers is the so-called weighted fair queuing (WFQ)\ndiscipline [Demers 1990; Parekh 1993. WFQ is illustrated in Figure 4.16.\nHere, arriving packets are classified and queued in the appropriate per-class\nwaiting area. As in round robin scheduling, a WFQ scheduler will serve\nclasses in a circular manner—first serving class 1, then serving class 2, then\nserving class 3, and then (assuming there are three classes) repeating the\nservice pattern. WFQ is also a work-conserving queuing discipline and thus\nwill immediately move on to the next class in the service sequence when it\nfinds an empty class queue.\nFigure 4.16 ♦Weighted fair queuing\nWFQ differs from round robin in that each class may receive a\ndifferential amount of service in any interval of time. Specifically, each\nclass, i, is assigned a weight, w . Under WFQ, during any interval of time\nduring which there are class i packets to send, class i will then be\nguaranteed to receive a fraction of service equal to w /(Σw ), where the sum\nin the denominator is taken over all classes that also have packets queued\nfor transmission. In the worst case, even if all classes have queued packets,\nclass i will still be guaranteed to receive a fraction w /(Σw ) of the\nbandwidth, where in this worst case the sum in the denominator is over all\nclasses. Thus, for a link with transmission rate R, class i will always achieve\na throughput of at least R · w  / (Σw ) Our description of WFQ has been\nidealized, as we have not considered the fact that packets are discrete and a\npacket’s transmission will not be interrupted to begin transmission of\nanother packet; [Demers 1990; Parekh 1993] discuss this packetization\n4.3 The Internet Protocol (IP): IPv4, Addressing,\nIPv6, and More\nOur study of the network layer thus far in Chapter 4—the notion of the data\nand control plane component of the network layer, our distinction between\nforwarding and routing, the identification of various network service\nmodels, and our look inside a router—have often been without reference to\nany specific computer network architecture or protocol. In this section,\nwe’ll focus on key aspects of the network layer on today’s Internet and the\ncelebrated Internet Protocol (IP).\nThere are two versions of IP in use today. We’ll first examine the\nwidely deployed IP protocol version 4, which is usually referred to simply\nas IPv4 [RFC 791] in Section 4.3.1. We’ll examine IP version 6 [RFC 2460;\nRFC 4291], which has been proposed to replace IPv4, in Section 4.3.4. In\nbetween, we’ll primarily cover Internet addressing—a topic that might\nseem rather dry and detail-oriented but we’ll see is crucial to understanding\nhow the Internet’s network layer works. To master IP addressing is to\nmaster the Internet’s network layer itself!\n4.3.1 IPv4 Datagram Format\nRecall that the Internet’s network-layer packet is referred to as a datagram.\nWe begin our study of IP with an overview of the syntax and semantics of\nthe IPv4 datagram. You might be thinking that nothing could be drier than\nthe syntax and semantics of a packet’s bits. Nevertheless, the datagram\nplays a central role in the Internet—every networking student and\nprofessional needs to see it, absorb it, and master it. (And just to see that\nprotocol headers can indeed be fun to study, check out [Pomeranz 2010]).\nThe IPv4 datagram format is shown in Figure 4.17. The key fields in the\nIPv4 datagram are the following:\nFigure 4.17 ♦IPv4 datagram format\nVersion number. These 4 bits specify the IP protocol version of the\ndatagram. By looking at the version number, the router can determine\nhow to interpret the remainder of the IP datagram. Different versions of\nIP use different datagram formats. The datagram format for IPv4 is\nshown in Figure 4.17. The datagram format for the new version of IP\n(IPv6) is discussed in Section 4.3.4.\nHeader length. Because an IPv4 datagram can contain a variable\nnumber of options (which are included in the IPv4 datagram header),\nthese 4 bits are needed to determine where in the IP datagram the\npayload (for example, the transport-layer segment being encapsulated in\nthis datagram) actually begins. Most IP datagrams do not contain\noptions, so the typical IP datagram has a 20-byte header.\nType of service. The type of service (TOS) bits were included in the\nIPv4 header to allow different types of IP datagrams to be distinguished\nfrom each other. For example, it might be useful to distinguish real-time\ndatagrams (such as those used by an IP telephony application) from\nnon-real-time traffic (e.g., FTP). The ­specific level of service to be\nprovided is a policy issue determined and configured by the network\nadministrator for that router. We also learned in Section 3.7.2 that two\nof the TOS bits are used for Explicit Congestion ­Notification.\nDatagram length. This is the total length of the IP datagram (header\nplus data), measured in bytes. Since this field is 16 bits long, the\ntheoretical maximum size of the IP datagram is 65,535 bytes. However,\ndatagrams are rarely larger than 1,500 bytes, which allows an IP\ndatagram to fit in the payload field of a maximally sized Ethernet frame.\nIdentifier, flags, fragmentation offset. These three fields have to do with\nso-called IP fragmentation, when a large IP datagram is broken into\nseveral smaller IP datagrams which are then forwarded independently to\nthe destination, where they are reassembled before their payload data\n(see below) is passed up to the transport layer at the destination host.\nInterestingly, the new version of IP, IPv6, does not allow for\nfragmentation. We’ll not cover fragmentation here; but readers can find\na detailed discussion online, among the “retired” material from earlier\nversions of this book.\nTime-to-live. The time-to-live (TTL) field is included to ensure that\ndatagrams do not circulate forever (due to, for example, a long-lived\nrouting loop) in the network. This field is decremented by one each time\nthe datagram is processed by a router. If the TTL field reaches 0, a\nrouter must drop that datagram.\nProtocol. This field is typically used only when an IP datagram reaches\nits final destination. The value of this field indicates the specific\ntransport-layer protocol to which the data portion of this IP datagram\nshould be passed. For example, a value of 6 indicates that the data\nportion is passed to TCP, while a value of 17 indicates that the data is\npassed to UDP. For a list of all possible values, see [IANA Protocol\nNumbers 2016]. Note that the protocol number in the IP datagram has a\nrole that is analogous to the role of the port number field in the\ntransport-layer segment. The protocol number is the glue that binds the\nnetwork and transport layers together, whereas the port number is the\nglue that binds the transport and application layers together. We’ll see in\nChapter 6 that the link-layer frame also has a special field that binds the\nlink layer to the network layer.\nHeader checksum. The header checksum aids a router in detecting bit\nerrors in a received IP datagram. The header checksum is computed by\ntreating each 2  bytes in the header as a number and summing these\nnumbers using 1s complement arithmetic. As discussed in Section 3.3,\nthe 1s complement of this sum, known as the Internet checksum, is\nstored in the checksum field. A router computes the header checksum\nfor each received IP datagram and detects an error condition if the\nchecksum carried in the datagram header does not equal the computed\nchecksum. Routers typically discard datagrams for which an error has\nbeen detected. Note that the checksum must be recomputed and stored\nagain at each router, since the TTL field, and possibly the options field\nas well, will change. An interesting discussion of fast algorithms for\ncomputing the Internet checksum is [RFC 1071]. A question often asked\nat this point is, why does TCP/IP perform error checking at both the\ntransport and network layers? There are several reasons for this\nrepetition. First, note that only the IP header is checksummed at the IP\nlayer, while the TCP/UDP checksum is computed over the entire\nTCP/UDP segment. Second, TCP/UDP and IP do not necessarily both\nhave to belong to the same protocol stack. TCP can, in principle, run\nover a different network-layer protocol (for example, ATM) [Black\n1995]) and IP can carry data that will not be passed to TCP/UDP.\nSource and destination IP addresses. When a source creates a datagram,\nit inserts its IP address into the source IP address field and inserts the\naddress of the ultimate destination into the destination IP address field.\nOften the source host determines the destination address via a DNS\nlookup, as discussed in Chapter 2. We’ll discuss IP addressing in detail\nin Section 4.3.2.\nOptions. The options fields allow an IP header to be extended. Header\noptions were meant to be used rarely—hence the decision to save\noverhead by not including the information in options fields in every\ndatagram header. However, the mere existence of options does\ncomplicate matters—since datagram headers can be of variable length,\none cannot determine a priori where the data field will start. Also, since\nsome datagrams may require options processing and others may not, the\namount of time needed to process an IP datagram at a router can vary\ngreatly. These considerations become particularly important for IP\nprocessing in high-performance routers and hosts. For these reasons and\nothers, IP options were not included in the IPv6 header, as discussed in\nSection 4.3.4.\nData (payload). Finally, we come to the last and most important field—\nthe raison d’etre for the datagram in the first place! In most\ncircumstances, the data field of the IP datagram contains the transport-\nlayer segment (TCP or UDP) to be delivered to the destination.\nHowever, the data field can carry other types of data, such as ICMP\nmessages (discussed in Section 5.6).\nNote that an IP datagram has a total of 20 bytes of header (assuming no\noptions). If the datagram carries a TCP segment, then each datagram carries\na total of 40 bytes of header (20 bytes of IP header plus 20 bytes of TCP\nheader) along with the application-layer message.\n4.3.2 IPv4 Addressing\nWe now turn our attention to IPv4 addressing. Although you may be\nthinking that addressing must be a straightforward topic, hopefully by the\nend of this section you’ll be convinced that Internet addressing is not only a\njuicy, subtle, and interesting topic but also one that is of central importance\nto the Internet. An excellent treatment of IPv4 addressing can be found in\nthe first chapter in [Stewart 1999].\nBefore discussing IP addressing, however, we’ll need to say a few\nwords about how hosts and routers are connected into the Internet. A host\ntypically has only a single link into the network; when IP in the host wants\nto send a datagram, it does so over this link. The boundary between the host\nand the physical link is called an interface. Now consider a router and its\ninterfaces. Because a router’s job is to receive a datagram on one link and\nforward the datagram on some other link, a router necessarily has two or\nmore links to which it is connected. The boundary between the router and\nany one of its links is also called an interface. A router thus has multiple\ninterfaces, one for each of its links. Because every host and router is\ncapable of sending and receiving IP datagrams, IP requires each host and\nrouter interface to have its own IP address. Thus, an IP address is\ntechnically associated with an interface, rather than with the host or router\ncontaining that interface.\nEach IP address is 32 bits long (equivalently, 4 bytes), and there are\nthus a total of 232 (or approximately 4 billion) possible IP addresses. These\naddresses are typically written in so-called dotted-decimal notation, in\nwhich each byte of the address is written in its decimal form and is\nseparated by a period (dot) from other bytes in the address. For example,\nconsider the IP address 193.32.216.9. The 193 is the decimal equivalent of\nthe first 8 bits of the address; the 32 is the decimal equivalent of the second\n8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary\nnotation is\n11000001 00100000 11011000 00001001\nEach interface on every host and router in the global Internet must have an\nIP address that is globally unique (except for interfaces behind NATs, as\ndiscussed in Section 4.3.3). These addresses cannot be chosen in a willy-\nnilly manner, however. A portion of an interface’s IP address will be\ndetermined by the subnet to which it is connected.\nFigure 4.18 provides an example of IP addressing and interfaces. In this\nfigure, one router (with three interfaces) is used to interconnect seven hosts.\nTake a close look at the IP addresses assigned to the host and router\ninterfaces, as there are several things to notice. The three hosts in the upper-\nleft portion of Figure 4.18, and the router interface to which they are\nconnected, all have an IP address of the form 223.1.1.xxx. That is, they all\nhave the same leftmost 24 bits in their IP address. These four interfaces are\nalso interconnected to each other by a network that contains no routers.\nThis network could be interconnected by an Ethernet LAN, in which case\nthe interfaces would be interconnected by an Ethernet switch (as we’ll\ndiscuss in Chapter 6), or by a wireless access point (as we’ll discuss in\nChapter 7). We’ll represent this routerless network connecting these hosts as\na cloud for now, and dive into the internals of such networks in Chapters 6\nFigure 4.18 ♦Interface addresses and subnets\nIn IP terms, this network interconnecting three host interfaces and one\nrouter interface forms a subnet [RFC 950]. (A subnet is also called an IP\nnetwork or simply a network in the Internet literature.) IP addressing assigns\nan address to this subnet: 223.1.1.0/24, where the /24 (“slash-24”) notation,\nsometimes known as a subnet mask, indicates that the leftmost 24 bits of\nthe 32-bit quantity define the subnet address. The 223.1.1.0/24 subnet thus\nconsists of the three host interfaces (223.1.1.1, 223.1.1.2, and 223.1.1.3)\nand one router interface (223.1.1.4). Any additional hosts attached to the\n223.1.1.0/24 subnet would be required to have an address of the form\n223.1.1.xxx. There are two additional subnets shown in Figure 4.18: the\n223.1.2.0/24 network and the 223.1.3.0/24 subnet. Figure 4.19 illustrates\nthe three IP subnets present in Figure 4.18.\nFigure 4.19 ♦Subnet addresses\nThe IP definition of a subnet is not restricted to Ethernet segments that\nconnect multiple hosts to a router interface. To get some insight here,\nconsider Figure 4.20, which shows three routers that are interconnected\nwith each other by point-to-point links. Each router has three interfaces, one\nfor each point-to-point link and one for the broadcast link that directly\nconnects the router to a pair of hosts. What subnets are present here? Three\nsubnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are similar to the\nsubnets we encountered in Figure 4.18. But note that there are three\nadditional subnets in this example as well: one subnet, 223.1.9.0/24, for the\ninterfaces that connect routers R1 and R2; another subnet, 223.1.8.0/24, for\nthe interfaces that connect routers R2 and R3; and a third subnet,\n223.1.7.0/24, for the interfaces that connect routers R3 and R1. For a\ngeneral interconnected system of routers and hosts, we can use the\nfollowing recipe to define the subnets in the system:\nTo determine the subnets, detach each interface from its host or router,\ncreating islands of isolated networks, with interfaces terminating the\nend points of the isolated networks. Each of these isolated networks is\ncalled a subnet.\nIf we apply this procedure to the interconnected system in Figure 4.20, we\nget six islands or subnets.\nFigure 4.20 ♦Three routers interconnecting six subnets\nFrom the discussion above, it’s clear that an organization (such as a\ncompany or academic institution) with multiple Ethernet segments and\npoint-to-point links will have multiple subnets, with all of the devices on a\ngiven subnet having the same subnet address. In principle, the different\nsubnets could have quite different subnet addresses. In practice, however,\ntheir subnet addresses often have much in common. To understand why,\nlet’s next turn our attention to how addressing is handled in the global\nThe Internet’s address assignment strategy is known as Classless\nInterdomain Routing (CIDR—pronounced cider) [RFC 4632]. CIDR\ngeneralizes the notion of subnet addressing. As with subnet addressing, the\n32-bit IP address is divided into two parts and again has the dotted-decimal\nform a.b.c.d/x, where x indicates the number of bits in the first part of the\nThe x most significant bits of an address of the form a.b.c.d/x constitute\nthe network portion of the IP address, and are often referred to as the prefix\n(or network prefix) of the address. An organization is typically assigned a\nblock of contiguous addresses, that is, a range of addresses with a common\nprefix (see the Principles in Practice feature). In this case, the IP addresses\nof devices within the organization will share the common prefix. When we\ncover the Internet’s BGP routing protocol in Section 5.4, we’ll see that only\nthese x leading prefix bits are considered by routers outside the\norganization’s network. That is, when a router outside the organization\nforwards a datagram whose destination address is inside the organization,\nonly the leading x bits of the address need be considered. This considerably\nreduces the size of the forwarding table in these routers, since a single entry\nof the form a.b.c.d/x will be sufficient to forward packets to any destination\nwithin the organization.\nThe remaining 32-x bits of an address can be thought of as\ndistinguishing among the devices within the organization, all of which have\nthe same network prefix. These are the bits that will be considered when\nforwarding packets at routers within the organization. These lower-order\nbits may (or may not) have an additional subnetting structure, such as that\ndiscussed above. For example, suppose the first 21 bits of the CIDRized\naddress a.b.c.d/21 specify the organization’s network prefix and are\ncommon to the IP addresses of all devices in that organization. The\nremaining 11 bits then identify the specific hosts in the organization. The\norganization’s internal structure might be such that these 11 rightmost bits\nare used for subnetting within the organization, as discussed above. For\nexample, a.b.c.d/24 might refer to a specific subnet within the organization.\nThis example of an ISP that connects eight organizations to the Internet nicely illustrates\nhow carefully allocated CIDRized addresses facilitate routing. Suppose, as shown in Figure\n4.21, that the ISP (which we’ll call Fly-By-Night-ISP) advertises to the outside world that it\nshould be sent any datagrams whose first 20 address bits match 200.23.16.0/20. The rest\nof the world need not know that within the address block 200.23.16.0/20 there are in fact\neight other organizations, each with its own subnets. This ability to use a single prefix to\nadvertise multiple networks is often referred to as address aggregation (also route\naggregation or route summarization).\nAddress aggregation works extremely well when addresses are allocated in blocks to\nISPs and then from ISPs to client organizations. But what happens when addresses are not\nallocated in such a hierarchical manner? What would happen, for example, if Fly-By-Night-\nISP acquires ISPs-R-Us and then has Organization 1 connect to the Internet through its\nsubsidiary ISPs-R-Us? As shown in Figure 4.21, the subsidiary ISPs-R-Us owns the\naddress block 199.31.0.0/16, but Organization 1’s IP addresses are unfortunately outside of\nthis address block. What should be done here? Certainly, Organization 1 could renumber all\nof its routers and hosts to have addresses within the ISPs-R-Us address block. But this is a\ncostly solution, and Organization 1 might well be reassigned to another subsidiary in the\nfuture. The solution typically adopted is for Organization 1 to keep its IP addresses in\n200.23.18.0/23. In this case, as shown in Figure 4.22, Fly-By-Night-ISP continues to\nadvertise the address block 200.23.16.0/20 and ISPs-R-Us continues to advertise\n199.31.0.0/16. However, ISPs-R-Us now also advertises the block of addresses for\nOrganization 1, 200.23.18.0/23. When other routers in the larger Internet see the address\nblocks 200.23.16.0/20 (from Fly-By-Night-ISP) and 200.23.18.0/23 (from ISPs-R-Us) and\nwant to route to an address in the block 200.23.18.0/23, they will use longest prefix\nmatching (see Section 4.2.1), and route toward ISPs-R-Us, as it advertises the longest (i.e.,\nmost-specific) address prefix that matches the destination address.\nFigure 4.21 ♦Hierarchical addressing and route aggregation\nFigure 4.22 ♦ISPs-R-Us has a more specific route to Organization\nBefore CIDR was adopted, the network portions of an IP address were\nconstrained to be 8, 16, or 24 bits in length, an addressing scheme known as\nclassful addressing, since subnets with 8-, 16-, and 24-bit subnet addresses\nwere known as class A, B, and C networks, respectively. The requirement\nthat the subnet portion of an IP address be exactly 1, 2, or 3 bytes long\nturned out to be problematic for supporting the rapidly growing number of\norganizations with small and medium-sized subnets. A class C (/24) subnet\ncould accommodate only up to 2  − 2 = 254 hosts (two of the 2  = 256\naddresses are reserved for special use)—too small for many organizations.\nHowever, a class B (/16) subnet, which supports up to 65,634 hosts, was too\nlarge. Under classful addressing, an organization with, say, 2,000 hosts was\ntypically allocated a class B (/16) subnet address. This led to a rapid\ndepletion of the class B address space and poor utilization of the assigned\naddress space. For example, the organization that used a class B address for\nits 2,000 hosts was allocated enough of the address space for up to 65,534\ninterfaces—leaving more than 63,000 addresses that could not be used by\nother organizations.\nWe would be remiss if we did not mention yet another type of IP\naddress, the IP broadcast address 255.255.255.255. When a host sends a\ndatagram with destination address 255.255.255.255, the message is\ndelivered to all hosts on the same subnet. Routers optionally forward the\nmessage into neighboring subnets as well (although they usually don’t).\nHaving now studied IP addressing in detail, we need to know how hosts\nand subnets get their addresses in the first place. Let’s begin by looking at\nhow an organization gets a block of addresses for its devices, and then look\nat how a device (such as a host) is assigned an address from within the\norganization’s block of addresses.\nObtaining a Block of Addresses\nIn order to obtain a block of IP addresses for use within an organization’s\nsubnet, a network administrator might first contact its ISP, which would\nprovide addresses from a larger block of addresses that had already been\nallocated to the ISP. For example, the ISP may itself have been allocated the\naddress block 200.23.16.0/20. The ISP, in turn, could divide its address\nblock into eight equal-sized contiguous address blocks and give one of\nthese address blocks out to each of up to eight organizations that are\nsupported by this ISP, as shown below. (We have underlined the subnet part\nof these addresses for your convenience.)\nWhile obtaining a set of addresses from an ISP is one way to get a\nblock of addresses, it is not the only way. Clearly, there must also be a way\nfor the ISP itself to get a block of addresses. Is there a global authority that\nhas ultimate responsibility for managing the IP address space and allocating\naddress blocks to ISPs and other organizations? Indeed there is! IP\naddresses are managed under the authority of the Internet Corporation for\nAssigned Names and Numbers (ICANN) [ICANN 2020], based on\nguidelines set forth in [RFC 7020]. The role of the nonprofit ICANN\norganization is not only to allocate IP addresses, but also to manage the\nDNS root servers. It also has the very contentious job of assigning domain\nnames and resolving domain name disputes. The ICANN allocates\naddresses to regional Internet registries (for example, ARIN, RIPE, APNIC,\nand LACNIC, which together form the Address Supporting Organization of\nICANN [ASO-ICANN 2020]), and handle the allocation/management of\naddresses within their regions.\nObtaining a Host Address: The Dynamic Host Configuration\nOnce an organization has obtained a block of addresses, it can assign\nindividual IP addresses to the host and router interfaces in its organization.\nA system administrator will typically manually configure the IP addresses\ninto the router (often remotely, with a network management tool). Host\naddresses can also be configured manually, but typically this is done using\nthe Dynamic Host Configuration Protocol (DHCP) [RFC 2131]. DHCP\nallows a host to obtain (be allocated) an IP address automatically. A network\nadministrator can configure DHCP so that a given host receives the same IP\naddress each time it connects to the network, or a host may be assigned a\ntemporary IP address that will be different each time the host connects to\nthe network. In addition to host IP address assignment, DHCP also allows a\nhost to learn additional information, such as its subnet mask, the address of\nits first-hop router (often called the default gateway), and the address of its\nlocal DNS server.\nBecause of DHCP’s ability to automate the network-related aspects of\nconnecting a host into a network, it is often referred to as a plug-and-play\nor zeroconf (zero-configuration) protocol. This capability makes it very\nattractive to the network administrator who would otherwise have to\nperform these tasks manually! DHCP is also enjoying widespread use in\nresidential Internet access networks, enterprise networks, and in wireless\nLANs, where hosts join and leave the network frequently. Consider, for\nexample, the student who carries a laptop from a dormitory room to a\nlibrary to a classroom. It is likely that in each location, the student will be\nconnecting into a new subnet and hence will need a new IP address at each\nlocation. DHCP is ideally suited to this situation, as there are many users\ncoming and going, and addresses are needed for only a limited amount of\ntime. The value of DHCP’s plug-and-play capability is clear, since it’s\nunimaginable that a system administrator would be able to reconfigure\nlaptops at each location, and few students (except those taking a computer\nnetworking class!) would have the expertise to configure their laptops\nDHCP is a client-server protocol. A client is typically a newly arriving\nhost wanting to obtain network configuration information, including an IP\naddress for itself. In the simplest case, each subnet (in the addressing sense\nof Figure 4.20) will have a DHCP server. If no server is present on the\nsubnet, a DHCP relay agent (typically a router) that knows the address of a\nDHCP server for that network is needed. Figure 4.23 shows a DHCP server\nattached to subnet 223.1.2/24, with the router serving as the relay agent for\narriving clients attached to subnets 223.1.1/24 and 223.1.3/24. In our\ndiscussion below, we’ll assume that a DHCP server is available on the\nFor a newly arriving host, the DHCP protocol is a four-step process, as\nshown in Figure 4.24 for the network setting shown in Figure 4.23. In this\nfigure, yiaddr (as in “your Internet address”) indicates the address being\nallocated to the newly arriving client. The four steps are:\nFigure 4.23 ♦DHCP client and server\nDHCP server discovery. The first task of a newly arriving host is to find\na DHCP server with which to interact. This is done using a DHCP\ndiscover message, which a client sends within a UDP packet to port 67.\nThe UDP packet is encapsulated in an IP datagram. But to whom should\nthis datagram be sent? The host doesn’t even know the IP address of the\nnetwork to which it is attaching, much less the address of a DHCP\nserver for this network. Given this, the DHCP client creates an IP\ndatagram containing its DHCP discover message along with the\nbroadcast destination IP address of 255.255.255.255 and a “this host”\nsource IP address of 0.0.0.0. The DHCP client passes the IP datagram to\nthe link layer, which then broadcasts this frame to all nodes attached to\nthe subnet (we will cover the details of link-layer broadcasting in\nSection 6.4).\nDHCP server offer(s). A DHCP server receiving a DHCP discover\nmessage responds to the client with a DHCP offer message that is\nbroadcast to all nodes on the subnet, again using the IP broadcast\naddress of 255.255.255.255. (You might want to think about why this\nserver reply must also be broadcast). Since several DHCP servers can\nbe present on the subnet, the client may find itself in the enviable\nposition of being able to choose from among several offers. Each server\noffer message contains the transaction ID of the received discover\nmessage, the proposed IP address for the client, the network mask, and\nan IP address lease time—the amount of time for which the IP address\nwill be valid. It is common for the server to set the lease time to several\nhours or days [Droms 2002].\nDHCP request. The newly arriving client will choose from among one\nor more server offers and respond to its selected offer with a DHCP\nrequest message, echoing back the configuration parameters.\nDHCP ACK. The server responds to the DHCP request message with a\nDHCP ACK message, confirming the requested parameters.\nFigure 4.24 ♦DHCP client-server interaction\nOnce the client receives the DHCP ACK, the interaction is complete\nand the client can use the DHCP-allocated IP address for the lease duration.\nSince a client may want to use its address beyond the lease’s expiration,\nDHCP also provides a mechanism that allows a client to renew its lease on\nan IP address.\nFrom a mobility aspect, DHCP does have one very significant\nshortcoming. Since a new IP address is obtained from DHCP each time a\nnode connects to a new subnet, a TCP connection to a remote application\ncannot be maintained as a mobile node moves between subnets. In Chapter\n7, we will learn how mobile cellular networks allow a host to retain its IP\naddress and ongoing TCP connections as it moves between base stations in\na provider’s cellular network. Additional details about DHCP can be found\nin [Droms 2002] and [dhc 2020]. An open source reference implementation\nof DHCP is available from the Internet Systems Consortium [ISC 2020].\n4.3.3 Network Address Translation (NAT)\nGiven our discussion about Internet addresses and the IPv4 datagram\nformat, we’re now well aware that every IP-capable device needs an IP\naddress. With the ­proliferation of small office, home office (SOHO)\nsubnets, this would seem to imply that whenever a SOHO wants to install a\nLAN to connect multiple machines, a range of addresses would need to be\nallocated by the ISP to cover all of the SOHO’s IP devices (including\nphones, tablets, gaming devices, IP TVs, printers and more). If the subnet\ngrew bigger, a larger block of addresses would have to be allocated. But\nwhat if the ISP had already allocated the contiguous portions of the SOHO ­-\nnetwork’s current address range? And what typical homeowner wants (or\nshould need) to know how to manage IP addresses in the first place?\nFortunately, there is a simpler approach to address allocation that has found\nincreasingly widespread use in such scenarios: network address\ntranslation (NAT) [RFC 2663; RFC 3022; Huston 2004, Zhang 2007;\nHuston 2017].\nFigure 4.25 shows the operation of a NAT-enabled router. The NAT-\nenabled router, residing in the home, has an interface that is part of the\nhome network on the right of Figure 4.25. Addressing within the home\nnetwork is exactly as we have seen above—all four interfaces in the home\nnetwork have the same subnet address of 10.0.0.0/24. The address space\n10.0.0.0/8 is one of three portions of the IP address space that is reserved in\n[RFC 1918] for a private network or a realm with private addresses,\nsuch as the home network in Figure 4.25. A realm with private addresses\nrefers to a network whose addresses only have meaning to devices within\nthat network. To see why this is important, consider the fact that there are\nhundreds of thousands of home networks, many using the same address\nspace, 10.0.0.0/24. Devices within a given home network can send packets\nto each other using 10.0.0.0/24 addressing. However, packets forwarded\nbeyond the home network into the larger global Internet clearly cannot use\nthese addresses (as either a source or a destination address) because there\nare hundreds of thousands of networks using this block of addresses. That\nis, the 10.0.0.0/24 addresses can only have meaning within the given home\nnetwork. But if private addresses only have meaning within a given\nnetwork, how is addressing handled when packets are sent to or received\nfrom the global Internet, where addresses are necessarily unique? The\nanswer lies in understanding NAT.\nFigure 4.25 ♦Network address translation\nThe NAT-enabled router does not look like a router to the outside\nworld. Instead the NAT router behaves to the outside world as a single\ndevice with a single IP address. In Figure 4.25, all traffic leaving the home\nrouter for the larger Internet has a source IP address of 138.76.29.7, and all\ntraffic entering the home router must have a destination address of\n138.76.29.7. In essence, the NAT-enabled router is hiding the details of the\nhome network from the outside world. (As an aside, you might wonder\nwhere the home network computers get their addresses and where the router\ngets its single IP address. Often, the answer is the same—DHCP! The router\ngets its address from the ISP’s DHCP server, and the router runs a DHCP\nserver to provide addresses to computers within the NAT-DHCP-router-\ncontrolled home network’s address space.)\nIf all datagrams arriving at the NAT router from the WAN have the\nsame destination IP address (specifically, that of the WAN-side interface of\nthe NAT router), then how does the router know the internal host to which it\nshould forward a given datagram? The trick is to use a NAT translation\ntable at the NAT router, and to include port numbers as well as IP addresses\nin the table entries.\nConsider the example in Figure 4.25. Suppose a user sitting in a home\nnetwork behind host 10.0.0.1 requests a Web page on some Web server\n(port 80) with IP address 128.119.40.186. The host 10.0.0.1 assigns the\n(arbitrary) source port number 3345 and sends the datagram into the LAN.\nThe NAT router receives the datagram, generates a new source port number\n5001 for the datagram, replaces the source IP address with its WAN-side IP\naddress 138.76.29.7, and replaces the original source port number 3345\nwith the new source port number 5001. When generating a new source port\nnumber, the NAT router can select any source port number that is not\ncurrently in the NAT translation table. (Note that because a port number\nfield is 16 bits long, the NAT protocol can support over 60,000\nsimultaneous connections with a single WAN-side IP address for the\nrouter!) NAT in the router also adds an entry to its NAT translation table.\nThe Web server, blissfully unaware that the arriving datagram containing\nthe HTTP request has been manipulated by the NAT router, responds with a\ndatagram whose destination address is the IP address of the NAT router, and\nwhose destination port number is 5001. When this datagram arrives at the\nas we have seen in Chapter 2, server processes wait for incoming requests\nat well-known port numbers and peers in a P2P protocol need to accept\nincoming connections when acting as servers. How can one peer connect to\nanother peer that is behind a NAT server, and has a DHCP-provided NAT\naddress? Technical solutions to these problems include NAT traversal tools\n[RFC 5389] [RFC 5389, RFC 5128, Ford 2005].\nMore “philosophical” arguments have also been raised against NAT by\narchitectural purists. Here, the concern is that routers are meant to be layer\n3 (i.e., network-layer) devices, and should process packets only up to the\nnetwork layer. NAT violates this principle that hosts should be talking\ndirectly with each other, without interfering nodes modifying IP addresses,\nmuch less port numbers. We’ll return to this debate later in Section 4.5,\nwhen we cover middleboxes.\nINSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION SYSTEMS\nSuppose you are assigned the task of administering a home, departmental, university,\nor corporate network. Attackers, knowing the IP address range of your network, can\neasily send IP datagrams to addresses in your range. These datagrams can do all\nkinds of devious things, including mapping your network with ping sweeps and port\nscans, crashing vulnerable hosts with malformed packets, scanning for open TCP/UDP\nports on servers in your network, and infecting hosts by including malware in the\npackets. As the network administrator, what are you going to do about all those bad\nguys out there, each capable of sending malicious packets into your network? Two\npopular defense mechanisms to malicious packet attacks are firewalls and intrusion\ndetection systems (IDSs).\nAs a network administrator, you may first try installing a firewall between your\nnetwork and the Internet. (Most access routers today have firewall capability.) Firewalls\ninspect the datagram and segment header fields, denying suspicious datagrams entry\ninto the internal network. For example, a firewall may be configured to block all ICMP\necho request packets (see Section 5.6), thereby preventing an attacker from doing a\ntraditional port scan across your IP address range. Firewalls can also block packets\nbased on source and destination IP addresses and port numbers. Additionally, firewalls\ncan be configured to track TCP connections, granting entry only to datagrams that\nbelong to approved connections.\nAdditional protection can be provided with an IDS. An IDS, typically situated at the\nnetwork boundary, performs “deep packet inspection,” examining not only header fields\nbut also the payloads in the datagram (including application-layer data). An IDS has a\ndatabase of packet signatures that are known to be part of attacks. This database is\nautomatically updated as new attacks are discovered. As packets pass through the\nIDS, the IDS attempts to match header fields and payloads to the signatures in its\nsignature database. If such a match is found, an alert is created. An intrusion\nprevention system (IPS) is similar to an IDS, except that it actually blocks packets in\naddition to creating alerts. We’ll explore firewalls and IDSs in more detail in Section 4.5\nand in again Chapter 8.\nCan firewalls and IDSs fully shield your network from all attacks? The answer is\nclearly no, as attackers continually find new attacks for which signatures are not yet\navailable. But firewalls and traditional signature-based IDSs are useful in protecting\nyour network from known attacks.\nIn the early 1990s, the Internet Engineering Task Force began an effort to\ndevelop a successor to the IPv4 protocol. A prime motivation for this effort\nwas the realization that the 32-bit IPv4 address space was beginning to be\nused up, with new subnets and IP nodes being attached to the Internet (and\nbeing allocated unique IP addresses) at a breathtaking rate. To respond to\nthis need for a large IP address space, a new IP protocol, IPv6, was\ndeveloped. The designers of IPv6 also took this opportunity to tweak and\naugment other aspects of IPv4, based on the accumulated operational\nexperience with IPv4.\nThe point in time when IPv4 addresses would be completely allocated\n(and hence no new networks could attach to the Internet) was the subject of\nconsiderable debate. The estimates of the two leaders of the IETF’s Address\nLifetime Expectations working group were that addresses would become\nexhausted in 2008 and 2018, respectively [Solensky 1996]. In February\n2011, IANA allocated out the last remaining pool of unassigned IPv4\naddresses to a regional registry. While these registries still have available\nIPv4 addresses within their pool, once these addresses are exhausted, there\nare no more available address blocks that can be allocated from a central\npool [Huston 2011a]. A recent survey of IPv4 address-space exhaustion, and\nthe steps taken to prolong the life of the address space is [Richter 2015]; a\nrecent analysis of IPv4 address use is [Huston 2019].\nAlthough the mid-1990s estimates of IPv4 address depletion suggested\nthat a considerable amount of time might be left until the IPv4 address\nspace was exhausted, it was realized that considerable time would be\nneeded to deploy a new technology on such an extensive scale, and so the\nprocess to develop IP version 6 (IPv6) [RFC 2460] was begun [RFC 1752].\n(An often-asked question is what happened to IPv5? It was initially\nenvisioned that the ST-2 protocol would become IPv5, but ST-2 was later\ndropped.) An excellent source of information about IPv6 is [Huitema 1998].\nIPv6 Datagram Format\nThe format of the IPv6 datagram is shown in Figure 4.26. The most\nimportant changes introduced in IPv6 are evident in the datagram format:\nExpanded addressing capabilities. IPv6 increases the size of the IP\naddress from 32 to 128 bits. This ensures that the world won’t run out\nof IP addresses. Now, every grain of sand on the planet can be IP-\naddressable. In addition to unicast and multicast addresses, IPv6 has\nintroduced a new type of address, called an anycast address, that\nallows a datagram to be delivered to any one of a group of hosts. (This\nfeature could be used, for example, to send an HTTP GET to the nearest\nof a number of mirror sites that contain a given document.)\nA streamlined 40-byte header. As discussed below, a number of IPv4\nfields have been dropped or made optional. The resulting 40-byte fixed-\nlength header allows for faster processing of the IP datagram by a\nrouter. A new encoding of options allows for more flexible options\nprocessing.\nFlow labeling. IPv6 has an elusive definition of a flow. RFC 2460 states\nthat this allows “labeling of packets belonging to particular flows for\nwhich the sender requests special handling, such as a non-default\nquality of service or real-time service.” For example, audio and video\ntransmission might likely be treated as a flow. On the other hand, the\nmore traditional applications, such as file transfer and e-mail, might not\nbe treated as flows. It is possible that the traffic carried by a high-\npriority user (for example, someone paying for better service for their\ntraffic) might also be treated as a flow. What is clear, however, is that\nthe designers of IPv6 foresaw the eventual need to be able to\ndifferentiate among the flows, even if the exact meaning of a flow had\nyet to be determined.\nAs noted above, a comparison of Figure 4.26 with Figure 4.17 reveals\nthe simpler, more streamlined structure of the IPv6 datagram. The following\nfields are defined in IPv6:\nFigure 4.26 ♦IPv6 datagram format\nVersion. This 4-bit field identifies the IP version number. Not\nsurprisingly, IPv6 carries a value of 6 in this field. Note that putting a 4\nin this field does not create a valid IPv4 datagram. (If it did, life would\nbe a lot simpler—see the discussion below regarding the transition from\nIPv4 to IPv6.)\nTraffic class. The 8-bit traffic class field, like the TOS field in IPv4, can\nbe used to give priority to certain datagrams within a flow, or it can be\nused to give priority to datagrams from certain applications (for\nexample, voice-over-IP) over datagrams from other applications (for\nexample, SMTP e-mail).\nFlow label. As discussed above, this 20-bit field is used to identify a\nflow of datagrams.\nPayload length. This 16-bit value is treated as an unsigned integer\ngiving the number of bytes in the IPv6 datagram following the fixed-\nlength, 40-byte datagram header.\nthat we’ll cover in Chapter 7—is the following. Suppose two IPv6 nodes (in\nthis example, B and E in Figure 4.27) want to interoperate using IPv6\ndatagrams but are connected to each other by intervening IPv4 routers. We\nrefer to the intervening set of IPv4 routers between two IPv6 routers as a\ntunnel, as illustrated in Figure 4.27. With tunneling, the IPv6 node on the\nsending side of the tunnel (in this example, B) takes the entire IPv6\ndatagram and puts it in the data (payload) field of an IPv4 datagram. This\nIPv4 datagram is then addressed to the IPv6 node on the receiving side of\nthe tunnel (in this example, E) and sent to the first node in the tunnel (in this\nexample, C). The intervening IPv4 routers in the tunnel route this IPv4\ndatagram among themselves, just as they would any other datagram,\nblissfully unaware that the IPv4 datagram itself contains a complete IPv6\ndatagram. The IPv6 node on the receiving side of the tunnel eventually\nreceives the IPv4 datagram (it is the destination of the IPv4 datagram!),\ndetermines that the IPv4 datagram contains an IPv6 datagram (by observing\nthat the protocol number field in the IPv4 datagram is 41 [RFC 4213],\nindicating that the IPv4 payload is a IPv6 datagram), extracts the IPv6\ndatagram, and then routes the IPv6 datagram exactly as it would if it had\nreceived the IPv6 datagram from a directly connected IPv6 neighbor.\nFigure 4.27 ♦Tunneling\nWe end this section by noting that while the adoption of IPv6 was\ninitially slow to take off [Lawton 2001; Huston 2008b], momentum has\nbeen building. NIST [NIST IPv6 2020] reports that more than a third of US\ngovernment second-level domains are IPv6-enabled. On the client side,\nGoogle reports that about 25 percent of the clients accessing Google\nservices do so via IPv6 [Google IPv6 2020]. Other recent measurements\n[Czyz 2014] indicate that IPv6 adoption has been accelerating. The\nproliferation of devices such as IP-enabled phones and other portable\ndevices provides an additional push for more widespread deployment of\nIPv6. Europe’s Third Generation Partnership Program [3GPP 2020] has\nspecified IPv6 as the standard addressing scheme for mobile multimedia.\nOne important lesson that we can learn from the IPv6 experience is that\nit is enormously difficult to change network-layer protocols. Since the early\n1990s, numerous new network-layer protocols have been trumpeted as the\nnext major revolution for the Internet, but most of these protocols have had\nlimited penetration to date. These protocols include IPv6, multicast\nprotocols, and resource reservation protocols; a discussion of these latter\ntwo classes of protocols can be found in the online supplement to this text.\nIndeed, introducing new protocols into the network layer is like replacing\nthe foundation of a house—it is difficult to do without tearing the whole\nhouse down or at least temporarily relocating the house’s residents. On the\nother hand, the Internet has witnessed rapid deployment of new protocols at\nthe application layer. The classic examples, of course, are the Web, instant\nmessaging, streaming media, distributed games, and various forms of social\nmedia. Introducing new application-layer protocols is like adding a new\nlayer of paint to a house—it is relatively easy to do, and if you choose an\nattractive color, others in the neighborhood will copy you. In summary, in\nthe future, we can certainly expect to see changes in the Internet’s network\nlayer, but these changes will likely occur on a time scale that is much\nslower than the changes that will occur at the application layer.\n4.4 Generalized Forwarding and SDN\nRecall that Section 4.2.1 characterized destination-based forwarding as the\ntwo steps of looking up a destination IP address (“match”), then sending the\npacket into the switching fabric to the specified output port (“action”). Let’s\nnow consider a significantly more general “match-plus-action” paradigm,\nwhere the “match” can be made over multiple header fields associated with\ndifferent protocols at different layers in the protocol stack. The “action” can\ninclude forwarding the packet to one or more output ports (as in\ndestination-based forwarding), load balancing packets across multiple\noutgoing interfaces that lead to a service (as in load balancing), rewriting\nheader values (as in NAT), purposefully blocking/dropping a packet (as in a\nfirewall), sending a packet to a special server for further processing and\naction (as in DPI), and more.\nIn generalized forwarding, a match-plus-action table generalizes the\nnotion of the destination-based forwarding table that we encountered in\nSection 4.2.1. Because forwarding decisions may be made using network-\nlayer and/or link-layer source and destination addresses, the forwarding\ndevices shown in Figure 4.28 are more accurately described as “packet\nswitches” rather than layer 3 “routers” or layer 2 “switches.” Thus, in the\nremainder of this section, and in Section 5.5, we’ll refer to these devices as\npacket switches, adopting the terminology that is gaining widespread\nadoption in SDN literature.\nFigure 4.28 shows a match-plus-action table in each packet switch, with\nthe table being computed, installed, and updated by a remote controller. We\nnote that while it is possible for the control components at the individual\npacket switches to interact with each other (e.g., in a manner similar to that\nin Figure 4.2), in practice, generalized match-plus-action capabilities are\nimplemented via a remote controller that computes, installs, and updates\nthese tables. You might take a minute to compare Figures 4.2, 4.3, and 4.28\n—what similarities and differences do you notice between destination-based\nforwarding shown in Figures 4.2 and 4.3, and generalized forwarding\nshown in Figure 4.28?\nFigure 4.28 ♦Generalized forwarding: Each packet switch contains\na match-plus-action table that is computed and\ndistributed by a remote controller\nOur following discussion of generalized forwarding will be based on\nOpenFlow [McKeown 2008, ONF 2020, Casado 2014, Tourrilhes 2014]—a\nhighly visible standard that has pioneered the notion of the match-plus-\naction forwarding abstraction and controllers, as well as the SDN revolution\nmore generally [Feamster 2013]. We’ll primarily consider OpenFlow 1.0,\nwhich introduced key SDN abstractions and functionality in a particularly\nclear and concise manner. Later versions of ­OpenFlow introduced\nadditional capabilities as a result of experience gained through\nimplementation and use; current and earlier versions of the OpenFlow\nstandard can be found at [ONF 2020].\nEach entry in the match-plus-action forwarding table, known as a flow\ntable in OpenFlow, includes:\nA set of header field values to which an incoming packet will be\nmatched. As in the case of destination-based forwarding, hardware-\nbased matching is most rapidly performed in TCAM memory, with\nmore than a million destination address entries being possible [Bosshart\n2013]. A packet that matches no flow table entry can be dropped or sent\nto the remote controller for more processing. In practice, a flow table\nmay be implemented by multiple flow tables for performance or cost\nreasons [Bosshart 2013], but we’ll focus here on the abstraction of a\nsingle flow table.\nA set of counters that are updated as packets are matched to flow table\nentries. These counters might include the number of packets that have\nbeen matched by that table entry, and the time since the table entry was\nlast updated.\nA set of actions to be taken when a packet matches a flow table entry.\nThese actions might be to forward the packet to a given output port, to\ndrop the packet, makes copies of the packet and sent them to multiple\noutput ports, and/or to rewrite selected header fields.\nWe’ll explore matching and actions in more detail in Sections 4.4.1 and\n4.4.2, respectively. We’ll then study how the network-wide collection of\nper-packet switch matching rules can be used to implement a wide range of\nfunctions including routing, layer-2 switching, firewalling, load-balancing,\nvirtual networks, and more in Section 4.4.3. In closing, we note that the\nflow table is essentially an API, the abstraction through which an individual\npacket switch’s behavior can be programmed; we’ll see in Section 4.4.3 that\nnetwork-wide behaviors can similarly be programmed by appropriately\nprogramming/configuring these tables in a collection of network packet\nswitches [Casado 2014].\n4.4.1 Match\nFigure 4.29 shows the 11 packet-header fields and the incoming port ID that\ncan be matched in an OpenFlow 1.0 match-plus-action rule. Recall from\nSection 1.5.2 that a link-layer (layer 2) frame arriving to a packet switch\nwill contain a network-layer (layer 3) datagram as its payload, which in turn\nwill typically contain a transport-layer (layer 4) segment. The first\nobservation we make is that OpenFlow’s match abstraction allows for a\nmatch to be made on selected fields from three layers of protocol headers\n(thus rather brazenly defying the layering principle we studied in Section\n1.5). Since we’ve not yet covered the link layer, suffice it to say that the\nsource and destination MAC addresses shown in Figure 4.29 are the link-\nlayer addresses associated with the frame’s sending and receiving\ninterfaces; by forwarding on the basis of Ethernet addresses rather than IP\naddresses, we can see that an OpenFlow-enabled device can equally\nperform as a router (layer-3 device) forwarding datagrams as well as a\nswitch (layer-2 device) forwarding frames. The Ethernet type field\ncorresponds to the upper layer protocol (e.g., IP) to which the frame’s\npayload will be de-multiplexed, and the VLAN fields are concerned with\nso-called virtual local area networks that we’ll study in Chapter 6. The set\nof 12 values that can be matched in the OpenFlow 1.0 specification has\ngrown to 41 values in more recent OpenFlow specifications [Bosshart\nFigure 4.29 ♦Packet matching fields, OpenFlow 1.0 flow table\nThe ingress port refers to the input port at the packet switch on which a\npacket is received. The packet’s IP source address, IP destination address,\nIP protocol field, and IP type of service fields were discussed earlier in\nSection 4.3.1. The transport-layer source and destination port number fields\ncan also be matched.\nFlow table entries may also have wildcards. For example, an IP address\nof 128.119.*.* in a flow table will match the corresponding address field of\nany datagram that has 128.119 as the first 16 bits of its address. Each flow\ntable entry also has an associated priority. If a packet matches multiple flow\ntable entries, the selected match and corresponding action will be that of the\nhighest priority entry with which the packet matches.\nLastly, we observe that not all fields in an IP header can be matched.\nFor example OpenFlow does not allow matching on the basis of TTL field\nor datagram length field. Why are some fields allowed for matching, while\nothers are not? Undoubtedly, the answer has to do with the tradeoff between\nfunctionality and complexity. The “art” in choosing an abstraction is to\nprovide for enough functionality to accomplish a task (in this case to\nimplement, configure, and manage a wide range of network-layer functions\nthat had previously been implemented through an assortment of ­network-\nlayer devices), without over-burdening the abstraction with so much detail\nand generality that it becomes bloated and unusable. Butler Lampson has\nfamously noted [Lampson 1983]:\nDo one thing at a time, and do it well. An interface should capture the\nminimum essentials of an abstraction. Don’t generalize; generalizations\nare generally wrong.\nGiven OpenFlow’s success, one can surmise that its designers indeed chose\ntheir abstraction well. Additional details of OpenFlow matching can be\nfound in [ONF 2020].\n4.4.2 Action\nAs shown in Figure 4.28, each flow table entry has a list of zero or more\nactions that determine the processing that is to be applied to a packet that\nmatches a flow table entry. If there are multiple actions, they are performed\nin the order specified in the list.\nAmong the most important possible actions are:\nForwarding. An incoming packet may be forwarded to a particular\nphysical output port, broadcast over all ports (except the port on which\nit arrived) or multicast over a selected set of ports. The packet may be\nencapsulated and sent to the remote controller for this device. That\ncontroller then may (or may not) take some action on that packet,\nincluding installing new flow table entries, and may return the packet to\nthe device for forwarding under the updated set of flow table rules.\nDropping. A flow table entry with no action indicates that a matched\npacket should be dropped.\nModify-field. The values in 10 packet-header fields (all layer 2, 3, and 4\nfields shown in Figure 4.29 except the IP Protocol field) may be re-\nwritten before the packet is forwarded to the chosen output port.\n4.4.3 OpenFlow Examples of Match-plus-action in\nHaving now considered both the match and action components of\ngeneralized forwarding, let’s put these ideas together in the context of the\nsample network shown in Figure 4.30. The network has 6 hosts (h1, h2, h3,\nh4, h5 and h6) and three packet switches (s1, s2 and s3), each with four\nlocal interfaces (numbered 1 through 4). We’ll consider a number of\nnetwork-wide behaviors that we’d like to implement, and the flow table\nentries in s1, s2 and s3 needed to implement this behavior.\nFigure 4.30 ♦OpenFlow match-plus-action network with three\npacket switches, 6 hosts, and an OpenFlow controller\nA First Example: Simple Forwarding\nAs a very simple example, suppose that the desired forwarding behavior is\nthat packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3\nto s1, and then from s1 to s2 (thus completely avoiding the use of the link\nbetween s3 and s2). The flow table entry in s1 would be:\nOf course, we’ll also need a flow table entry in s3 so that datagrams\nsent from h5 or h6 are forwarded to s1 over outgoing interface 3:\nLastly, we’ll also need a flow table entry in s2 to complete this first\nexample, so that datagrams arriving from s1 are forwarded to their\ndestination, either host h3 or h4:\nA Second Example: Load Balancing\nAs a second example, let’s consider a load-balancing scenario, where\ndatagrams from h3 destined to 10.1.*.* are to be forwarded over the direct\nlink between s2 and s1, while datagrams from h4 destined to 10.1.*.* are to\nbe forwarded over the link between s2 and s3 (and then from s3 to s1). Note\nthat this behavior couldn’t be achieved with IP’s destination-based\nforwarding. In this case, the flow table in s2 would be:\nFlow table entries are also needed at s1 to forward the datagrams\nreceived from s2 to either h1 or h2; and flow table entries are needed at s3\nto forward datagrams received on interface 4 from s2 over interface 3\ntoward s1. See if you can figure out these flow table entries at s1 and s3.\nA Third Example: Firewalling\nAs a third example, let’s consider a firewall scenario in which s2 wants only\nto receive (on any of its interfaces) traffic sent from hosts attached to s3.\nIf there were no other entries in s2’s flow table, then only traffic from\n10.3.*.* would be forwarded to the hosts attached to s2.\nAlthough we’ve only considered a few basic scenarios here, the\nversatility and advantages of generalized forwarding are hopefully apparent.\nIn homework problems, we’ll explore how flow tables can be used to create\nmany different logical behaviors, including virtual networks—two or more\nlogically separate networks (each with their own independent and distinct\nforwarding behavior)—that use the same physical set of packet switches\nand links. In Section 5.5, we’ll return to flow tables when we study the\nSDN controllers that compute and distribute the flow tables, and the\nprotocol used for communicating between a packet switch and its controller.\nThe match-plus-action flow tables that we’ve seen in this section are\nactually a limited form of programmability, specifying how a router should\nforward and manipulate (e.g., change a header field) a datagram, based on\nthe match between the datagram’s header values and the matching\nconditions. One could imagine an even richer form of programmability—a\nprogramming language with higher-level constructs such as variables,\ngeneral purpose arithmetic and Boolean operations, variables, functions,\nand conditional statements, as well as constructs specifically designed for\ndatagram processing at line rate. P4 (Programming Protocol-independent\nPacket Processors) [P4 2020] is such a language, and has gained\nconsiderable interest and traction since its introduction five years ago\n[Bosshart 2014].\n4.5 Middleboxes\nRouters are the workhorses of the network layer, and in this chapter, we’ve\nlearned how they accomplish their “bread and butter” job of forwarding IP\ndatagrams toward their destination. But in this chapter, and in earlier\nchapters, we’ve also encountered other network equipment (“boxes”) within\nthe network that sit on the data path and perform functions other than\nforwarding. We encountered Web caches in Section 2.2.5; TCP connection\nsplitters in section 3.7; and network address translation (NAT), firewalls,\nand intrusion detection systems in Section 4.3.4. We learned in Section 4.4\nthat generalized forwarding allows a modern router to easily and naturally\nperform firewalling and load balancing with generalized “match plus\naction” operations.\nIn the past 20 years, we’ve seen tremendous growth in such\nmiddleboxes, which RFC 3234 defines as:\n“any intermediary box performing functions apart from normal,\nstandard functions of an IP router on the data path between a source\nhost and destination host”\nWe can broadly identify three types of services performed by middleboxes:\nNAT Translation. As we saw in Section 4.3.4, NAT boxes implement\nprivate network addressing, rewriting datagram header IP addresses and\nport numbers.\nSecurity Services. Firewalls block traffic based on header-field values or\nredirect packets for additional processing, such as deep packet\ninspection (DPI). Intrusion Detection Systems (IDS) are able to detect\npredetermined patterns and filter packets accordingly. Application-level\ne-mail filters block e-mails considered to be junk, phishing or otherwise\nposing a security threat.\nPerformance Enhancement. These middleboxes perform services such\nas compression, content caching, and load balancing of service requests\n(e.g., an HTTP request, or a search engine query) to one of a set of\nservers that can provide the desired service.\nMany other middleboxes [RFC 3234] provide capabilities belonging to\nthese three types of services, in both wired and wireless cellular [Wang\n2011] networks.\nWith the proliferation of middleboxes comes the attendant need to\noperate, manage, and upgrade this equipment. Separate specialized\nmanagement/operation skills translate to significant operational and capital\ncosts. It is perhaps not surprising then that researchers are exploring the use\nof commodity hardware (networking, computing, and storage) with\nspecialized software built on top of a common software stack—exactly the\napproach taken in SDN a decade earlier—to implement these services. This\napproach has become known as network function virtualization (NFV)\n[Mijumbi 2016]. An alternate approach that has also been explored is to\noutsource middlebox functionality to the cloud [Sherry 2012].\nFor many years, the Internet architecture had a clear separation between\nthe network layer and the transport/application layers. In these “good old\ndays,” the network layer consisted of routers, operating within the network\ncore, to forward datagrams toward their destinations using fields only in the\nIP datagram header. The transport and application layers were implemented\nin hosts operating at the network edge. Hosts exchanged packets among\nthemselves in transport-layer segments and application-layer messages.\nToday’s middleboxes clearly violate this separation: a NAT box, sitting\nbetween a router and host, rewrites network-layer IP addresses and\ntransport-layer port numbers; an in-network firewall blocks suspect\ndatagrams using application-layer (e.g., HTTP), transport-layer, and\nnetwork-layer header fields; e-mail security gateways are injected between\nthe e-mail sender (whether malicious or not) and the intended e-mail\napplication-layer \nwhitelisted/blacklisted IP addresses as well as e-mail message content.\nWhile there are those who have considered such middleboxes as a bit of an\narchitectural abomination [Garfinkel 2003], others have adopted the\nphilosophy that such middleboxes “exist for important and permanent\nreasons”—that they fill an important need—and that we’ll have more, not\nfewer, middleboxes in the future [Walfish 2004]. See the section in attached\nsidebar on “The end-to-end argument” for a slightly different lens on the\nquestion of where to place service functionality in a network.\nGiven the phenomenal success of the Internet, one might naturally wonder about the\narchitectural principles that have guided the development of what is arguably the largest\nand most complex engineered system ever built by humankind. RFC 1958, entitled\n“Architectural Principles of the Internet,” suggests that these principles, if indeed they exist,\nare truly minimal:\n“Many members of the Internet community would argue that there is no architecture,\nbut only a tradition, which was not written down for the first 25 years (or at least not by\nthe IAB). However, in very general terms, the community believes that the goal is\nconnectivity, the tool is the Internet Protocol, and the intelligence is end to end rather\nthan hidden in the network.” [RFC 1958]\nSo there we have it! The goal was to provide connectivity, there would be just one network-\nlayer protocol (the celebrated IP protocol we have studied in this chapter), and “intelligence”\n(one might say the “complexity”) would be placed at the network edge, rather than in the\nnetwork core. Let’s look these last two considerations in a bit more detail.\nBy now, we’re well acquainted with the five-layer Internet protocol stack that we first\nencountered in Figure 1.23. Another visualization of this stack, shown in Figure 4.31 and\nsometimes known as the “IP hourglass,” illustrates the “narrow waist” of the layered\nInternet architecture. While the Internet has many protocols in the physical, link, transport,\nand application layers, there is only one network layer protocol—the IP protocol. This is the\none protocol that must be implemented by each and every of the billions of Internet-\nconnected devices. This narrow waist has played a critical role in the phenomenal growth of\nthe Internet. The relative simplicity of the IP protocol, and the fact that it is the only universal\nrequirement for Internet connectivity has allowed a rich variety of networks—with very\ndifferent underlying link-layer technologies, from Ethernet to WiFi to cellular to optical\nnetworks to become part of the Internet. [Clark 1997] notes that role of the narrow waist,\nwhich he refers to as a “spanning layer,” is to “… hide the detailed differences among these\nvarious [underlying] technologies and present a uniform service interface to the applications\nabove.” For the IP layer in particular: “How does the IP spanning layer achieve its purpose?\nIt defines a basic set of services, which were carefully designed so that they could be\nconstructed from a wide range of underlying network technologies. Software, as a part of\nthe Internet [i.e., network] layer, translates what each of these lower-layer technologies\noffers into the common service of the Internet layer.”\nFor a discussion the narrow waist, including examples beyond the Internet, see [Beck\n2019; Akhshabi 2011]. We note here that as the Internet architecture enters mid-life\n(certainly, the Internet’s age of 40 to 50 years qualifies it for middle age!), one might observe\nthat its “narrow waist” may indeed be widening a bit (as often happens in middle age!) via\nthe rise of middleboxes.\nFigure 4.31 ♦The narrow-waisted Internet hourglass\nTHE END-TO-END ARGUMENT\nThe third principle in RFC 1958—that “intelligence is end to end rather than hidden in the\nnetwork”—speaks to the placement of functionality within the network. Here, we’ve seen\nthat until the recent rise of middleboxes, most Internet functionality was indeed placed at the\nnetwork’s edge. It’s worth noting that, in direct contrast with the 20th century telephone\nnetwork—which had “dumb” (non-programmable) endpoints and smart switches—the\nInternet has always had smart endpoints (programmable computers), enabling complex\nfunctionality to be placed at those endpoints. But a more principled argument for actually\nplacing functionality at the endpoints was made in an extremely influential paper [Saltzer\n1984] that articulated the “end-to-end argument.” It stated:\n“ . . . there is a list of functions each of which might be implemented in any of several\nways: by the communication subsystem, by its client, as a joint venture, or perhaps\nredundantly, each doing its own version. In reasoning about this choice, the\nrequirements of the application provide the basis for a class of arguments, which go as\nThe function in question can completely and correctly be implemented only with\nthe knowledge and help of the application standing at the end points of the\ncommunication system. Therefore, providing that questioned function as a feature\nof the communication system itself is not possible. (Sometimes an incomplete\nversion of the function provided by the communication system may be useful as a\nperformance enhancement.)\nWe call this line of reasoning against low-level function implementation the “end-to-end\nAn example illustrating the end-to-end argument is that of reliable data transfer. Since\npackets can be lost within the network (e.g., even without buffer overflows, a router holding\na queued packet could crash, or a portion of the network in which a packet is queued\nbecomes detached due to link failures), the endpoints (in this case via the TCP protocol)\nmust perform error control. As we will see in Chapter 6, some link-layer protocols do indeed\nperform local error control, but this local error control alone is “incomplete” and not sufficient\nto provide end-to-end reliable data transfer. And so reliable data transfer must be\nimplemented end to end.\nwe’re now ready to dive into the network layer’s control plane in Chapter 5!\nHomework Problems and Questions\nChapter 4 Review Questions\nSECTION 4.1\nR1. Let’s review some of the terminology used in this textbook. Recall\nthat the name of a transport-layer packet is segment and that the name\nof a link-layer packet is frame. What is the name of a network-layer\npacket? Recall that both routers and link-layer switches are called\npacket switches. What is the fundamental difference between a router\nand link-layer switch?\nR2. We noted that network layer functionality can be broadly divided into\ndata plane functionality and control plane functionality. What are the\nmain functions of the data plane? Of the control plane?\nR3. We made a distinction between the forwarding function and the\nrouting function performed in the network layer. What are the key\ndifferences between routing and forwarding?\nR4. What is the role of the forwarding table within a router?\nR5. We said that a network layer’s service model “defines the\ncharacteristics of end-to-end transport of packets between sending\nand receiving hosts.” What is the service model of the Internet’s\nnetwork layer? What guarantees are made by the Internet’s service\nmodel regarding the host-to-host delivery of datagrams?\nSECTION 4.2\nR6. In Section 4.2, we saw that a router typically consists of input ports,\noutput ports, a switching fabric and a routing processor. Which of\nthese are implemented in hardware and which are implemented in\nsoftware? Why? Returning to the notion of the network layer’s data\nplane and control plane, which are implemented in hardware and\nwhich are implemented in software? Why?\nR7. How can the input ports of a high-speed router facilitate fast\nforwarding ­decisions?\nR8. What is meant by destination-based forwarding? How does this differ\nfrom generalized forwarding (assuming you’ve read Section 4.4,\nwhich of the two approaches are adopted by Software-Defined\nNetworking)?\nR9. Suppose that an arriving packet matches two or more entries in a\nrouter’s forwarding table. With traditional destination-based\nforwarding, what rule does a router apply to determine which of these\nrules should be applied to determine the output port to which the\narriving packet should be switched?\nR10. Switching in a router forwards data from an input port to an output\nport. What is the advantage of switching via an interconnection\nnetwork over switching via memory and switching via bus?\nR11. What is the role of a packet scheduler at the output port of a router?\na. What is a drop-tail policy?\nb. What are AQM algorithms?\nc. Name one of the most widely studied and implemented AQM\nalgorithms and explain how it works.\nR13. What is HOL blocking? Does it occur in input ports or output ports?\nR14. In Section 4.2, we studied FIFO, Priority, Round Robin (RR), and\nWeighted Fair Queuing (WFQ) packet scheduling disciplines? Which\nof these queuing disciplines ensure that all packets depart in the order\nin which they arrived?\nR15. Give an example showing why a network operator might want one\nclass of packets to be given priority over another class of packets.\nR16. What is an essential different between RR and WFQ packet\nscheduling? Is there a case (Hint: Consider the WFQ weights) where\nRR and WFQ will behave exactly the same?\nSECTION 4.3\nR17. Suppose Host A sends Host B a TCP segment encapsulated in an IP\ndatagram. When Host B receives the datagram, how does the network\nlayer in Host B know it should pass the segment (that is, the payload\nof the datagram) to TCP rather than to UDP or to some other upper-\nlayer protocol?\nR18. What field in the IP header can be used to ensure that a packet is\nforwarded through no more than N routers?\nR19. Recall that we saw the Internet checksum being used in both\ntransport-layer segment (in UDP and TCP headers, Figures 3.7 and\n3.29 respectively) and in network-layer datagrams (IP header, Figure\n4.17). Now consider a transport layer segment encapsulated in an IP\ndatagram. Are the checksums in the segment header and datagram\nheader computed over any common bytes in the IP datagram?\nExplain your answer.\nR20. When a large datagram is fragmented into multiple smaller\ndatagrams, where are these smaller datagrams reassembled into a\nsingle larger datagram?\nR21. How many IP addresses does a router have?\nR22. What is the 32-bit binary equivalent of the IP address 202.3.14.25?\nR23. Visit a host that uses DHCP to obtain its IP address, network mask,\ndefault router, and IP address of its local DNS server. List these\nR24. Suppose there are four routers between a source host and a\ndestination host. Ignoring fragmentation, an IP datagram sent from\nthe source host to the ­destination host will travel over how many\nintroduction to Chapter 4, software-defined networking (SDN) makes\na clear separation between the data and control planes, implementing\ncontrol-plane functions in a separate “controller” service that is\ndistinct, and remote, from the forwarding components of the routers it\ncontrols. We’ll cover SDN controllers in Section 5.5.\nIn Sections 5.6 and 5.7, we’ll cover some of the nuts and bolts of\nmanaging an IP network: ICMP (the Internet Control Message\nProtocol) and SNMP (the Simple Network Management Protocol).\n5.1 Introduction\nLet’s quickly set the context for our study of the network control plane by\nrecalling Figures 4.2 and 4.3. There, we saw that the forwarding table (in\nthe case of ­destination-based forwarding) and the flow table (in the case of\ngeneralized forwarding) were the principal elements that linked the network\nlayer’s data and control planes. We learned that these tables specify the\nlocal data-plane forwarding behavior of a router. We saw that in the case of\ngeneralized forwarding, the actions taken could include not only forwarding\na packet to a router’s output port, but also dropping a packet, replicating a\npacket, and/or rewriting layer 2, 3 or 4 packet-header fields.\nIn this chapter, we’ll study how those forwarding and flow tables are\ncomputed, maintained and installed. In our introduction to the network\nlayer in Section 4.1, we learned that there are two possible approaches for\nFigure 5.1 ♦Per-router control: Individual routing algorithm\ncomponents interact in the control plane\nPer-router control. Figure 5.1 illustrates the case where a routing\nalgorithm runs in each and every router; both a forwarding and a\nrouting function are contained within each router. Each router has a\nrouting component that communicates with the routing components in\nother routers to compute the values for its forwarding table. This per-\nrouter control approach has been used in the Internet for decades. The\nOSPF and BGP protocols that we’ll study in Sections 5.3 and 5.4 are\nbased on this per-router approach to control.\nLogically centralized control. Figure 5.2 illustrates the case in which a\nlogically centralized controller computes and distributes the forwarding\ntables to be used by each and every router. As we saw in Sections 4.4\nand 4.5, the generalized match-plus-action abstraction allows the router\nto perform traditional IP forwarding as well as a rich set of other\nfunctions (load sharing, firewalling, and NAT) that had been previously\nimplemented in separate middleboxes.\nFigure 5.2 ♦Logically centralized control: A distinct, typically\nremote, controller interacts with local control agents\nThe controller interacts with a control agent (CA) in each of the routers\nvia a well-defined protocol to configure and manage that router’s flow\ntable. Typically, the CA has minimum functionality; its job is to\ncommunicate with the controller, and to do as the controller commands.\nUnlike the routing algorithms in Figure 5.1, the CAs do not directly interact\nwith each other nor do they actively take part in computing the forwarding\ntable. This is a key distinction between per-router control and logically\ncentralized control.\nBy “logically centralized” control [Levin 2012] we mean that the\nrouting control service is accessed as if it were a single central service\npoint, even though the service is likely to be implemented via multiple\nservers for fault-tolerance, and performance scalability reasons. As we will\nsee in Section 5.5, SDN adopts this notion of a logically centralized\ncontroller—an approach that is finding increased use in production\ndeployments. Google uses SDN to control the routers in its internal B4\nglobal wide-area network that interconnects its data centers [Jain 2013].\nSWAN [Hong 2013], from Microsoft Research, uses a logically ­centralized\ncontroller to manage routing and forwarding between a wide area network\nand a data center network. Major ISP deployments, including COMCAST’s\nActiveCore and Deutsche Telecom’s Access 4.0 are actively integrating\nSDN into their networks. And as we’ll see in Chapter 8, SDN control is\ncentral to 4G/5G cellular networking as well. [AT&T 2019] notes, “ …\nSDN, isn’t a vision, a goal, or a promise. It’s a reality. By the end of next\nyear, 75% of our network functions will be fully virtualized and software-\ncontrolled.” China Telecom and China Unicom are using SDN both within\ndata centers and between data centers [Li 2015].\n5.2 Routing Algorithms\nIn this section, we’ll study routing algorithms, whose goal is to determine good paths\n(equivalently, routes), from senders to receivers, through the network of routers.\nTypically, a “good” path is one that has the least cost. We’ll see that in practice,\nhowever, real-world concerns such as policy issues (for example, a rule such as “router\nx, belonging to organization Y, should not forward any packets originating from the\nnetwork owned by organization Z ”) also come into play. We note that whether the\nnetwork control plane adopts a per-router control approach or a logically centralized\napproach, there must always be a well-defined sequence of routers that a packet will\ncross in traveling from sending to receiving host. Thus, the routing algorithms that\ncompute these paths are of fundamental importance, and another candidate for our top-\n10 list of fundamentally important networking concepts.\nA graph is used to formulate routing problems. Recall that a graph G = (N, E) is a\nset N of nodes and a collection E of edges, where each edge is a pair of nodes from N.\nIn the context of network-layer routing, the nodes in the graph represent routers—the\npoints at which packet-forwarding decisions are made—and the edges connecting\nthese nodes represent the physical links between these routers. Such a graph\nabstraction of a computer network is shown in Figure 5.3. When we study the BGP\ninter-domain routing protocol, we’ll see that nodes represent networks, and the edge\nconnecting two such nodes represents direction connectivity (know as peering)\nbetween the two networks. To view some graphs representing real network maps, see\n[CAIDA 2020]; for a discussion of how well different graph-based models model the\nInternet, see [Zegura 1997, Faloutsos 1999, Li 2004].\nFigure 5.3 ♦Abstract graph model of a computer network\nAs shown in Figure 5.3, an edge also has a value representing its cost. Typically,\nan edge’s cost may reflect the physical length of the corresponding link (for example,\na transoceanic link might have a higher cost than a short-haul terrestrial link), the link\nspeed, or the monetary cost associated with a link. For our purposes, we’ll simply take\nthe edge costs as a given and won’t worry about how they are determined. For any\nedge (x, y) in E, in E, we denote c(x, y) as the cost of the edge between nodes x and y.\nIf the pair (x, y) does not belong to E, we set c(x, y) = ∞. Also, we’ll only consider\nundirected graphs (i.e., graphs whose edges do not have a direction) in our discussion\nhere, so that edge (x, y) is the same as edge (y, x) and that c(x, y) = c(y, x); however, the\nalgorithms we’ll study can be easily extended to the case of directed links with a\ndifferent cost in each direction. Also, a node y is said to be a neighbor of node x if (x,\ny) belongs to E.\nGiven that costs are assigned to the various edges in the graph abstraction, a\nnatural goal of a routing algorithm is to identify the least costly paths between sources\nand destinations. To make this problem more precise, recall that a path in a graph G =\n(N, E) is a sequence of nodes (x , x , ..., x ) such that each of the pairs (x , x ), (x , x ),\n, x ) are edges in E. The cost of a path (x , x , ..., x ) is simply the sum of all the\nedge costs along the path, that is, c(x , x ) + c(x , x ) + ...+ c(x\n, x ). Given any two\nnodes x and y, there are typically many paths between the two nodes, with each path\nhaving a cost. One or more of these paths is a least-cost path. The least-cost problem\nis therefore clear: Find a path between the source and destination that has least cost. In\nFigure 5.3, for example, the least-cost path between source node u and destination\nnode w is (u, x, y, w) with a path cost of 3. Note that if all edges in the graph have the\nsame cost, the least-cost path is also the shortest path (that is, the path with the\nsmallest number of links between the source and the destination).\nAs a simple exercise, try finding the least-cost path from node u to z in Figure 5.3\nand reflect for a moment on how you calculated that path. If you are like most people,\nyou found the path from u to z by examining Figure 5.3, tracing a few routes from u to\nz, and somehow convincing yourself that the path you had chosen had the least cost\namong all possible paths. (Did you check all of the 17 possible paths between u and z?\nProbably not!) Such a calculation is an example of a centralized routing algorithm—\nthe routing algorithm was run in one location, your brain, with complete information\nabout the network. Broadly, one way in which we can classify routing algorithms is\naccording to whether they are centralized or decentralized.\nA centralized routing algorithm computes the least-cost path between a source\nand destination using complete, global knowledge about the network. That is, the\nalgorithm takes the connectivity between all nodes and all link costs as inputs.\nThis then requires that the algorithm somehow obtain this information before\nactually performing the calculation. The calculation itself can be run at one site\n(e.g., a logically centralized controller as in Figure 5.2) or could be replicated in\nthe routing component of each and every router (e.g., as in Figure 5.1). The key\ndistinguishing feature here, however, is that the algorithm has complete\ninformation about connectivity and link costs. Algorithms with global state\ninformation are often referred to as link-state (LS) algorithms, since the\nalgorithm must be aware of the cost of each link in the network. We’ll study LS\nalgorithms in Section 5.2.1.\nIn a decentralized routing algorithm, the calculation of the least-cost path is\ncarried out in an iterative, distributed manner by the routers. No node has complete\ninformation about the costs of all network links. Instead, each node begins with\nonly the knowledge of the costs of its own directly attached links. Then, through\nan iterative process of calculation and exchange of information with its\nneighboring nodes, a node gradually calculates the least-cost path to a destination\nor set of destinations. The decentralized routing algorithm we’ll study below in\nSection 5.2.2 is called a distance-vector (DV) algorithm, because each node\nmaintains a vector of estimates of the costs (distances) to all other nodes in the\nnetwork. Such decentralized algorithms, with interactive message exchange\nbetween neighboring routers is perhaps more naturally suited to control planes\nwhere the routers interact directly with each other, as in Figure 5.1.\nA second broad way to classify routing algorithms is according to whether they\nare static or dynamic. In static routing algorithms, routes change very slowly over\ntime, often as a result of human intervention (for example, a human manually editing a\nlink costs). Dynamic routing algorithms change the routing paths as the network\ntraffic loads or topology change. A dynamic algorithm can be run either periodically or\nin direct response to topology or link cost changes. While dynamic algorithms are\nmore responsive to network changes, they are also more susceptible to problems such\nas routing loops and route oscillation.\nA third way to classify routing algorithms is according to whether they are load-\nsensitive or load-insensitive. In a load-sensitive algorithm, link costs vary\ndynamically to reflect the current level of congestion in the underlying link. If a high\ncost is associated with a link that is currently congested, a routing algorithm will tend\nto choose routes around such a congested link. While early ARPAnet routing\nalgorithms were load-sensitive [McQuillan 1980], a number of difficulties were\nencountered [Huitema 1998]. Today’s Internet routing algorithms (such as RIP, OSPF,\nand BGP) are load-insensitive, as a link’s cost does not explicitly reflect its current (or\nrecent past) level of congestion.\n5.2.1 The Link-State (LS) Routing Algorithm\nRecall that in a link-state algorithm, the network topology and all link costs are\nknown, that is, available as input to the LS algorithm. In practice, this is accomplished\nby having each node broadcast link-state packets to all other nodes in the network,\nwith each link-state packet containing the identities and costs of its attached links. In\npractice (for example, with the Internet’s OSPF routing protocol, discussed in Section\n5.3), this is often accomplished by a link-state broadcast algorithm ­[Perlman 1999].\nThe result of the nodes’ broadcast is that all nodes have an identical and complete view\nof the network. Each node can then run the LS algorithm and compute the same set of\nleast-cost paths as every other node.\nThe link-state routing algorithm we present below is known as Dijkstra’s\nalgorithm, named after its inventor. A closely related algorithm is Prim’s algorithm;\nsee [Cormen 2001] for a general discussion of graph algorithms. Dijkstra’s algorithm\ncomputes the least-cost path from one node (the source, which we will refer to as u) to\nall other nodes in the network. Dijkstra’s algorithm is iterative and has the property\nthat after the kth iteration of the algorithm, the least-cost paths are known to k\ndestination nodes, and among the least-cost paths to all destination nodes, these k\npaths will have the k smallest costs. Let us define the following notation:\nD(v): cost of the least-cost path from the source node to destination v as of this\niteration of the algorithm.\np(v): previous node (neighbor of v) along the current least-cost path from the\nsource to v.\nN': subset of nodes; v is in N' if the least-cost path from the source to v is\ndefinitively known.\nThe centralized routing algorithm consists of an initialization step followed by a\nloop. The number of times the loop is executed is equal to the number of nodes in the\nnetwork. Upon termination, the algorithm will have calculated the shortest paths from\nthe source node u to every other node in the network.\nLink-State (LS) Algorithm for Source Node u\nInitialization:\nfor all nodes v\nif v is a neighbor of u\nthen D(v) = c(u,v)\nelse D(v) = ∞\nfind w not in N’ such that D(w) is a minimum\n10 add w to N’\n11 update D(v) for each neighbor v of w and not in N’:\n12  D(v) = min(D(v), D(w)+ c(w,v) )\n13  /* new cost to v is either old cost to v or known\n14  least path cost to w plus cost from w to v */\n15 until N’= N\nAs an example, let’s consider the network in Figure 5.3 and compute the least-cost\npaths from u to all possible destinations. A tabular summary of the algorithm’s\ncomputation is shown in Table 5.1, where each line in the table gives the values of the\nalgorithm’s variables at the end of the iteration. Let’s consider the few first steps in\nIn the initialization step, the currently known least-cost paths from u to its directly\nattached neighbors, v, x, and w, are initialized to 2, 1, and 5, respectively. Note in\nparticular that the cost to w is set to 5 (even though we will soon see that a lesser-\ncost path does indeed exist) since this is the cost of the direct (one hop) link from u\nto w. The costs to y and z are set to infinity because they are not directly connected\nIn the first iteration, we look among those nodes not yet added to the set N' and\nfind that node with the least cost as of the end of the previous iteration. That node\nis x, with a cost of 1, and thus x is added to the set N'. Line 12 of the LS algorithm\nis then performed to update D(v) for all nodes v, yielding the results shown in the\nsecond line (Step 1) in Table 5.1. The cost of the path to v is unchanged. The cost\nof the path to w (which was 5 at the end of the initialization) through node x is\nfound to have a cost of 4. Hence this lower-cost path is selected and w’s\npredecessor along the shortest path from u is set to x. Similarly, the cost to y\n(through x) is computed to be 2, and the table is updated accordingly.\nIn the second iteration, nodes v and y are found to have the least-cost paths (2), and\nwe break the tie arbitrarily and add y to the set N' so that N' now contains u, x, and\ny. The cost to the remaining nodes not yet in N', that is, nodes v, w, and z, are\nupdated via line 12 of the LS algorithm, yielding the results shown in the third row\nin Table 5.1.\nAnd so on . . .\nTable 5.1 ♦Running the link-state algorithm on the network in Figure 5.3\nWhen the LS algorithm terminates, we have, for each node, its predecessor along\nthe least-cost path from the source node. For each predecessor, we also have its\npredecessor, and so in this manner we can construct the entire path from the source to\nall destinations. The forwarding table in a node, say node u, can then be constructed\nfrom this information by storing, for each destination, the next-hop node on the least-\ncost path from u to the destination. Figure 5.4 shows the resulting least-cost paths and\nforwarding table in u for the network in Figure 5.3.\nFigure 5.4 ♦Least cost path and forwarding table for node u\nWhat is the computational complexity of this algorithm? That is, given n nodes\n(not counting the source), how much computation must be done in the worst case to\nfind the least-cost paths from the source to all destinations? In the first iteration, we\nneed to search through all n nodes to determine the node, w, not in N' that has the\nminimum cost. In the second iteration, we need to check n − 1 nodes to determine the\nminimum cost; in the third iteration n − 2 nodes, and so on. Overall, the total number\nof nodes we need to search through over all the iterations is n(n + 1)/2, and thus we\nsay that the preceding implementation of the LS algorithm has worst-case complexity\nof order n squared: O(n ). (A more sophisticated implementation of this algorithm,\nusing a data structure known as a heap, can find the minimum in line 9 in logarithmic\nrather than linear time, thus reducing the complexity.)\nBefore completing our discussion of the LS algorithm, let us consider a pathology\nthat can arise. Figure 5.5 shows a simple network topology where link costs are equal\nto the load carried on the link, for example, reflecting the delay that would be\nexperienced. In this example, link costs are not symmetric; that is, c(u,v) equals c(v,u)\nonly if the load carried on both directions on the link (u,v) is the same. In this example,\nnode z originates a unit of traffic destined for w, node x also originates a unit of traffic\ndestined for w, and node y injects an amount of traffic equal to e, also destined for w.\nThe initial routing is shown in Figure 5.5(a) with the link costs corresponding to the\namount of traffic carried.\nWhen the LS algorithm is next run, node y determines (based on the link costs\nshown in Figure 5.5(a)) that the clockwise path to w has a cost of 1, while the\ncounterclockwise path to w (which it had been using) has a cost of 1 + e. Hence y’s\nleast-cost path to w is now clockwise. Similarly, x determines that its new least-cost\npath to w is also clockwise, resulting in costs shown in Figure 5.5(b). When the LS\nalgorithm is run next, nodes x, y, and z all detect a zero-cost path to w in the\ncounterclockwise direction, and all route their traffic to the counterclockwise routes.\nThe next time the LS algorithm is run, x, y, and z all then route their traffic to the\nclockwise routes.\nFigure 5.5 ♦Oscillations with congestion-sensitive routing\nWhat can be done to prevent such oscillations (which can occur in any algorithm,\nnot just an LS algorithm, that uses a congestion or delay-based link metric)? One\nsolution would be to mandate that link costs not depend on the amount of traffic\ncarried—an unacceptable solution since one goal of routing is to avoid highly\ncongested (for example, high-delay) links. Another solution is to ensure that not all\nrouters run the LS algorithm at the same time. This seems a more reasonable solution,\nsince we would hope that even if routers ran the LS algorithm with the same\nperiodicity, the execution instance of the algorithm would not be the same at each\nnode. Interestingly, researchers have found that routers in the Internet can self-\nsynchronize among themselves [Floyd Synchronization 1994]. That is, even though\nthey initially execute the algorithm with the same period but at different instants of\ntime, the algorithm execution instance can eventually become, and remain,\nsynchronized at the routers. One way to avoid such self-synchronization is for each\nrouter to randomize the time it sends out a link advertisement.\nHaving studied the LS algorithm, let’s consider the other major routing algorithm\nthat is used in practice today—the distance-vector routing algorithm.\n5.2.2 The Distance-Vector (DV) Routing Algorithm\nWhereas the LS algorithm is an algorithm using global information, the distance-\nvector (DV) algorithm is iterative, asynchronous, and distributed. It is distributed in\nthat each node receives some information from one or more of its directly attached\nneighbors, performs a calculation, and then distributes the results of its calculation\nback to its neighbors. It is iterative in that this process continues on until no more\ninformation is exchanged between neighbors. (Interestingly, the algorithm is also self-\nterminating—there is no signal that the computation should stop; it just stops.) The\nalgorithm is asynchronous in that it does not require all of the nodes to operate in\nlockstep with each other. We’ll see that an asynchronous, iterative, self-terminating,\ndistributed algorithm is much more interesting and fun than a centralized algorithm!\nBefore we present the DV algorithm, it will prove beneficial to discuss an\nimportant relationship that exists among the costs of the least-cost paths. Let d (y) be\nthe cost of the least-cost path from node x to node y. Then the least costs are related by\nthe celebrated Bellman-Ford equation, namely,\nwhere the min  in the equation is taken over all of x’s neighbors. The Bellman-Ford\nequation is rather intuitive. Indeed, after traveling from x to v, if we then take the least-\ncost path from v to y, the path cost will be c(x, v) + d (y). Since we must begin by\ndx(y) = minv{c(x,  v) + dv( y)},\ntraveling to some neighbor v, the least cost from x to y is the minimum of c(x, v) +\nd (y) taken over all neighbors v.\nBut for those who might be skeptical about the validity of the equation, let’s check\nit for source node u and destination node z in Figure 5.3. The source node u has three\nneighbors: nodes v, x, and w. By walking along various paths in the graph, it is easy to\nsee that d (z) = 5, d (z) = 3, and d (z) = 3. Plugging these values into Equation 5.1,\nalong with the costs c(u, v) = 2, c(u, x) = 1, and c(u, w) = 5, gives d (z) = min{2 + 5, 5\n+ 3, 1 + 3} = 4, which is obviously true and which is exactly what the Dijskstra\nalgorithm gave us for the same network. This quick verification should help relieve\nany skepticism you may have.\nThe Bellman-Ford equation is not just an intellectual curiosity. It actually has\nsignificant practical importance: the solution to the Bellman-Ford equation provides\nthe entries in node x’s forwarding table. To see this, let v* be any neighboring node\nthat achieves the minimum in Equation 5.1. Then, if node x wants to send a packet to\nnode y along a least-cost path, it should first forward the packet to node v*. Thus, node\nx’s forwarding table would specify node v* as the next-hop router for the ultimate\ndestination y. Another important practical contribution of the Bellman-Ford equation is\nthat it suggests the form of the neighbor-to-neighbor communication that will take\nplace in the DV algorithm.\nThe basic idea is as follows. Each node x begins with D (y), an estimate of the cost\nof the least-cost path from itself to node y, for all nodes, y, in N. Let D  = [D (y): y in\nN] be node x’s distance vector, which is the vector of cost estimates from x to all other\nnodes, y, in N. With the DV algorithm, each node x maintains the following routing\ninformation:\nFor each neighbor v, the cost c(x,v) from x to directly attached neighbor, v\nNode x’s distance vector, that is, D  = [D (y): y in N], containing x’s estimate of its\ncost to all destinations, y, in N\nThe distance vectors of each of its neighbors, that is, D  = [D (y): y in N] for each\nneighbor v of x\nIn the distributed, asynchronous algorithm, from time to time, each node sends a copy\nof its distance vector to each of its neighbors. When a node x receives a new distance\nvector from any of its neighbors w, it saves w’s distance vector, and then uses the\nBellman-Ford equation to update its own distance vector as follows:\nIf node x’s distance vector has changed as a result of this update step, node x will then\nsend its updated distance vector to each of its neighbors, which can in turn update their\nown distance vectors. Miraculously enough, as long as all the nodes continue to\nexchange their distance vectors in an asynchronous fashion, each cost estimate D (y)\nconverges to d (y), the actual cost of the least-cost path from node x to node y\n[Bertsekas 1991]!\nDistance-Vector (DV) Algorithm\nAt each node, x:\nInitialization:\n for all destinations y in N:\n Dx(y)= c(x,y)/* if y is not a neighbor then c(x,y)= ∞\n for each neighbor w\n D w(y) = ? for all destinations y in N\n for each neighbor w\n send distance vector Dx = [Dx(y): y in N] to w\n10 wait (until I see a link cost change to some neighbor w\n11  until I receive a distance vector from some neighbor\n13  for each y in N:\n14  D x(y) = minv{c(x,v) + Dv(y)}\nDx(y) = minv{c(x,  v) + Dv(y)}\nfor each node y in N\n16 if Dx(y) changed for any destination y\n17  send distance vector Dx = [Dx(y): y in N] to all\nIn the DV algorithm, a node x updates its distance-vector estimate when it either\nsees a cost change in one of its directly attached links or receives a distance-vector\nupdate from some neighbor. But to update its own forwarding table for a given\ndestination y, what node x really needs to know is not the shortest-path distance to y\nbut instead the neighboring node v*(y) that is the next-hop router along the shortest\npath to y. As you might expect, the next-hop router v*(y) is the neighbor v that\nachieves the minimum in Line 14 of the DV algorithm. (If there are multiple neighbors\nv that achieve the minimum, then v*(y) can be any of the minimizing neighbors.)\nThus, in Lines 13–14, for each destination y, node x also determines v*(y) and updates\nits forwarding table for destination y.\nRecall that the LS algorithm is a centralized algorithm in the sense that it requires\neach node to first obtain a complete map of the network before running the Dijkstra\nalgorithm. The DV algorithm is decentralized and does not use such global\ninformation. Indeed, the only information a node will have is the costs of the links to\nits directly attached neighbors and information it receives from these neighbors. Each\nnode waits for an update from any neighbor (Lines 10–11), calculates its new distance\nvector when receiving an update (Line 14), and distributes its new distance vector to\nits neighbors (Lines 16–17). DV-like algorithms are used in many routing protocols in\npractice, including the Internet’s RIP and BGP, ISO IDRP, Novell IPX, and the\noriginal ARPAnet.\nFigure 5.6 illustrates the operation of the DV algorithm for the simple three-node\nnetwork shown at the top of the figure. The operation of the algorithm is illustrated in\na synchronous manner, where all nodes simultaneously receive distance vectors from\ntheir neighbors, compute their new distance vectors, and inform their neighbors if their\ndistance vectors have changed. After studying this example, you should convince\nyourself that the algorithm operates correctly in an asynchronous manner as well, with\nnode computations and update generation/reception occurring at any time.\nFigure 5.6 ♦Distance-vector (DV) algorithm in operation\nThe leftmost column of the figure displays three initial routing tables for each of\nthe three nodes. For example, the table in the upper-left corner is node x’s initial\nrouting table. Within a specific routing table, each row is a distance vector—\nspecifically, each node’s routing table includes its own distance vector and that of each\nof its neighbors. Thus, the first row in node x’s initial routing table is D  = [D (x),\nD (y), D (z)] = [0, 2, 7]. The second and third rows in this table are the most recently\nreceived distance vectors from nodes y and z, respectively. Because at initialization\nnode x has not received anything from node y or z, the entries in the second and third\nrows are initialized to infinity.\nAfter initialization, each node sends its distance vector to each of its two\nneighbors. This is illustrated in Figure 5.6 by the arrows from the first column of\ntables to the second column of tables. For example, node x sends its distance vector D\n= [0, 2, 7] to both nodes y and z. After receiving the updates, each node recomputes its\nown distance vector. For example, node x computes\nD (y) = min{c(x,y) + D (y), c(x,z) + D (y)} = min{2 + 0, 7 + 1} = 2\nD (z) = min{c(x,y) + D (z), c(x,z) + D (z)} = min{2 + 1, 7 + 0} = 3\nThe second column therefore displays, for each node, the node’s new distance vector\nalong with distance vectors just received from its neighbors. Note, for example, that\nnode x’s estimate for the least cost to node z, D (z), has changed from 7 to 3. Also note\nthat for node x, neighboring node y achieves the minimum in line 14 of the DV\nalgorithm; thus, at this stage of the algorithm, we have at node x that v (y) = y and v (z)\nAfter the nodes recompute their distance vectors, they again send their updated\ndistance vectors to their neighbors (if there has been a change). This is illustrated in\nFigure 5.6 by the arrows from the second column of tables to the third column of\ntables. Note that only nodes x and z send updates: node y’s distance vector didn’t\nchange so node y doesn’t send an update. After receiving the updates, the nodes then\nrecompute their distance vectors and update their routing tables, which are shown in\nthe third column.\nThe process of receiving updated distance vectors from neighbors, recomputing\nrouting table entries, and informing neighbors of changed costs of the least-cost path\nto a destination continues until no update messages are sent. At this point, since no\nupdate messages are sent, no further routing table calculations will occur and the\nalgorithm will enter a quiescent state; that is, all nodes will be performing the wait in\nLines 10–11 of the DV algorithm. The algorithm remains in the quiescent state until a\nlink cost changes, as discussed next.\nDistance-Vector Algorithm: Link-Cost Changes and Link Failure\nWhen a node running the DV algorithm detects a change in the link cost from itself to\na neighbor (Lines 10–11), it updates its distance vector (Lines 13–14) and, if there’s a\nchange in the cost of the least-cost path, informs its neighbors (Lines 16–17) of its new\ndistance vector. Figure 5.7(a) illustrates a scenario where the link cost from y to x\nchanges from 4 to 1. We focus here only on y’ and z’s distance table entries to\ndestination x. The DV algorithm causes the following sequence of events to occur:\nAt time t , y detects the link-cost change (the cost has changed from 4 to 1),\nupdates its distance vector, and informs its neighbors of this change since its\ndistance vector has changed.\nAt time t , z receives the update from y and updates its table. It computes a new\nleast cost to x (it has decreased from a cost of 5 to a cost of 2) and sends its new\ndistance vector to its neighbors.\nAt time t , y receives z’s update and updates its distance table. y’s least costs do not\nchange and hence y does not send any message to z. The algorithm comes to a\nquiescent state.\nThus, only two iterations are required for the DV algorithm to reach a quiescent state.\nThe good news about the decreased cost between x and y has propagated quickly\nthrough the network.\nLet’s now consider what can happen when a link cost increases. Suppose that the\nlink cost between x and y increases from 4 to 60, as shown in Figure 5.7(b).\nFigure 5.7 ♦Changes in link cost\n1. Before the link cost changes, D (x) = 4, D (z) = 1, D (y) = 1, and D (x) = 5. At time\nt , y detects the link-cost change (the cost has changed from 4 to 60). y computes\nits new minimum-cost path to x to have a cost of\nDy(x) = min{c(y, x) + Dx(x), c(y, z) + Dz(x)} = min{60 + 0, 1 + 5} = 6\nOf course, with our global view of the network, we can see that this new cost via z\nis wrong. But the only information node y has is that its direct cost to x is 60 and\nthat z has last told y that z could get to x with a cost of 5. So in order to get to x, y\nwould now route through z, fully expecting that z will be able to get to x with a cost\nof 5. As of t  we have a routing loop—in order to get to x, y routes through z, and z\nroutes through y. A routing loop is like a black hole—a packet destined for x\narriving at y or z as of t  will bounce back and forth between these two nodes\nforever (or until the forwarding tables are changed).\n2. Since node y has computed a new minimum cost to x, it informs z of its new\ndistance vector at time t .\n3. Sometime after t , z receives y’s new distance vector, which indicates that y’s\nminimum cost to x is 6. z knows it can get to y with a cost of 1 and hence computes\na new least cost to x of D (x) = min {50 + 0,1 + 6} = 7. Since z’s least cost to x has\nincreased, it then informs y of its new distance vector at t .\n4. In a similar manner, after receiving z’s new distance vector, y determines D (x) = 8\nand sends z its distance vector. z then determines D (x) = 9 and sends y its distance\nvector, and so on.\nHow long will the process continue? You should convince yourself that the loop will\npersist for 44 iterations (message exchanges between y and z)—until z eventually\ncomputes the cost of its path via y to be greater than 50. At this point, z will (finally!)\ndetermine that its least-cost path to x is via its direct connection to x. y will then route\nto x via z. The result of the bad news about the increase in link cost has indeed traveled\nslowly! What would have happened if the link cost c(y, x) had changed from 4 to\n10,000 and the cost c(z, x) had been 9,999? Because of such scenarios, the problem we\nhave seen is sometimes referred to as the count-to-infinity ­problem.\nDistance-Vector Algorithm: Adding Poisoned Reverse\nThe specific looping scenario just described can be avoided using a technique known\nas poisoned reverse. The idea is simple—if z routes through y to get to destination x,\nthen z will advertise to y that its distance to x is infinity, that is, z will advertise to y\nthat D (x) = ∞ (even though z knows D (x) = 5 in truth). z will continue telling this\nlittle white lie to y as long as it routes to x via y. Since y believes that z has no path to\nx, y will never attempt to route to x via z, as long as z continues to route to x via y (and\nlies about doing so).\nLet’s now see how poisoned reverse solves the particular looping problem we\nencountered before in Figure 5.5(b). As a result of the poisoned reverse, y’s distance\ntable indicates D (x) = ∞. When the cost of the (x, y) link changes from 4 to 60 at time\nt , y updates its table and continues to route directly to x, albeit at a higher cost of 60,\nand informs z of its new cost to x, that is, D (x) = 60. After receiving the update at t , z\nimmediately shifts its route to x to be via the direct (z, x) link at a cost of 50. Since this\nis a new least-cost path to x, and since the path no longer passes through y, z now\ninforms y that D (x) = 50 at t . After receiving the update from z, y updates its distance\ntable with D (x) = 51. Also, since z is now on y’s least-cost path to x, y poisons the\nreverse path from z to x by informing z at time t  that D (x) = ∞ (even though y knows\nthat D (x) = 51 in truth).\nDoes poisoned reverse solve the general count-to-infinity problem? It does not.\nYou should convince yourself that loops involving three or more nodes (rather than\nsimply two immediately neighboring nodes) will not be detected by the poisoned\nreverse technique.\nA Comparison of LS and DV Routing Algorithms\nThe DV and LS algorithms take complementary approaches toward computing\nrouting. In the DV algorithm, each node talks to only its directly connected neighbors,\nbut it provides its neighbors with least-cost estimates from itself to all the nodes (that\nit knows about) in the network. The LS algorithm requires global information.\nConsequently, when implemented in each and every router, for example, as in Figures\n4.2 and 5.1, each node would need to communicate with all other nodes (via\nbroadcast), but it tells them only the costs of its directly connected links. Let’s\nconclude our study of LS and DV algorithms with a quick comparison of some of their\nattributes. Recall that N is the set of nodes (routers) and E is the set of edges (links).\nMessage complexity. We have seen that LS requires each node to know the cost of\neach link in the network. This requires O(|N| |E|) messages to be sent. Also,\nwhenever a link cost changes, the new link cost must be sent to all nodes. The DV\nalgorithm requires message exchanges between directly connected neighbors at\neach iteration. We have seen that the time needed for the algorithm to converge\ncan depend on many factors. When link costs change, the DV algorithm will\npropagate the results of the changed link cost only if the new link cost results in a\nchanged least-cost path for one of the nodes attached to that link.\nSpeed of convergence. We have seen that our implementation of LS is an O(|N|2)\nalgorithm requiring O(|N| |E|)) messages. The DV algorithm can converge slowly\nand can have routing loops while the algorithm is converging. DV also suffers\nfrom the count-to-infinity problem.\nRobustness. What can happen if a router fails, misbehaves, or is sabotaged? Under\nLS, a router could broadcast an incorrect cost for one of its attached links (but no\nothers). A node could also corrupt or drop any packets it received as part of an LS\nbroadcast. But an LS node is computing only its own forwarding tables; other\nnodes are performing similar calculations for themselves. This means route\ncalculations are somewhat separated under LS, providing a degree of robustness.\nUnder DV, a node can advertise incorrect least-cost paths to any or all destinations.\n(Indeed, in 1997, a malfunctioning router in a small ISP provided national\nbackbone routers with erroneous routing information. This caused other routers to\nflood the malfunctioning router with traffic and caused large portions of the\nInternet to become disconnected for up to several hours [Neumann 1997].) More\ngenerally, we note that, at each iteration, a node’s calculation in DV is passed on to\nits neighbor and then indirectly to its neighbor’s neighbor on the next iteration. In\nthis sense, an incorrect node calculation can be diffused through the entire network\nIn the end, neither algorithm is an obvious winner over the other; indeed, both\nalgorithms are used in the Internet.\n5.3 Intra-AS Routing in the Internet: OSPF\nIn our study of routing algorithms so far, we’ve viewed the network simply\nas a collection of interconnected routers. One router was indistinguishable\nfrom another in the sense that all routers executed the same routing\nalgorithm to compute routing paths through the entire network. In practice,\nthis model and its view of a homogenous set of routers all executing the\nsame routing algorithm is simplistic for two important reasons:\nScale. As the number of routers becomes large, the overhead involved\nin communicating, computing, and storing routing information becomes\nprohibitive. Today’s Internet consists of hundreds of millions of routers.\nStoring routing information for possible destinations at each of these\nrouters would clearly require enormous amounts of memory. The\noverhead required to broadcast connectivity and link cost updates\namong all of the routers would be huge! A distance-vector algorithm\nthat iterated among such a large number of routers would surely never\nconverge. Clearly, something must be done to reduce the complexity of\nroute computation in a network as large as the Internet.\nAdministrative autonomy. As described in Section 1.3, the Internet is a\nnetwork of ISPs, with each ISP consisting of its own network of routers.\nAn ISP generally desires to operate its network as it pleases (for\nexample, to run whatever routing algorithm it chooses within its\nnetwork) or to hide aspects of its network’s internal organization from\nthe outside. Ideally, an organization should be able to operate and\nadminister its network as it wishes, while still being able to connect its\nnetwork to other outside networks.\nBoth of these problems can be solved by organizing routers into\nautonomous ­systems (ASs), with each AS consisting of a group of routers\nthat are under the same administrative control. Often the routers in an ISP,\nand the links that interconnect them, constitute a single AS. Some ISPs,\nhowever, partition their network into multiple ASs. In particular, some tier-1\nISPs use one gigantic AS for their entire network, whereas others break up\ntheir ISP into tens of interconnected ASs. An autonomous system is\nidentified by its globally unique autonomous system number (ASN) [RFC\n1930]. AS numbers, like IP addresses, are assigned by ICANN regional\nregistries [ICANN 2020].\nRouters within the same AS all run the same routing algorithm and\nhave information about each other. The routing algorithm ­running within an\nautonomous system is called an intra-autonomous system routing ­-\nOpen Shortest Path First (OSPF)\nOSPF routing and its closely related cousin, IS-IS, are widely used for\nintra-AS routing in the Internet. The Open in OSPF indicates that the\nrouting protocol specification is publicly available (for example, as opposed\nto Cisco’s EIGRP protocol, which was only recently became open [Savage\n2015], after roughly 20 years as a Cisco-proprietary protocol). The most\nrecent version of OSPF, version 2, is defined in [RFC 2328], a public\nOSPF is a link-state protocol that uses flooding of link-state\ninformation and a Dijkstra’s least-cost path algorithm. With OSPF, each\nrouter constructs a complete topological map (that is, a graph) of the entire\nautonomous system. Each router then locally runs Dijkstra’s shortest-path\nalgorithm to determine a shortest-path tree to all subnets, with itself as the\nroot node. Individual link costs are configured by the network administrator\n(see sidebar, Principles and Practice: Setting OSPF Weights). The\nadministrator might choose to set all link costs to 1, thus achieving\nminimum-hop routing, or might choose to set the link weights to be\ninversely proportional to link capacity in order to discourage traffic from\nusing low-bandwidth links. OSPF does not mandate a policy for how link\nweights are set (that is the job of the ­network administrator), but instead\nprovides the mechanisms (protocol) for determining least-cost path routing\nfor the given set of link weights.\nOur discussion of link-state routing has implicitly assumed that link weights are set, a\nrouting algorithm such as OSPF is run, and traffic flows according to the routing tables\ncomputed by the LS algorithm. In terms of cause and effect, the link weights are given (i.e.,\nthey come first) and result (via Dijkstra’s algorithm) in routing paths that minimize overall\ncost. In this viewpoint, link weights reflect the cost of using a link (for example, if link\nweights are inversely proportional to capacity, then the use of high-capacity links would\nhave smaller weight and thus be more attractive from a routing standpoint) and Dijsktra’s\nalgorithm serves to minimize overall cost.\nIn practice, the cause and effect relationship between link weights and routing paths may\nbe reversed, with network operators configuring link weights in order to obtain routing paths\nthat achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For example,\nsuppose a network operator has an estimate of traffic flow entering the network at each\ningress point and destined for each egress point. The operator may then want to put in\nplace a specific routing of ingress-to-egress flows that minimizes the maximum utilization\nover all of the network’s links. But with a routing algorithm such as OSPF, the operator’s\nmain “knobs” for tuning the routing of flows through the network are the link weights. Thus,\nin order to achieve the goal of minimizing the maximum link utilization, the operator must\nfind the set of link weights that achieves this goal. This is a reversal of the cause and effect\nrelationship—the desired routing of flows is known, and the OSPF link weights must be\nfound such that the OSPF routing algorithm results in this desired routing of flows.\nWith OSPF, a router broadcasts routing information to all other routers\nin the autonomous system, not just to its neighboring routers. A router\nbroadcasts link-state information whenever there is a change in a link’s state\n(for example, a change in cost or a change in up/down status). It also\nbroadcasts a link’s state periodically (at least once every 30 minutes), even\nif the link’s state has not changed. RFC 2328 notes that “this periodic\nupdating of link state advertisements adds robustness to the link state\nalgorithm.” OSPF advertisements are contained in OSPF messages that are\ncarried directly by IP, with an upper-layer protocol of 89 for OSPF. Thus,\nthe OSPF protocol must itself implement functionality such as reliable\nmessage transfer and link-state broadcast. The OSPF protocol also checks\nthat links are operational (via a HELLO message that is sent to an attached\nneighbor) and allows an OSPF router to obtain a neighboring router’s\ndatabase of network-wide link state.\nSome of the advances embodied in OSPF include the following:\nSecurity. Exchanges between OSPF routers (for example, link-state\nupdates) can be authenticated. With authentication, only trusted routers\ncan participate in the OSPF protocol within an AS, thus preventing\nmalicious intruders (or networking students taking their newfound\nknowledge out for a joyride) from injecting incorrect information into\nrouter tables. By default, OSPF packets between routers are  not\nauthenticated and could be forged. Two types of authentication can be\nconfigured—simple and MD5 (see Chapter 8 for a discussion on MD5\nand authentication in general). With simple authentication, the same\npassword is configured on each router. When a router sends an OSPF\npacket, it includes the password in plaintext. Clearly, simple\nauthentication is not very secure. MD5 authentication is based on\nshared secret keys that are configured in all the routers. For each OSPF\npacket that it sends, the router computes the MD5 hash of the content of\nthe OSPF packet appended with the secret key. (See the discussion of\nmessage authentication codes in Chapter 8.) Then the router includes\nthe resulting hash value in the OSPF packet. The receiving router, using\nthe preconfigured secret key, will compute an MD5 hash of the packet\nand compare it with the hash value that the packet carries, thus\nverifying the packet’s authenticity. Sequence numbers are also used\nwith MD5 authentication to protect against replay attacks.\nMultiple same-cost paths. When multiple paths to a destination have the\nsame cost, OSPF allows multiple paths to be used (that is, a single path\nneed not be chosen for carrying all traffic when multiple equal-cost\npaths exist).\nIntegrated support for unicast and multicast routing. Multicast OSPF\n(MOSPF) [RFC 1584] provides simple extensions to OSPF to provide\nfor multicast routing. MOSPF uses the existing OSPF link database and\nadds a new type of link-state advertisement to the existing OSPF link-\nstate broadcast mechanism.\nSupport for hierarchy within a single AS. An OSPF autonomous system\ncan be configured hierarchically into areas. Each area runs its own\nOSPF link-state routing algorithm, with each router in an area\nbroadcasting its link state to all other routers in that area. Within each\narea, one or more area border routers are responsible for routing packets\noutside the area. Lastly, exactly one OSPF area in the AS is configured\nto be the backbone area. The primary role of the backbone area is to\nroute traffic between the other areas in the AS. The backbone always\ncontains all area border routers in the AS and may contain non-border\nrouters as well. Inter-area routing within the AS requires that the packet\nbe first routed to an area border router (intra-area routing), then routed\nthrough the backbone to the area border router that is in the destination\narea, and then routed to the final destination.\nOSPF is a relatively complex protocol, and our coverage here has been\nnecessarily brief; [Huitema 1998; Moy 1998; RFC 2328] provide additional\n5.4 Routing Among the ISPs: BGP\nWe just learned that OSPF is an example of an intra-AS routing protocol.\nWhen routing a packet between a source and destination within the same\nAS, the route the packet follows is entirely determined by the intra-AS\nrouting protocol. However, to route a packet across multiple ASs, say from\na smartphone in Timbuktu to a server in a datacenter in Silicon Valley, we\nneed an inter-autonomous ­system routing protocol. Since an inter-AS\ncoordination \ncommunicating ASs must run the same inter-AS routing protocol. In fact, in\nthe Internet, all ASs run the same inter-AS routing protocol, called the\nBorder Gateway Protocol, more commonly known as BGP [RFC 4271;\nStewart 1999].\nGluing the Internet Together: BGP\nBGP is arguably the most important of all the Internet protocols (the\nonly other contender would be the IP protocol that we studied in Section\n4.3), as it is the protocol that glues the thousands of ISPs in the Internet\ntogether. As we will soon see, BGP is a decentralized and asynchronous\nprotocol in the vein of distance-vector routing described in Section 5.2.2.\nAlthough BGP is a complex and challenging protocol, to understand the\nInternet on a deep level, we need to become familiar with its underpinnings\nand operation. The time we devote to learning BGP will be well worth the\n5.4.1 The Role of BGP\nTo understand the responsibilities of BGP, consider an AS and an arbitrary\nrouter in that AS. Recall that every router has a forwarding table, which\nplays the central role in the process of forwarding arriving packets to\noutbound router links. As we have learned, for destinations that are within\nthe same AS, the entries in the router’s forwarding table are determined by\nthe AS’s intra-AS routing protocol. But what about destinations that are\noutside of the AS? This is precisely where BGP comes to the rescue.\nIn BGP, packets are not routed to a specific destination address, but\ninstead to CIDRized prefixes, with each prefix representing a subnet or a\ncollection of subnets. In the world of BGP, a destination may take the form\n138.16.68/22, which for this example includes 1,024 IP addresses. Thus, a\nrouter’s forwarding table will have entries of the form (x, I), where x is a\nprefix (such as 138.16.68/22) and I is an interface number for one of the\nrouter’s interfaces.\nAs an inter-AS routing protocol, BGP provides each router a means to:\n1. Obtain prefix reachability information from neighboring ASs. In\nparticular, BGP allows each subnet to advertise its existence to the rest\nof the Internet. A subnet screams, “I exist and I am here,” and BGP\nmakes sure that all the routers in the Internet know about this subnet. If\nit weren’t for BGP, each subnet would be an isolated island—alone,\nunknown and unreachable by the rest of the Internet.\n2. Determine the “best” routes to the prefixes. A router may learn about\ntwo or more different routes to a specific prefix. To determine the best\nroute, the router will locally run a BGP route-selection procedure (using\nthe prefix reachability information it obtained via neighboring routers).\nThe best route will be determined based on policy as well as the\nreachability information.\nLet us now delve into how BGP carries out these two tasks.\n5.4.2 Advertising BGP Route Information\nConsider the network shown in Figure 5.8. As we can see, this simple\nnetwork has three autonomous systems: AS1, AS2, and AS3. As shown,\nAS3 includes a subnet with prefix x. For each AS, each router is either a\ngateway router or an internal router. A gateway router is a router on the\nedge of an AS that directly connects to one or more routers in other ASs. An\ninternal router connects only to hosts and routers within its own AS. In\nAS1, for example, router 1c is a gateway router; routers 1a, 1b, and 1d are\ninternal routers.\nFigure 5.8 ♦Network with three autonomous systems. AS3 includes\na subnet with prefix x\nLet’s consider the task of advertising reachability information for prefix\nx to all of the routers shown in Figure 5.8. At a high level, this is\nstraightforward. First, AS3 sends a BGP message to AS2, saying that x\nexists and is in AS3; let’s denote this message as “AS3 x”. Then AS2 sends\na BGP message to AS1, saying that x exists and that you can get to x by\nfirst passing through AS2 and then going to AS3; let’s denote that message\nas “AS2 AS3 x”. In this manner, each of the autonomous systems will not\nonly learn about the existence of x, but also learn about a path of\nautonomous systems that leads to x.\nAlthough the discussion in the above paragraph about advertising BGP\nreachability information should get the general idea across, it is not precise\nin the sense that autonomous systems do not actually send messages to each\nother, but instead routers do. To understand this, let’s now re-examine the\nexample in Figure 5.8. In BGP, pairs of routers exchange routing\ninformation over semi-permanent TCP connections using port 179. Each\nsuch TCP connection, along with all the BGP messages sent over the\nconnection, is called a BGP connection. Furthermore, a BGP connection\nthat spans two ASs is called an external BGP (eBGP) connection, and a\nBGP session between routers in the same AS is called an internal BGP\n(iBGP) connection. Examples of BGP connections for the network in\nFigure 5.8 are shown in Figure 5.9. There is typically one eBGP connection\nfor each link that directly connects gateway routers in different ASs; thus, in\nFigure 5.9, there is an eBGP connection between gateway routers 1c and 2a\nand an eBGP connection between gateway routers 2c and 3a.\nFigure 5.9 ♦eBGP and iBGP connections\nThere are also iBGP connections between routers within each of the\nASs. In particular, Figure 5.9 displays a common configuration of one BGP\nconnection for each pair of routers internal to an AS, creating a mesh of\nTCP connections within each AS. In Figure 5.9, the eBGP connections are\nshown with the long dashes; the iBGP connections are shown with the short\ndashes. Note that iBGP connections do not always correspond to physical\nIn order to propagate the reachability information, both iBGP and\neBGP sessions are used. Consider again advertising the reachability\ninformation for prefix x to all routers in AS1 and AS2. In this process,\ngateway router 3a first sends an eBGP message “AS3 x” to gateway router\n2c. Gateway router 2c then sends the iBGP message “AS3 x” to all of the\nother routers in AS2, including to gateway router 2a. Gateway router 2a\nthen sends the eBGP message “AS2 AS3 x” to gateway router 1c. Finally,\ngateway router 1c uses iBGP to send the message “AS2 AS3 x” to all the\nrouters in AS1. After this process is complete, each router in AS1 and AS2\nis aware of the existence of x and is also aware of an AS path that leads to\nOf course, in a real network, from a given router there may be many\ndifferent paths to a given destination, each through a different sequence of\nASs. For example, consider the network in Figure 5.10, which is the\noriginal network in Figure 5.8, with an additional physical link from router\n1d to router 3d. In this case, there are two paths from AS1 to x: the path\n“AS2 AS3 x” via router 1c; and the new path “AS3 x” via the router 1d.\n5.4.3 Determining the Best Routes\nAs we have just learned, there may be many paths from a given router to a\ndestination subnet. In fact, in the Internet, routers often receive reachability\ninformation about dozens of different possible paths. How does a router\nchoose among these paths (and then configure its forwarding table\naccordingly)?\nBefore addressing this critical question, we need to introduce a little\nmore BGP terminology. When a router advertises a prefix across a BGP\nconnection, it includes with the prefix several BGP attributes. In BGP\njargon, a prefix along with its attributes is called a route. Two of the more\nimportant attributes are AS-PATH and NEXT-HOP. The AS-PATH attribute\ncontains the list of ASs through which the advertisement has passed, as\nwe’ve seen in our examples above. To generate the AS-PATH value, when a\nprefix is passed to an AS, the AS adds its ASN to the existing list in the AS-\nPATH. For example, in Figure 5.10, there are two routes from AS1 to\nsubnet x: one which uses the AS-PATH “AS2 AS3”; and another that uses\nthe AS-PATH “A3”. BGP routers also use the AS-PATH attribute to detect\nand prevent looping advertisements; specifically, if a router sees that its\nown AS is contained in the path list, it will reject the advertisement.\nFigure 5.10 ♦Network augmented with peering link between AS1\nProviding the critical link between the inter-AS and intra-AS routing\nprotocols, the NEXT-HOP attribute has a subtle but important use. The\nNEXT-HOP is the IP address of the router interface that begins the AS-\nPATH. To gain insight into this attribute, let’s again refer to Figure 5.10. As\nindicated in Figure 5.10, the NEXT-HOP attribute for the route “AS2 AS3\nx” from AS1 to x that passes through AS2 is the IP address of the left\ninterface on router 2a. The NEXT-HOP attribute for the route “AS3 x” from\nAS1 to x that bypasses AS2 is the IP address of the leftmost interface of\nrouter 3d. In summary, in this toy example, each router in AS1 becomes\naware of two BGP routes to prefix x:\nIP address of leftmost interface for router 2a; AS2 AS3; x\nIP address of leftmost interface of router 3d; AS3; x\nHere, each BGP route is written as a list with three components: NEXT-\nHOP; AS-PATH; destination prefix. In practice, a BGP route includes\nadditional attributes, which we will ignore for the time being. Note that the\nNEXT-HOP attribute is an IP address of a router that does not belong to\nAS1; however, the subnet that contains this IP address directly attaches to\nHot Potato Routing\nWe are now finally in position to talk about BGP routing algorithms in a\nprecise manner. We will begin with one of the simplest routing algorithms,\nnamely, hot potato routing.\nConsider router 1b in the network in Figure 5.10. As just described, this\nrouter will learn about two possible BGP routes to prefix x. In hot potato\nrouting, the route chosen (from among all possible routes) is that route with\nthe least cost to the NEXT-HOP router beginning that route. In this\nexample, router 1b will consult its intra-AS routing information to find the\nleast-cost intra-AS path to NEXT-HOP router 2a and the least-cost intra-AS\npath to NEXT-HOP router 3d, and then select the route with the smallest of\nthese least-cost paths. For example, suppose that cost is defined as the\nnumber of links traversed. Then the least cost from router 1b to router 2a is\n2, the least cost from router 1b to router 2d is 3, and router 2a would\ntherefore be selected. Router 1b would then consult its forwarding table\n(configured by its intra-AS algorithm) and find the interface I that is on the\nleast-cost path to router 2a. It then adds (x, I) to its forwarding table.\nThe steps for adding an outside-AS prefix in a router’s forwarding table\nfor hot potato routing are summarized in Figure 5.11. It is important to note\nthat when adding an outside-AS prefix into a forwarding table, both the\ninter-AS routing protocol (BGP) and the intra-AS routing protocol (e.g.,\nOSPF) are used.\nFigure 5.11 ♦Steps in adding outside-AS destination in a router’s\nforwarding table\nThe idea behind hot-potato routing is for router 1b to get packets out of\nits AS as quickly as possible (more specifically, with the least cost possible)\nwithout worrying about the cost of the remaining portions of the path\noutside of its AS to the destination. In the name “hot potato routing,” a\npacket is analogous to a hot potato that is burning in your hands. Because it\nis burning hot, you want to pass it off to another person (another AS) as\nquickly as possible. Hot potato routing is thus a selfish ­algorithm—it tries\nto reduce the cost in its own AS while ignoring the other components of the\nend-to-end costs outside its AS. Note that with hot potato routing, two\nrouters in the same AS may choose two different AS paths to the same\nprefix. For example, we just saw that router 1b would send packets through\nAS2 to reach x. However, router 1d would bypass AS2 and send packets\ndirectly to AS3 to reach x.\nRoute-Selection Algorithm\nIn practice, BGP uses an algorithm that is more complicated than hot potato\nrouting, but nevertheless incorporates hot potato routing. For any given\ndestination prefix, the input into BGP’s route-selection algorithm is the set\nof all routes to that prefix that have been learned and accepted by the router.\nIf there is only one such route, then BGP obviously selects that route. If\nthere are two or more routes to the same prefix, then BGP sequentially\ninvokes the following elimination rules until one route remains:\n1. A route is assigned a local preference value as one of its attributes (in\naddition to the AS-PATH and NEXT-HOP attributes). The local\npreference of a route could have been set by the router or could have\nbeen learned from another router in the same AS. The value of the local\npreference attribute is a policy decision that is left entirely up to the\nAS’s network administrator. (We will shortly discuss BGP policy issues\nin some detail.) The routes with the highest local preference values are\n2. From the remaining routes (all with the same highest local preference\nvalue), the route with the shortest AS-PATH is selected. If this rule were\nthe only rule for route selection, then BGP would be using a DV\nalgorithm for path determination, where the distance metric uses the\nnumber of AS hops rather than the number of router hops.\n3. From the remaining routes (all with the same highest local preference\nvalue and the same AS-PATH length), hot potato routing is used, that is,\nthe route with the closest NEXT-HOP router is selected.\n4. If more than one route still remains, the router uses BGP identifiers to\nselect the route; see [Stewart 1999].\nAs an example, let’s again consider router 1b in Figure 5.10. Recall that\nthere are exactly two BGP routes to prefix x, one that passes through AS2\nand one that bypasses AS2. Also recall that if hot potato routing on its own\nwere used, then BGP would route packets through AS2 to prefix x. But in\nthe above route-selection algorithm, rule 2 is applied before rule 3, causing\nBGP to select the route that bypasses AS2, since that route has a shorter AS\nPATH. So we see that with the above route-selection algorithm, BGP is no\nlonger a selfish algorithm—it first looks for routes with short AS paths\n(thereby likely reducing end-to-end delay).\nAs noted above, BGP is the de facto standard for inter-AS routing for\nthe many other access solutions described in Chapter 1. Your local ISP will\nalso provide you with an IP address range, for example, a /24 address range\nconsisting of 256 addresses. Once you have your physical connectivity and\nyour IP address range, you will assign one of the IP addresses (in your\naddress range) to your Web server, one to your mail server, one to your\nDNS server, one to your gateway router, and other IP addresses to other\nservers and ­networking devices in your company’s network.\nIn addition to contracting with an ISP, you will also need to contract\nwith an Internet registrar to obtain a domain name for your company, as\ndescribed in Chapter 2. For example, if your company’s name is, say,\nXanadu Inc., you will naturally try to obtain the domain name xanadu.com.\nYour company must also obtain presence in the DNS system. Specifically,\nbecause outsiders will want to contact your DNS server to obtain the IP\naddresses of your servers, you will also need to provide your registrar with\nthe IP address of your DNS server. Your registrar will then put an entry for\nyour DNS server (domain name and corresponding IP address) in the .com\ntop-level-domain servers, as described in Chapter 2. After this step is\ncompleted, any user who knows your domain name (e.g., xanadu.com) will\nbe able to obtain the IP address of your DNS server via the DNS system.\nSo that people can discover the IP addresses of your Web server, in\nyour DNS server you will need to include entries that map the host name of\nyour Web server (e.g., www.xanadu.com) to its IP address. You will want to\nhave similar entries for other publicly available servers in your company,\nincluding your mail server. In this manner, if Alice wants to browse your\nWeb server, the DNS system will contact your DNS server, find the IP\naddress of your Web server, and give it to Alice. Alice can then establish a\nTCP connection directly with your Web server.\nHowever, there still remains one other necessary and crucial step to\nallow outsiders from around the world to access your Web server. Consider\nwhat happens when Alice, who knows the IP address of your Web server,\nsends an IP datagram (e.g., a TCP SYN segment) to that IP address. This\ndatagram will be routed through the Internet, visiting a series of routers in\nmany different ASs, and eventually reach your Web server. When any one\nof the routers receives the datagram, it is going to look for an entry in its\nforwarding table to determine on which outgoing port it should forward the\ndatagram. Therefore, each of the routers needs to know about the existence\nof your company’s /24 prefix (or some aggregate entry). How does a router\nbecome aware of your company’s prefix? As we have just seen, it becomes\naware of it from BGP! Specifically, when your company contracts with a\nlocal ISP and gets assigned a prefix (i.e., an address range), your local ISP\nwill use BGP to advertise your prefix to the ISPs to which it connects.\nThose ISPs will then, in turn, use BGP to propagate the advertisement.\nEventually, all Internet routers will know about your prefix (or about some\naggregate that includes your prefix) and thus be able to appropriately\nforward datagrams destined to your Web and mail servers.\n5.5 The SDN Control Plane\nIn this section, we’ll dive into the SDN control plane—the network-wide\nlogic that controls packet forwarding among a network’s SDN-enabled\ndevices, as well as the configuration and management of these devices and\ntheir services. Our study here builds on our earlier discussion of generalized\nSDN forwarding in Section 4.4, so you might want to first review that\nsection, as well as Section 5.1 of this chapter, before continuing on. As in\nSection 4.4, we’ll again adopt the terminology used in the SDN literature\nand refer to the network’s forwarding devices as “packet switches” (or just\nswitches, with “packet” being understood), since forwarding decisions can\nbe made on the basis of network-layer source/destination addresses, link-\nlayer source/destination addresses, as well as many other values in\ntransport-, network-, and link-layer packet-header fields.\nFour key characteristics of an SDN architecture can be identified\n[Kreutz 2015]:\nFlow-based forwarding. Packet forwarding by SDN-controlled switches\ncan be based on any number of header field values in the transport-\nlayer, network-layer, or link-layer header. We saw in Section 4.4 that the\nOpenFlow1.0 abstraction allows forwarding based on eleven different\nheader field values. This contrasts sharply with the traditional approach\nto router-based forwarding that we studied in Sections 5.2–5.4, where\nforwarding of IP datagrams was based solely on a datagram’s\ndestination IP address. Recall from Figure 5.2 that packet forwarding\nrules are specified in a switch’s flow table; it is the job of the SDN\ncontrol plane to compute, manage and install flow table entries in all of\nthe network’s switches.\nSeparation of data plane and control plane. This separation is shown\nclearly in Figures 5.2 and 5.14. The data plane consists of the network’s\nswitches—relatively simple (but fast) devices that execute the “match\nplus action” rules in their flow tables. The control plane consists of\nservers and software that determine and manage the switches’ flow\nNetwork control functions: external to data-plane switches. Given that\nthe “S” in SDN is for “software,” it’s perhaps not surprising that the\nSDN control plane is implemented in software. Unlike traditional\nrouters, however, this software executes on servers that are both distinct\nand remote from the network’s switches. As shown in Figure 5.14, the\ncontrol plane itself consists of two components—an SDN controller (or\nnetwork operating system [Gude 2008]) and a set of network-control\napplications. The controller maintains accurate network state\ninformation (e.g., the state of remote links, switches, and hosts);\nprovides this information to the network-control applications running in\nthe control plane; and provides the means through which these\napplications can monitor, program, and control the underlying network\ndevices. Although the controller in Figure 5.14 is shown as a single\ncentral server, in practice the controller is only logically centralized; it\nis typically implemented on several servers that provide coordinated,\nscalable performance and high availability.\nA programmable network. The network is programmable through the\nnetwork-control applications running in the control plane. These\napplications represent the “brains” of the SDN control plane, using the\nAPIs provided by the SDN controller to specify and control the data\nplane in the network devices. For example, a routing network-control\napplication might determine the end-end paths between sources and\ndestinations (for example, by executing Dijkstra’s algorithm using the\nnode-state and link-state information maintained by the SDN\ncontroller). Another network application might perform access control,\nthat is, determine which packets are to be blocked at a switch, as in our\nthird example in Section 4.4.3. Yet another application might have\nswitches forward packets in a manner that performs server load\nbalancing (the second example we considered in Section 4.4.3).\nFrom this discussion, we can see that SDN represents a significant\n“unbundling” of network functionality—data plane switches, SDN\ncontrollers, and network-control applications are separate entities that may\neach be provided by different vendors and organizations. This contrasts\nwith the pre-SDN model in which a switch/router (together with its\nembedded control plane software and protocol implementations) was\nmonolithic, vertically integrated, and sold by a single vendor. This\nunbundling of network functionality in SDN has been likened to the earlier\nevolution from mainframe computers (where hardware, system software,\nand applications were provided by a single vendor) to personal computers\n(with their separate hardware, operating systems, and applications). The\nunbundling of computing hardware, system software, and applications has\nled to a rich, open ecosystem driven by innovation in all three of these\nareas; one hope for SDN is that it will continue to drive and enable such\nrich innovation.\nGiven our understanding of the SDN architecture of Figure 5.14, many\nquestions naturally arise. How and where are the flow tables actually\ncomputed? How are these tables updated in response to events at SDN-\ncontrolled devices (e.g., an attached link going up/down)? And how are the\nflow table entries at multiple switches coordinated in such a way as to result\nin orchestrated and consistent network-wide functionality (e.g., end-to-end\npaths for forwarding packets from sources to destinations, or coordinated\ndistributed firewalls)? It is the role of the SDN control plane to provide\nthese, and many other, capabilities.\nFigure 5.14 ♦Components of the SDN architecture: SDN-controlled\nswitches, the SDN controller, network-control\napplications\n5.5.1 The SDN Control Plane: SDN Controller and\nSDN Network-control Applications\nLet’s begin our discussion of the SDN control plane in the abstract, by\nconsidering the generic capabilities that the control plane must provide. As\nwe’ll see, this abstract, “first principles” approach will lead us to an overall\narchitecture that reflects how SDN control planes have been implemented\nin practice.\nAs noted above, the SDN control plane divides broadly into two\ncomponents—the \nnetwork-control\napplications. Let’s explore the controller first. Many SDN controllers have\nbeen developed since the earliest SDN controller [Gude 2008]; see [Kreutz\n2015] for an extremely thorough survey. Figure 5.15 provides a more\ndetailed view of a generic SDN controller. A controller’s functionality can\nbe broadly organized into three layers. Let’s consider these layers in an\nuncharacteristically bottom-up fashion:\nFigure 5.15 ♦Components of an SDN controller\nA communication layer: communicating between the SDN controller\nand controlled network devices. Clearly, if an SDN controller is going\nto control the operation of a remote SDN-enabled switch, host, or other\ndevice, a protocol is needed to transfer information between the\ncontroller and that device. In addition, a device must be able to\ncommunicate locally-observed events to the controller (for example, a\nmessage indicating that an attached link has gone up or down, that a\ndevice has just joined the network, or a heartbeat indicating that a\ndevice is up and operational). These events provide the SDN controller\nwith an up-to-date view of the network’s state. This protocol constitutes\nthe lowest layer of the controller architecture, as shown in Figure 5.15.\nThe communication between the controller and the controlled devices\ncross what has come to be known as the controller’s “southbound”\ninterface. In Section 5.5.2, we’ll study OpenFlow—a specific protocol\nthat provides this communication functionality. OpenFlow is\nimplemented in most, if not all, SDN controllers.\nA network-wide state-management layer. The ultimate control decisions\nmade by the SDN control plane—for example, configuring flow tables\nin all switches to achieve the desired end-end forwarding, to implement\nload balancing, or to implement a particular firewalling capability—will\nrequire that the controller have up-to-date information about state of the\nnetworks’ hosts, links, switches, and other SDN-controlled devices. A\nswitch’s flow table contains counters whose values might also be\nprofitably used by network-control applications; these values should\nthus be available to the applications. Since the ultimate aim of the\ncontrol plane is to determine flow tables for the various controlled\ndevices, a controller might also maintain a copy of these tables. These\npieces of information all constitute examples of the network-wide\n“state” maintained by the SDN controller.\nThe interface to the network-control application layer. The controller\ninteracts with network-control applications through its “northbound”\ninterface. This API allows network-control applications to read/write\nnetwork state and flow tables within the state-management layer.\nApplications can register to be notified when state-change events occur,\nso that they can take actions in response to network event notifications\nsent from SDN-controlled devices. Different types of APIs may be\nprovided; we’ll see that two popular SDN controllers communicate with\ntheir applications using a REST [Fielding 2000] request-response\nWe have noted several times that an SDN controller can be considered\nto be ­“logically centralized,” that is, that the controller may be viewed\nexternally (for example, from the point of view of SDN-controlled devices\nand external network-control applications) as a single, monolithic service.\nHowever, these services and the databases used to hold state information\nare implemented in practice by a distributed set of servers for fault\ntolerance, high availability, or for performance reasons. With controller\nfunctions being implemented by a set of servers, the semantics of the\ncontroller’s internal operations (e.g., maintaining logical time ordering of\nevents, consistency, consensus, and more) must be considered [Panda\n2013]. Such concerns are common across many different distributed\nsystems; see [Lamport 1989, Lampson 1996] for elegant solutions to these\nchallenges. Modern controllers such as OpenDaylight [OpenDaylight 2020]\nand ONOS [ONOS 2020] (see sidebar) have placed considerable emphasis\non architecting a logically centralized but physically distributed controller\nplatform that provides scalable services and high availability to the\ncontrolled devices and network-control applications alike.\nThe architecture depicted in Figure 5.15 closely resembles the\narchitecture of the originally proposed NOX controller in 2008 [Gude\n2008], as well as that of today’s OpenDaylight [OpenDaylight 2020] and\nONOS [ONOS 2020] SDN controllers (see sidebar). We’ll cover an\nexample of controller operation in Section 5.5.3. First, however, let’s\nexamine the OpenFlow protocol, the earliest and now one of several pro­-\ntocols that can be used for communication between an SDN controller and a\ncontrolled device, which lies in the controller’s communication layer.\n5.5.2 OpenFlow Protocol\nThe OpenFlow protocol [OpenFlow 2009, ONF 2020] operates between an\nSDN controller and an SDN-controlled switch or other device\nimplementing the OpenFlow API that we studied earlier in Section 4.4. The\nOpenFlow protocol operates over TCP, with a default port number of 6653.\nAmong the important messages flowing from the controller to the\ncontrolled switch are the following:\nConfiguration. This message allows the controller to query and set a\nswitch’s configuration parameters.\nModify-State. This message is used by a controller to add/delete or\nmodify entries in the switch’s flow table, and to set switch port\nproperties.\nRead-State. This message is used by a controller to collect statistics and\ncounter values from the switch’s flow table and ports.\nSend-Packet. This message is used by the controller to send a specific\npacket out of a specified port at the controlled switch. The message\nitself contains the packet to be sent in its payload.\nAmong the messages flowing from the SDN-controlled switch to the\ncontroller are the following:\nFlow-Removed. This message informs the controller that a flow table\nentry has been removed, for example by a timeout or as the result of a\nreceived modify-state message.\nPort-status. This message is used by a switch to inform the controller of\na change in port status.\nPacket-in. Recall from Section 4.4 that a packet arriving at a switch port\nand not matching any flow table entry is sent to the controller for\nadditional processing. Matched packets may also be sent to the\ncontroller, as an action to be taken on a match. The packet-in message is\nused to send such packets to the controller.\nAdditional OpenFlow messages are defined in [OpenFlow 2009, ONF\nGOOGLE’S SOFTWARE-DEFINED GLOBAL NETWORK\nRecall from the case study in Section 2.6 that Google deploys a dedicated wide-area\nnetwork (WAN) that interconnects its data centers and server clusters (in IXPs and ISPs).\nThis network, called B4, has a Google-designed SDN control plane built on OpenFlow.\nGoogle’s network is able to drive WAN links at near 70% utilization over the long run (a two\nto three fold increase over typical link utilizations) and split application flows among multiple\npaths based on application priority and existing flow demands [Jain 2013].\nThe Google B4 network is particularly it well-suited for SDN: (i) Google controls all\ndevices from the edge servers in IXPs and ISPs to routers in their network core; (ii) the\nmost bandwidth-intensive applications are large-scale data copies between sites that can\ndefer to higher-priority interactive applications during times of resource congestion; (iii) with\nonly a few dozen data centers being connected, centralized control is feasible.\nGoogle’s B4 network uses custom-built switches, each implementing a slightly extended\nversion of OpenFlow, with a local Open Flow Agent (OFA) that is similar in spirit to the\ncontrol agent we encountered in Figure 5.2. Each OFA in turn connects to an Open Flow\nController (OFC) in the network control server (NCS), using a separate “out of band”\nnetwork, distinct from the network that carries data-center traffic between data centers. The\nOFC thus provides the services used by the NCS to communicate with its controlled\nswitches, similar in spirit to the lowest layer in the SDN architecture shown in Figure 5.15. In\nB4, the OFC also performs state management functions, keeping node and link status in a\nNetwork Information Base (NIB). Google’s implementation of the OFC is based on the ONIX\nSDN controller [Koponen 2010]. Two routing protocols, BGP (for routing between the data\ncenters) and IS-IS (a close relative of OSPF, for routing within a data center), are\nimplemented. Paxos [Chandra 2007] is used to execute hot replicas of NCS components to\nprotect against failure.\nA traffic engineering network-control application, sitting logically above the set of network\ncontrol servers, interacts with these servers to provide global, network-wide bandwidth\nprovisioning for groups of application flows. With B4, SDN made an important leap forward\ninto the operational networks of a global network provider. See [Jain 2013; Hong 2018] for a\ndetailed description of B4.\n5.5.3 Data and Control Plane Interaction: An\nIn order to solidify our understanding of the interaction between SDN-\ncontrolled switches and the SDN controller, let’s consider the example\nshown in Figure 5.16, in which Dijkstra’s algorithm (which we studied in\nSection 5.2) is used to determine shortest path routes. The SDN scenario in\nFigure 5.16 has two important differences from the earlier per-router-\ncontrol scenario of Sections 5.2.1 and 5.3, where ­Dijkstra’s algorithm was\nimplemented in each and every router and link-state updates were flooded\namong all network routers:\nFigure 5.16 ♦SDN controller scenario: Link-state change\nDijkstra’s algorithm is executed as a separate application, outside of the\npacket switches.\nPacket switches send link updates to the SDN controller and not to each\nIn this example, let’s assume that the link between switch s1 and s2\ngoes down; that shortest path routing is implemented, and consequently and\nthat incoming and outgoing flow forwarding rules at s1, s3, and s4 are\naffected, but that s2’s operation is unchanged. Let’s also assume that\nOpenFlow is used as the communication layer protocol, and that the control\nplane performs no other function other than link-state routing.\n1. Switch s1, experiencing a link failure between itself and s2, notifies the\nSDN controller of the link-state change using the OpenFlow port-status\n2. The SDN controller receives the OpenFlow message indicating the link-\nstate change, and notifies the link-state manager, which updates a link-\nstate ­database.\n3. The network-control application that implements Dijkstra’s link-state\nrouting has previously registered to be notified when link state changes.\nThat application receives the notification of the link-state change.\n4. The link-state routing application interacts with the link-state manager\nto get updated link state; it might also consult other components in the\nstate-­management layer. It then computes the new least-cost paths.\n5. The link-state routing application then interacts with the flow table\nmanager, which determines the flow tables to be updated.\n6. The flow table manager then uses the OpenFlow protocol to update flow\ntable entries at affected switches—s1 (which will now route packets\ndestined to s2 via s4), s2 (which will now begin receiving packets from\ns1 via intermediate switch s4), and s4 (which must now forward packets\nfrom s1 destined to s2).\nThis example is simple but illustrates how the SDN control plane provides\ncontrol-plane services (in this case, network-layer routing) that had been\npreviously implemented with per-router control exercised in each and every\nnetwork router. One can now easily appreciate how an SDN-enabled ISP\ncould easily switch from least-cost path routing to a more hand-tailored\napproach to routing. Indeed, since the controller can tailor the flow tables as\nit pleases, it can implement any form of forwarding that it pleases—simply\nby changing its application-control software. This ease of change should be\ncontrasted to the case of a traditional per-router control plane, where\nsoftware in all routers (which might be provided to the ISP by multiple\nindependent vendors) must be changed.\n5.5.4 SDN: Past and Future\nAlthough the intense interest in SDN is a relatively recent phenomenon, the\ntechnical roots of SDN, and the separation of the data and control planes in\nparticular, go back considerably further. In 2004, [Feamster 2004,\nLakshman 2004, RFC 3746] all argued for the separation of the network’s\ndata and control planes. [van der Merwe 1998] describes a control\nframework for ATM networks [Black 1995] with multiple controllers, each\ncontrolling a number of ATM switches. The Ethane project [Casado 2007]\npioneered the notion of a network of simple flow-based Ethernet switches\nwith  match-plus-action flow tables, a centralized controller that managed\nflow admission and routing, and the forwarding of unmatched packets from\nthe switch to the controller. A network of more than 300 Ethane switches\nwas operational in 2007. Ethane quickly evolved into the OpenFlow\nproject, and the rest (as the saying goes) is history!\nNumerous research efforts are aimed at developing future SDN\narchitectures and capabilities. As we have seen, the SDN revolution is\nleading to the disruptive replacement of dedicated monolithic switches and\nrouters (with both data and control planes) by simple commodity switching\nhardware and a sophisticated software control plane. A generalization of\nSDN known as network functions virtualization (NFV) (which we\ndiscussed earlier in Section 4.5) similarly aims at disruptive replacement of\nsophisticated middleboxes (such as middleboxes with dedicated hardware\nand proprietary software for media caching/service) with simple commodity\nservers, switching, and storage. A second area of important research seeks\nto extend SDN concepts from the intra-AS setting to the inter-AS setting\n[Gupta 2014].\nSDN CONTROLLER CASE STUDIES: THE OPENDAYLIGHT AND ONOS\nIn the earliest days of SDN, there was a single SDN protocol (OpenFlow [McKeown 2008;\nOpenFlow 2009]) and a single SDN controller (NOX [Gude 2008]). Since then, the number\nof SDN controllers in particular has grown significantly [Kreutz 2015]. Some SDN controllers\nare company-specific and proprietary, particularly when used to control internal proprietary\nnetworks (e.g., within or among a company’s data centers). But many more controllers are\nopen-source and implemented in a variety of programming languages [Erickson 2013]. Most\nrecently, the OpenDaylight controller [OpenDaylight 2020] and the ONOS controller [ONOS\n2020] have found considerable industry support. They are both open-source and are being\ndeveloped in partnership with the Linux Foundation.\nThe OpenDaylight Controller\nFigure 5.17 presents a simplified view of the OpenDaylight (ODL) controller platform\n[OpenDaylight 2020, Eckel 2017].\nFigure 5.17 ♦A simplified view of the OpenDaylight controller\nODL’s Basic Network Functions are at the heart of the controller, and correspond closely\nto the network-wide state management capabilities that we encountered in Figure 5.15. The\nService Abstraction Layer (SAL) is the controller’s nerve center, allowing controller\ncomponents and applications to invoke each other’s services, access configuration and\noperational data, and to subscribe to events they generate. The SAL also provides a\nuniform abstract interface to specific protocols operating between the ODL controller and\nthe controlled devices. These protocols include OpenFlow (which we covered in Section\n4.5), and the Simple Network Management Protocol (SNMP) and the Network Configuration\n(NETCONF) protocol, both of which we’ll cover in Section 5.7. The Open vSwitch Database\nManagement Protocol (OVSDB) is used to manage data center switching, an important\napplication area for SDN technology. We’ll introduce data center networking in Chapter 6. \nNetwork Orchestrations and Applications determine how data-plane forwarding and other\nservices, such as firewalling and load balancing, are accomplished in the controlled\ndevices. ODL provides two ways in which applications can interoperate with native\ncontroller services (and hence devices) and with each other. In the API-Driven (AD-SAL)\napproach, shown in Figure 5.17, applications communicate with controller modules using a\nREST request-response API running over HTTP. Initial releases of the OpenDaylight\ncontroller provided only the AD-SAL. As ODL became increasingly used for network\nconfiguration and management, later ODL releases introduced a Model-Driven (MD-SAL)\napproach. Here, the YANG data modeling language [RFC 6020] defines models of device,\nprotocol, and network configuration and operational state data. Devices are then configured\nand managed by manipulating this data using the NETCONF protocol.\nThe ONOS Controller\nFigure 5.18 presents a simplified view of the ONOS controller ONOS 2020]. Similar to the\ncanonical controller in Figure 5.15, three layers can be identified in the ONOS ­controller:\nFigure 5.18 ♦ONOS controller architecture\nNorthbound abstractions and protocols. A unique feature of ONOS is its intent\nframework, which allows an application to request a high-level service (e.g., to setup a\nconnection between host A and Host B, or conversely to not allow Host A and host B to\ncommunicate) without having to know the details of how this service is performed. State\ninformation is provided to network-control applications across the northbound API either\nsynchronously (via query) or asynchronously (via listener callbacks, e.g., when network\nstate changes).\nDistributed core. The state of the network’s links, hosts, and devices is maintained in\nONOS’s distributed core. ONOS is deployed as a service on a set of interconnected\nservers, with each server running an identical copy of the ONOS software; an increased\nnumber of servers offers an increased service capacity. The ONOS core provides the\nmechanisms for service replication and coordination among instances, providing the\napplications above and the network devices below with the abstraction of logically\ncentralized core services.\nSouthbound abstractions and protocols. The southbound abstractions mask the\nheterogeneity of the underlying hosts, links, switches, and protocols, allowing the\ndistributed core to be both device and protocol agnostic. Because of this abstraction, the\nsouthbound interface below the distributed core is logically higher than in our canonical\ncontroller in Figure 5.14 or the ODL controller in Figure 5.17.\n5.6 ICMP: The Internet Control Message Protocol\nThe Internet Control Message Protocol (ICMP), specified in [RFC 792], is\nused by hosts and routers to communicate network-layer information to\neach other. The most typical use of ICMP is for error reporting. For\nexample, when running an HTTP session, you may have encountered an\nerror message such as “Destination network unreachable.” This message\nhad its origins in ICMP. At some point, an IP router was unable to find a\npath to the host specified in your HTTP request. That router created and\nsent an ICMP message to your host indicating the error.\nICMP is often considered part of IP, but architecturally it lies just above\nIP, as ICMP messages are carried inside IP datagrams. That is, ICMP\nmessages are carried as IP payload, just as TCP or UDP segments are\ncarried as IP payload. Similarly, when a host receives an IP datagram with\nICMP specified as the upper-layer protocol (an upper-layer protocol number\nnot a process. Chapter 11 of [Stevens 1990] provides the source code for the\nping client program. Note that the client program needs to be able to\ninstruct the operating system to generate an ICMP message of type 8 code\nAnother interesting ICMP message is the source quench message. This\nmessage is seldom used in practice. Its original purpose was to perform\ncongestion control—to allow a congested router to send an ICMP source\nquench message to a host to force that host to reduce its transmission rate.\nWe have seen in Chapter 3 that TCP has its own congestion-control\nmechanism that operates at the transport layer, and that Explicit Congestion\nNotification bits can be used by network-later devices to signal congestion.\nIn Chapter 1, we introduced the Traceroute program, which allows us to\ntrace a route from a host to any other host in the world. Interestingly,\nTraceroute is implemented with ICMP messages. To determine the names\nand addresses of the routers between source and destination, Traceroute in\nthe source sends a series of ordinary IP datagrams to the destination. Each\nof these datagrams carries a UDP segment with an unlikely UDP port\nnumber. The first of these datagrams has a TTL of 1, the second of 2, the\nthird of 3, and so on. The source also starts timers for each of the\ndatagrams. When the nth datagram arrives at the nth router, the nth router\nobserves that the TTL of the datagram has just expired. According to the\nrules of the IP protocol, the router discards the datagram and sends an\nICMP warning message to the source (type 11 code 0). This warning\nmessage includes the name of the router and its IP address. When this\nICMP message arrives back at the source, the source obtains the round-trip\ntime from the timer and the name and IP address of the nth router from the\nICMP message.\nHow does a Traceroute source know when to stop sending UDP\nsegments? Recall that the source increments the TTL field for each\ndatagram it sends. Thus, one of the datagrams will eventually make it all the\nway to the destination host. Because this datagram contains a UDP segment\nwith an unlikely port number, the destination host sends a port unreachable\nICMP message (type 3 code 3) back to the source. When the source host\nreceives this particular ICMP message, it knows it does not need to send\nadditional probe packets. (The standard Traceroute program actually sends\nsets of three packets with the same TTL; thus, the Traceroute output\nprovides three results for each TTL.)\nIn this manner, the source host learns the number and the identities of\nrouters that lie between it and the destination host and the round-trip time\nbetween the two hosts. Note that the Traceroute client program must be able\nto instruct the operating system to generate UDP datagrams with specific\nTTL values and must also be able to be notified by its operating system\nwhen ICMP messages arrive. Now that you understand how Traceroute\nworks, you may want to go back and play with it some more.\nA new version of ICMP has been defined for IPv6 in RFC 4443. In\naddition to reorganizing the existing ICMP type and code definitions,\nICMPv6 also added new types and codes required by the new IPv6\nfunctionality. These include the “Packet Too Big” type and an\n“unrecognized IPv6 options” error code.\n5.7 Network Management and SNMP,\nNETCONF/YANG\nHaving now made our way to the end of our study of the network layer,\nwith only the link-layer before us, we’re well aware that a network consists\nof many complex, interacting pieces of hardware and software—from the\nlinks, switches, routers, hosts, and other devices that comprise the physical\ncomponents of the network to the many protocols that control and\ncoordinate these devices. When hundreds or thousands of such components\nare brought together by an organization to form a network, the job of the\nnetwork administrator to keep the network “up and running” is surely a\nchallenge. We saw in Section  5.5 that the logically centralized controller\ncan help with this process in an SDN context. But the challenge of network\nmanagement has been around long before SDN, with a rich set of network\nmanagement tools and approaches that help the network administrator\nmonitor, manage, and control the network. We’ll study these tools and\ntechniques in this section, as well as new tools and techniques that have co-\nevolved along with SDN.\nAn often-asked question is “What is network management?” A well-\nconceived, single-sentence (albeit a rather long run-on sentence) definition\nof network management from [Saydam 1996] is:\nNetwork management includes the deployment, integration, and\ncoordination of the hardware, software, and human elements to\nmonitor, test, poll, configure, analyze, evaluate, and control the network\nand element resources to meet the real-time, operational performance,\nand Quality of Service requirements at a reasonable cost.\nGiven this broad definition, we’ll cover only the rudiments of network\nmanagement in this section—the architecture, protocols, and data used by a\nnetwork administrator in performing their task. We’ll not cover the\nadministrator’s decision-making processes, where topics such as fault\nidentification [Labovitz 1997; Steinder 2002; Feamster 2005; Wu 2005;\nTeixeira 2006], anomaly detection [Lakhina 2005; Barford 2009], network\ndesign/engineering to meet contracted Service Level Agreements (SLA’s)\n[Huston 1999a], and more come into consideration. Our focus is thus\nagain, we see that ­security—a topic we’ll cover in detail in Chapter 8 — is\nof critical concern, but once again a concern whose importance had been\nrealized perhaps a bit late and only then “added on.”\nThe Management Information Base (MIB)\nWe learned earlier that a managed device’s operational state data (and to\nsome extent its configuration data) in the SNMP/MIB approach to network\nmanagement are represented as objects that are gathered together into an\nMIB for that device. An MIB object might be a counter, such as the number\nof IP datagrams discarded at a router due to errors in an IP datagram header;\nor the number of carrier sense errors in an Ethernet interface card;\ndescriptive information such as the version of the software running on a\nDNS server; status information such as whether a particular device is\nfunctioning correctly; or protocol-specific information such as a routing\npath to a destination. Related MIB objects are gathered into MIB modules.\nThere are over 400 MIB modules defined in various IETC RFC’s; there are\nmany more device- and vendor-specific MIBs. [RFC 4293] specifies the\nipSystemStatsInDelivers) for managing implementations of the Internet\nProtocol (IP) and its associated Internet Control Message Protocol (ICMP).\n[RFC 4022] specifies the MIB module for TCP, and [RFC 4113] specifies\nthe MIB module for UDP.\nWhile MIB-related RFCs make for rather tedious and dry reading, it is\nnonetheless instructive (i.e., like eating vegetables, it is “good for you”) to\nStatsInDelivers object-type definition from [RFC 4293] defines a\n32-bit read-only counter that keeps track of the number of IP datagrams that\nwere received at the managed device and were successfully delivered to an\nupper-layer protocol. In the example below, Counter32 is one of the basic\ndata types defined in the SMI.\nipSystemStatsInDelivers OBJECT-TYPE\n     SYNTAX Counter32\n     MAX-ACCESS read-only\n     STATUS current\n“The total number of datagrams\nsuccessfully delivered to IPuser-\nprotocols (including ICMP).\nWhen tracking interface statistics, the\ncounter of the interface to which these\ndatagrams were addressed is\nincremented. This interface might not\nbe the same as the input interface for\nsome of the datagrams.\nDiscontinuities in the value of this\ncounter can occur at re-initialization\nof the management system, and at other\ntimes as indicated by the value of\nipSystemStatsDiscontinuityTime.”\n::= { ipSystemStatsEntry 18 }\n5.7.3 The Network Configuration Protocol\n(NETCONF) and YANG\nThe NETCONF protocol operates between the managing server and the\nmanaged network devices, providing messaging to (i) retrieve, set, and\nmodify configuration data at managed devices; (ii) to query operational data\nand statistics at managed devices; and (iii) to subscribe to notifications\ngenerated by managed devices. The managing server actively controls a\nmanaged device by sending it configurations, which are specified in a\nstructured XML document, and activating a configuration at the managed\ndevice. NETCONF uses a remote procedure call (RPC) paradigm, where\nprotocol messages are also encoded in XML and exchanged between the\nmanaging server and a managed device over a secure, connection-oriented\nsession such as the TLS (Transport Layer Security) protocol (discussed in\nChapter 8) over TCP.\nFigure 5.22 shows an example NETCONF session. First, the managing\nserver establishes a secure connection to the managed device. (In\nNETCONF parlance, the managing server is actually referred to as the\n“client” and the managed device as the “server,” since the managing server\nestablishes the connection to the managed device. But we’ll ignore that here\nfor consistency with the longer-standing network-management server/client\nterminology shown in Figure 5.20.) Once a secure connection has been\nestablished, the managing server and the managed device exchange <hello>\nmessages, declaring their “capabilities”—NETCONF functionality that\nsupplements the base NETCONF specification in [RFC 6241]. Interactions\nbetween the managing server and managed device take the form of a remote\nprocedure call, using the <rpc> and <rpc-response> messages. These\nmessages are used to retrieve, set, query and modify device configurations,\noperational data and statistics, and to subscribe to device notifications.\nDevice notifications themselves are proactively sent from managed device\nto the managing server using NETCONF <notification> messages. A\nsession is closed with the <session-close message>.\nFigure 5.22 ♦NETCONF session between managing\nserver/controller and managed device\nTable 5.3 shows a number of the important NETCONF operations that\na managing server can perform at a managed device. As in the case of\nSNMP, we see operations for retrieving operational state data (<get>), and\nfor event notification. However, the <get-config>, <edit-config>, <lock>\nand <unlock> operation demon­strate NETCONF’s particular emphasis on\ndevice configuration. Using the basic operations shown in Table 5.3, it is\nalso possible to create a set of more sophisticated network management\ntransactions that either complete atomically (i.e., as a group) and\nsuccessfully on a set of devices, or are fully reversed and leave the devices\nin their pre-transaction state. Such multi-device transactions—“enabl[ing]\noperators to concentrate on the configuration of the network as a whole\nrather than individual devices” was an important operator requirement put\nforth in [RFC 3535].\nTable 5.3 ♦Selected NETCONF operations\nA full description of NETCONF is beyond our scope here; [RFC 6241,\nRFC 5277, Claise 2019; Schonwalder 2010] provide more in-depth\nBut since this is the first time we’ve seen protocol messages formatted\nas an XML document (rather than the traditional message with header fields\nand message body, e.g., as shown in Figure 5.21 for the SNMP PDU), let’s\nconclude our brief study of NETCONF with two examples. \nIn the first example, the XML document sent from the managing server\nto the managed device is a NETCONF <get> command requesting all\ndevice configuration and operational data. With this command, the server\ncan learn about the device’s configuration.\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc message-id=”101”\n03    xmlns=”urn:ietf:params:xml:ns:netconf:base\nAlthough few people can completely parse XML directly, we see that\nthe NETCONF command is relatively human-readable, and is much more\nreminiscent of HTTP and HTML than the protocol message formats that we\nsaw for SNMP PDU format in Figure 5.21. The RPC message itself spans\nlines 02–05 (we have added line numbers here for pedagogical purposes).\nThe RPC has a message ID value of 101, declared in line 02, and contains a\nsingle NETCONF <get> command. The reply from the device contains a\nmatching ID number (101), and all of the device’s configuration data (in\nXML format, of course), starting in line 04, ultimately with a closing </rpc-\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc-reply message-id=”101”\n03    xmlns=”urn:ietf:params:xml:ns:netconf:base\n04  <!-- . . . all configuration data\nreturned... -->\n</rpc-reply>\nIn the second example below, adapted from [RFC 6241], the XML\ndocument sent from the managing server to the managed device sets the\nMaximum Transmission Unit (MTU) of an interface named “Ethernet0/0”\nto 1500 bytes:\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc message-id=”101”\n  xmlns=”urn:ietf:params:xml:ns:netconf:base:1.0\n04   <edit-config>\n05     <target>\n06       <running/>\n07     </target>\n08     <config>\n09       <top\nxmlns=”http://example.com/schema/1.2/config”>\n10          <interface>\n11             <name>Ethernet0/0</name>\n12             <mtu>1500</mtu>\n13          </interface>\n14       </top>\n15     </config>\n16   </edit-config>\nThe RPC message itself spans lines 02–17, has a message ID value of\n101, and contains a single NETCONF <edit-config> command, spanning\nlines 04–15. Line 06 indicates that the running device configuration at the\nmanaged device will be changed. Lines 11 and 12 specify the MTU size to\nbe set of the Ethernet0/0 interface.\nOnce the managed device has changed the interface’s MTU size in the\nconfiguration, it responds back to the managing server with an OK reply\n(line 04 below), again within an XML document:\n01 <?xml version=”1.0” encoding=”UTF-8”?>\n02 <rpc-reply message-id=”101”\nxmlns=”urn:ietf:params:xml:ns:netconf:base:1.0”>\n05 </rpc-reply>\nYANG is the data modeling language used to precisely specify the\nstructure, syntax, and semantics of network management data used by\nNETCONF, in much the same way that the SMI is used to specify MIBs in\nSNMP. All YANG definitions are contained in modules, and an XML\ndocument describing a device and its capabilities can be generated from a\nYANG module.\nYANG features a small set of built-in data types (as in the case of SMI)\nand also allows data modelers to express constraints that must be satisfied\nby a valid NETCONF configuration—a powerful aid in helping ensure that\nNETCONF configurations satisfy specified correctness and consistency\nconstraints. YANG is also used to specify NETCONF notifications.\nA fuller discussion of YANG is beyond our scope here. For more\ninformation, we refer the interested reader to the excellent book [Claise\n5.8 Summary\nWe have now completed our two-chapter journey into the network core—a\njourney that began with our study of the network layer’s data plane in\nChapter 4 and finished here with our study of the network layer’s control\nplane. We learned that the control plane is the network-wide logic that\ncontrols not only how a datagram is forwarded among routers along an end-\nto-end path from the source host to the destination host, but also how\nnetwork-layer components and services are configured and managed.\nWe learned that there are two broad approaches towards building a\ncontrol plane: traditional per-router control (where a routing algorithm runs\nin each and every router and the routing component in the router\ncommunicates with the routing components in other routers) and software-\ndefined networking (SDN) control (where a logically centralized controller\ncomputes and distributes the forwarding tables to be used by each and every\nrouter). We studied two fundamental routing algorithms for computing least\ncost paths in a graph—link-state routing and distance-vector routing—in\nSection 5.2; these algorithms find application in both per-router control and\nin SDN control. These algorithms are the basis for two widely deployed\nInternet routing protocols, OSPF and BGP, that we covered in Sections 5.3\nand 5.4. We covered the SDN approach to the network-layer control plane\nin Section 5.5, investigating SDN network-control applications, the SDN\ncontroller, and the OpenFlow protocol for communicating between the\ncontroller and SDN-controlled devices. In Sections 5.6 and 5.7, we covered\nsome of the nuts and bolts of managing an IP network: ICMP (the Internet\nControl Message Protocol) and network management using SNMP and\nNETCONF/YANG.\nHaving completed our study of the network layer, our journey now\ntakes us one step further down the protocol stack, namely, to the link layer.\nLike the network layer, the link layer is part of each and every network-\nconnected device. But we will see in the next chapter that the link layer has\nthe much more localized task of moving packets between nodes on the\nsame link or LAN. Although this task may appear on the surface to be\nrather simple compared with that of the network layer’s tasks, we will see\nthat the link layer involves a number of important and fascinating issues\nthat can keep us busy for a long time.\nHomework Problems and Questions\nSECTION 5.1\nR1. What is meant by a control plane that is based on per-router control?\nIn such cases, when we say the network control and data planes are\nimplemented “monolithically,” what do we mean?\nR2. What is meant by a control plane that is based on logically\ncentralized control? In such cases, are the data plane and the control\nplane implemented within the same device or in separate devices?\nSECTION 5.2\nR3. Compare and contrast the properties of a centralized and a distributed\nrouting algorithm. Give an example of a routing protocol that takes a\ncentralized and a decentralized approach.\nR4. Compare and contrast static and dynamic routing algorithms.\nR5. What is the “count to infinity” problem in distance vector routing?\nR6. How is a least cost path calculated in a decentralized routing\nSECTIONS 5.3-5.4\nR7. Why are different inter-AS and intra-AS protocols used in the\nR8. True or false: When an OSPF route sends its link state information, it\nis sent only to those nodes directly attached neighbors. Explain.\nR9. What is meant by an area in an OSPF autonomous system? Why was\nthe concept of an area introduced?\nR10. Define and contrast the following terms: subnet, prefix, and BGP\nR11. How does BGP use the NEXT-HOP attribute? How does it use the\nAS-PATH attribute?\nR12. Describe how a network administrator of an upper-tier ISP can\nimplement policy when configuring BGP.\nR13. True or false: When a BGP router receives an advertised path from its\nneighbor, it must add its own identity to the received path and then\nsend that new path on to all of its neighbors. Explain.\nSECTION 5.5\nR14. Describe the main role of the communication layer, the network-wide\nstate-­management layer, and the network-control application layer in\nan SDN controller.\nR15. Suppose you wanted to implement a new routing protocol in the SDN\ncontrol plane. At which layer would you implement that protocol?\nR16. What types of messages flow across an SDN controller’s northbound\nand southbound APIs? Who is the recipient of these messages sent\nfrom the controller across the southbound interface, and who sends\nmessages to the controller across the northbound interface?\nR17. Describe the purpose of two types of OpenFlow messages (of your\nchoosing) that are sent from a controlled device to the controller.\nDescribe the purpose of two types of Openflow messages (of your\nchoosing) that are send from the controller to a controlled device.\nR18. What is the purpose of the service abstraction layer in the\nOpenDaylight SDN controller?\nSECTIONS 5.6-5.7\nR19. Names four different types of ICMP messages\nR20. What two types of ICMP messages are received at the sending host\nexecuting the Traceroute program?\nR21. Define the following terms in the context of SNMP: managing server,\nmanaged device, network management agent and MIB.\nR22. What are the purposes of the SNMP GetRequest and SetRequest\nR23. What is the purpose of the SNMP trap message?\nP1. Consider the figure below.\nEnumerate all paths from A to D that do not contain any loops\nP2. Repeat Problem P1 for paths from C to D, B to F, and C to F.\nP3. Consider the following network. With the indicated link costs, use\nDijkstra’s shortest-path algorithm to compute the shortest path from x\nto all network nodes. Show how the algorithm works by computing a\ntable similar to Table 5.1.\nDijkstra’s algorithm: discussion and example\nP4. Consider the network shown in Problem P3. Using Dijkstra’s\nalgorithm, and showing your work using a table similar to Table 5.1,\ndo the following:\na. Compute the shortest path from t to all network nodes.\nb. Compute the shortest path from u to all network nodes.\na. Compute the shortest path from v to all network nodes.\nd. Compute the shortest path from w to all network nodes.\ne. Compute the shortest path from y to all network nodes.\nf. Compute the shortest path from z to all network nodes.\nP5. Consider the network shown below. Assume that each node initially\nknows the costs to each of its neighbors. Consider the distance-\nvector ­algorithm and show the distance table entries at node z.\nP6. Consider a general topology (that is, not the specific network shown\nabove) and a synchronous version of the distance-vector algorithm.\nSuppose that at each iteration, a node exchanges its distance vectors\nwith its neighbors and receives their distance vectors. Assuming that\nthe algorithm begins with each node knowing only the costs to its\nimmediate neighbors, what is the maximum number of iterations\nrequired before the distributed algorithm converges? Justify your\nP7. Consider the network fragment shown below. x has only two\nattached ­neighbors, w and y. w has a minimum-cost path to\ndestination u ­(illustrated with the dotted line through the remaining\nnetwork) of 9, and y has a minimum-cost path to u of 11. The\ncomplete paths from w and y to u (and between w and y) are pictured\nwith dotted lines, as they are irrelevant to the solution.\na. Give x’s distance vector for destinations w, y, and u.\nb. Give a link-cost change for either c(x,w) or c(x,y) such that x will\ninform its neighbors of a new minimum-cost path to u as a result\nof executing the distance-vector algorithm.\nc. Give a link-cost change for either c(x,w) or c(x,y) such that x will\nnot inform its neighbors of a new minimum-cost path to u as a\nresult of executing the distance-vector algorithm.\nP8. Consider the three-node topology shown in Figure 5.6. Rather than\nhaving the link costs shown in Figure 5.6, the link costs are c(x,y) = 3,\nc(y,z) = 6, c(z,x) = 4. Compute the distance tables after the\ninitialization step and after each iteration of a synchronous version of\nthe distance-vector algorithm (as we did in our earlier discussion of\nFigure 5.6).\nP9. Can the poisoned reverse solve the general count-to-infinity problem?\nJustify your answer.\nP10. Argue that for the distance-vector algorithm in Figure 5.6, each value\nin the distance vector D(x) is non-increasing and will eventually\nstabilize in a finite number of steps.\nP11. Consider Figure 5.7. Suppose there is another router w, connected to\nrouter y and z. The costs of all links are given as follows: c(x,y) = 4,\nc(x,z) = 50, c(y,w) = 1, c(z,w) = 1, c(y,z) = 3. Suppose that poisoned\nreverse is used in the distance-vector routing algorithm.\na. When the distance vector routing is stabilized, router w, y, and z\ninform their distances to x to each other. What distance values do\nthey tell each other?\nb. Now suppose that the link cost between x and y increases to 60.\nWill there be a count-to-infinity problem even if poisoned\nreverse is used? Why or why not? If there is a count-to-infinity\nproblem, then how many iterations are needed for the distance-\nvector routing to reach a stable state again? Justify your answer.\nc. How do you modify c(y,z) such that there is no count-to-infinity\nproblem at all if c(y,x) changes from 4 to 60?\nP12. What is the message complexity of LS routing algorithm?\nP13. Will a BGP router always choose the loop-free route with the shortest\nASpath length? Justify your answer.\nP14. Consider the network shown below. Suppose AS3 and AS2 are\nrunning OSPF for their intra-AS routing protocol. Suppose AS1 and\nAS4 are running RIP for their intra-AS routing protocol. Suppose\neBGP and iBGP are used for the inter-AS routing protocol. Initially\nsuppose there is no physical link between AS2 and AS4.\na. Router 3c learns about prefix x from which routing protocol:\nOSPF, RIP, eBGP, or iBGP?\nb. Router 3a learns about x from which routing protocol?\nc. Router 1c learns about x from which routing protocol?\nd. Router 1d learns about x from which routing protocol?\nP15. Referring to the previous problem, once router 1d learns about x it\nwill put an entry (x, I) in its forwarding table.\na. Will I be equal to I  or I  for this entry? Explain why in one\nb. Now suppose that there is a physical link between AS2 and AS4,\nshown by the dotted line. Suppose router 1d learns that x is\naccessible via AS2 as well as via AS3. Will I be set to I  or I ?\nExplain why in one sentence.\nc. Now suppose there is another AS, called AS5, which lies on the\npath between AS2 and AS4 (not shown in diagram). Suppose\nrouter 1d learns that x is accessible via AS2 AS5 AS4 as well as\nvia AS3 AS4. Will I be set to I  or I ? Explain why in one\nP16. Consider the following network. ISP B provides national backbone\nservice to regional ISP A. ISP C provides national backbone service\nto regional ISP D. Each ISP consists of one AS. B and C peer with\neach other in two places using BGP. Consider traffic going from A to\nD. B would prefer to hand that traffic over to C on the West Coast (so\nthat C would have to absorb the cost of carrying the traffic cross-\ncountry), while C would prefer to get the traffic via its East Coast\npeering point with B (so that B would have carried the traffic across\nthe country). What BGP mechanism might C use, so that B would\nhand over A-to-D traffic at its East Coast peering point? To answer\nthis question, you will need to dig into the BGP ­specification.\nP17. In Figure 5.13, consider the path information that reaches stub\nnetworks W, X, and Y. Based on the information available at W and\nX, what are their respective views of the network topology? Justify\nyour answer. The topology view at Y is shown below.\nP18. Consider Figure 5.13. B would never forward traffic destined to Y via\nX based on BGP routing. But there are some very popular\napplications for which data packets go to X first and then flow to Y.\nIdentify one such application, and describe how data packets follow a\npath not given by BGP routing.\nP19. In Figure 5.13, suppose that there is another stub network V that is a\ncustomer of ISP A. Suppose that B and C have a peering relationship,\nand A is a customer of both B and C. Suppose that A would like to\nhave the traffic destined to W to come from B only, and the traffic\ndestined to V from either B or C. How should A advertise its routes to\nB and C? What AS routes does C receive?\nP20. Suppose ASs X and Z are not directly connected but instead are\nconnected by AS Y. Further suppose that X has a peering agreement\nwith Y, and that Y has a peering agreement with Z. Finally, suppose\nthat Z wants to transit all of Y’s traffic but does not want to transit X’s\ntraffic. Does BGP allow Z to ­implement this policy?\nP21. Consider the two ways in which communication occurs between a\nmanaging entity and a managed device: request-response mode and\ntrapping. What are the pros and cons of these two approaches, in\nterms of (1) overhead, (2) notification time when exceptional events\noccur, and (3) robustness with respect to lost messages between the\nmanaging entity and the device?\nP22. In Section 5.7, we saw that it was preferable to transport SNMP\nmessages in unreliable UDP datagrams. Why do you think the\ndesigners of SNMP chose UDP rather than TCP as the transport\nprotocol of choice for SNMP?\nSocket Programming Assignment 5: ICMP Ping\nAt the end of Chapter 2, there are four socket programming assignments.\nHere you  will find a fifth assignment which employs ICMP, a protocol\ndiscussed in this chapter.\nPing is a popular networking application used to test from a remote\nlocation whether a particular host is up and reachable. It is also often used\nto measure latency between the client host and the target host. It works by\nsending ICMP “echo request” packets (i.e., ping packets) to the target host\nand listening for ICMP “echo response” replies (i.e., pong packets). Ping\nmeasures the RRT, records packet loss, and calculates a statistical summary\nof multiple ping-pong exchanges (the minimum, mean, max, and standard\ndeviation of the round-trip times).\nIn this lab, you will write your own Ping application in Python. Your\napplication will use ICMP. But in order to keep your program simple, you\nwill not exactly follow the official specification in RFC 1739. Note that you\nwill only need to write the client side of the program, as the functionality\nneeded on the server side is built into almost all operating systems. You can\nfind full details of this assignment, as well as important snippets of the\nPython code, at the Web site http://www.pearsonglobaleditions.com.\nProgramming Assignment: Routing\nIn this programming assignment, you will be writing a “distributed” set of\nprocedures that implements a distributed asynchronous distance-vector\nrouting for the network shown below.\nYou are to write the following routines that will “execute”\nasynchronously within the emulated environment provided for this\nassignment. For node 0, you will write the routines:\nrtinit0(). This routine will be called once at the beginning of the\nemulation. rtinit0() has no arguments. It should initialize your distance\ntable in node 0 to reflect the direct costs of 1, 3, and 7 to nodes 1, 2, and\n3, respectively. In the figure above, all links are bidirectional and the\ncosts in both directions are identical. After initializing the distance table\nand any other data structures needed by your node 0 routines, it should\nthen send its directly connected neighbors (in this case, 1, 2, and 3) the\ncost of its minimum-cost paths to all other network nodes. This\nminimum-cost information is sent to neighboring nodes in a routing\nupdate packet by calling the routine tolayer2(), as described in the full\nassignment. The format of the routing update packet is also described in\nthe full assignment.\nrtupdate0(struct rtpkt *rcvdpkt). This routine will be called when node\n0 receives a routing packet that was sent to it by one of its directly\nconnected neighbors. The parameter *rcvdpkt is a pointer to the packet\nthat was received. rtupdate0() is the “heart” of the distance-vector\nalgorithm. The values it receives in a routing update packet from some\nother node i contain i’s current shortest-path costs to all other network\nnodes. rtupdate0() uses these received values to update its own distance\ntable (as specified by the distance-vector algorithm). If its own\nminimum cost to another node changes as a result of the update, node 0\ninforms its directly connected neighbors of this change in minimum\ncost by sending them a routing packet. Recall that in the distance-vector\nalgorithm, only directly connected nodes will exchange routing packets.\nThus, nodes 1 and 2 will communicate with each other, but nodes 1 and\n3 will not communicate with each other.\nSimilar routines are defined for nodes 1, 2, and 3. Thus, you will write eight\nprocedures in all: rtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(),\nrtupdate1(), rtupdate2(), and rtupdate3(). These routines will together\nimplement a distributed, asynchronous computation of the distance tables\nfor the topology and costs shown in the figure on the preceding page.\nYou can find the full details of the programming assignment, as well as\nC code that you will need to create the simulated hardware/software\nenvironment, at http://www.pearsonglobaleditions.com. A Java version of\nthe assignment is also available.\nWireshark Lab: ICMP\nIn the Web site for this textbook, www.pearsonglobaleditions.com, you’ll\nfind a Wireshark lab assignment that examines the use of the ICMP protocol\nin the ping and traceroute commands.\nAN INTERVIEW WITH…\nJennifer Rexford\nJennifer Rexford is a Professor in the Computer Science\ndepartment at Princeton University. Her research has the\nbroad goal of making computer networks easier to design and\nmanage, with particular emphasis on programmable neworks.\nFrom 1996–2004, she was a member of the Network\nManagement and Performance department at AT&T Labs–\nResearch. While at AT&T, she designed techniques and tools\nfor network measurement, traffic engineering, and router\nconfiguration that were deployed in AT&T’s backbone\nnetwork. Jennifer is co-author of the book “Web Protocols\nand Practice: Networking Protocols, Caching, and Traffic\nMeasurement,” published by Addison-Wesley in May 2001.\nShe served as the chair of ACM SIGCOMM from 2003 to\n2007. She received her BSE degree in electrical engineering\nfrom Princeton University in 1991, and her PhD degree in\nelectrical engineering and computer science from the\nUniversity of Michigan in 1996. Jennifer was the 2004 winner\nof ACM’s Grace Murray Hopper Award for outstanding young\ncomputer professional, the ACM Athena Lecturer Award\n(2016), the NCWIT Harrold and Notkin Research and\nGraduate Mentoring Award (2017), the ACM SIGCOMM\naward for lifetime contributions (2018), and the IEEE Internet\nAward (2019). She is an ACM Fellow (2008), an IEEE Fellow\n(2018), and the National Academy of Engineering (2014).\nCourtesy of Jennifer Rexford\nPlease describe one or two of the most\nexciting projects you have worked on\nduring your career. What were the biggest\nchallenges?\nWhen I was a researcher at AT&T, a group of us\ndesigned a new way to manage routing in Internet\nService Provider backbone networks. Traditionally,\nnetwork operators configure each router\nindividually, and these routers run distributed\nprotocols to compute paths through the network.\nWe believed that network management would be\nsimpler and more flexible if network operators\ncould exercise direct control over how routers\nforward traffic based on a network-wide view of\nthe topology and traffic. The Routing Control\nPlatform (RCP) we designed and built could\ncompute the routes for all of AT&T’s backbone on\na single commodity computer, and could control\nlegacy routers without modification. To me, this\nproject was exciting because we had a provocative\nidea, a working system, and ultimately a real\ndeployment in an operational network. Fast\nforward a few years, and software-defined\nnetworking (SDN) has become a mainstream\ntechnology, and standard protocols (like standard\nprotocols (like OpenFlow) and languages (like P4)\nhave made it much easier to tell the underlying\nswitches what to do.\nHow do you think software-defined\nnetworking should evolve in the future?\nIn a major break from the past, the software\ncontrolling network devices can be created by\nmany different programmers, not just at companies\nselling network equipment. Yet, unlike the\napplications running on a server or a smart phone,\nSDN applications must work together to handle the\nsame traffic. Network operators do not want to\nperform load balancing on some traffic and routing\non other traffic; instead, they want to perform load\nbalancing and routing, together, on the same\ntraffic. Future SDN platforms should offer good\nprogramming abstractions for composing\nindependently written multiple applications\ntogether. More broadly, good programming\nabstractions can make it easier to create\napplications, without having to worry about low-\nlevel details like flow table entries, traffic counters,\nbit patterns in packet headers, and so on. Also,\nwhile an SDN controller is logically centralized,\nthe network still consists of a distributed collection\nof devices. Future programmable networks should\noffer good abstractions for updating a distributed\nset of devices, so network administrators can\nreason about what happens to packets in flight\nwhile the devices are updated. Programming\nabstractions for programmable network is an\nexciting area for interdisciplinary\nresearch between computer networking, distributed\nsystems, and programming languages, with a real\nchance for practical impact in the years ahead.\nWhere do you see the future of networking\nand the Internet?\nNetworking is an exciting field because the\napplications and the underlying technologies\nchange all the time. We are always reinventing\nourselves! Who would have predicted even ten\nyears ago the dominance of smart phones, allowing\nmobile users to access existing applications as well\nas new location-based services? The emergence of\ncloud computing is fundamentally changing the\nrelationship between users and the applications\nthey run, and networked sensors and actuators (the\n“Internet of Things”) are enabling a wealth of new\napplications (and security vulnerabilities!). The\npace of innovation is truly inspiring.\nThe underlying network is a crucial\ncomponent in all of these innovations. Yet, the\nnetwork is notoriously “in the way”—limiting\nperformance, compromising reliability,\nconstraining applications, and complicating the\ndeployment and management of services. We\nshould strive to make the network of the future as\ninvisible as the air we breathe, so it never stands in\nthe way of new ideas and valuable services. To do\nthis, we need to raise the level of abstraction above\nindividual network devices and protocols (and\ntheir attendant acronyms!), so we can reason about\nthe network and the user’s high-level goals as a\nWhat people inspired you professionally?\nI’ve long been inspired by Sally Floyd who\nworked for many years at the International\nComputer Science Institute. Her research was\nalways purposeful, focusing on the important\nchallenges facing the Internet. She dug deeply into\nhard questions until she understood the problem\nand the space of solutions completely, and she\ndevoted serious energy into “making things\nhappen,” such as pushing her ideas into protocol\nstandards and network equipment. Also, she gave\nback to the community, through professional\nservice in numerous standards and research\norganizations and by creating tools (such as the\nwidely used ns-2 and ns-3 simulators) that enable\nother researchers to succeed. She retired in 2009,\nand passed away in 2019, but her influence on the\nfield will be felt for years to come.\nWhat are your recommendations for\nstudents who want careers in computer\nscience and networking?\nNetworking is an inherently interdisciplinary field.\nApplying techniques from other discipline’s\nbreakthroughs in networking come from such\ndiverse areas as queuing theory, game theory,\ncontrol theory, distributed systems, network\noptimization, programming languages, machine\nlearning, algorithms, data structures, and so on. I\nthink that becoming conversant in a related field,\nor collaborating closely with experts in those\nfields, is a wonderful way to put networking on a\nstronger foundation, so we can learn how to build\nnetworks that are worthy of society’s trust. Beyond\nthe theoretical disciplines, networking is exciting\nbecause we create real artifacts that real people\nuse. Mastering how to design and build systems—\nby gaining experience in operating systems,\ncomputer architecture, and so on—is another\nfantastic way to amplify your knowledge of\nnetworking to help make the world a better place.\nThe Link Layer and\nIn the previous two chapters, we learned that the network layer\nprovides a communication service between any two network hosts.\nBetween the two hosts, datagrams travel over a series of\ncommunication links, some wired and some wireless, starting at the\nsource host, passing through a series of packet switches (switches and\nrouters) and ending at the destination host. As we continue down the\nprotocol stack, from the network layer to the link layer, we naturally\nwonder how packets are sent across the individual links that make up\nthe end-to-end communication path. How are the network-layer\ndatagrams encapsulated in the link-layer frames for transmission over\na single link? Are different link-layer protocols used in the different\nlinks along the communication path? How are transmission conflicts in\nbroadcast links resolved? Is there addressing at the link layer and, if\nso, how does the link-layer addressing operate with the network-layer\naddressing we learned about in Chapter 4? And what exactly is the\ndifference between a switch and a router? We’ll answer these and other\nimportant questions in this chapter.\nIn discussing the link layer, we’ll see that there are two\nfundamentally ­different types of link-layer channels. The first type are\nbroadcast channels, which connect multiple hosts in wireless LANs, in\nsatellite networks, and in hybrid fiber-coaxial cable (HFC) access\nnetworks. Since many hosts are connected to the same broadcast\ncommunication channel, a so-called medium access protocol is needed\nto coordinate frame transmission. In some cases, a central controller\nmay be used to coordinate transmissions; in other cases, the hosts\nthemselves coordinate transmissions. The second type of link-layer\nchannel is the point-to-point communication link, such as that often\nfound between two routers connected by a long-distance link, or\nbetween a user’s office computer and the nearby Ethernet switch to\nwhich it is connected. Coordinating access to a point-to-point link is\nsimpler; the reference material on this book’s Web site has a detailed\ndiscussion of the Point-to-Point Protocol (PPP), which is used in\nsettings ranging from dial-up service over a telephone line to high-\nspeed point-to-point frame transport over fiber-optic links.\nWe’ll explore several important link-layer concepts and technologies in\nthis ­chapter. We’ll dive deeper into error detection and correction, a topic\nwe touched on briefly in Chapter 3. We’ll consider multiple access\nnetworks and switched LANs, including Ethernet—by far the most\nprevalent wired LAN technology. We’ll also look at virtual LANs, and data\ncenter networks. Although WiFi, and more generally wireless LANs, are\nlink-layer topics, we’ll postpone our study of these important topics until\n6.1 Introduction to the Link Layer\nLet’s begin with some important terminology. We’ll find it convenient in\nthis chapter to refer to any device that runs a link-layer (i.e., layer 2)\nprotocol as a node. Nodes include hosts, routers, switches, and WiFi access\npoints (discussed in Chapter 7). We will also refer to the communication\nchannels that connect adjacent nodes along the communication path as\nlinks. In order for a datagram to be transferred from source host to\ndestination host, it must be moved over each of the individual links in the\nend-to-end path. As an example, in the company network shown at the\nbottom of Figure 6.1, consider sending a datagram from one of the wireless\nhosts to one of the servers. This datagram will actually pass through six\nlinks: a WiFi link between sending host and WiFi access point, an Ethernet\nlink between the access point and a link-layer switch; a link between the\nlink-layer switch and the router, a link between the two routers; an Ethernet\nlink between the router and a link-layer switch; and finally an Ethernet link\nbetween the switch and the server. Over a given link, a transmitting node\nencapsulates the datagram in a link-layer frame and transmits the frame\ninto the link.\nFigure 6.1 ♦Six link-layer hops between wireless host and server\nIn order to gain further insight into the link layer and how it relates to\nthe ­network layer, let’s consider a transportation analogy. Consider a travel\nagent who is planning a trip for a tourist traveling from Princeton, New\nJersey, to Lausanne, Switzerland. The travel agent decides that it is most\nconvenient for the tourist to take a limousine from Princeton to JFK airport,\nthen a plane from JFK airport to Geneva’s airport, and finally a train from\nGeneva’s airport to Lausanne’s train station. Once the travel agent makes\nthe three reservations, it is the responsibility of the Princeton limousine\ncompany to get the tourist from Princeton to JFK; it is the responsibility of\nthe airline company to get the tourist from JFK to Geneva; and it is the\nresponsibility of the Swiss train service to get the tourist from Geneva to\nLausanne. Each of the three segments of the trip is “direct” between two\n“adjacent” locations. Note that the three transportation segments are\nmanaged by different companies and use entirely different transportation\nmodes (limousine, plane, and train). Although the transportation modes are\ndifferent, they each provide the basic service of moving passengers from\none location to an adjacent location. In this transportation analogy, the\ntourist is a datagram, each transportation segment is a link, the\ntransportation mode is a link-layer protocol, and the travel agent is a routing\n6.1.1 The Services Provided by the Link Layer\nAlthough the basic service of any link layer is to move a datagram from one\nnode to an adjacent node over a single communication link, the details of\nthe provided service can vary from one link-layer protocol to the next.\nPossible services that can be offered by a link-layer protocol include:\nFraming. Almost all link-layer protocols encapsulate each network-\nlayer datagram within a link-layer frame before transmission over the\nlink. A frame consists of a data field, in which the network-layer\ndatagram is inserted, and a number of header fields. The structure of the\nframe is specified by the link-layer protocol. We’ll see several different\nframe formats when we examine specific link-layer protocols in the\nsecond half of this chapter.\nLink access. A medium access control (MAC) protocol specifies the\nrules by which a frame is transmitted onto the link. For point-to-point\nlinks that have a single sender at one end of the link and a single\nreceiver at the other end of the link, the MAC protocol is simple (or\nnonexistent)—the sender can send a frame whenever the link is idle.\nThe more interesting case is when multiple nodes share a single\nbroadcast link—the so-called multiple access problem. Here, the MAC\nprotocol serves to coordinate the frame transmissions of the many\nReliable delivery. When a link-layer protocol provides reliable delivery\nservice, it guarantees to move each network-layer datagram across the\nlink without error. Recall that certain transport-layer protocols (such as\nTCP) also provide a reliable delivery service. Similar to a transport-\nlayer reliable delivery service, a link-layer reliable delivery service can\nwe’ll study in Chapter 7.\nFigure 6.2 ♦Network adapter: Its relationship to other host\ncomponents and to protocol stack functionality\nOn the sending side, the controller takes a datagram that has been\ncreated and stored in host memory by the higher layers of the protocol\nstack, encapsulates the datagram in a link-layer frame (filling in the frame’s\nvarious fields), and then transmits the frame into the communication link,\nfollowing the link-access protocol. On the receiving side, a controller\nreceives the entire frame, and extracts the network-layer datagram. If the\nlink layer performs error detection, then it is the sending controller that sets\nthe error-detection bits in the frame header and it is the receiving controller\nthat performs error detection.\nFigure 6.2 shows that while most of the link layer is implemented in\nhardware, part of the link layer is implemented in software that runs on the\nhost’s CPU. The software components of the link layer implement higher-\nlevel link-layer functionality such as assembling link-layer addressing\ninformation and activating the controller hardware. On the receiving side,\nlink-layer software responds to controller interrupts (for example, due to the\nreceipt of one or more frames), handling error conditions and passing a\ndatagram up to the network layer. Thus, the link layer is a combination of\nhardware and software—the place in the protocol stack where software\nmeets hardware. [Intel 2020] provides a readable overview (as well as a\ndetailed description) of the XL710 controller from a software-programming\npoint of view.\n6.2 Error-Detection and -Correction Techniques\nIn the previous section, we noted that bit-level error detection and\ncorrection—detecting and correcting the corruption of bits in a link-layer\nframe sent from one node to another physically connected neighboring node\n—are two services often ­provided by the link layer. We saw in Chapter 3\nthat error-detection and -correction services are also often offered at the\ntransport layer as well. In this section, we’ll examine a few of the simplest\ntechniques that can be used to detect and, in some cases, correct such bit\nerrors. A full treatment of the theory and implementation of this topic is\nitself the topic of many textbooks (e.g., [Schwartz 1980] or [Bertsekas\n1991]), and our treatment here is necessarily brief. Our goal here is to\ndevelop an intuitive feel for the capabilities that error-detection and -\ncorrection techniques provide and to see how a few simple techniques work\nand are used in practice in the link layer.\nFigure 6.3 illustrates the setting for our study. At the sending node,\ndata, D, to be protected against bit errors is augmented with error-detection\nand -correction bits (EDC). Typically, the data to be protected includes not\nonly the datagram passed down from the network layer for transmission\nacross the link, but also link-level addressing information, sequence\nnumbers, and other fields in the link frame header. Both D and EDC are\nsent to the receiving node in a link-level frame. At the receiving node, a\nsequence of bits, D' and EDC' is received. Note that D' and EDC' may differ\nfrom the original D and EDC as a result of in-transit bit flips.\nFigure 6.3 ♦Error-detection and -correction scenario\nThe receiver’s challenge is to determine whether or not D' is the same\nas the original D, given that it has only received D' and EDC'. The exact\nwording of the receiver’s decision in Figure 6.3 (we ask whether an error is\ndetected, not whether an error has occurred!) is important. Error-detection\nand -correction techniques allow the receiver to sometimes, but not always,\ndetect that bit errors have occurred. Even with the use of error-detection bits\nthere still may be undetected bit errors; that is, the receiver may be\nunaware that the received information contains bit errors. As a\nconsequence, the receiver might deliver a corrupted datagram to the\nlink-layer ARQ techniques similar to those we examined in Chapter 3. FEC\ntechniques are valuable because they can decrease the number of sender\nretransmissions required. Perhaps more important, they allow for immediate\ncorrection of errors at the receiver. This avoids having to wait for the round-\ntrip propagation delay needed for the sender to receive a NAK packet and\nfor the retransmitted packet to propagate back to the receiver—a potentially\nimportant advantage for real-time network applications [Rubenstein 1998]\nor links (such as deep-space links) with long propagation delays. Research\nexamining the use of FEC in error-control protocols includes [Biersack\n1992; Nonnenmacher 1998; Byers 1998; Shacham 1990].\n6.2.2 Checksumming Methods\nIn checksumming techniques, the d bits of data in Figure 6.4 are treated as a\nsequence of k-bit integers. One simple checksumming method is to simply\nsum these k-bit integers and use the resulting sum as the error-detection\nbits. The Internet checksum is based on this approach—bytes of data are\ntreated as 16-bit integers and summed. The 1s complement of this sum then\nforms the Internet checksum that is carried in the segment header. As\ndiscussed in Section 3.3, the receiver checks the checksum by taking the 1s\ncomplement of the sum of the received data (including the checksum) and\nchecking whether the result is all 0 bits. If any of the bits are 1, an error is\nindicated. RFC 1071 discusses the Internet checksum algorithm and its\nimplementation in detail. In the TCP and UDP protocols, the Internet\nchecksum is computed over all fields (header and data fields included). In\nIP, the checksum is computed over the IP header (since the UDP or TCP\nsegment has its own checksum). In other protocols, for example, XTP\n[Strayer 1992], one checksum is computed over the header and another\nchecksum is computed over the entire packet.\nChecksumming methods require relatively little packet overhead. For\nexample, the checksums in TCP and UDP use only 16 bits. However, they\nprovide relatively weak protection against errors as compared with cyclic\nredundancy check, which is discussed below and which is often used in the\nlink layer. A natural question at this point is, Why is checksumming used at\nthe transport layer and cyclic redundancy check used at the link layer?\nRecall that the transport layer is typically implemented in software in a host\nas part of the host’s operating system. Because transport-layer error\ndetection is implemented in software, it is important to have a simple and\nfast error-detection scheme such as checksumming. On the other hand, error\ndetection at the link layer is implemented in dedicated hardware in adapters,\nwhich can rapidly perform the more complex CRC operations. Feldmeier\n[Feldmeier 1995] presents fast software implementation techniques for not\nonly weighted checksum codes, but CRC (see below) and other codes as\n6.2.3 Cyclic Redundancy Check (CRC)\nAn error-detection technique used widely in today’s computer networks is\nbased on cyclic redundancy check (CRC) codes. CRC codes are also\nknown as polynomial codes, since it is possible to view the bit string to be\nsent as a polynomial whose coefficients are the 0 and 1 values in the bit\nstring, with operations on the bit string interpreted as polynomial\narithmetic.\nCRC codes operate as follows. Consider the d-bit piece of data, D, that\nthe sending node wants to send to the receiving node. The sender and\nreceiver must first agree on an r + 1 bit pattern, known as a generator,\nwhich we will denote as G. We will require that the most significant\n(leftmost) bit of G be a 1. The key idea behind CRC codes is shown in\nFigure 6.6. For a given piece of data, D, the sender will choose r additional\nbits, R, and append them to D such that the resulting d + r bit pattern\n(interpreted as a binary number) is exactly divisible by G (i.e., has no\nremainder) using modulo-2 arithmetic. The process of error checking with\nCRCs is thus simple: The receiver divides the d + r received bits by G. If\nthe remainder is nonzero, the receiver knows that an error has occurred;\notherwise the data is accepted as being correct.\nAll CRC calculations are done in modulo-2 arithmetic without carries\nin addition or borrows in subtraction. This means that addition and\nsubtraction are identical, and both are equivalent to the bitwise exclusive-or\n(XOR) of the operands. Thus, for example,\n1011 XOR 0101 = 1110\n1001 XOR 1101 = 0100\nAlso, we similarly have\n1011 − 0101 = 1110\n1001 − 1101 = 0100\nMultiplication and division are the same as in base-2 arithmetic, except that\nany required addition or subtraction is done without carries or borrows. As\nin regular binary arithmetic, multiplication by 2  left shifts a bit pattern by k\nplaces. Thus, given D and R, the quantity D · 2  XOR R yields the d + r bit\npattern shown in Figure 6.6. We’ll use this algebraic characterization of the\nd + r bit pattern from Figure 6.6 in our discussion below.\nFigure 6.6 ♦CRC\nLet us now turn to the crucial question of how the sender computes R.\nRecall that we want to find R such that there is an n such that\nD ⋅2r XOR R = nG\nThat is, we want to choose R such that G divides into D · 2  XOR R\nwithout remainder. If we XOR (that is, add modulo-2, without carry) R to\nboth sides of the above equation, we get\nD ⋅2r = nG XOR R\nThis equation tells us that if we divide D · 2  by G, the value of the\nremainder is precisely R. In other words, we can calculate R as\nR = remainder D⋅2r\nFigure 6.7 illustrates this calculation for the case of D = 101110, d = 6,\nG = 1001, and r = 3. The 9 bits transmitted in this case are 101 110 011.\nYou should check these calculations for yourself and also check that indeed\nD · 2  = 101011 · G XOR R.\nFigure 6.7 ♦A sample CRC calculation\nInternational standards have been defined for 8-, 12-, 16-, and 32-bit\ngenerators, G. The CRC-32 32-bit standard, which has been adopted in a\nnumber of link-level IEEE protocols, uses a generator of\nGCRC-32 = 100000100110000010001110110110111\nEach of the CRC standards can detect burst errors of fewer than r + 1\nbits. (This means that all consecutive bit errors of r bits or fewer will be\ndetected.) Furthermore, under appropriate assumptions, a burst of length\ngreater than r + 1 bits is detected with probability 1 − 0.5 . Also, each of the\nCRC standards can detect any odd number of bit errors. See [Williams\n1993] for a discussion of implementing CRC checks. The theory behind\nCRC codes and even more powerful codes is beyond the scope of this text.\nThe text [Schwartz 1980] provides an excellent introduction to this topic.\n6.3 Multiple Access Links and Protocols\nIn the introduction to this chapter, we noted that there are two types of\nnetwork links: point-to-point links and broadcast links. A point-to-point\nlink consists of a single sender at one end of the link and a single receiver\nat the other end of the link. Many link-layer protocols have been designed\nfor point-to-point links; the point-to-point protocol (PPP) and high-level\ndata link control (HDLC) are two such protocols. The second type of link, a\nbroadcast link, can have multiple sending and receiving nodes all\nconnected to the same, single, shared broadcast channel. The term\nbroadcast is used here because when any one node transmits a frame, the\nchannel broadcasts the frame and each of the other nodes receives a copy.\nEthernet and wireless LANs are examples of broadcast link-layer\ntechnologies. In this section, we’ll take a step back from specific link-layer\nprotocols and first examine a problem of central importance to the link\nlayer: how to coordinate the access of multiple sending and receiving nodes\nto a shared broadcast channel—the multiple access problem. Broadcast\nchannels are often used in LANs, networks that are geographically\nconcentrated in a single building (or on a corporate or university campus).\nThus, we’ll look at how multiple access channels are used in LANs at the\nend of this section.\nWe are all familiar with the notion of broadcasting—television has been\nusing it since its invention. But traditional television is a one-way broadcast\n(that is, one fixed node transmitting to many receiving nodes), while nodes\non a computer network broadcast channel can both send and receive.\nPerhaps a more apt human analogy for a broadcast channel is a cocktail\nparty, where many people gather in a large room (the air providing the\nbroadcast medium) to talk and listen. A second good analogy is something\nmany readers will be familiar with—a classroom—where teacher(s) and\nstudent(s) similarly share the same, single, broadcast medium. A central\nproblem in both scenarios is that of determining who gets to talk (that is,\ntransmit into the channel) and when. As humans, we’ve evolved an\nelaborate set of protocols for sharing the broadcast channel:\n“Give everyone a chance to speak.”\n“Don’t speak until you are spoken to.”\n“Don’t monopolize the conversation.”\n“Raise your hand if you have a question.”\n“Don’t interrupt when someone is speaking.”\n“Don’t fall asleep when someone is talking.”\nComputer networks similarly have protocols—so-called multiple\naccess ­protocols—by which nodes regulate their transmission into the\nshared broadcast channel. As shown in Figure 6.8, multiple access protocols\nare needed in a wide variety of network settings, including both wired and\nwireless access networks, and satellite networks. Although technically each\nnode accesses the broadcast channel through its adapter, in this section, we\nwill refer to the node as the sending and receiving device. In practice,\nhundreds or even thousands of nodes can directly communicate over a\nbroadcast channel.\nFigure 6.8 ♦Various multiple access channels\nBecause all nodes are capable of transmitting frames, more than two\nnodes can transmit frames at the same time. When this happens, all of the\nnodes receive multiple frames at the same time; that is, the transmitted\nframes collide at all of the receivers. Typically, when there is a collision,\nnone of the receiving nodes can make any sense of any of the frames that\nwere transmitted; in a sense, the signals of the colliding frames become\ninextricably tangled together. Thus, all the frames involved in the collision\nare lost, and the broadcast channel is wasted during the collision interval.\nClearly, if many nodes want to transmit frames frequently, many\ntransmissions will result in collisions, and much of the bandwidth of the\nbroadcast channel will be wasted.\nIn order to ensure that the broadcast channel performs useful work\nwhen multiple nodes are active, it is necessary to somehow coordinate the\ntransmissions of the active nodes. This coordination job is the responsibility\nof the multiple access protocol. Over the past 40 years, thousands of papers\nand hundreds of PhD dissertations have been written on multiple access\nprotocols; a comprehensive survey of the first 20 years of this body of work\nis [Rom 1990]. Furthermore, active research in multiple access protocols\ncontinues due to the continued emergence of new types of links, particularly\nnew wireless links.\nOver the years, dozens of multiple access protocols have been\nimplemented in a variety of link-layer technologies. Nevertheless, we can\nclassify just about any multiple access protocol as belonging to one of three\ncategories: channel partitioning protocols, random access protocols, and\ntaking-turns protocols. We’ll cover these categories of multiple access\nprotocols in the following three subsections.\nLet’s conclude this overview by noting that, ideally, a multiple access\nprotocol for a broadcast channel of rate R bits per second should have the\nfollowing desirable characteristics:\n1. When only one node has data to send, that node has a throughput of R\n2. When M nodes have data to send, each of these nodes has a throughput\nof R/M bps. This need not necessarily imply that each of the M nodes\nalways has an instantaneous rate of R/M, but rather that each node\nshould have an average transmission rate of R/M over some suitably\ndefined interval of time.\n3. The protocol is decentralized; that is, there is no master node that\nrepresents a single point of failure for the network.\n4. The protocol is simple, so that it is inexpensive to implement.\n6.3.1 Channel Partitioning Protocols\nRecall from our early discussion back in Section 1.3 that time-division ­-\nmultiplexing (TDM) and frequency-division multiplexing (FDM) are two\ntechniques that can be used to partition a broadcast channel’s bandwidth\namong all nodes sharing that channel. As an example, suppose the channel\nsupports N nodes and that the transmission rate of the channel is R bps.\nTDM divides time into time frames and further divides each time frame\ninto N time slots. (The TDM time frame should not be confused with the\nlink-layer unit of data exchanged between sending and receiving adapters,\nwhich is also called a frame. In order to reduce confusion, in this subsection\nwe’ll refer to the link-layer unit of data exchanged as a packet.) Each time\nslot is then assigned to one of the N nodes. Whenever a node has a packet to\nsend, it transmits the packet’s bits during its assigned time slot in the\nrevolving TDM frame. Typically, slot sizes are chosen so that a single\npacket can be transmitted during a slot time. Figure 6.9 shows a simple\nfour-node TDM example. Returning to our cocktail party analogy, a TDM-\nregulated cocktail party would allow one partygoer to speak for a fixed\nperiod of time, then allow another partygoer to speak for the same amount\nof time, and so on. Once everyone had had a chance to talk, the ­pattern\nwould repeat.\nFigure 6.9 ♦A four-node TDM and FDM example\nTDM is appealing because it eliminates collisions and is perfectly fair:\nEach node gets a dedicated transmission rate of R/N bps during each frame\ntime. However, it has two major drawbacks. First, a node is limited to an\naverage rate of R/N bps even when it is the only node with packets to send.\nA second drawback is that a node must always wait for its turn in the\ntransmission sequence—again, even when it is the only node with a frame\nto send. Imagine the partygoer who is the only one with anything to say\n(and imagine that this is the even rarer circumstance where everyone wants\nto hear what that one person has to say). Clearly, TDM would be a poor\nchoice for a multiple access protocol for this particular party.\nWhile TDM shares the broadcast channel in time, FDM divides the R\nbps channel into different frequencies (each with a bandwidth of R/N) and\nassigns each frequency to one of the N nodes. FDM thus creates N smaller\nchannels of R/N bps out of the single, larger R bps channel. FDM shares\nboth the advantages and drawbacks of TDM. It avoids collisions and\ndivides the bandwidth fairly among the N nodes. However, FDM also\nshares a principal disadvantage with TDM—a node is limited to a\nbandwidth of R/N, even when it is the only node with packets to send.\nA third channel partitioning protocol is code division multiple access\n(CDMA). While TDM and FDM assign time slots and frequencies,\nrespectively, to the nodes, CDMA assigns a different code to each node.\nEach node then uses its unique code to encode the data bits it sends. If the\ncodes are chosen carefully, CDMA networks have the wonderful property\nthat different nodes can transmit simultaneously and yet have their\nrespective receivers correctly receive a sender’s encoded data bits\n(assuming the receiver knows the sender’s code) in spite of interfering\ntransmissions by other nodes. CDMA has been used in military systems for\nsome time (due to its anti-jamming properties) and now has widespread\ncivilian use, particularly in cellular telephony. Because CDMA’s use is so\ntightly tied to wireless channels, we’ll save our discussion of the technical\ndetails of CDMA until Chapter 7. For now, it will suffice to know that\nCDMA codes, like time slots in TDM and frequencies in FDM, can be\nallocated to the multiple access channel users.\n6.3.2 Random Access Protocols\nThe second broad class of multiple access protocols are random access\nprotocols. In a random access protocol, a transmitting node always\ntransmits at the full rate of the channel, namely, R bps. When there is a\ncollision, each node involved in the collision repeatedly retransmits its\nframe (that is, packet) until its frame gets through without a collision. But\nwhen a node experiences a collision, it doesn’t necessarily retransmit the\nframe right away. Instead it waits a random delay before retransmitting the\nframe. Each node involved in a collision chooses independent random\ndelays. Because the random delays are independently chosen, it is possible\nthat one of the nodes will pick a delay that is sufficiently less than the\ndelays of the other colliding nodes and will therefore be able to sneak its\nframe into the channel without a collision.\nThere are dozens if not hundreds of random access protocols described\nin the literature [Rom 1990; Bertsekas 1991]. In this section we’ll describe\na few of the most commonly used random access protocols—the ALOHA\nprotocols [Abramson 1970; Abramson 1985; Abramson 2009] and the\ncarrier sense multiple access (CSMA) protocols [Kleinrock 1975b].\nEthernet [Metcalfe 1976] is a popular and widely deployed CSMA protocol.\nSlotted ALOHA\nLet’s begin our study of random access protocols with one of the simplest\nrandom access protocols, the slotted ALOHA protocol. In our description of\nslotted ALOHA, we assume the following:\nAll frames consist of exactly L bits.\nTime is divided into slots of size L/R seconds (that is, a slot equals the\ntime to transmit one frame).\nNodes start to transmit frames only at the beginnings of slots.\nThe nodes are synchronized so that each node knows when the slots\nIf two or more frames collide in a slot, then all the nodes detect the\ncollision event before the slot ends.\nLet p be a probability, that is, a number between 0 and 1. The operation of\nslotted ALOHA in each node is simple:\nWhen the node has a fresh frame to send, it waits until the beginning of\nthe next slot and transmits the entire frame in the slot.\nIf there isn’t a collision, the node has successfully transmitted its frame\nand thus need not consider retransmitting the frame. (The node can\nprepare a new frame for transmission, if it has one.)\nIf there is a collision, the node detects the collision before the end of the\nslot. The node retransmits its frame in each subsequent slot with\nprobability p until the frame is transmitted without a collision.\nBy retransmitting with probability p, we mean that the node effectively\ntosses a biased coin; the event heads corresponds to “retransmit,” which\noccurs with probability p. The event tails corresponds to “skip the slot and\ntoss the coin again in the next slot”; this occurs with probability (1 − p). All\nnodes involved in the collision toss their coins independently.\nSlotted ALOHA would appear to have many advantages. Unlike\nchannel partitioning, slotted ALOHA allows a node to transmit\ncontinuously at the full rate, R, when that node is the only active node. (A\nnode is said to be active if it has frames to send.) Slotted ALOHA is also\ndecentralized, \nindependently decides when to retransmit. (Slotted ALOHA does, however,\nrequire the slots to be synchronized in the nodes; shortly we’ll discuss an\nunslotted version of the ALOHA protocol, as well as CSMA protocols, none\nof which require such synchronization.) Slotted ALOHA is also an\nextremely simple protocol.\nSlotted ALOHA works well when there is only one active node, but\nhow ­efficient is it when there are multiple active nodes? There are two\npossible efficiency concerns here. First, as shown in Figure 6.10, when\nthere are multiple active nodes, a certain fraction of the slots will have\ncollisions and will therefore be “wasted.” The second concern is that\nanother fraction of the slots will be empty because all active nodes refrain\nfrom transmitting as a result of the probabilistic transmission policy. The\nonly “unwasted” slots will be those in which exactly one node transmits. A\nslot in which exactly one node transmits is said to be a successful slot. The\nefficiency of a slotted multiple access protocol is defined to be the long-run\nfraction of successful slots in the case when there are a large number of\nactive nodes, each always having a large number of frames to send. Note\nthat if no form of access control were used, and each node were to\nimmediately retransmit after each collision, the efficiency would be zero.\nSlotted ALOHA clearly increases the efficiency beyond zero, but by how\nFigure 6.10 ♦Nodes 1, 2, and 3 collide in the first slot. Node 2\nfinally succeeds in the fourth slot, node 1 in the eighth\nslot, and node 3 in the ninth slot\nWe now proceed to outline the derivation of the maximum efficiency of\nslotted ALOHA. To keep this derivation simple, let’s modify the protocol a\nlittle and assume that each node attempts to transmit a frame in each slot\nwith probability p. (That is, we assume that each node always has a frame\nto send and that the node transmits with probability p for a fresh frame as\nwell as for a frame that has already suffered a collision.) Suppose there are\nN nodes. Then the probability that a given slot is a successful slot is the\nprobability that one of the nodes transmits and that the remaining N − 1\nnodes do not transmit. The probability that a given node transmits is p; the\nprobability that the remaining nodes do not transmit is (1 − p)\n. Therefore,\nthe probability a given node has a success is p(1 − p)\n. Because there are\nN nodes, the probability that any one of the N nodes has a success is Np(1 −\nThus, when there are N active nodes, the efficiency of slotted ALOHA\nis Np(1 − p)\n. To obtain the maximum efficiency for N active nodes, we\nhave to find the p* that maximizes this expression. (See the homework\nproblems for a general outline of this derivation.) And to obtain the\nmaximum efficiency for a large number of active nodes, we take the limit of\nNp*(1 − p*)\n as N approaches infinity. (Again, see the homework\nproblems.) After performing these calculations, we’ll find that the\nmaximum efficiency of the protocol is given by 1/e = 0.37. That is, when a\nlarge number of nodes have many frames to transmit, then (at best) only 37\npercent of the slots do useful work. Thus, the effective transmission rate of\nthe channel is not R bps but only 0.37 R bps! A similar analysis also shows\nthat 37 percent of the slots go empty and 26 percent of slots have collisions.\nImagine the poor network administrator who has purchased a 100-Mbps\nslotted ALOHA system, expecting to be able to use the network to transmit\ndata among a large number of users at an aggregate rate of, say, 80 Mbps!\nAlthough the channel is capable of transmitting a given frame at the full\nchannel rate of 100 Mbps, in the long run, the successful throughput of this\nchannel will be less than 37 Mbps.\nThe slotted ALOHA protocol required that all nodes synchronize their\ntransmissions to start at the beginning of a slot. The first ALOHA protocol\n[Abramson 1970] was actually an unslotted, fully decentralized protocol. In\npure ALOHA, when a frame first arrives (that is, a network-layer datagram\nis passed down from the network layer at the sending node), the node\nimmediately transmits the frame in its entirety into the broadcast channel. If\na transmitted frame experiences a collision with one or more other\ntransmissions, the node will then immediately (after completely\ntransmitting its collided frame) retransmit the frame with probability p.\nOtherwise, the node waits for a frame transmission time. After this wait, it\nthen transmits the frame with probability p, or waits (remaining idle) for\nanother frame time with probability 1 – p.\nTo determine the maximum efficiency of pure ALOHA, we focus on an\nindividual node. We’ll make the same assumptions as in our slotted\nALOHA analysis and take the frame transmission time to be the unit of\ntime. At any given time, the probability that a node is transmitting a frame\nis p. Suppose this frame begins transmission at time t . As shown in Figure\n6.11, in order for this frame to be successfully transmitted, no other nodes\ncan begin their transmission in the interval of time [t  − 1, t ]. Such a\ntransmission would overlap with the beginning of the transmission of node\ni’s frame. The probability that all other nodes do not begin a transmission in\nthis interval is (1 − p)\n. Similarly, no other node can begin a transmission\nwhile node i is transmitting, as such a transmission would overlap with the\nlatter part of node i’s transmission. The probability that all other nodes do\nnot begin a transmission in this interval is also (1 − p)\n. Thus, the\nprobability that a given node has a successful transmission is p(1 − p)\nBy taking limits as in the slotted ALOHA case, we find that the maximum\nefficiency of the pure ALOHA protocol is only 1/(2e)—exactly half that of\nslotted ALOHA. This then is the price to be paid for a fully decentralized\nALOHA protocol.\nFigure 6.11 ♦Interfering transmissions in pure ALOHA\nCarrier Sense Multiple Access (CSMA)\nIn both slotted and pure ALOHA, a node’s decision to transmit is made\nindependently of the activity of the other nodes attached to the broadcast\nchannel. In particular, a node neither pays attention to whether another node\nhappens to be transmitting when it begins to transmit, nor stops transmitting\nif another node begins to interfere with its transmission. In our cocktail\nparty analogy, ALOHA protocols are quite like a boorish partygoer who\ncontinues to chatter away regardless of whether other people are talking. As\nhumans, we have human protocols that allow us not only to behave with\nmore civility, but also to decrease the amount of time spent “colliding” with\neach other in conversation and, consequently, to increase the amount of data\nwe exchange in our conversations. Specifically, there are two important\nrules for polite human conversation:\nListen before speaking. If someone else is speaking, wait until they are\nfinished. In the networking world, this is called carrier sensing—a\nnode listens to the channel before transmitting. If a frame from another\nnode is currently being transmitted into the channel, a node then waits\nuntil it detects no transmissions for a short amount of time and then\nbegins transmission.\nIf someone else begins talking at the same time, stop talking. In the\nnetworking world, this is called collision detection—a transmitting\nnode listens to the channel while it is transmitting. If it detects that\nanother node is transmitting an interfering frame, it stops transmitting\nand waits a random amount of time before repeating the sense-and-\ntransmit-when-idle cycle.\nThese two rules are embodied in the family of carrier sense multiple\naccess (CSMA) and CSMA with collision detection (CSMA/CD)\nprotocols [Kleinrock 1975b; Metcalfe 1976; Lam 1980; Rom 1990]. Many\nvariations on CSMA and CSMA/CD have been proposed. Here, we’ll\nconsider a few of the most important, and fundamental, characteristics of\nCSMA and CSMA/CD.\nNorm Abramson, a PhD engineer, had a passion for surfing and an interest in packet\nswitching. This combination of interests brought him to the University of Hawaii in\n1969. Hawaii consists of many mountainous islands, making it difficult to install and\noperate land-based networks. When not surfing, Abramson thought about how to\ndesign a network that does packet switching over radio. The network he designed had\none central host and several secondary nodes scattered over the Hawaiian Islands.\nThe network had two channels, each using a different frequency band. The downlink\nchannel broadcasted packets from the central host to the secondary hosts; and the\nupstream channel sent packets from the secondary hosts to the central host. In\naddition to sending informational packets, the central host also sent on the downstream\nsurprising, recalling from Chapter 4 that hosts and routers have network-\nlayer addresses as well. You might be asking, why in the world do we need\nto have addresses at both the network and link layers? In addition to\ndescribing the syntax and function of the link-layer addresses, in this\nsection we hope to shed some light on why the two layers of addresses are\nuseful and, in fact, indispensable. We’ll also cover the Address Resolution\nProtocol (ARP), which provides a mechanism to translate IP addresses to\nlink-layer addresses.\nMAC Addresses\nIn truth, it is not hosts and routers that have link-layer addresses but rather\ntheir adapters (that is, network interfaces) that have link-layer addresses. A\nhost or router with multiple network interfaces will thus have multiple link-\nlayer addresses associated with it, just as it would also have multiple IP\naddresses associated with it. It’s important to note, however, that link-layer\nswitches do not have link-layer addresses associated with their interfaces\nthat connect to hosts and routers. This is because the job of the link-layer\nswitch is to carry datagrams between hosts and routers; a switch does this\njob transparently, that is, without the host or router having to explicitly\naddress the frame to the intervening switch. This is illustrated in Figure\n6.16. A link-layer address is variously called a LAN address, a physical\naddress, or a MAC address. Because MAC address seems to be the most\npopular term, we’ll henceforth refer to link-layer addresses as MAC\naddresses. For most LANs (including Ethernet and 802.11 wireless LANs),\nthe MAC address is 6 bytes long, giving 2  possible MAC addresses. As\nshown in Figure 6.16, these 6-byte addresses are typically expressed in\nhexadecimal notation, with each byte of the address expressed as a pair of\nhexadecimal numbers. Although MAC addresses were designed to be\npermanent, it is now possible to change an adapter’s MAC address via\nsoftware. For the rest of this section, however, we’ll assume that an\nadapter’s MAC address is fixed.\nOne interesting property of MAC addresses is that no two adapters\nhave the same address. This might seem surprising given that adapters are\nmanufactured in many countries by many companies. How does a company\nmanufacturing adapters in Taiwan make sure that it is using different\naddresses from a company manufacturing adapters in Belgium? The answer\nis that the IEEE manages the MAC address space. In particular, when a\ncompany wants to manufacture adapters, it purchases a chunk of the\naddress space consisting of 2  addresses for a nominal fee. IEEE allocates\nthe chunk of 2  addresses by fixing the first 24 bits of a MAC address and\nletting the company create unique combinations of the last 24 bits for each\nAn adapter’s MAC address has a flat structure (as opposed to a\nhierarchical structure) and doesn’t change no matter where the adapter goes.\nA laptop with an Ethernet interface always has the same MAC address, no\nmatter where the computer goes. A smartphone with an 802.11 interface\nalways has the same MAC address, no matter where the smartphone goes.\nRecall that, in contrast, IP addresses have a hierarchical structure (that is, a\nnetwork part and a host part), and a host’s IP addresses needs to be changed\nwhen the host moves, i.e., changes the network to which it is attached. An\nadapter’s MAC address is analogous to a person’s social security number,\nwhich also has a flat addressing structure and which doesn’t change no\nmatter where the person goes. An IP address is analogous to a person’s\npostal address, which is hierarchical and which must be changed whenever\na person moves. Just as a person may find it useful to have both a postal\naddress and a social security number, it is useful for a host and router\ninterfaces to have both a network-layer address and a MAC address.\nFigure 6.16 ♦Each interface connected to a LAN has a unique\nMAC address\nWhen an adapter wants to send a frame to some destination adapter, the\nsending adapter inserts the destination adapter’s MAC address into the\nframe and then sends the frame into the LAN. As we will soon see, a switch\noccasionally broadcasts an incoming frame onto all of its interfaces. We’ll\nsee in Chapter 7 that 802.11 also broadcasts frames. Thus, an adapter may\nreceive a frame that isn’t addressed to it. Thus, when an adapter receives a\nframe, it will check to see whether the destination MAC address in the\nframe matches its own MAC address. If there is a match, the adapter\nextracts the enclosed datagram and passes the datagram up the protocol\nstack. If there isn’t a match, the adapter discards the frame, without passing\nthe network-layer datagram up. Thus, the destination only will be\ninterrupted when the frame is received.\nHowever, sometimes a sending adapter does want all the other adapters\non the LAN to receive and process the frame it is about to send. In this case,\nthe sending adapter inserts a special MAC broadcast address into the\ndestination address field of the frame. For LANs that use 6-byte addresses\n(such as Ethernet and 802.11), the broadcast address is a string of 48\nconsecutive 1s (that is, FF-FF-FF-FF-FF-FF in hexadecimal notation).\nAddress Resolution Protocol (ARP)\nBecause there are both network-layer addresses (for example, Internet IP\naddresses) and link-layer addresses (that is, MAC addresses), there is a need\nto translate between them. For the Internet, this is the job of the Address\nResolution Protocol (ARP) [RFC 826].\nTo understand the need for a protocol such as ARP, consider the\nnetwork shown in Figure 6.17. In this simple example, each host and router\nhas a single IP address and single MAC address. As usual, IP addresses are\nshown in dotted-decimal notation and MAC addresses are shown in\nhexadecimal notation. For the purposes of this discussion, we will assume\nin this section that the switch broadcasts all frames; that is, whenever a\nswitch receives a frame on one interface, it forwards the frame on all of its\nother interfaces. In the next section, we will provide a more accurate\nexplanation of how switches operate.\nFigure 6.17 ♦Each interface on a LAN has an IP address and a\nMAC address\nThere are several reasons why hosts and router interfaces have MAC addresses in ­addition\nto network-layer addresses. First, LANs are designed for arbitrary network-layer protocols,\nnot just for IP and the Internet. If adapters were assigned IP addresses rather than “neutral”\nMAC addresses, then adapters would not easily be able to support other network-layer\nprotocols (for example, IPX or DECnet). Second, if adapters were to use network-layer\naddresses instead of MAC addresses, the network-layer address would have to be stored in\nthe adapter RAM and reconfigured every time the adapter was moved (or powered up).\nAnother option is to not use any addresses in the adapters and have each adapter pass the\ndata (typically, an IP datagram) of each frame it receives up the protocol stack. The network\nlayer could then check for a matching network-layer address. One problem with this option\nis that the host would be interrupted by every frame sent on the LAN, including by frames\nthat were destined for other hosts on the same broadcast LAN. In summary, in order for the\nlayers to be largely independent building blocks in a network architecture, different layers\nneed to have their own addressing scheme. We have now seen three types of addresses:\nhost names for the application layer, IP addresses for the network layer, and MAC\naddresses for the link layer.\nNow suppose that the host with IP address 222.222.222.220 wants to\nsend an IP datagram to host 222.222.222.222. In this example, both the\nsource and destination are in the same subnet, in the addressing sense of\nSection 4.3.3. To send a datagram, the source must give its adapter not only\nthe IP datagram but also the MAC address for destination 222.222.222.222.\nThe sending adapter will then construct a link-layer frame containing the\ndestination’s MAC address and send the frame into the LAN.\nThe important question addressed in this section is, How does the\nsending host determine the MAC address for the destination host with IP\naddress 222.222.222.222? As you might have guessed, it uses ARP. An ARP\nmodule in the sending host takes any IP address on the same LAN as input,\nand returns the corresponding MAC address. In the example at hand,\nsending host 222.222.222.220 provides its ARP module the IP address\n222.222.222.222, and the ARP module returns the corresponding MAC\naddress 49-BD-D2-C7-56-2A.\nSo we see that ARP resolves an IP address to a MAC address. In many\nways it is analogous to DNS (studied in Section 2.5), which resolves host\nnames to IP addresses. However, one important difference between the two\nresolvers is that DNS resolves host names for hosts anywhere in the\nInternet, whereas ARP resolves IP addresses only for hosts and router\ninterfaces on the same subnet. If a node in California were to try to use ARP\nto resolve the IP address for a node in Mississippi, ARP would return with\nNow that we have explained what ARP does, let’s look at how it works.\nEach host and router has an ARP table in its memory, which contains\nmappings of IP addresses to MAC addresses. Figure 6.18 shows what an\nARP table in host 222.222.222.220 might look like. The ARP table also\ncontains a time-to-live (TTL) value, which indicates when each mapping\nwill be deleted from the table. Note that a table does not necessarily contain\nan entry for every host and router on the subnet; some may have never been\nentered into the table, and others may have expired. A typical expiration\ntime for an entry is 20 minutes from when an entry is placed in an ARP\nFigure 6.18 ♦A possible ARP table in 222.222.222.220\nNow suppose that host 222.222.222.220 wants to send a datagram that\nis IP-addressed to another host or router on that subnet. The sending host\nneeds to obtain the MAC address of the destination given the IP address.\nThis task is easy if the sender’s ARP table has an entry for the destination\nnode. But what if the ARP table doesn’t currently have an entry for the\ndestination? In particular, suppose 222.222.222.220 wants to send a\ndatagram to 222.222.222.222. In this case, the sender uses the ARP protocol\nto resolve the address. First, the sender constructs a special packet called an\nARP packet. An ARP packet has several fields, including the sending and\nreceiving IP and MAC addresses. Both ARP query and response packets\nhave the same format. The purpose of the ARP query packet is to query all\nthe other hosts and routers on the subnet to determine the MAC address\ncorresponding to the IP address that is being resolved.\nReturning to our example, 222.222.222.220 passes an ARP query\npacket to the adapter along with an indication that the adapter should send\nthe packet to the MAC broadcast address, namely, FF-FF-FF-FF-FF-FF.\nThe adapter encapsulates the ARP packet in a link-layer frame, uses the\nbroadcast address for the frame’s destination address, and transmits the\nframe into the subnet. Recalling our social security ­number/postal address\nanalogy, an ARP query is equivalent to a person shouting out in a crowded\nroom of cubicles in some company (say, AnyCorp): “What is the social\nsecurity number of the person whose postal address is Cubicle 13, Room\n112, AnyCorp, Palo Alto, California?” The frame containing the ARP query\nis received by all the other adapters on the subnet, and (because of the\nbroadcast address) each adapter passes the ARP packet within the frame up\nto its ARP module. Each of these ARP modules checks to see if its IP\naddress matches the destination IP address in the ARP packet. The one with\na match sends back to the querying host a response ARP packet with the\ndesired mapping. The querying host 222.222.222.220 can then update its\nARP table and send its IP datagram, encapsulated in a link-layer frame\nwhose destination MAC is that of the host or router responding to the\nearlier ARP query.\nThere are a couple of interesting things to note about the ARP protocol.\nFirst, the query ARP message is sent within a broadcast frame, whereas the\nresponse ARP message is sent within a standard frame. Before reading on\nyou should think about why this is so. Second, ARP is plug-and-play; that\nis, an ARP table gets built ­automatically—it doesn’t have to be configured\nby a system administrator. And if a host becomes disconnected from the\nsubnet, its entry is eventually deleted from the other ARP tables in the\nStudents often wonder if ARP is a link-layer protocol or a network-\nlayer protocol. As we’ve seen, an ARP packet is encapsulated within a link-\nlayer frame and thus lies architecturally above the link layer. However, an\nARP packet has fields containing link-layer addresses and thus is arguably a\nlink-layer protocol, but it also contains network-layer addresses and thus is\nalso arguably a network-layer protocol. In the end, ARP is probably best\nconsidered a protocol that straddles the boundary between the link and\nnetwork layers—not fitting neatly into the simple layered protocol stack we\nstudied in Chapter 1. Such are the complexities of real-world protocols!\nSending a Datagram off the Subnet\nIt should now be clear how ARP operates when a host wants to send a\ndatagram to another host on the same subnet. But now let’s look at the more\ncomplicated situation when a host on a subnet wants to send a network-\nlayer datagram to a host off the subnet (that is, across a router onto another\nsubnet). Let’s discuss this issue in the context of Figure 6.19, which shows\na simple network consisting of two subnets interconnected by a router.\nThere are several interesting things to note about Figure 6.19. Each host\nhas exactly one IP address and one adapter. But, as discussed in Chapter 4,\na router has an IP address for each of its interfaces. For each router interface\nthere is also an ARP module (in the router) and an adapter. Because the\nrouter in Figure 6.19 has two interfaces, it has two IP addresses, two ARP\nmodules, and two adapters. Of course, each adapter in the network has its\nown MAC address.\nFigure 6.19 ♦Two subnets interconnected by a router\nAlso note that Subnet 1 has the network address 111.111.111/24 and\nthat Subnet 2 has the network address 222.222.222/24. Thus, all of the\ninterfaces connected to Subnet 1 have addresses of the form\n111.111.111.xxx and all of the interfaces connected to Subnet 2 have\naddresses of the form 222.222.222.xxx.\nNow let’s examine how a host on Subnet 1 would send a datagram to a\nhost on Subnet 2. Specifically, suppose that host 111.111.111.111 wants to\nsend an IP datagram to a host 222.222.222.222. The sending host passes the\ndatagram to its adapter, as usual. But the sending host must also indicate to\nits adapter an appropriate destination MAC address. What MAC address\nshould the adapter use? One might be tempted to guess that the appropriate\nMAC address is that of the adapter for host 222.222.222.222, namely, 49-\nBD-D2-C7-56-2A. This guess, however, would be wrong! If the sending\nadapter were to use that MAC address, then none of the ­adapters on Subnet\n1 would bother to pass the IP datagram up to its network layer, since the\nframe’s destination address would not match the MAC address of any\nadapter on Subnet 1. The datagram would just die and go to datagram\nIf we look carefully at Figure 6.19, we see that in order for a datagram\nto go from 111.111.111.111 to a host on Subnet 2, the datagram must first be\nsent to the router interface 111.111.111.110, which is the IP address of the\nfirst-hop router on the path to the final destination. Thus, the appropriate\nMAC address for the frame is the address of the adapter for router interface\n111.111.111.110, namely, E6-E9-00-17-BB-4B. How does the sending host\nacquire the MAC address for 111.111.111.110? By using ARP, of course!\nOnce the sending adapter has this MAC address, it creates a frame\n(containing the datagram addressed to 222.222.222.222) and sends the\nframe into Subnet 1. The router adapter on Subnet 1 sees that the link-layer\nframe is addressed to it, and therefore passes the frame to the network layer\nof the router. Hooray—the IP datagram has successfully been moved from\nsource host to the router! But we are not finished. We still have to move the\ndatagram from the router to the destination. The router now has to\ndetermine the correct interface on which the datagram is to be forwarded.\nAs discussed in Chapter 4, this is done by consulting a forwarding table in\nthe router. The forwarding table tells the router that the datagram is to be\nforwarded via router interface 222.222.222.220. This interface then passes\nthe datagram to its adapter, which encapsulates the datagram in a new frame\nand sends the frame into Subnet 2. This time, the destination MAC address\nof the frame is indeed the MAC address of the ultimate destination. And\nhow does the router obtain this destination MAC address? From ARP, of\nARP for Ethernet is defined in RFC 826. A nice introduction to ARP is\ngiven in the TCP/IP tutorial, RFC 1180. We’ll explore ARP in more detail in\nthe homework problems.\n6.4.2 Ethernet\nEthernet has pretty much taken over the wired LAN market. In the 1980s\nand the early 1990s, Ethernet faced many challenges from other LAN\ntechnologies, ­including token ring, FDDI, and ATM. Some of these other\ntechnologies succeeded in capturing a part of the LAN market for a few\nyears. But since its invention in the mid-1970s, Ethernet has continued to\nevolve and grow and has held on to its dominant position. Today, Ethernet\nis by far the most prevalent wired LAN technology, and it is likely to\nremain so for the foreseeable future. One might say that Ethernet has been\nto local area networking what the Internet has been to global networking.\nThere are many reasons for Ethernet’s success. First, Ethernet was the\nfirst widely deployed high-speed LAN. Because it was deployed early,\nnetwork administrators became intimately familiar with Ethernet—its\nwonders and its quirks—and were reluctant to switch over to other LAN\ntechnologies when they came on the scene. Second, token ring, FDDI, and\nATM were more complex and expensive than Ethernet, which further\ndiscouraged network administrators from switching over. Third, the most\ncompelling reason to switch to another LAN technology (such as FDDI or\nATM) was usually the higher data rate of the new technology; however,\nEthernet always fought back, producing versions that operated at equal data\nrates or higher. Switched Ethernet was also introduced in the early 1990s,\nwhich further increased its effective data rates. Finally, because Ethernet\nhas been so popular, Ethernet hardware (in particular, adapters and\nswitches) has become a commodity and is remarkably cheap.\nThe original Ethernet LAN was invented in the mid-1970s by Bob\nMetcalfe and David Boggs. The original Ethernet LAN used a coaxial bus\nto interconnect the nodes. Bus topologies for Ethernet actually persisted\nthroughout the 1980s and into the mid-1990s. Ethernet with a bus topology\nis a broadcast LAN—all transmitted frames travel to and are processed by\nall adapters connected to the bus. Recall that we covered Ethernet’s\nCSMA/CD multiple access protocol with binary exponential backoff in\nSection 6.3.2.\nBy the late 1990s, most companies and universities had replaced their\nLANs with Ethernet installations using a hub-based star topology. In such\nan installation the hosts (and routers) are directly connected to a hub with\ntwisted-pair copper wire. A hub is a physical-layer device that acts on\nindividual bits rather than frames. When a bit, representing a zero or a one,\narrives from one interface, the hub simply re-creates the bit, boosts its\nenergy strength, and transmits the bit onto all the other interfaces. Thus,\nEthernet with a hub-based star topology is also a broadcast LAN—\nwhenever a hub receives a bit from one of its interfaces, it sends a copy out\non all of its other interfaces. In particular, if a hub receives frames from two\ndifferent interfaces at the same time, a collision occurs and the nodes that\ncreated the frames must retransmit.\nIn the early 2000s, Ethernet experienced yet another major evolutionary\nchange. Ethernet installations continued to use a star topology, but the hub\nat the center was replaced with a switch. We’ll be examining switched\nEthernet in depth later in this chapter. For now, we only mention that a\nswitch is not only “collision-less” but is also a bona-fide store-and-forward\npacket switch; but unlike routers, which operate up through layer 3, a\nswitch operates only up through layer 2.\nEthernet Frame Structure\nWe can learn a lot about Ethernet by examining the Ethernet frame, which\nis shown in Figure 6.20. To give this discussion about Ethernet frames a\ntangible context, let’s consider sending an IP datagram from one host to\nanother host, with both hosts on the same Ethernet LAN (for example, the\nEthernet LAN in Figure 6.17.) (Although the payload of our Ethernet frame\nis an IP datagram, we note that an Ethernet frame can carry other network-\nlayer packets as well.) Let the sending adapter, adapter A, have the MAC\naddress AA-AA-AA-AA-AA-AA and the receiving adapter, adapter B, have\nBB-BB-BB-BB-BB-BB. \nencapsulates the IP datagram within an Ethernet frame and passes the frame\nto the physical layer. The receiving adapter receives the frame from the\nphysical layer, extracts the IP datagram, and passes the IP datagram to the\nnetwork layer. In this context, let’s now examine the six fields of the\nEthernet frame, as shown in Figure 6.20.\nFigure 6.20 ♦Ethernet frame structure\nData field (46 to 1,500 bytes). This field carries the IP datagram. The\nmaximum transmission unit (MTU) of Ethernet is 1,500 bytes. This\nmeans that if the IP datagram exceeds 1,500 bytes, then the host has to\nfragment the datagram, as discussed in Section 4.3.2. The minimum size\nof the data field is 46 bytes. This means that if the IP datagram is less\nthan 46 bytes, the data field has to be “stuffed” to fill it out to 46 bytes.\nWhen stuffing is used, the data passed to the network layer contains the\nstuffing as well as an IP datagram. The network layer uses the length\nfield in the IP datagram header to remove the stuffing.\nDestination address (6 bytes). This field contains the MAC address of\nthe destination adapter, BB-BB-BB-BB-BB-BB. When adapter B\nreceives an Ethernet frame whose destination address is either BB-BB-\nat Host B see gaps as well? As we learned in Chapter 3, this depends on\nwhether the application is using UDP or TCP. If the application is using\nUDP, then the application in Host B will indeed see gaps in the data. On the\nother hand, if the application is using TCP, then TCP in Host B will not\nacknowledge the data contained in discarded frames, causing TCP in Host A\nto retransmit. Note that when TCP retransmits data, the data will eventually\nreturn to the Ethernet adapter at which it was discarded. Thus, in this sense,\nEthernet does retransmit data, although Ethernet is unaware of whether it is\ntransmitting a brand-new datagram with brand-new data, or a datagram that\ncontains data that has already been transmitted at least once.\nEthernet Technologies\nIn our discussion above, we’ve referred to Ethernet as if it were a single\nprotocol standard. But in fact, Ethernet comes in many different flavors,\nwith somewhat bewildering acronyms such as 10BASE-T, 10BASE-2,\n100BASE-T, 1000BASE-LX, 10GBASE-T and 40GBASE-T. These and\nmany other Ethernet technologies have been standardized over the years by\nthe IEEE 802.3 CSMA/CD (Ethernet) working group [IEEE 802.3 2020].\nWhile these acronyms may appear bewildering, there is actually\nconsiderable order here. The first part of the acronym refers to the speed of\nthe standard: 10, 100, 1000, or 10G, for 10 Megabit (per second), 100\nMegabit, Gigabit, 10 Gigabit and 40 Gigibit Ethernet, respectively. “BASE”\nrefers to baseband Ethernet, meaning that the physical media only carries\nEthernet traffic; almost all of the 802.3 standards are for baseband Ethernet.\nThe final part of the acronym refers to the physical media itself; Ethernet is\nboth a link-layer and a physical-layer specification and is carried over a\nvariety of physical media including coaxial cable, copper wire, and fiber.\nGenerally, a “T” refers to twisted-pair copper wires.\nHistorically, an Ethernet was initially conceived of as a segment of\ncoaxial cable. The early 10BASE-2 and 10BASE-5 standards specify 10\nMbps Ethernet over two types of coaxial cable, each limited in length to\n500 meters. Longer runs could be obtained by using a repeater—a\nphysical-layer device that receives a signal on the input side, and\nregenerates the signal on the output side. A coaxial cable corresponds nicely\nto our view of Ethernet as a broadcast medium—all frames transmitted by\none interface are received at other interfaces, and Ethernet’s CDMA/CD\nprotocol nicely solves the multiple access problem. Nodes simply attach to\nthe cable, and voila, we have a local area network!\nEthernet has passed through a series of evolutionary steps over the\nyears, and today’s Ethernet is very different from the original bus-topology\ndesigns using coaxial cable. In most installations today, nodes are\nconnected to a switch via point-to-point segments made of twisted-pair\ncopper wires or fiber-optic cables, as shown in Figures 6.15–6.17.\nIn the mid-1990s, Ethernet was standardized at 100 Mbps, 10 times\nfaster than 10 Mbps Ethernet. The original Ethernet MAC protocol and\nframe format were preserved, but higher-speed physical layers were defined\nfor copper wire (100BASE-T) and fiber (100BASE-FX, 100BASE-SX,\n100BASE-BX). Figure 6.21 shows these different standards and the\ncommon Ethernet MAC protocol and frame format. 100 Mbps Ethernet is\nlimited to a 100-meter distance over twisted pair, and to several kilometers\nover fiber, allowing Ethernet switches in different buildings to be\nFigure 6.21 ♦100 Mbps Ethernet standards: A common link layer,\ndifferent physical layers\nGigabit Ethernet is an extension to the highly successful 10 Mbps and\n100 Mbps Ethernet standards. Offering a raw data rate of 40,000 Mbps, 40\nGigabit Ethernet maintains full compatibility with the huge installed base of\nEthernet equipment. The standard for Gigabit Ethernet, referred to as IEEE\n802.3z, does the following:\nUses the standard Ethernet frame format (Figure 6.20) and is backward\ncompatible with 10BASE-T and 100BASE-T technologies. This allows\nfor easy integration of Gigabit Ethernet with the existing installed base\nof Ethernet equipment.\nAllows for point-to-point links as well as shared broadcast channels.\nPoint-to-point links use switches while broadcast channels use hubs, as\ndescribed earlier. In Gigabit Ethernet jargon, hubs are called buffered\ndistributors.\nUses CSMA/CD for shared broadcast channels. In order to have\nacceptable efficiency, the maximum distance between nodes must be\nseverely restricted.\nAllows for full-duplex operation at 40 Gbps in both directions for point-\nto-point channels.\nInitially operating over optical fiber, Gigabit Ethernet is now able to run\nover category 5 UTP cabling (for 1000BASE-T and 10GBASE-T).\nLet’s conclude our discussion of Ethernet technology by posing a\nquestion that may have begun troubling you. In the days of bus topologies\nand hub-based star topologies, Ethernet was clearly a broadcast link (as\ndefined in Section 6.3) in which frame collisions occurred when nodes\ntransmitted at the same time. To deal with these collisions, the Ethernet\nstandard included the CSMA/CD protocol, which is particularly effective\nfor a wired broadcast LAN spanning a small geographical region. But if the\nprevalent use of Ethernet today is a switch-based star topology, using store-\nand-forward packet switching, is there really a need anymore for an\nEthernet MAC protocol? As we’ll see shortly, a switch coordinates its\ntransmissions and never forwards more than one frame onto the same\ninterface at any time. Furthermore, modern switches are full-duplex, so that\na switch and a node can each send frames to each other at the same time\nwithout interference. In other words, in a switch-based Ethernet LAN there\nare no collisions and, therefore, there is no need for a MAC protocol!\nAs we’ve seen, today’s Ethernets are very different from the original\nEthernet conceived by Metcalfe and Boggs more than 40 years ago—speeds\nhave increased by three orders of magnitude, Ethernet frames are carried\nover a variety of media, switched-Ethernets have become dominant, and\nnow even the MAC protocol is often unnecessary! Is all of this really still\nEthernet? The answer, of course, is “yes, by definition.” It is interesting to\nnote, however, that through all of these changes, there has indeed been one\nenduring constant that has remained unchanged over 30 years—Ethernet’s\nframe format. Perhaps this then is the one true and timeless centerpiece of\nthe Ethernet standard.\n6.4.3 Link-Layer Switches\nUp until this point, we have been purposefully vague about what a switch\nactually does and how it works. The role of the switch is to receive\nincoming link-layer frames and forward them onto outgoing links; we’ll\nstudy this forwarding function in detail in this subsection. We’ll see that the\nswitch itself is transparent to the hosts and routers in the subnet; that is, a\nhost/router addresses a frame to another host/router (rather than addressing\nthe frame to the switch) and happily sends the frame into the LAN, unaware\nthat a switch will be receiving the frame and forwarding it. The rate at\nwhich frames arrive to any one of the switch’s output interfaces may\ntemporarily exceed the link capacity of that interface. To accommodate this\nproblem, switch output interfaces have buffers, in much the same way that\nrouter output interfaces have buffers for datagrams. Let’s now take a closer\nlook at how switches operate.\nForwarding and Filtering\nFiltering is the switch function that determines whether a frame should be\nforwarded to some interface or should just be dropped. Forwarding is the\nswitch function that determines the interfaces to which a frame should be\ndirected, and then moves the frame to those interfaces. Switch filtering and\nforwarding are done with a switch table. The switch table contains entries\nfor some, but not necessarily all, of the hosts and routers on a LAN. An\nentry in the switch table contains (1)  a  MAC address, (2) the switch\ninterface that leads toward that MAC address, and (3) the time at which the\nentry was placed in the table. An example switch table for the uppermost\nswitch in Figure 6.15 is shown in Figure 6.22. This description of frame\nforwarding may sound similar to our discussion of datagram forwarding in\nChapter 4. Indeed, in our discussion of generalized forwarding in Section\n4.4, we learned that many modern packet switches can be configured to\nforward on the basis of layer-2 destination MAC addresses (i.e., function as\na layer-2 switch) or layer-3 IP destination addresses (i.e., function as a\nlayer-3 router). Nonetheless, we’ll make the important distinction that\nswitches forward packets based on MAC addresses rather than on IP\naddresses. We will also see that a traditional (i.e., in a non-SDN context)\nswitch table is constructed in a very different manner from a router’s\nforwarding table.\nFigure 6.22 ♦Portion of a switch table for the uppermost switch in\nFigure 6.15\nTo understand how switch filtering and forwarding work, suppose a\nframe with destination address DD-DD-DD-DD-DD-DD arrives at the\nAs we learned in Chapter 4, routers are store-and-forward packet switches\nthat forward packets using network-layer addresses. Although a switch is\nalso a store-and-forward packet switch, it is fundamentally different from a\nrouter in that it forwards packets using MAC addresses. Whereas a router is\na layer-3 packet switch, a switch is a layer-2 packet switch. Recall,\nhowever, that we learned in Section 4.4 that modern switches using the\n“match plus action” operation can be used to forward a layer-2 frame based\non the frame's destination MAC address, as well as a layer-3 datagram\nusing the datagram's destination IP address. Indeed, we saw that switches\nusing the OpenFlow standard can perform generalized packet forwarding\nbased on any of eleven different frame, datagram, and transport-layer\nheader fields.\nEven though switches and routers are fundamentally different, network\nadministrators must often choose between them when installing an\ninterconnection device. For example, for the network in Figure 6.15, the\nnetwork administrator could just as easily have used a router instead of a\nswitch to connect the department LANs, servers, and internet gateway\nrouter. Indeed, a router would permit interdepartmental communication\nwithout creating collisions. Given that both switches and routers are\ncandidates for interconnection devices, what are the pros and cons of the\ntwo approaches?\nFirst consider the pros and cons of switches. As mentioned above,\nswitches are plug-and-play, a property that is cherished by all the\noverworked network administrators of the world. Switches can also have\nrelatively high filtering and forwarding rates—as shown in Figure 6.24,\nswitches have to process frames only up through layer 2, whereas routers\nhave to process datagrams up through layer 3. On the other hand, to prevent\nthe cycling of broadcast frames, the active topology of a switched network\nis restricted to a spanning tree. Also, a large switched network would\nrequire large ARP tables in the hosts and routers and would generate\nsubstantial ARP traffic and processing. Furthermore, switches are\nsusceptible to broadcast storms—if one host goes haywire and transmits an\nendless stream of Ethernet broadcast frames, the switches will forward all\nof these frames, causing the entire network to collapse.\nFigure 6.24 ♦Packet processing in switches, routers, and hosts\nNow consider the pros and cons of routers. Because network addressing\nis often hierarchical (and not flat, as is MAC addressing), packets do not\nnormally cycle through routers even when the network has redundant paths.\n(However, packets can cycle when router tables are misconfigured; but as\nwe learned in Chapter 4, IP uses a special datagram header field to limit the\ncycling.) Thus, packets are not restricted to a spanning tree and can use the\nbest path between source and destination. Because routers do not have the\nspanning tree restriction, they have allowed the Internet to be built with a\nrich topology that includes, for example, multiple active links between\nEurope and North America. Another feature of routers is that they provide\nfirewall protection against layer-2 broadcast storms. Perhaps the most\nsignificant drawback of routers, though, is that they are not plug-and-play—\nthey and the hosts that connect to them need their IP addresses to be\nconfigured. Also, routers often have a larger per-packet processing time\nthan switches, because they have to process up through the layer-3 fields.\nFinally, there are two different ways to pronounce the word router, either as\n“rootor” or as “rowter,” and people waste a lot of time arguing over the\nproper pronunciation [Perlman 1999].\nGiven that both switches and routers have their pros and cons (as\nsummarized in Table 6.1), when should an institutional network (for\nexample, a university campus network or a corporate campus network) use\nswitches, and when should it use routers? Typically, small networks\nconsisting of a few hundred hosts have a few LAN segments. Switches\nsuffice for these small networks, as they localize traffic and increase\naggregate throughput without requiring any configuration of IP addresses.\nBut larger networks consisting of thousands of hosts typically include\nrouters within the network (in addition to switches). The routers provide a\nmore robust isolation of traffic, control broadcast storms, and use more\n“intelligent” routes among the hosts in the network.\nTable 6.1 ♦Comparison of the typical features of popular\ninterconnection devices\nFor more discussion of the pros and cons of switched versus routed\nnetworks, as well as a discussion of how switched LAN technology can be\nextended to accommodate two orders of magnitude more hosts than today’s\nEthernets, see [Meyers 2004; Kim 2008].\n6.4.4 Virtual Local Area Networks (VLANs)\nIn our earlier discussion of Figure 6.15, we noted that modern institutional\nLANs are often configured hierarchically, with each workgroup\n(department) having its own switched LAN connected to the switched\nLANs of other groups via a switch hierarchy. While such a configuration\nworks well in an ideal world, the real world is often far from ideal. Three\ndrawbacks can be identified in the configuration in Figure 6.15:\nLack of traffic isolation. Although the hierarchy localizes group traffic\nto within a single switch, broadcast traffic (e.g., frames carrying ARP\nand DHCP messages or frames whose destination has not yet been\nlearned by a self-learning switch) must still traverse the entire\ninstitutional network. Limiting the scope of such broadcast traffic would\nimprove LAN performance. Perhaps more importantly, it also may be\ndesirable to limit LAN broadcast traffic for security/privacy reasons.\nFor example, if one group contains the company’s executive\nmanagement team and another group contains disgruntled employees\nrunning Wireshark packet sniffers, the network manager may well\nprefer that the executives’ traffic never even reaches employee hosts.\nThis type of isolation could be provided by replacing the center switch\nin Figure 6.15 with a router. We’ll see shortly that this isolation also can\nbe achieved via a switched (layer 2) solution.\nInefficient use of switches. If instead of three groups, the institution had\n10 groups, then 10 first-level switches would be required. If each group\nwere small, say less than 10 people, then a single 96-port switch would\nlikely be large enough to accommodate everyone, but this single switch\nwould not provide traffic isolation.\nManaging users. If an employee moves between groups, the physical\ncabling must be changed to connect the employee to a different switch\nin Figure 6.15. Employees belonging to two groups make the problem\neven harder.\nFortunately, each of these difficulties can be handled by a switch that\nsupports virtual local area networks (VLANs). As the name suggests, a\nswitch that supports VLANs allows multiple virtual local area networks to\nbe defined over a single physical local area network infrastructure. Hosts\nwithin a VLAN communicate with each other as if they (and no other hosts)\nwere connected to the switch. In a port-based VLAN, the switch’s ports\n(interfaces) are divided into groups by the network manager. Each group\nconstitutes a VLAN, with the ports in each VLAN forming a broadcast\ndomain (i.e., broadcast traffic from one port can only reach other ports in\nthe group). Figure 6.25 shows a single switch with 16 ports. Ports 2 to 8\nbelong to the EE VLAN, while ports 9 to 15 belong to the CS VLAN (ports\n1 and 16 are unassigned). This VLAN solves all of the difficulties noted\nabove—EE and CS VLAN frames are isolated from each other, the two\nswitches in Figure 6.15 have been replaced by a single switch, and if the\nuser at switch port 8 joins the CS Department, the network operator simply\nreconfigures the VLAN software so that port 8 is now associated with the\nCS VLAN. One can easily imagine how the VLAN switch is configured\nand operates—the network manager declares a port to belong to a given\nVLAN (with undeclared ports belonging to a default VLAN) using switch\nmanagement software, a table of port-to-VLAN mappings is maintained\nwithin the switch; and switch hardware only delivers frames between ports\nbelonging to the same VLAN.\nFigure 6.25 ♦A single switch with two configured VLANs\nBut by completely isolating the two VLANs, we have introduced a new\ndifficulty! How can traffic from the EE Department be sent to the CS\nDepartment? One way to handle this would be to connect a VLAN switch\nport (e.g., port 1 in Figure 6.25) to an external router and configure that port\nto belong both the EE and CS VLANs. In this case, even though the EE and\nCS departments share the same physical switch, the logical configuration\nwould look as if the EE and CS departments had separate switches\nconnected via a router. An IP datagram going from the EE to the CS\ndepartment would first cross the EE VLAN to reach the router and then be\nforwarded by the router back over the CS VLAN to the CS host.\nFortunately, switch vendors make such configurations easy for the network\nmanager by building a single device that contains both a VLAN switch and\na router, so a separate external router is not needed. A homework problem at\nthe end of the chapter explores this scenario in more detail.\nReturning again to Figure 6.15, let’s now suppose that rather than\nhaving a separate Computer Engineering department, some EE and CS\nfaculty are housed in a separate building, where (of course!) they need\nnetwork access, and (of course!) they’d like to be part of their department’s\nVLAN. Figure 6.26 shows a second 8-port switch, where the switch ports\nhave been defined as belonging to the EE or the CS VLAN, as needed. But\nhow should these two switches be interconnected? One easy solution would\nbe to define a port belonging to the CS VLAN on each switch (similarly for\nthe EE VLAN) and to connect these ports to each other, as shown in Figure\n6.26(a). This solution doesn’t scale, however, since N VLANS would\nrequire N ports on each switch simply to interconnect the two switches.\nFigure 6.26 ♦Connecting two VLAN switches with two VLANs: (a)\ntwo cables (b) trunked\nFigure 6.27 ♦Original Ethernet frame (top), 802.1Q-tagged\nEthernet VLAN frame (below)\nA more scalable approach to interconnecting VLAN switches is known\nas VLAN trunking. In the VLAN trunking approach shown in Figure\n6.26(b), a special port on each switch (port 16 on the left switch and port 1\non the right switch) is configured as a trunk port to interconnect the two\nVLAN switches. The trunk port belongs to all VLANs, and frames sent to\nany VLAN are forwarded over the trunk link to the other switch. But this\nraises yet another question: How does a switch know that a frame arriving\non a trunk port belongs to a particular VLAN? The IEEE has defined an\nextended Ethernet frame format, 802.1Q, for frames crossing a VLAN\ntrunk. As shown in Figure 6.27, the 802.1Q frame consists of the standard\nEthernet frame with a four-byte VLAN tag added into the header that\ncarries the identity of the VLAN to which the frame belongs. The VLAN\ntag is added into a frame by the switch at the sending side of a VLAN trunk,\nparsed, and removed by the switch at the receiving side of the trunk. The\nVLAN tag itself consists of a 2-byte Tag Protocol Identifier (TPID) field\n(with a fixed hexadecimal value of 81-00), a 2-byte Tag Control\nInformation field that contains a 12-bit VLAN identifier field, and a 3-bit\npriority field that is similar in intent to the IP datagram TOS field.\nIn this discussion, we’ve only briefly touched on VLANs and have\nfocused on port-based VLANs. We should also mention that VLANs can be\ndefined in several other ways. In MAC-based VLANs, the network manager\nspecifies the set of MAC addresses that belong to each VLAN; whenever a\ndevice attaches to a port, the port is connected into the appropriate VLAN\nbased on the MAC address of the device. VLANs can also be defined based\non network-layer protocols (e.g., IPv4, IPv6, or Appletalk) and other\ncriteria. It is also possible for VLANs to be extended across IP routers,\nallowing islands of LANs to be connected together to form a single VLAN\nthat could span the globe [Yu 2011]. See the 802.1Q standard [IEEE 802.1q\n2005] for more details.\n6.5 Link Virtualization: A Network as a Link Layer\nBecause this chapter concerns link-layer protocols, and given that we’re\nnow nearing the chapter’s end, let’s reflect on how our understanding of the\nterm link has evolved. We began this chapter by viewing the link as a\nphysical wire connecting two communicating hosts. In studying multiple\naccess protocols, we saw that multiple hosts could be connected by a shared\nwire and that the “wire” connecting the hosts could be radio spectra or other\nmedia. This led us to consider the link a bit more abstractly as a channel,\nrather than as a wire. In our study of Ethernet LANs (Figure 6.15), we saw\nthat the interconnecting media could actually be a rather complex switched\ninfrastructure. Throughout this evolution, however, the hosts themselves\nmaintained the view that the interconnecting medium was simply a link-\nlayer channel connecting two or more hosts. We saw, for example, that an\nEthernet host can be blissfully unaware of whether it is connected to other\nLAN hosts by a single short LAN segment (Figure 6.17) or by a\ngeographically dispersed switched LAN (Figure 6.15) or by a VLAN\n(Figure 6.26).\nIn the case of a dialup modem connection between two hosts, the link\nconnecting the two hosts is actually the telephone network—a logically\nseparate, global telecommunications network with its own switches, links,\nand protocol stacks for data transfer and signaling. From the Internet link-\nlayer point of view, however, the dial-up connection through the telephone\nnetwork is viewed as a simple “wire.” In this sense, the Internet virtualizes\nthe telephone network, viewing the telephone network as a link-layer\ntechnology providing link-layer connectivity between two Internet hosts.\nYou may recall from our discussion of overlay networks in Chapter 2 that\nan overlay network similarly views the Internet as a means for providing\nconnectivity between overlay nodes, seeking to overlay the Internet in the\nsame way that the Internet overlays the telephone network.\nIn this section, we’ll consider Multiprotocol Label Switching (MPLS)\nnetworks. Unlike the circuit-switched telephone network, MPLS is a\npacket-switched, virtual-circuit network in its own right. It has its own\npacket formats and forwarding behaviors. Thus, from a pedagogical\nviewpoint, a discussion of MPLS fits well into a study of either the network\nlayer or the link layer. From an Internet viewpoint, however, we can\nconsider MPLS, like the telephone network and switched-­Ethernets, as a\nlink-layer technology that serves to interconnect IP devices. Thus, we’ll\nconsider MPLS in our discussion of the link layer. Frame-relay and ATM\nnetworks can also be used to interconnect IP devices, though they represent\na slightly older (but still deployed) technology and will not be covered here;\nsee the very readable book [Goralski 1999] for details. Our treatment of\nMPLS will be necessarily brief, as entire books could be (and have been)\nwritten on these networks. We recommend [Davie 2000] for details on\nMPLS. We’ll focus here primarily on how MPLS ­servers interconnect to IP\ndevices, although we’ll dive a bit deeper into the underlying technologies as\n6.5.1 Multiprotocol Label Switching (MPLS)\nMultiprotocol Label Switching (MPLS) evolved from a number of industry\nefforts in the mid-to-late 1990s to improve the forwarding speed of IP\nrouters by adopting a key concept from the world of virtual-circuit\nnetworks: a fixed-length label. The goal was not to abandon the destination-\nbased IP datagram-forwarding infrastructure for one based on fixed-length\nlabels and virtual circuits, but to augment it by selectively labeling\ndatagrams and allowing routers to forward datagrams based on fixed-length\nlabels (rather than destination IP addresses) when possible. Importantly,\nthese techniques work hand-in-hand with IP, using IP addressing and\nrouting. The IETF unified these efforts in the MPLS protocol [RFC 3031,\nRFC 3032], effectively blending VC techniques into a routed datagram\nLet’s begin our study of MPLS by considering the format of a link-\nlayer frame that is handled by an MPLS-capable router. Figure 6.28 shows\nthat a link-layer frame transmitted between MPLS-capable devices has a\nsmall MPLS header added between the layer-2 (e.g., Ethernet) header and\nlayer-3 (i.e., IP) header. RFC 3032 defines the format of the MPLS header\nfor such links; headers are defined for ATM and frame-relayed networks as\nwell in other RFCs. Among the fields in the MPLS header are the label, 3\nbits reserved for experimental use, a single S bit, which is used to indicate\nthe end of a series of “stacked” MPLS headers (an advanced topic that we’ll\nnot cover here), and a time-to-live field.\nFigure 6.28 ♦MPLS header: Located between link- and network-\nlayer headers\nIt’s immediately evident from Figure 6.28 that an MPLS-enhanced\nframe can only be sent between routers that are both MPLS capable (since a\nnon-MPLS-capable router would be quite confused when it found an MPLS\nheader where it had expected to find the IP header!). An MPLS-capable\nrouter is often referred to as a label-switched router, since it forwards an\nMPLS frame by looking up the MPLS label in its forwarding table and then\nimmediately passing the datagram to the appropriate output interface. Thus,\nthe MPLS-capable router need not extract the destination IP address and\nperform a lookup of the longest prefix match in the forwarding table. But\nhow does a router know if its neighbor is indeed MPLS capable, and how\ndoes a router know what label to associate with the given IP destination? To\nanswer these questions, we’ll need to take a look at the interaction among a\ngroup of MPLS-capable routers.\nIn the example in Figure 6.29, routers R1 through R4 are MPLS\ncapable. R5 and R6 are standard IP routers. R1 has advertised to R2 and R3\nthat it (R1) can route to destination A, and that a received frame with MPLS\nlabel 6 will be forwarded to destination A. Router R3 has advertised to\nrouter R4 that it can route to destinations A and D, and that incoming\nframes with MPLS labels 10 and 12, respectively, will be switched toward\nthose destinations. Router R2 has also advertised to router R4 that it (R2)\ncan reach destination A, and that a received frame with MPLS label 8 will\nbe switched toward A. Note that router R4 is now in the interesting position\nof having two MPLS paths to reach A: via interface 0 with outbound MPLS\nlabel 10, and via interface 1 with an MPLS label of 8. The broad picture\npainted in Figure 6.29 is that IP devices R5, R6, A, and D are connected\ntogether via an MPLS infrastructure (MPLS-capable routers R1, R2, R3,\nand R4) in much the same way that a switched LAN or an ATM network\ncan connect together IP devices. And like a switched LAN or ATM network,\nthe MPLS-capable routers R1 through R4 do so without ever touching the\nIP header of a packet.\nFigure 6.29 ♦MPLS-enhanced forwarding\nIn our discussion above, we’ve not specified the specific protocol used\nto distribute labels among the MPLS-capable routers, as the details of this\nsignaling are well beyond the scope of this book. We note, however, that the\nIETF working group on MPLS has specified in [RFC 3468] that an\nextension of the RSVP protocol, known as RSVP-TE [RFC 3209], will be\nthe focus of its efforts for MPLS signaling. We’ve also not discussed how\nMPLS actually computes the paths for packets among MPLS capable\nrouters, nor how it gathers link-state information (e.g., amount of link\nbandwidth unreserved by MPLS) to use in these path computations.\nExisting link-state routing algorithms (e.g., OSPF) have been extended to\nflood this information to MPLS-capable routers. Interestingly, the actual\npath computation algorithms are not standardized, and are currently vendor-\nThus far, the emphasis of our discussion of MPLS has been on the fact\nthat MPLS performs switching based on labels, without needing to consider\nthe IP address of a packet. The true advantages of MPLS and the reason for\ncurrent interest in MPLS, however, lie not in the potential increases in\nswitching speeds, but rather in the new traffic management capabilities that\nMPLS enables. As noted above, R4 has two MPLS paths to A. If forwarding\nwere performed up at the IP layer on the basis of IP address, the IP routing\nprotocols we studied in Chapter 5 would specify only a single, least-cost\npath to A. Thus, MPLS provides the ability to forward packets along routes\nthat would not be possible using standard IP routing protocols. This is one\nsimple form of traffic engineering using MPLS [RFC 3346; RFC 3272;\nRFC 2702; Xiao 2000], in which a network operator can override normal IP\nrouting and force some of the traffic headed toward a given destination\nalong one path, and other traffic destined toward the same destination along\nanother path (whether for policy, performance, or some other reason).\nIt is also possible to use MPLS for many other purposes as well. It can\nbe used to perform fast restoration of MPLS forwarding paths, e.g., to\nreroute traffic over a precomputed failover path in response to link failure\n[Kar 2000; Huang 2002; RFC 3469]. Finally, we note that MPLS can, and\nhas, been used to implement so-called ­virtual private networks (VPNs). In\nimplementing a VPN for a customer, an ISP uses its MPLS-enabled\nnetwork to connect together the customer’s various networks. MPLS can be\nused to isolate both the resources and addressing used by the customer’s\nVPN from that of other users crossing the ISP’s network; see [DeClercq\n2002] for details.\nOur discussion of MPLS has been brief, and we encourage you to\nwe studied in Chapter 5, and that many of MPLS’ traffic engineering\ncapabilities can also be achieved via SDN and the generalized forwarding\nparadigm we studied in Chapter 4. Only the future will tell whether MPLS\nand SDN will continue to co-exist, or whether newer technologies (such as\nSDN) will eventually replace MPLS.\n6.6 Data Center Networking\nInternet companies such as Google, Microsoft, Amazon, and Alibaba have\nbuilt massive data centers, each housing tens to hundreds of thousands of\nhosts. As briefly discussed in the sidebar in Section 1.2, data centers are not\nonly connected to the Internet, but also internally include complex\ncomputer networks, called data center networks, which interconnect their\ninternal hosts. In this section, we provide a brief introduction to data center\nnetworking for cloud applications.\nBroadly speaking, data centers serve three purposes. First, they provide\ncontent such as Web pages, search results, e-mail, or streaming video to\nusers. Second, they serve as massively-parallel computing infrastructures\npresented in Chapter 4 are actually necessary.)\n6.7.2 Still Getting Started: DNS and ARP\nWhen Bob types the URL for www.google.com into his Web browser, he\nbegins the long chain of events that will eventually result in Google’s home\npage being displayed by his Web browser. Bob’s Web browser begins the\nprocess by creating a TCP socket (Section 2.7) that will be used to send the\nHTTP request (Section 2.2) to  www.google.com. In order to create the\nsocket, Bob’s laptop will need to know the IP address of www.google.com.\nWe learned in Section 2.5, that the DNS ­protocol is used to provide this\nname-to-IP-address translation service.\n8. The operating system on Bob’s laptop thus creates a DNS query\nmessage (Section 2.5.3), putting the string “www.google.com” in the\nquestion section of the DNS message. This DNS message is then placed\nwithin a UDP segment with a destination port of 53 (DNS server). The\nUDP segment is then placed within an IP datagram with an IP\ndestination address of 68.87.71.226 (the address of the DNS server\nreturned in the DHCP ACK in step 5) and a source IP address of\n68.85.2.101.\n9. Bob’s laptop then places the datagram containing the DNS query\nmessage in an Ethernet frame. This frame will be sent (addressed, at the\nlink layer) to the gateway router in Bob’s school’s network. However,\neven though Bob’s laptop knows the IP address of the school’s gateway\nrouter (68.85.2.1) via the DHCP ACK message in step 5 above, it\ndoesn’t know the gateway router’s MAC address. In order to obtain the\nMAC address of the gateway router, Bob’s ­laptop will need to use the\nARP protocol (Section 6.4.1).\n10. Bob’s laptop creates an ARP query message with a target IP address of\n68.85.2.1 (the default gateway), places the ARP message within an\nEthernet frame with a broadcast destination address\n(FF:FF:FF:FF:FF:FF) and sends the Ethernet frame to the switch, which\ndelivers the frame to all connected devices, including the gateway router.\n11. The gateway router receives the frame containing the ARP query\nmessage on the interface to the school network, and finds that the target\nIP address of 68.85.2.1 in the ARP message matches the IP address of its\ninterface. The gateway router thus prepares an ARP reply, indicating that\nits MAC address of 00:22:6B:45:1F:1B corresponds to IP address\n68.85.2.1. It places the ARP reply message in an Ethernet frame, with a\ndestination address of 00:16:D3:23:68:8A (Bob’s laptop) and sends the\nframe to the switch, which delivers the frame to Bob’s laptop.\n12. Bob’s laptop receives the frame containing the ARP reply message and\nextracts the MAC address of the gateway router (00:22:6B:45:1F:1B)\nfrom the ARP reply message.\n13. Bob’s laptop can now (finally!) address the Ethernet frame containing\nthe DNS query to the gateway router’s MAC address. Note that the IP\ndatagram in this frame has an IP destination address of 68.87.71.226 (the\nDNS server), while the frame has a destination address of\n00:22:6B:45:1F:1B (the gateway router). Bob’s laptop sends this frame to\nthe switch, which delivers the frame to the gateway router.\n6.7.3 Still Getting Started: Intra-Domain Routing to\nthe DNS Server\n14. The gateway router receives the frame and extracts the IP datagram\ncontaining the DNS query. The router looks up the destination address of\nthis datagram (68.87.71.226) and determines from its forwarding table\nthat the datagram should be sent to the leftmost router in the Comcast\nnetwork in Figure 6.32. The IP datagram is placed inside a link-layer\nframe appropriate for the link connecting the school’s router to the\nleftmost Comcast router and the frame is sent over this link.\n15. The leftmost router in the Comcast network receives the frame, extracts\nthe IP datagram, examines the datagram’s destination address\n(68.87.71.226) and determines the outgoing interface on which to\nforward the datagram toward the DNS server from its forwarding table,\nwhich has been filled in by ­Comcast’s intra-domain protocol (such as\nRIP, OSPF or IS-IS, Section 5.3) as well as the Internet’s inter-\ndomain protocol, BGP (Section 5.4).\n16. Eventually the IP datagram containing the DNS query arrives at the\nDNS server. The DNS server extracts the DNS query message, looks up\nthe name www.google.com in its DNS database (Section 2.5), and finds\nthe DNS resource record that contains the IP address (64.233.169.105)\nfor www.google.com. (assuming that it is currently cached in the DNS\nserver). Recall that this cached data originated in the authoritative DNS\nserver (Section 2.5) for google.com. The DNS server forms a DNS reply\nmessage containing this hostname-to-IP-address mapping, and places the\nDNS reply message in a UDP segment, and the segment within an IP\ndatagram addressed to Bob’s laptop (68.85.2.101). This datagram will be\nforwarded back through the Comcast network to the school’s router and\nfrom there, via the Ethernet switch to Bob’s laptop.\n17. Bob’s laptop extracts the IP address of the server www.google.com from\nthe DNS message. Finally, after a lot of work, Bob’s laptop is now ready\nto contact the www.google.com server!\n6.7.4 Web Client-Server Interaction: TCP and HTTP\n18. Now that Bob’s laptop has the IP address of www.google.com, it can\ncreate the TCP socket (Section 2.7) that will be used to send the HTTP\nGET message (Section 2.2.3) to www.google.com. When Bob creates the\nTCP socket, the TCP in Bob’s laptop must first perform a three-way\nhandshake (Section 3.5.6) with the TCP in www.google.com. Bob’s\nlaptop thus first creates a TCP SYN segment with destination port 80\n(for HTTP), places the TCP segment inside an IP datagram with a\ndestination IP address of 64.233.169.105 (www.google.com), places the\ndatagram inside a frame with a destination MAC address of\n00:22:6B:45:1F:1B (the gateway router) and sends the frame to the\n19. The routers in the school network, Comcast’s network, and Google’s\nnetwork forward the datagram containing the TCP SYN toward\nwww.google.com, using the forwarding table in each router, as in steps\n14–16 above. Recall that the router forwarding table entries governing\nforwarding of packets over the inter-domain link between the Comcast\nand Google networks are determined by the BGP protocol (Chapter 5).\n20. Eventually, the datagram containing the TCP SYN arrives at\nwww.google.com. The TCP SYN message is extracted from the datagram\nand demultiplexed to the welcome socket associated with port 80. A\nconnection socket (Section 2.7) is created for the TCP connection\nbetween the Google HTTP server and Bob’s laptop. A TCP SYNACK\n(Section 3.5.6) segment is generated, placed inside a datagram addressed\nto Bob’s laptop, and finally placed inside a link-layer frame appropriate\nfor the link connecting www.google.com to its first-hop router.\n21. The datagram containing the TCP SYNACK segment is forwarded\nthrough the Google, Comcast, and school networks, eventually arriving\nat the Ethernet controller in Bob’s laptop. The datagram is demultiplexed\nwithin the operating system to the TCP socket created in step 18, which\nenters the connected state.\n22. With the socket on Bob’s laptop now (finally!) ready to send bytes to\nwww.google.com, Bob’s browser creates the HTTP GET message\n(Section 2.2.3) containing the URL to be fetched. The HTTP GET\nmessage is then written into the socket, with the GET message becoming\nthe payload of a TCP segment. The TCP segment is placed in a datagram\nand sent and delivered to www.google.com as in steps 18–20 above.\n23. The HTTP server at www.google.com reads the HTTP GET message\nfrom the TCP socket, creates an HTTP response message (Section 2.2),\nplaces the requested Web page content in the body of the HTTP response\nmessage, and sends the message into the TCP socket.\n24. The datagram containing the HTTP reply message is forwarded through\nthe Google, Comcast, and school networks, and arrives at Bob’s laptop.\nBob’s Web browser program reads the HTTP response from the socket,\nextracts the html for the Web page from the body of the HTTP response,\nand finally (finally!) displays the Web page!\nOur scenario above has covered a lot of networking ground! If you’ve\nunderstood most or all of the above example, then you’ve also covered a lot\nof ground since you first read Section 1.1, where we wrote “much of this\nbook is concerned with computer network protocols” and you may have\nwondered what a protocol actually was! As detailed as the above example\nmight seem, we’ve omitted a number of possible additional protocols (e.g.,\nNAT running in the school’s gateway router, wireless access to the school’s\nnetwork, security protocols for accessing the school network or encrypting\nprotocols), \nconsiderations (Web caching, the DNS hierarchy) that one would encounter\nin the public ­Internet. We’ll cover a number of these topics and more in the\nsecond part of this book.\nLastly, we note that our example above was an integrated and holistic,\nbut also very “nuts and bolts,” view of many of the protocols that we’ve\nstudied in the first part of this book. The example focused more on the\n“how” than the “why.” For a broader, more reflective view on the design of\nnetwork protocols in general, you might want to re-read the “Architectural\nand in Chapter 1 (our discussion of physical media in Section 1.2). We’ll\nconsider the physical layer again when we study wireless link\ncharacteristics in the next chapter.\nAlthough our journey down the protocol stack is over, our study of\ncomputer networking is not yet at an end. In the following three chapters,\nwe cover wireless networking, network security, and multimedia\nnetworking. These four topics do not fit conveniently into any one layer;\nindeed, each topic crosscuts many layers. Understanding these topics (billed\nas advanced topics in some networking texts) thus requires a firm\nfoundation in all layers of the protocol stack—a foundation that our study\nof the link layer has now completed!\nHomework Problems and Questions\nChapter 6 Review Questions\nSECTION 6.1-6.2\nR1. What is framing in link layer?\nR2. If all the links in the Internet were to provide reliable delivery\nservice, would the TCP reliable delivery service be redundant? Why\nor why not?\nR3. Name three error-detection strategies employed by link layer.\nSECTION 6.3\nR4. Suppose two nodes start to transmit at the same time a packet of\nlength L over a broadcast channel of rate R. Denote the propagation\ndelay between the two nodes as d\n. Will there be a collision if d\nL/R? Why or why not?\nR5. In Section 6.3, we listed four desirable characteristics of a broadcast\nchannel. Which of these characteristics does slotted ALOHA have?\nWhich of these characteristics does token passing have?\nR6. In CSMA/CD, after the fifth collision, what is the probability that a\nnode chooses K = 4? The result K = 4 corresponds to a delay of how\nmany ­seconds on a 10 Mbps Ethernet?\nR7. While TDM and FDM assign time slots and frequencies, CDMA\nassigns a ­different code to each node. Explain the basic principle in\nwhich CDMA works.\nR8. Why does collision occur in CSMA, if all nodes perform carrier\nsensing before transmission?\nSECTION 6.4\nR9. How big is the MAC address space? The IPv4 address space? The\nIPv6 address space?\nR10. Suppose nodes A, B, and C each attach to the same broadcast LAN\n(through their adapters). If A sends thousands of IP datagrams to B\nwith each encapsulating frame addressed to the MAC address of B,\nwill C’s adapter process these frames? If so, will C’s adapter pass the\nIP datagrams in these frames to the network layer C? How would\nyour answers change if A sends frames with the MAC broadcast\nR11. IEEE manages the MAC address space, allocating chunks of it to\ncompanies manufacturing network adapters. The first half of the bits\nof the addresses in these chunks are fixed, ensuring that the address\nspace is unique. How long will a chunk last for a company\nmanufacturing 1,000,000 network adapters per year?\nR12. For the network in Figure 6.19, the router has two ARP modules,\neach with its own ARP table. Is it possible that the same MAC\naddress appears in both tables?\nR13. What is a hub used for?\nR14. Consider Figure 6.15. How many subnetworks are there, in the\naddressing sense of Section 4.3?\nR15. Each host and router has an ARP table in its memory. What are the\nRecall from Chapter 6 that when hosts communicate over a shared medium,\na protocol is needed so that the signals sent by multiple senders do not\ninterfere at the receivers. In Chapter 6, we described three classes of\nmedium access protocols: channel partitioning, random access, and taking\nturns. Code division multiple access (CDMA) belongs to the family of\nchannel partitioning protocols. It is prevalent in wireless LAN and cellular\ntechnologies. Because CDMA is so important in the wireless world, we’ll\ntake a quick look at CDMA now, before getting into specific wireless access\ntechnologies in the subsequent sections.\nIn a CDMA protocol, each bit being sent is encoded by multiplying the\nbit by a signal (the code) that changes at a much faster rate (known as the\nchipping rate) than the original sequence of data bits. Figure 7.5 shows a\nsimple, idealized CDMA encoding/decoding scenario. Suppose that the rate\nat which original data bits reach the CDMA encoder defines the unit of\ntime; that is, each original data bit to be transmitted requires a one-bit slot\ntime. Let d  be the value of the data bit for the ith bit slot. For mathematical\nconvenience, we represent a data bit with a 0 value as −1. Each bit slot is\nfurther subdivided into M mini-slots; in Figure 7.5, M = 8, although in\npractice M is much larger. The CDMA code used by the sender consists of a\nsequence of M values, c , m = 1, . . . , M, each taking a +1 or −1 value. In\nthe example in Figure 7.5, the M-bit CDMA code being used by the sender\nis (1, 1, 1, −1, 1, −1, −1, −1).\nFigure 7.5 ♦A simple CDMA example: Sender encoding, receiver\nTo illustrate how CDMA works, let us focus on the ith data bit, d . For\nthe mth mini-slot of the bit-transmission time of d , the output of the CDMA\nencoder, Z , is the value of d  multiplied by the mth bit in the assigned\nCDMA code, c :\nZi,m = di ⋅cm\nIn a simple world, with no interfering senders, the receiver would receive\nthe encoded bits, Z , and recover the original data bit, d , by computing:\nThe reader might want to work through the details of the example in Figure\n7.5 to see that the original data bits are indeed correctly recovered at the\nreceiver using Equation 7.2.\nThe world is far from ideal, however, and as noted above, CDMA must\nwork in the presence of interfering senders that are encoding and\ntransmitting their data using a different assigned code. But how can a\nCDMA receiver recover a sender’s original data bits when those data bits\nare being tangled with bits being transmitted by other senders? CDMA\nworks under the assumption that the interfering transmitted bit signals are\nadditive. This means, for example, that if three senders send a 1 value, and a\nfourth sender sends a −1 value during the same mini-slot, then the received\nsignal at all receivers during that mini-slot is a 2 (since 1 + 1 + 1 − 1 = 2).\nIn the presence of multiple senders, sender s computes its encoded\ntransmissions, Z s\ni,m, in exactly the same manner as in Equation 7.1. The\nvalue received at a receiver during the\nmth mini-slot of the ith bit slot, however, is now the sum of the transmitted\nbits from all N senders during that mini-slot:\nAmazingly, if the senders’ codes are chosen carefully, each receiver can\nrecover the data sent by a given sender out of the aggregate signal simply\nby using the sender’s code in exactly the same manner as in Equation 7.2:\nas shown in Figure 7.6, for a two-sender CDMA example. The M-bit\nCDMA code being used by the upper sender is (1, 1, 1, −1, 1, −1, −1, −1),\nwhile the CDMA code being used by the lower sender is (1, −1, 1, 1, 1, −1,\n1, 1). Figure 7.6 illustrates a receiver recovering the original data bits from\nthe upper sender. Note that the receiver is able to extract the data from\nsender 1 in spite of the interfering transmission from sender 2.\nFigure 7.6 ♦A two-sender CDMA example\nRecall our cocktail analogy from Chapter 6. A CDMA protocol is\nsimilar to having partygoers speaking in multiple languages; in such\ncircumstances humans are actually quite good at locking into the\nconversation in the language they understand, while filtering out the\nremaining conversations. We see here that CDMA is a partitioning protocol\nin that it partitions the codespace (as opposed to time or frequency) and\nassigns each node a dedicated piece of the codespace.\nOur discussion here of CDMA is necessarily brief; in practice a number\nof difficult issues must be addressed. First, in order for the CDMA receivers\nto be able to extract a particular sender’s signal, the CDMA codes must be\ncarefully chosen. ­Second, our discussion has assumed that the received\nsignal strengths from various senders are the same; in reality, this can be\ndifficult to achieve. There is a considerable body of literature addressing\nthese and other issues related to CDMA; see ­[Pickholtz 1982; Viterbi 1995]\nfor details.\n7.3 WiFi: 802.11 Wireless LANs\nPervasive in the workplace, the home, educational institutions, cafés,\nairports, and street corners, wireless LANs are now one of the most\nimportant access network technologies in the Internet today. Although many\ntechnologies and standards for wireless LANs were developed in the 1990s,\none particular class of standards has clearly emerged as the winner: the\nIEEE 802.11 wireless LAN, also known as WiFi. In this section, we’ll take\na close look at 802.11 wireless LANs, examining its frame structure, its\nmedium access protocol, and its internetworking of 802.11 LANs with\nwired Ethernet LANs.\nAs summarized in Table 7.1, there are several 802.11 standards [IEEE\n802.11 2020]. The 802.11 b, g, n, ac, ax are successive generations of\n802.11 technology aimed for wireless local area networks (WLANs),\ntypically less than 70 m range in a home office, workplace, or business\nsetting. The 802.11 n, ac, and ax standards have recently been branded as\nWiFi 4, 5 and 6, respectively—no doubt competing with 4G and 5G cellular\nnetwork branding. The 802.11 af, ah standards operate over longer distances\nand are aimed at Internet of Things, sensor networks, and metering\napplications.\nThe different 802.11 b, g, n, ac, ax standards all share some common\ncharacteristics, including the 802.11 frame format that we will study shortly,\nand are backward compatible, meaning, for example, that a mobile capable\nonly of 802.11 g may still interact with a newer 802.11 ac or 802.11 ax base\nstation. They also all use the same medium access protocol, CSMA/CA,\nwhich we’ll also discuss shortly, while also 802.11 ax also supports\ncentralized scheduling by the base station of transmissions from associated\nwireless devices.\nHowever, as shown in Table 7.1, the standards have some major\ndifferences at the physical layer. 802.11 devices operate in two different\nfrequency ranges: 2.4–2.485 GHz (referred to as the 2.4 GHz range) and\n5.1–5.8 GHz (referred to as the 5 GHz range). The 2.4 GHz range is an\nunlicensed frequency band, where 802.11 devices may compete for\nfrequency spectrum with 2.4 GHz phones and appliances such as\nmicrowave ovens. At 5 GHz, 802.11 LANs have a shorter transmission\ndistance for a given power level and suffer more from multipath\npropagation. The 802.11n, 802.11ac, and 802.11ax standards use multiple\ninput multiple-output (MIMO) antennas; that is, two or more antennas on\nthe sending side and two or more antennas on the receiving side that are\ntransmitting/receiving different signals [Diggavi 2004]. 802.11ac and\n802.11 ax base stations may transmit to multiple stations simultaneously,\nand use “smart” antennas to adaptively beamform to target transmissions in\nthe direction of a receiver. This decreases interference and increases the\ndistance reached at a given data rate. The data rates shown in Table 7.1 are\nfor an idealized environment, for example, a receiver close to the base\nstation, with no ­interference—a scenario that we’re unlikely to experience\nin practice! So as the saying goes, YMMV: Your Mileage (or in this case\nyour wireless data rate) May Vary.\nTable 7.1 ♦Summary of IEEE 802.11 standards\n7.3.1 The 802.11 Wireless LAN Architecture\nFigure 7.7 illustrates the principal components of the 802.11 wireless LAN\narchitecture. The fundamental building block of the 802.11 architecture is\nthe basic service set (BSS). A BSS contains one or more wireless stations\nand a central base station, known as an access point (AP) in 802.11\nparlance. Figure 7.7 shows the AP in each of two BSSs connecting to an\ninterconnection device (such as a switch or router), which in turn leads to\nthe Internet. In a typical home network, there is one AP and one router\n(typically integrated together as one unit) that connects the BSS to the\nFigure 7.7 ♦IEEE 802.11 LAN architecture\nAs with Ethernet devices, each 802.11 wireless station has a 6-byte\nMAC address that is stored in the firmware of the station’s adapter (that is,\n802.11 network interface card). Each AP also has a MAC address for its\nwireless interface. As with Ethernet, these MAC addresses are administered\nby IEEE and are (in theory) ­globally unique.\nAs noted in Section 7.1, wireless LANs that deploy APs are often\nreferred to as infrastructure wireless LANs, with the “infrastructure”\nbeing the APs along with the wired Ethernet infrastructure that\ninterconnects the APs and a router. Figure 7.8 shows that IEEE 802.11\nstations can also group themselves together to form an ad hoc network—a\nnetwork with no central control and with no connections to the ­“outside\nworld.” Here, the network is formed “on the fly,” by mobile devices that\nhave found themselves in proximity to each other, that have a need to\ncommunicate, and that find no preexisting network infrastructure in their\nlocation. An ad hoc network might be formed when people with laptops get\ntogether (e.g., in a conference room, a train, or a car) and want to exchange\ndata in the absence of a centralized AP. There has been tremendous interest\nin ad hoc networking, as communicating portable devices continue to\nproliferate. In this section, though, we’ll focus our attention on\ninfrastructure wireless LANs.\nFigure 7.8 ♦An IEEE 802.11 ad hoc network\nChannels and Association\nIn 802.11, each wireless station needs to associate with an AP before it can\nsend or receive network-layer data. Although all of the 802.11 standards use\nassociation, we’ll discuss this topic specifically in the context of IEEE\n802.11b, g, n, ac, ax.\nWhen a network administrator installs an AP, the administrator assigns\na one- or two-word Service Set Identifier (SSID) to the access point.\n(When you choose Wi-Fi under Setting on your iPhone, for example, a list\nis displayed showing the SSID of each AP in range.) The administrator must\nalso assign a channel number to the AP. To understand channel numbers,\nrecall that 802.11 operates in the frequency range of 2.4 GHz to 2.4835\nGHz. Within this 85 MHz band, 802.11 defines 11 partially overlapping\nchannels. Any two channels are non-overlapping if and only if they are\nseparated by four or more channels. In particular, the set of channels 1, 6,\nand 11 is the only set of three non-overlapping channels. This means that an\nadministrator could create a wireless LAN with an aggregate maximum\ntransmission rate of three times the maximum transmission rate shown in\nTable 7.1 by installing three 802.11 APs at the same physical location,\nassigning channels 1, 6, and 11 to the APs, and interconnecting each of the\nAPs with a switch.\nNow that we have a basic understanding of 802.11 channels, let’s\ndescribe an interesting (and not completely uncommon) situation—that of a\nWiFi jungle. A WiFi jungle is any physical location where a wireless\nstation receives a sufficiently strong signal from two or more APs. For\nexample, in many cafés in New York City, a wireless station can pick up a\nsignal from numerous nearby APs. One of the APs might be managed by\nthe café, while the other APs might be in residential apartments near the\ncafé. Each of these APs would likely be located in a different IP subnet and\nwould have been independently assigned a channel.\nNow suppose you enter such a WiFi jungle with your smartphone,\ntablet, or ­laptop, seeking wireless Internet access and a blueberry muffin.\nSuppose there are five APs in the WiFi jungle. To gain Internet access, your\nwireless device needs to join exactly one of the subnets and hence needs to\nassociate with exactly one of the APs. ­Associating means the wireless\ndevice creates a virtual wire between itself and the AP. Specifically, only the\nassociated AP will send data frames (that is, frames containing data, such as\na datagram) to your wireless device, and your wireless device will send data\nframes into the Internet only through the associated AP. But how does your\nwireless device associate with a particular AP? And more fundamentally,\nhow does your wireless device know which APs, if any, are out there in the\nThe 802.11 standard requires that an AP periodically send beacon\nframes, each of which includes the AP’s SSID and MAC address. Your\nwireless device, knowing that APs are sending out beacon frames, scans the\n11 channels, seeking beacon frames from any APs that may be out there\n(some of which may be transmitting on the same channel—it’s a jungle out\nthere!). Having learned about available APs from the beacon frames, you\n(or your wireless device) select one of the APs for association.\nThe 802.11 standard does not specify an algorithm for selecting which\nof the available APs to associate with; that algorithm is left up to the\ndesigners of the 802.11 firmware and software in your wireless device.\nTypically, the device chooses the AP whose beacon frame is received with\nthe highest signal strength. While a high signal strength is good (see, e.g.,\nFigure 7.3), signal strength is not the only AP characteristic that will\ndetermine the performance a device receives. In particular, it’s possible that\nthe selected AP may have a strong signal, but may be overloaded with other\naffiliated devices (that will need to share the wireless bandwidth at that AP),\nwhile an unloaded AP is not selected due to a slightly weaker signal. A\nnumber of alternative ways of choosing APs have thus recently been\nproposed [Vasudevan 2005; Nicholson 2006; Sundaresan 2006]. For an\ninteresting and down-to-earth discussion of how signal strength is\nmeasured, see [Bardwell 2004].\nThe process of scanning channels and listening for beacon frames is\nknown as passive scanning (see Figure 7.9a). A wireless device can also\nperform active scanning, by broadcasting a probe frame that will be\nreceived by all APs within the wireless device’s range, as shown in Figure\n7.9b. APs respond to the probe request frame with a probe response frame.\nThe wireless device can then choose the AP with which to associate from\namong the responding APs.\nFigure 7.9 ♦Active and passive scanning for access points\nAfter selecting the AP with which to associate, the wireless device\nsends an association request frame to the AP, and the AP responds with an\nassociation response frame. Note that this second request/response\nhandshake is needed with active scanning, since an AP responding to the\ninitial probe request frame doesn’t know which of the (possibly many)\nresponding APs the device will choose to associate with, in much the same\nway that a DHCP client can choose from among multiple DHCP servers\n(see Figure 4.21). Once associated with an AP, the device will want to join\nthe subnet (in the IP addressing sense of Section 4.3.3) to which the AP\nbelongs. Thus, the device will typically send a DHCP discovery message\n(see Figure 4.21) into the subnet via the AP in order to obtain an IP address\non the subnet. Once the address is obtained, the rest of the world then views\nthat device simply as another host with an IP address in that subnet.\nIn order to create an association with a particular AP, the wireless\ndevice may be required to authenticate itself to the AP. 802.11 wireless\nLANs provide a number of alternatives for authentication and access. One\napproach, used by many companies, is to permit access to a wireless\nnetwork based on a device’s MAC address. A second approach, used by\nmany Internet cafés, employs usernames and passwords. In both cases, the\nAP typically communicates with an authentication server, relaying\ninformation between the wireless device and the authentication server using\na protocol such as RADIUS [RFC 2865] or DIAMETER [RFC 6733].\nSeparating the authentication server from the AP allows one authentication\nserver to serve many APs, centralizing the (often sensitive) decisions of\nauthentication and access within the single server, and keeping AP costs and\ncomplexity low. We’ll see in Chapter 8 that the new IEEE 802.11i protocol\ndefining security aspects of the 802.11 protocol family takes precisely this\n7.3.2 The 802.11 MAC Protocol\nOnce a wireless device is associated with an AP, it can start sending and\nreceiving data frames to and from the access point. But because multiple\nwireless devices, or the AP itself may want to transmit data frames at the\nsame time over the same channel, a multiple access protocol is needed to\ncoordinate the transmissions. In the following, we'll refer to the devices or\nthe AP as wireless “stations” that share the multiple access channel. As\ndiscussed in Chapter 6 and Section 7.2.1, broadly speaking there are three\nclasses of multiple access protocols: channel partitioning (including\nCDMA), random access, and taking turns. Inspired by the huge success of\nEthernet and its random access protocol, the designers of 802.11 chose a\nrandom access protocol for 802.11 wireless LANs. This random access\nprotocol is referred to as CSMA with collision avoidance, or more\nsuccinctly as CSMA/CA. As with Ethernet’s CSMA/CD, the “CSMA” in\nCSMA/CA stands for “carrier sense multiple access,” meaning that each\nstation senses the channel before transmitting, and refrains from\ntransmitting when the channel is sensed busy. Although both ­Ethernet and\n802.11 use carrier-sensing random access, the two MAC protocols have\nimportant differences. First, instead of using collision detection, 802.11 uses\ncollision-avoidance techniques. Second, because of the relatively high bit\nerror rates of wireless channels, 802.11 (unlike Ethernet) uses a link-layer\nin Chapter 3.\nRecall that the 802.11 protocol allows a transmitting station to reserve\nthe channel for a period of time that includes the time to transmit its data\nused or not (WEP is discussed in Chapter 8).\n7.3.4 Mobility in the Same IP Subnet\nIn order to increase the physical range of a wireless LAN, companies and\nuniversities will often deploy multiple BSSs within the same IP subnet. This\nnaturally raises the issue of mobility among the BSSs—how do wireless\nstations seamlessly move from one BSS to another while maintaining\nongoing TCP sessions? As we’ll see in this subsection, mobility can be\nhandled in a relatively straightforward manner when the BSSs are part of\nthe subnet. When stations move between subnets, more sophisticated\nmobility management protocols will be needed, such as those we’ll study in\nSections 7.5 and 7.6.\nLet’s now look at a specific example of mobility between BSSs in the\nsame subnet. Figure 7.15 shows two interconnected BSSs with a host, H1,\nmoving from BSS1 to BSS2. Because in this example the interconnection\ndevice that connects the two BSSs is not a router, all of the stations in the\ntwo BSSs, including the APs, belong to the same IP subnet. Thus, when H1\nmoves from BSS1 to BSS2, it may keep its IP address and all of its ongoing\nTCP connections. If the interconnection device were a router, then H1\nwould have to obtain a new IP address in the subnet in which it was\nmoving. This address change would disrupt (and eventually terminate) any\non-going TCP connections at H1. In Section 7.6, we’ll see how a network-\nlayer mobility protocol, such as mobile IP, can be used to avoid this\nFigure 7.15 ♦Mobility in the same subnet\nBut what specifically happens when H1 moves from BSS1 to BSS2? As\nH1 wanders away from AP1, H1 detects a weakening signal from AP1 and\nstarts to scan for a stronger signal. H1 receives beacon frames from AP2\n(which in many corporate and university settings will have the same SSID\nas AP1). H1 then disassociates with AP1 and associates with AP2, while\nkeeping its IP address and maintaining its ongoing TCP sessions.\nThis addresses the handover problem from the host and AP viewpoint.\nBut what about the switch in Figure 7.15? How does it know that the host\nhas moved from one AP to another? As you may recall from Chapter 6,\nswitches are “self-learning” and automatically build their forwarding tables.\nThis self-learning feature nicely handles occasional moves (for example,\nwhen an employee gets transferred from one department to another);\nhowever, switches were not designed to support highly mobile users who\nwant to maintain TCP connections while moving between BSSs. To\nappreciate the problem here, recall that before the move, the switch has an\nentry in its forwarding table that pairs H1’s MAC address with the outgoing\nswitch interface through which H1 can be reached. If H1 is initially in\nBSS1, then a datagram destined to H1 will be directed to H1 via AP1. Once\nH1 associates with BSS2, however, its frames should be directed to AP2.\nOne solution (a bit of a hack, really) is for AP2 to send a broadcast Ethernet\nframe with H1’s source address to the switch just after the new association.\nWhen the switch receives the frame, it updates its forwarding table,\nallowing H1 to be reached via AP2. The 802.11f standards group is\ndeveloping an inter-AP protocol to handle these and related issues.\nOur discussion above has focused on mobility with the same LAN\nsubnet. Recall that VLANs, which we studied in Section 6.4.4, can be used\nto connect together islands of LANs into a large virtual LAN that can span a\nlarge geographical region. Mobility among base stations within such a\nVLAN can be handled in exactly the same manner as above [Yu 2011].\nLOCATION DISCOVERY: GPS AND WIFI POSITIONING\nMany of the most useful and important smartphone apps today are location-based\nmobile apps, including Foursquare, Yelp, Uber, Pokémon Go, and Waze. These ­-\nsoftware apps all make use of an API that allows them to extract their current\ngeographical position directly from the smartphone. Have you ever wondered how your\nsmartphone obtains its geographical position? Today, it is done by combining two\nsystems, the Global Positioning System (GPS) and the WiFi Positioning System\nThe GPS, with a constellation of 30+ satellites, broadcasts satellite location and\ntiming information, which in turn is used by each GPS receiver to estimate its\ngeolocation. The United States government created the system, maintains it, and\nmakes it freely accessible to anyone with a GPS receiver. The satellites have very\nstable atomic clocks that are synchronized with one another and with ground clocks.\nThe satellites also know their locations with great precision. Each GPS satellite\ncontinuously broadcasts a radio signal containing its current time and position. If a\nGPS receiver obtains this information from at least four satellites, it can solve\ntriangulation equations to estimate its position.\nGPS, however, cannot always provide accurate geolocations if it does not have line-\nof-sight with at least four GPS satellites or when there is interference from other high-\nfrequency communication systems. This is particularly true in urban environments,\nwhere tall buildings frequently block GPS signals. This is where WiFi positioning ­-\nsystems come to the rescue. WiFi positioning systems make use of databases of WiFi\naccess points, which are independently maintained by various Internet companies,\nincluding Google, Apple, and Microsoft. Each database contains information about\nmillions of WiFi access points, including each access point’s SSID and an estimate of\nits geographic location. To understand how a WiFi positioning system makes use of\nsuch a database, consider an Android smartphone along with the Google location\nservice. From each nearby access point, the smartphone receives and measures the\nsignal strength of beacon signals (see Section 7.3.1), which contain the access point’s\nSSID. The smartphone can therefore continually send messages to the Google\nlocation service (in the cloud) that include the SSIDs of nearby access points and the\ncorresponding signal strengths. It will also send its GPS position (obtained via the\nsatellite broadcast signals, as described above) when available. Using the signal-\nstrength information, Google will estimate the distance between the smartphone and\neach of the WiFi access points. Leveraging these estimated distances, it can then\nsolve triangulation equations to estimate the smartphone’s geolocation. Finally, this\nWiFi-based estimate is combined with the GPS satellite-based estimate to form an\naggregate estimate, which is then sent back to the smartphone and used by the\nlocation-based mobile apps.\nBut you may still be wondering how Google (and Apple, Microsoft, and so on) obtain\nand maintain the database of access points, and in particular, the access point’s\ngeographic location? Recall that for a given access point, every nearby Android\nsmartphone will send to the Google location service the strength of the ­signal received\nfrom the access point as well as the smartphone’s estimated location. Given that\nthousands of smartphones may be passing by the access point during any single day,\nGoogle’s location service will have lots of data at its disposition to use in estimating the\naccess point’s position, again by solving triangulation equations. Thus, the access\npoints help the smartphones determine their locations, and in turn the smartphones\nhelp the access points determine their locations!\n7.3.5 Advanced Features in 802.11\nWe’ll wrap up our coverage of 802.11 with a short discussion of two\nadvanced capabilities found in 802.11 networks. As we’ll see, these\ncapabilities are not completely specified in the 802.11 standard, but rather\nare made possible by mechanisms specified in the standard. This allows\ndifferent vendors to implement these capabilities using their own\n(proprietary) approaches, presumably giving them an edge over the\ncompetition.\n802.11 Rate Adaptation\nWe saw earlier in Figure 7.3 that different modulation techniques (with the\ndifferent transmission rates that they provide) are appropriate for different\nSNR scenarios. Consider, for example, a mobile 802.11 user who is initially\n20 meters away from the base station, with a high signal-to-noise ratio.\nGiven the high SNR, the user can communicate with the base station using\na physical-layer modulation technique that provides high transmission rates\nwhile maintaining a low BER. This is one happy user! Suppose now that the\nuser becomes mobile, walking away from the base station, with the SNR\nfalling as the distance from the base station increases. In this case, if the\nmodulation technique used in the 802.11 protocol operating between the\nbase station and the user does not change, the BER will become\nunacceptably high as the SNR decreases, and eventually no transmitted\nframes will be received correctly.\nFor this reason, some 802.11 implementations have a rate adaptation\ncapability that adaptively selects the underlying physical-layer modulation\ntechnique to use based on current or recent channel characteristics. If a node\nIP protocol we studied in Chapter 4. As with earlier 2G and 3G networks, 4G LTE is full\nof rather obtuse acronyms and element names. We’ll try to cut through that jumble by\nfirst focusing on element functions and how the various elements of a 4G LTE network\ninteract with each other in both the data and the control planes:\nMobile Device. This is a smartphone, tablet, laptop, or IoT device that connects into\na cellular carrier’s network. This is where applications such as web browsers, map\napps, voice and videoconference apps, mobile payment apps, and so much more are\nrun. The mobile device typically implements the full 5-layer Internet protocol stack,\nincluding the transport and application layers, as we saw with hosts at the Internet’s\nnetwork edge. The mobile device is a network endpoint, with an IP address (obtained\nthrough NAT, as we’ll see). The mobile device also has a globally unique 64-bit\nidentifier called the International Mobile ­Subscriber Identity (IMSI), which is\nstored on its SIM (Subscriber Identity Module) card. The IMSI identifies the\nsubscriber in the worldwide cellular carrier network system, including the country\nand home cellular carrier network to which the subscriber belongs. In some ways, the\nIMSI is analogous to a MAC address. The SIM card also stores information about the\nservices that the subscriber is able to access and encryption key information for that\nsubscriber. In the official 4G LTE jargon, the mobile device is referred to as User\nEquipment (UE). However, in this textbook, we’ll use the more reader-friendly term\n“mobile device” throughout. We also note here that a mobile device is not always\nmobile; for example, the device might be a fixed temperature sensor or a surveillance\nBase Station. The base station sits at the “edge” of the carrier’s network and is\nresponsible for managing the wireless radio resources and the mobile devices with its\ncoverage area (shown as a hexagonal cell in Figure 7.17). As we’ll see, a mobile\ndevice will interact with a base station to attach to the carrier’s network. The base\nstation coordinates device authentication and allocation of resources (channel access)\nin the radio access network. In this sense, cellular base station functions are\ncomparable (but by no means identical) to those of APs in wireless LANs. But\ncellular base stations have several other important roles not found in wireless LANs.\nIn particular, base stations create device-specific IP tunnels from the mobile device to\ngateways and interact among themselves to handle device mobility among cells.\nNearby base stations also coordinate among themselves to manage the radio\nspectrum to minimize interference between cells. In the official 4G LTE terminology,\nthe base station is referred to as an “eNode-B,” which is rather opaque and non-\ndescriptive. In this textbook, we will instead use the reader-friendlier term “base\nstation” throughout.\nAs an aside, if you find LTE terminology a bit opaque, you aren’t alone! The\netymology of “eNode-B” is rooted in earlier 3G terminology, where network function\npoints were referred to as “nodes,” with “B” harkening back to earlier “Base Station\n(BS)” 1G terminology or “Base Transceiver Station (BTS)” in 2G terminology. 4G\nLTE is an “e”volution over 3G, and hence, an “e” now precedes “Node-B” in 4G LTE\nterminology. This name opaqueness shows no signs in stopping! In 5G systems,\neNode-B functions are now referred to as “ng-eNB”; perhaps you can guess what that\nacronym stands for!\nFigure 7.17 ♦Elements of the 4G LTE architecture\nHome Subscriber Server (HSS). As shown in Figure 7.18, the HSS is a ­control-\nplane element. The HSS is a database, storing information about the mobile devices\nfor which the HSS’s network is their home network. It is used in conjunction with the\nMME (discussed below) for device authentication.\nServing Gateway (S-GW), Packet Data Network Gateway (P-GW), and\nother  network routers. As shown in Figure 7.18, the Serving Gateway and the\nPacket Data Network Gateway are two routers (often collocated in practice) that lie\non the data path between the mobile device and the Internet. The PDN Gateway also\nprovides NAT IP addresses to mobile devices and performs NAT functions (see\nSection 4.3.4). The PDN Gateway is the last LTE element that a datagram originating\nat a mobile device encounters before entering the larger Internet. To the outside\nworld, the P-GW looks like any other gateway router; the mobility of the mobile\nnodes within the cellular carrier’s LTE network is hidden from the outside world\nbehind the P-GW. In addition to these gateway routers, a cellular carrier’s all-IP core\nwill have additional routers whose role is similar to that of traditional IP routers—to\nforward IP datagrams among themselves along paths that will typically terminate at\nelements of the LTE core network.\nMobility Management Entity (MME). The MME is also a control-plane element,\nas shown in Figure 7.18. Along with the HSS, it plays an important role in\nauthenticating a device wanting to connect into its network. It also sets up the tunnels\non the data path from/to the device and the PDN Internet gateway router, and\nmaintains information about an active mobile device’s cell location within the\ncarrier’s cellular network. But, as shown in Figure 7.18, it is not in the forwarding\npath for the mobile device’s datagrams being sent to and from the Internet.\nAuthentication. It is important for the network and the mobile device attaching to\nthe network to mutually authenticate each other—for the network to know that\nthe attaching device is indeed the device associated with a given IMSI, and for\nthe mobile device to know that the network to which it is attaching is also a\nlegitimate cellular carrier network. We will cover authentication in Chapter 8 and\ncover 4G authentication in Section 8.8. Here, we simply note that the MME plays\na middleman role between the mobile and Home Subscriber Service (HSS) in the\nmobile’s home network. Specifically, after receiving an attach request from\nmobile device, the local MME contacts the HSS in the mobile’s home network.\nThe mobile’s home HSS then returns enough encrypted information to the local\nMME to prove to the mobile device that the home HSS is performing\nauthentication through this MME, and for the mobile device to prove to the MME\nthat it is indeed the mobile associated with that IMSI. When a mobile device is\nattached to its home network, the HSS to be contacted during authentication is\nlocated within that same home network. However, when a mobile device is\nroaming on a visited network operated by a different cellular network carrier, the\nMME in that roaming network will need to contact the HSS in the mobile\ndevice’s home network.\nPath setup. As shown in the bottom half of Figure 7.18, the data path from the\nmobile device to the carrier’s gateway router consists of a wireless first hop\nbetween the mobile device and the base station, and concatenated IP tunnels\nbetween the base station and the Serving Gateway, and the Serving Gateway and\nthe PDN Gateway. Tunnels are setup under the control of the MME and used for\ndata forwarding (rather than direct forwarding among network routers) to\nfacilitate device mobility—when a device moves, only the tunnel endpoint\nterminating at the base station needs to be changed, while other tunnel endpoints,\nand the Quality of Service associated with a tunnel, remain unchanged.\nCell location tracking. As the device moves between cells, the base stations will\nupdate the MME on the device’s location. If the mobile device is in a sleep mode\nbut nonetheless moving between cells, the base stations can no longer track the\ndevice’s location. In this case, it will be the responsibility of the MME to locate\nthe device for wakeup, through a process known as paging.\nFigure 7.18 ♦LTE data-plane and control-plane elements\nTable 7.2 summarizes the key LTE architectural elements that we have discussed\nabove and compares these functions with those we encountered in our study of WiFi\nwireless LANs (WLANs).\nTable 7.2 ♦LTE Elements, and similar WLAN (WiFi) functions\nTHE ARCHITECTURAL EVOLUTION FROM 2G TO 3G TO 4G\nIn a relatively short span of 20 years, cellular carrier networks have undergone an astonishing\ntransition from being almost exclusively circuit-switched telephone networks to being all-IP packet-\nswitched data networks which include voice as just one of many applications. How did this transition\nhappen from an architectural standpoint? Was there a “flag day,” when the previous telephony-oriented\nnetworks were turned “off” and the all-IP cellular network was turned “on”? Or did elements in the\nprevious telephony-oriented networks begin taking on dual circuit (legacy) and packet (new)\nfunctionality, as we saw with the IPv4-to-IPv6 transition in Section 4.3.4?\nFigure 7.19 is taken from the earlier 7th edition of this textbook, which covered both 2G and 3G\ncellular networks. (We have retired this historical material, which is still available on this book’s website,\nin favor of a deeper coverage of 4G LTE in this 8th edition). Although the 2G network is a circuit-\nswitched mobile telephone network, a comparison of Figures 7.17 and 7.19 illustrates a similar\nconceptual structure, albeit for voice rather than for data services—a wireless edge controlled by a\nbase station, a gateway from the carrier’s network to the outside world, and aggregation points\nbetween the base stations and the gateway.\nFigure 7.19 ♦Elements of the 2G cellular architecture, supporting circuit-\nswitched voice service with the carrier’s core network\nFigure 7.20 (also taken from the 7th edition of this textbook) shows the main architectural\ncomponents of the 3G cellular architecture, which supports both circuit-switched voice service and\npacket-switched data services. Here, the transition from a voice-only network to a combined voice and\ndata network is clear: the existing core 2G cellular voice network elements remained untouched.\nHowever, additional cellular data functionality was added in parallel to, and functioned independently\nfrom, the existing core voice network at that time. As shown in Figure 7.20, the splitting point into these\ntwo separate core voice and data networks happened at the network edge, at the base station in the\nradio access network. The alternative—integrating new data services directly into the core elements of\nthe existing cellular voice network—would have raised the same challenges encountered in integrating\nnew (IPv6) and legacy (IPv4) technologies in the Internet. The carriers also wanted to leverage and\nexploit their considerable investment of existing infrastructure (and profitable services!) in their existing\ncellular voice network.\nFigure 7.20 ♦3G system architecture: supporting separate circuit-switched\nvoice service and packet-switched data service with the\ncarrier’s core network\n7.4.2 LTE Protocols Stacks\nSince the 4G LTE architecture is an all-IP architecture, we’re already very familiar with\nthe higher-layer protocols in the LTE protocol stack, in particular IP, TCP, UDP, and\nvarious application layer protocols, from our studies in Chapters 2 through 5.\nConsequently, the new LTE protocols that we’ll focus on here are primarily at the link\nand physical layers, and in mobility management.\nFigure 7.21 shows the user-plane protocol stacks at the LTE mobile node, the base\nstation, and the serving gateway. We’ll touch on several of LTE’s control-plane protocols\nlater when we study LTE mobility management (Section 7.6) and security (Section 8.8).\nAs we can see from Figure 7.21, most of the new and interesting user-plane protocol\nactivity is happening at the wireless radio link between the mobile device and the base\nFigure 7.21 ♦LTE data-plane protocol stacks\nLTE divides the mobile device’s link layer into three sublayers:\nPacket Data Convergence. This uppermost sublayer of the link layer sits just below\nIP. The Packet Data Convergence Protocol (PDCP) [3GPP PDCP 2019] performs IP\nheader/compression in order to decrease the number of bits sent over the wireless\nlink, and encryption/decryption of the IP datagram using keys that were established\nvia signaling messages between the LTE mobile device and the Mobility\nManagement Entity (MME) when the mobile device first attached to the network;\nwe’ll cover aspects of LTE security in Section 8.8.2.\nRadio Link Control. The Radio Link Control (RLC) Protocol [3GPP RLCP\n2018] performs two important functions: (i) fragmenting (on the sending side) and\nreassembly (on the receiving) of IP datagrams that are too large to fit into the\nunderlying link-layer frames, and (ii) link-layer reliable data transfer at the through\nthe use of an ACK/NAK-based ARQ protocol. Recall the we’ve studied the basic\nelements of ARQ protocols in Section 3.4.1.\nMedium Access Control (MAC). The MAC layer performs transmission scheduling,\nthat is, the requesting and use of the radio transmission slots described in Section\n7.4.4. The MAC sublayer also performs additional error detection/­correction\nfunctions, including the use of redundant bit transmission as a forward error-\ncorrection technique. The amount of redundancy can be adapted to channel\nconditions.\nFigure 7.21 also shows the use of tunnels in the user data path. As discussed above,\nthese tunnels are established, under MME control, when the mobile device first attaches\nto the network. Each tunnel between two endpoints has a unique tunnel endpoint\nidentifier (TEID). When the base station receives datagrams from the mobile device, it\nencapsulates them using the GPRS Tunneling Protocol [3GPP GTPv1-U 2019], including\nthe TEID, and sends them in UDP segments to the Serving Gateway at the other end of\nthe tunnel. On the receiving side, the base station decapsulates tunneled UDP datagrams,\nextracts the encapsulated IP datagram destined for the mobile device, and forwards that\nIP datagram over the wireless hop to the mobile device.\n7.4.3 LTE Radio Access Network\nLTE uses a combination of frequency division multiplexing and time division\nmultiplexing on the downstream channel, known as orthogonal frequency division\nmultiplexing (OFDM) [Hwang 2009]. (The term “orthogonal” comes from the fact the\nsignals being sent on different frequency channels are created so that they interfere very\nlittle with each other, even when channel frequencies are tightly spaced). In LTE, each\nactive mobile device is allocated one or more 0.5 ms time slots in one or more of the\nchannel frequencies. Figure 7.22 shows an allocation of eight time slots over four\nfrequencies. By being allocated increasingly more time slots (whether on the same\nfrequency or on different frequencies), a mobile device is able to achieve increasingly\nhigher transmission rates. Slot (re)allocation among mobile devices can be performed as\noften as once every millisecond. Different modulation schemes can also be used to\nchange the transmission rate; see our earlier discussion of Figure 7.3 and dynamic\nselection of modulation schemes in WiFi networks.\nFigure 7.22 ♦Twenty 0.5-ms slots organized into 10 ms frames at each\nfrequency. An eight-slot allocation is shown shaded.\nThe particular allocation of time slots to mobile devices is not mandated by the LTE\nstandard. Instead, the decision of which mobile devices will be allowed to transmit in a\ngiven time slot on a given frequency is determined by the scheduling algorithms\nprovided by the LTE equipment vendor and/or the network operator. With opportunistic\nscheduling [Bender 2000; Kolding 2003; Kulkarni 2005], matching the physical-layer\nprotocol to the channel conditions between the sender and receiver and choosing the\nreceivers to which packets will be sent based on channel conditions allow the base station\nto make best use of the wireless medium. In addition, user priorities and contracted levels\nof service (e.g., silver, gold, or platinum) can be used in scheduling downstream packet\ntransmissions. In addition to the LTE capabilities described above, LTE-Advanced allows\nfor downstream bandwidths of hundreds of Mbps by allocating aggregated channels to a\nmobile device [Akyildiz 2010].\n7.4.4 Additional LTE Functions: Network Attachment and\nPower Management\nLet’s conclude or study of 4G LTE here by considering two additional important LTE\nfunctions: (i) the process with which a mobile device first attaches to the network and (ii)\nthe techniques used by the mobile device, in conjunction with core network elements, to\nmanage its power use.\nNetwork Attachment\nThe process by which a mobile device attaches to the cellular carrier’s network divides\nbroadly into three phases:\nAttachment to a Base Station. This first phase of device attachment is similar in\npurpose to, but quite different in practice from, the 802.11 association protocol that\nwe studied in Section 7.3.1. A mobile device wishing to attach to a cellular carrier\nnetwork will begin a bootstrap process to learn about, and then associate with, a\nnearby base station. The mobile device initially searches all channels in all frequency\nbands for a primary synchronization signal that is periodically broadcast every 5 ms\nby a base station. Once this signal is found, the mobile device remains on this\nfrequency and locates the secondary synchronization signal. With information found\nin this second signal, the device can locate (following several further steps)\nadditional information such as channel bandwidth, channel configurations, and the\ncellular carrier information of that base station. Armed with this information, the\nmobile device can select a base station to associate with (preferentially attaching to\nits home network, if available) and establish a control-plane signaling connection\nacross the wireless hop with that base station. This mobile-to-base-station channel\nwill be used through the remainder of the network attachment process.\nMutual Authentication. In our earlier description of the Mobility Management Entity\n(MME) in Section 7.4.1, we noted that the base station contacts the local MME to\nperform mutual authentication—a process that we’ll study in further detail in Section\n8.8.2. This is the second phase of network attachment, allowing the network to know\nthat the attaching device is indeed the device associated with a given IMSI, and the\nmobile device to know that the network to which it is attaching is also a legitimate\ncellular carrier network. Once this second phase of network attachment is complete,\nthe MME and mobile device have mutually authenticated each other, and the MME\nalso knows the identity of the base station to which the mobile is attached. Armed\nwith this information, the MME is now ready to configure the Mobile-device-to-\nPDN-gateway data path.\nMobile-device-to-PDN-gateway Data Path Configuration. The MME contacts the\nPDN gateway (which also provides a NAT address for the mobile device), the\nServing gateway, and the base station to establish the two tunnels shown in Figure\n7.21. Once this phase is complete, the mobile device is able to send/receive IP\ndatagrams via the base station through these tunnels to and from the Internet!\nPower Management: Sleep Modes\nRecall in our earlier discussion of advanced features in 802.11 (Section 7.3.5) and\nBluetooth (Section 7.3.6) that a radio in a wireless device may enter a sleep state to save\npower when it is not transmitting or receiving in order to minimize the amount of time\nthat the mobile device’s circuitry needs to be “on” for sending/receiving data, and for\nchannel sensing. In 4G LTE, a sleeping mobile device can be in one of two different\nsleep states. In the discontinuous reception state, which is typically entered after several\nhundred milliseconds of inactivity [Sauter 2014], the mobile device and the base station\nwill schedule periodic times in advance (typically several hundred milliseconds apart) at\nwhich the mobile device will wake up and actively monitor the channel for downstream\n(base station to mobile device) transmissions; apart from these scheduled times, however,\nthe mobile device’s radio will be sleeping.\nIf the discontinuous reception state might be considered a “light sleep,” the second\nsleep state—the Idle state—which follows even longer periods of 5 to 10 seconds of\ninactivity, might be thought of as a “deep sleep.” While in this deep sleep, the mobile\ndevice’s radio wakes up and monitors the channel even less frequently. Indeed, this sleep\nis so deep that if the mobile device moves into a new cell in the carrier’s network while\nsleeping, it need not inform the base station with which it was previous associated. Thus,\nwhen waking up periodically from this deep sleep, the mobile device will need to re-\nestablish an association with a (potentially new) base station in order to check for paging\nmessages broadcast by the MME to base stations nearby the base station with which the\nmobile was last associated. These control-plane paging messages, which are broadcast by\nthese base stations to all mobile devices in their cells, indicate which mobile devices\nshould fully wake up and re-establish a new data-plane connection to a base station (see\nFigure 7.18) in order to receive incoming packets.\n7.4.5 The Global Cellular Network: A Network of Networks\nHaving now studied the 4G cellular network architecture, let’s take a step back at take a\nlook at how the global cellular network—itself a “network of networks” like the Internet\n—is organized.\nFigure 7.23 shows a user’s mobile smartphone connected via a 4G base station into\nits home network. The user’s home mobile network is operated by a cellular carrier such\nas Verizon, AT&T, T-Mobile, or Sprint in the United States; Orange in France; or SK\nTelecom in Korea. The user’s home network, in turn, is connected to the networks of\nother cellular carriers and to the global Internet, though one or more gateway routers in\nthe home network, as shown in Figure 7.23. The mobile networks themselves\ninterconnect with each other either via the public Internet or via an Internet Protocol\nPacket eXchange (IPX) Network [GSMA 2018a]. An IPX is a managed network\nspecifically for interconnecting cellular carriers, similar to Internet eXchange Points (see\nFigure 1.15) for peering among ISPs. From Figure 7.23, we can see that the global\ncellular network is indeed a “network of networks”—just like the Internet (recall Figure\n1.15 and Section 5.4). 4G networks can also peer with 3G cellular voice/data networks\nand earlier voice-only networks.\nFigure 7.23 ♦The global cellular data network: a network of networks.\nWe’ll return shortly to additional 4G LTE topics—mobility management in Section\n7.6, and 4G security in Section 8.8.2—later, after developing the basic principles needed\nfor these topics. Let’s now take a quick look at the emerging 5G networks.\n7.4.6 5G Cellular Networks\nThe ultimate wide-area data service would be one with ubiquitous gigabit connection\nspeeds, extremely low latency, and unrestricted limitations on the number of users and\ndevices that could be supported in any region. Such a service would open the door to all\nkinds of new applications, including pervasive augmented reality and virtual reality,\ncontrol of autonomous vehicles via wireless connections, control of robots in factories\nvia wireless connections, and replacement of residential access technologies, such as\nDSL and cable, with fixed wireless Internet services (that is, residential wireless\nconnections from base stations to modems in homes).\nIt is expected that 5G, for which progressively improved versions are likely to be\nrolled out in the 2020 decade, will make a big step towards achieving the goals of the\nultimate wide-area data service. It is predicted that 5G will provide roughly a 10x\nincrease in peak bitrate, a 10x decrease in latency, and a 100x increase in traffic capacity\nover 4G [Qualcomm 2019].\nPrincipally, 5G refers to “5G NR (New Radio),” which is the standard adopted by\n3GPP. Other 5G technologies besides NR do exist, however. For example, Verizon’s\nproprietary 5G TF network operates on 28 and 39 GHz frequencies and is used only for\nfixed wireless Internet service, not in smartphones.\n5G standards divide frequencies into two groups: FR1 (450 MHz–6 GHz) and FR2\n(24 GHz–52 GHz). Most early deployments will be in the FR1 space, although there are\nearly deployments as of 2020 in the FR2 space for fixed Internet residential access as\nmentioned just above. Importantly, the physical layer (that is, wireless) aspects of 5G are\nnot backward-compatible with 4G mobile communications systems such as LTE: in\nparticular, it can’t be delivered to existing smartphones by deploying base station\nupgrades or software updates. Therefore, in the transition to 5G, wireless carriers will\nneed to make substantial investments in physical infrastructure.\nFR2 frequencies are also known as millimeter wave frequencies. While millimeter\nwave frequencies allow for much faster data speeds, they come with two major\nMillimeter wave frequencies have much shorter range from base station to receivers.\nThis makes millimeter wave technology unsuitable in rural areas and requires denser\ndeployments of base stations in urban areas.\nMillimeter wave communication is highly susceptible to atmospheric interference.\nNearby foliage and rain can cause problems for outdoor use.\n5G is not one cohesive standard, but instead consists of three co-existing standards\n[Dahlman 2018]:\neMBB (Enhanced Mobile Broadband). Initial deployments of 5G NR have focused\non eMBB, which provides for increased bandwidth for higher download and upload\nspeeds, as well as a moderate reduction in latency when compared to 4G LTE. eMBB\nenables rich media applications, such as mobile augmented reality and virtual reality,\nas well as mobile 4K resolution and 360° video streaming.\nURLLC (Ultra Reliable Low-Latency Communications). URLLC is targeted towards\napplications that are highly latency-sensitive, such as factory automation and\nautonomous driving. URLLC is targeting latencies of  1msec. As of this writing,\ntechnologies that enable URLLC are still being standardized.\nmMTC (Massive Machine Type Communications). mMTC is a narrowband access\ntype for sensing, metering, and monitoring applications. One priority for the design\nof 5G networks is to lower barriers for network connectivity for IoT devices. In\naddition to lowering latency, emerging technologies for 5G networks are focusing on\nreducing power requirements, making the use of IoT devices more pervasive than has\nbeen with 4G LTE.\n5G and Millimeter Wave Frequencies\nMany 5G innovations will be a direct result of working in the millimeter wave\nfrequencies in the 24 GHz–52 GHz band. For example, these frequencies offer the\npotential of achieving 100x increase in capacity over 4G. To get some insight into this,\ncapacity can be defined as the product of three terms [Björnson 2017]:\ncapacity = cell density  ×  available spectrum  ×  spectral efficiency\nwhere cell density is in units of cells/km2, available spectrum is in units of Hertz, and\nspectral efficiency is a measure of how efficiently each base station can communicate\nwith users and is in units of bps/Hz/cell. By multiplying these units out, it is easy to see\nthat capacity is in units of bps/km2. For each of these three terms, the values will be\nlarger for 5G than for 4G:\nBecause millimeter frequencies have much shorter range than 4G LTE frequencies,\nmore base stations are required, which in turn increases the cell density.\nBecause 5G FR2 operates in a much larger frequency band (52 − 24 = 28 GHz) than\n4G LTE (up to about 2 GHz), it has more available spectrum.\nWith regard to spectral efficiency, information theory says that if you want to double\nspectral efficiency, a 17-fold increase in power is needed [Björnson 2017]. Instead of\nincreasing power, 5G uses MIMO-technology (the same technology we encountered\nin our study of 802.11 networks in Section 7.3), which uses multiple antennas at each\nbase station. Rather than broadcasting signals in all directions, each MIMO antenna\nemploys beam forming and directs the signal at the user. MIMO technology allows a\nbase station to send to 10–20 users at the same time in the same frequency band.\nBy increasing all three terms in the capacity equation, 5G is expected to provide a\n100x increase in capacity in urban areas. Similarly, owing to the much wider frequency\nband, 5G is expected to provide peak download rates of 1 Gbps or higher.\nMillimeter wave signals are, however, easily blocked by buildings and trees. Small\ncell stations are needed to fill in coverage gaps between base stations and users. In a\nhighly populous region, the distance between two small cells could vary from 10 to 100\nmeters [Dahlman 2018].\n5G Core Network\nThe 5G Core network is the data network that manages all of the 5G mobile voice, data\nand Internet connections. The 5G Core network is being redesigned to better integrate\nwith the Internet and cloud-based services, and also includes distributed servers and\ncaches across the network, thereby reducing latency. Network function virtualization (as\ndiscussed in Chapters 4 and 5), and network slicing for different applications and\nservices, will be managed in the core.\nThe new 5G Core specification introduces major changes in the way mobile\nnetworks support a wide variety of services with varied performance. As in the case of\nthe 4G core network (recall Figures 7.17 and 7.18), the 5G core relays data traffic from\nend devices, authenticates devices, and manages device mobility. The 5G core also\ncontains all of the network elements that we encountered in Section 7.4.2—the mobile\ndevices, the cells, the base stations, and the Mobility Management Entity (now divided\ninto two sub-elements, as discussed below), the HSS, and the Serving and PDN\nAlthough the 4G and 5G core networks perform similar functions, there are some\nmajor differences in that the new 5G core architecture. The 5G Core is designed for\ncomplete control and user-plane separation (see Chapter 5). The 5G Core consists purely\nof virtualized software-based network functions. This new architecture will give\noperators the flexibility to meet the diverse requirements of the different 5G applications.\nSome of the new 5G core network functions include [Rommer 2019]:\nUser-Plane Function (UPF). Control and user-plane separation (see Chapter 5)\nallows packet processing to be distributed and pushed to the network edge.\nAccess and Mobility Management Function (AMF). The 5G Core essentially\ndecomposes the 4G Mobility Management Entity (MME) into two functional\nelements: AMF and SMF. The AMF receives all the connection and session\ninformation from end-user equipment but only handles connection and mobility\nmanagement tasks.\nSession Management Function (SMF). Session management is handled by the\nSession Management Function (SMF). The SMF is responsible for interacting with\nthe decoupled data plane. The SMF also performs IP address management and plays\nthe role of DHCP.\nAs of this writing (2020), 5G is in its early stages of deployment, and many 5G\nstandards have yet to be finalized. Only time will tell whether 5G will become a\npervasive broadband wireless service, whether it will successfully compete with WiFi for\nindoor wireless service, whether it will become a critical component of factory\nautomation and the autonomous vehicle infrastructure, and whether it will take us a big\nstep forward toward the ultimate wide-area wireless service.\n7.5 Mobility Management: Principles\nHaving covered the wireless nature of the communication links in a\nwireless network, it’s now time to turn our attention to the mobility that\nthese wireless links enable. In the broadest sense, a mobile device is one\nthat changes its point of attachment into the network over time. Because the\nterm mobility has taken on many meanings in both the computer and\ntelephony worlds, it will serve us well first to carefully consider forms of\n7.5.1 Device Mobility: a Network-layer Perspective\nFrom the network layer’s standpoint, a physically mobile device will\npresent a very different set of challenges to the network layer, depending on\nhow active the device is as it moves between points of attachment to the\nnetwork. At the one end of the spectrum, scenario (a) in Figure 7.24 is the\nmobile user who himself/herself physically moves between networks, but\npowers down the mobile device when moving. For example, a student\nmight disconnect from a wireless classroom network and power down\nhis/her device, head to the dining commons and connect to the wireless\naccess network there while eating, and then disconnect and power down\nfrom the dining commons network, walk to the library, and connect to the\nlibrary’s wireless network while studying. From a networking perspective,\nthis device is not mobile—it attaches to an access network and remains in\nthat access network while on. In this case, the device serially associates\nwith, and later disassociates from, each wireless access network\nencountered. This case of device (non-)mobility can be completely handled\nusing the networking mechanisms we’ve already studied in Sections 7.3 and\nIn scenario (b) in Figure 7.24, the device is physically mobile but\nremains attached to the same access network. This device is also not mobile\nfrom a network-layer perspective. Additionally, if the device remains\nassociated with the same 802.11 AP or LTE base station, the device is not\neven mobile from a link-layer perspective.\nFigure 7.24 ♦Various degrees of mobility, from a network-layer\nperspective\nFrom a network standpoint, our interest in device mobility really starts\nwith case (c), where a device changes its access network (e.g., 802.11\nWLAN or LTE cell) while continuing to send and receiving IP datagrams,\nand while maintaining higher-level (e.g., TCP) connections. Here, the\nnetwork will need to provide handover—a transfer of responsibility for\nforwarding datagrams to/from one AP or base station to the mobile device\n—as the device moves among WLANs or among LTE cells. We’ll cover\nhandover in detail in Section 7.6. If the handover occurs within access\nnetworks belonging to a single network provider, that provider can\norchestrate handover on its own. When a mobile device roams between\nmultiple provider networks, as in scenario (d), the providers must\norchestrate handover together, which considerably complicates the\nhandover process.\n7.5.2 Home Networks and Roaming on Visited\nAs we learned in our discussions of cellular 4G LTE networks in Section\n7.4.1, every subscriber has a “home” with some cellular provider. We\nlearned that the Home Subscriber Service (HSS) stores information about\neach of its subscribers, including a globally unique device ID (embedded in\na subscriber’s SIM card), information about services that the subscriber\nmay access, cryptographic keys to be used for communication, and\nbilling/charging information. When a device is connected to a cellular\nnetwork, other than its home network, that device is said to be roaming on\na visited network. When a mobile device attaches to, and roams on, a\nvisited network, coordination will be required between the home network\nand the visited network.\nThe Internet does not have a similarly strong notion of a home network\nor a visited network. In practice, a student’s home network might be the\nnetwork operated by his/her school; for mobile professionals, their home\nnetwork might be their company network. The visited network might be the\nnetwork of a school or a company they are visiting. But there is no notion\nof a home/visited network deeply embedded in the Internet’s architecture.\nThe Mobile IP protocol [Perkins 1998, RFC 5944], which we will cover\nbriefly in Section 7.6, was a proposal that strongly incorporated the notion\nof home/visited networks. But Mobile IP has seen limited deployment/use\nin practice. There are also activities underway that are built on top of the\nexisting IP infrastructure to provide authenticated network access across\nvisited IP networks. Eduroam [Eduroam 2020] is one such activity.\nThe notion of a mobile device having a home network provides two\nimportant advantages: the home network provides a single location where\ninformation about that device can be found, and (as we will see) it can serve\nas a coordination point for communication to/from a roaming mobile\nTo appreciate the potential value of the central point of information and\ncoordination, consider the human analogy of a 20-something adult Bob\nmoving out of the family home. Bob becomes mobile, living in a series of\ndormitories and apartments, and often changing addresses. If an old friend\nAlice wants to get in touch, how can Alice find the current address of Bob?\nOne common way is to contact the family, since a mobile 20-something\nadult will often register his or her current address with the family (if for no\nother reason than so that the parents can send money to help pay the rent!).\nThe family home becomes that unique location that others can go to as a\nfirst step in communicating with Bob. Additionally, later postal\ncommunication from Alice may be either indirect (e.g., with mail being sent\nfirst to Bob’s family home and then forwarded to Bob) or direct (e.g., with\nAlice using the address obtained from Bob’s parents to send mail directly to\n7.5.3 Direct and Indirect Routing to/from a Mobile\nLet us now consider the conundrum faced by the Internet-connected host\n(that we will refer to as a correspondent) in Figure 7.25 wishing to\ncommunicate with a mobile device that might be located within that mobile\ndevice’s cellular home network, or might be roaming in a visited network.\nIn our development below, we’ll adopt a 4G/5G cellular network\nperspective, since these networks have such a long history of supporting\ndevice mobility. But as we’ll see, the fundamental challenges and basic\nsolution approaches for supporting device mobility are equally applicable in\nboth cellular networks and in the Internet.\nAs shown in Figure 7.25, we’ll assume that the mobile device has a\nglobally unique identifier associated with it. In 4G, LTE cellular networks\n(see Section 7.4), this would be the International Mobile Subscriber Identity\n(IMSI) and an associated phone number, stored on a mobile device’s SIM\ncard. For mobile Internet users, this would be a permanent IP address in the\nIP address range of its home network, as in the case of the Mobile IP\narchitecture.\nFigure 7.25 ♦Elements of a mobile network architecture\nWhat approaches might be used in a mobile network architecture that\nwould allow a datagram sent by the correspondent to reach that mobile\ndevice? Three basic approaches can be identified and are discussed below.\nAs we will see, the latter two of these are adopted in practice.\nLeveraging the Existing IP Address Infrastructure\nPerhaps the simplest approach to routing to a mobile device in a visited\nnetwork is to simply use the existing IP addressing infrastructure—to add\nnothing new to the architecture. What could be easier!\nRecall from our discussion of Figure 4.21 that an ISP uses BGP to\nadvertise routes to destination networks by enumerating the CIDRized\naddress ranges of reachable networks. A visited network could thus\nadvertise to all other networks that a particular mobile device is resident in\nits network simply by advertising a highly specific address—the mobile\ndevice’s full 32-bit IP permanent address—essentially informing other\nnetworks that it has the path to be used to forward datagrams to that mobile\ndevice. These neighboring networks would then propagate this routing\ninformation throughout the network as part of the normal BGP procedure of\nupdating routing information and forwarding tables. Since datagrams will\nalways be forwarded to the router advertising the most specific destination\nfor that address (see Section 4.3), all datagrams addressed to that mobile\ndevice will be forwarded to the visited network. If the mobile device leaves\none visited network and joins another, the new visited network can\nadvertise a new, highly specific route to the mobile device, and the old\nvisited network can withdraw its routing information regarding the mobile\nThis solves two problems at once, and does so without making changes\nto the network-layer infrastructure! Other networks know the location of the\nmobile device, and it is easy to route datagrams to the mobile device, since\nthe forwarding tables will direct datagrams to the visited network. The killer\ndrawback, however, is that of scalability—network routers would have to\nmaintain forwarding table entries for potentially billions of mobile devices,\nand update a device’s entry each time it roams to a different network.\nClearly, this approach would not work in practice. Some additional\ndrawbacks are explored in the problems at the end of this chapter.\nAn alternative, more practical, approach (and one that has been adopted\nin practice) is to push mobility functionality from the network core to the\nnetwork edge—a recurring theme in our study of Internet architecture. A\nnatural way to do this is via the mobile device’s home network. In much the\nsame way that parents of the mobile 20-something adult track their child’s\nlocation, a mobility management entity (MME) in the mobile device’s home\nnetwork could track the visited network in which the mobile device resides.\nThis information might reside in a database, shown as the HSS database in\nFigure 7.25. A protocol operating between the visited network and the home\nnetwork will be needed to update the network in which the mobile device\nresides. You might recall that we encountered the MME and HSS elements\nin our study of 4G LTE. We’ll reuse their element names here, since they\nare so descriptive, and also because they are pervasively deployed in 4G\nLet’s next consider the visited network elements shown in Figure 7.25\nin more detail. The mobile device will clearly need an IP address in the\nvisited network. The possibilities here include using a permanent address\nassociated with the mobile device’s home network, allocating a new address\nin the address range of the visited network, or providing an IP address via\nNAT (see Section 4.3.4). In the latter two cases, a mobile device has a\ntransient identifier (a newly allocated IP address) in addition to its\npermanent identifiers stored in the HSS in its home network. These cases\nare analogous to a writer addressing a letter to the address of the house in\nwhich our mobile 20-something adult is currently living. In the case of a\nNAT address, datagrams destined to the mobile device would eventually\nreach the NAT gateway router in the visited network, which would then\nperform NAT address translation and forward the datagram to the mobile\nWe have now seen a number of elements of a solution to the\ncorrespondent’s dilemma in Figure 7.24: home and visited networks, the\nMME and HSS, and mobile device addressing. But how should datagrams\nbe addressed and forwarded to the mobile device? Since only the HSS (and\nnot network-wide routers) knows the location of the mobile device, the\ncorrespondent cannot simply address a datagram to the mobile device’s\npermanent address and send it into the network. Something more must be\ndone. Two approaches can be identified: indirect and direct routing.\nIndirect Routing to a Mobile Device\nLet’s again consider the correspondent that wants to send a datagram to a\nmobile device. In the indirect routing approach, the correspondent simply\naddresses the datagram to the mobile device’s permanent address and sends\nthe datagram into the network, blissfully unaware of whether the mobile\ndevice is resident in its home network or in a visited network; mobility is\nthus completely transparent to the correspondent. Such datagrams are first\nrouted, as usual, to the mobile device’s home network. This is illustrated in\nstep 1 in Figure 7.26.\nFigure 7.26 ♦Indirect routing to a mobile device\nLet’s now turn our attention to the HSS, which is responsible for\ninteracting with visited networks to track the mobile device’s location, and\nthe home network’s gateway router. One job of this gateway router is to be\non the lookout for an arriving datagram addressed to a device whose home\nis in that network, but that currently resides in a visited network. The home\nnetwork gateway intercepts this datagram, consults with the HSS to\ndetermine the visited network where the mobile device is resident, and\nforwards the datagram toward the visited network gateway router—step 2\nin Figure 7.26. The visited network gateway router then forwards the\ndatagram toward the mobile device—step 3 in Figure 7.26. If NAT\ntranslation is used, as in Figure 7.26, the visited network gateway router\nperforms NAT translation.\nIt is instructive to consider the rerouting at the home network in bit\nmore detail. Clearly, the home network gateway will need to forward the\narriving datagram to the gateway router in the visited network. On the other\nhand, it is desirable to leave the correspondent’s datagram intact, since the\napplication receiving the datagram should be unaware that the datagram\nwas forwarded via the home network. Both goals can be satisfied by having\nthe home gateway encapsulate the correspondent’s original complete\ndatagram within a new (larger) datagram. This larger datagram is then\naddressed and delivered to the visited network’s gateway router, which will\ndecapsulate the datagram—that is, remove the correspondent’s original\ndatagram from within the larger encapsulating datagram—and forward (step\n3 in Figure 7.26) the original datagram to the mobile device. The sharp\nreader will note that the encapsulation/decapsulation described here is\nprecisely the notion of tunneling, discussed in Section 4.3 in the context of\nIPv6; indeed, we also discussed the use of tunneling in the context of Figure\n7.18, when we introduced the 4G LTE data plane.\nFinally, let’s consider how the mobile device sends datagrams to the\ncorrespondent. In the context of Figure 7.26, the mobile device will clearly\nneed to forward the datagram through the visited gateway router, in order to\nperform NAT translation. But how then should the visited gateway router\nforward the datagram to the correspondent? As shown in Figure 7.26, there\nare two options here: (4a) the datagram could be tunneled back to the home\ngateway router, and sent to the correspondent from there, or (4b) the\ndatagram could be transmitted from the visited network directly to the\ncorrespondent—an approach known as local breakout [GSMA 2019a] in\nLet’s summarize our discussion of indirect routing by reviewing the\nnew network-layer functionality required to support mobility.\nA mobile-device–to–visited-network association protocol. The mobile\ndevice will need to associate with the visited network, and will similarly\nneed to disassociate when leaving the visited network.\nA visited-network–to–home-network-HSS registration protocol. The\nvisited network will need to register the mobile device’s location with\nthe HSS in the home network, and perhaps use information obtained\nfrom the HSS in performing device authentication.\nA datagram tunneling protocol between in the home network gateway\nand the visited network gateway router. The sending side performs\nencapsulation and forwarding of the correspondent’s original datagram;\non the receiving side, the gateway router performs decapsulation, NAT\ntranslation, and forwarding of the original datagram to the mobile\nThe previous discussion provides all the needed elements for a mobile\ndevice to maintain an ongoing connection with a correspondent as the\ndevice moves among networks. When a device roams from one visited\nnetwork to another, the new visited network information needs to be\nupdated in the home network HSS, and the home-gateway-router-to-visited-\ngateway-router tunnel endpoint needs to be moved. But will the mobile\ndevice see an interrupted flow of datagrams as it moves between networks?\nAs long as the time between the mobile device disconnection from one\nvisited network and its attachment to the next visited network is small, few\ndatagrams will be lost. Recall from Chapter 3 that end-to-end connections\ncan experience datagram loss due to network congestion. Hence, occasional\ndatagram loss within a connection when a device moves between networks\nis by no means a catastrophic problem. If loss-free communication is\nrequired, upper-layer mechanisms will recover from datagram loss, whether\nsuch loss results from network congestion or from device mobility.\nOur discussion above has been purposefully somewhat generic. An\nindirect routing approach is used in the mobile IP standard [RFC 5944], as\nwell as in 4G LTE networks [Sauter 2014]. Their details, in particular the\ntunneling procedures employed, differ just a bit from our generic discussion\nDirect Routing to a Mobile Device\nThe indirect routing approach illustrated in Figure 7.26 suffers from an\ninefficiency known as the triangle routing problem—datagrams addressed\nto the mobile device must be forwarded first to the home network and then\nto the visited network, even when a much more efficient route exists\nbetween the correspondent and the roaming mobile device. In the worst\ncase, imagine a mobile user who is roaming on the same network that is the\nhome network for an overseas colleague who our mobile user is visiting.\nThe two are sitting side-by-side and exchanging data. Datagrams between\nthe mobile user and his overseas colleague will be forwarded to the mobile\nuser’s home network and then back again to the visited network!\nDirect routing overcomes the inefficiency of triangle routing, but does\nso at the cost of additional complexity. In the direct routing approach,\nshown in Figure 7.27, the correspondent first discovers the visited network\nin which the mobile is resident. This is done by querying the HSS in the\nmobile device’s home network, assuming (as in the case of indirect routing)\nthat the mobile device’s visited network is registered in the HSS. This is\nshown as steps 1 and 2 in Figure 7.27. The correspondent then tunnels\ndatagrams from its network directly to the gateway router in the mobile\ndevice’s visited network.\nFigure 7.27 ♦Direct routing to a mobile device\nWhile direct routing overcomes the triangle routing problem, it\nintroduces two important additional challenges:\nA mobile-user location protocol is needed for the correspondent to\nquery the HSS to obtain the mobile device’s visited network (steps 1\nand 2 in Figure 7.27). This is in addition to the protocol needed for the\nmobile device to register its location with its HSS.\nWhen the mobile device moves from one visited network to another,\nhow will the correspondent know to now forward datagrams to the new\nvisited network? In the case of indirect routing, this problem was easily\nsolved by updating the HSS in the home network, and changing the\ntunnel endpoint to terminate at the gateway router of the new visited\nnetwork. However, with direct routing, this change in visited networks\nis not so easily handled, as the HSS is queried by the correspondent\nonly at the beginning of the session. Thus, additional protocol\nmechanisms would be required to proactively update the correspondent\neach time the mobile device moves. Two problems at the end of this\nchapter explore solutions to this problem.\n7.6 Mobility Management in Practice\nIn the previous section, we identified key fundamental challenges and\npotential solutions in developing a network architecture to support device\nmobility: the notions of home and visited networks; the home network’s\nrole as a central point of information and control for mobile devices\nsubscribed to that home network; control-plane functions needed by a home\nnetwork’s mobility management entity to track a mobile device roaming\namong visited networks; and data-plane approaches of direct and indirect\nrouting to enable a correspondent and a mobile device to exchange\ndatagrams. Let’s now look at how these principles are put into practice! In\nSection 7.6.1, we’ll study mobility management in 4G/5G networks; in\nSection 7.6.2, we’ll look at Mobile IP, which has been proposed for the\n7.6.1 Mobility Management in 4G/5G Networks\nOur earlier study of 4G and emerging 5G architectures in Section 7.4\nacquainted us with all of the network elements that play a central role in\n4G/5G mobility management. Let’s now illustrate how those elements\ninteroperate with each other to provide mobility services in today’s 4G/5G\nnetworks [Sauter 2014; GSMA 2019b], which have their roots in earlier 3G\ncellular voice and data networks [Sauter 2014], and even earlier 2G voice-\nonly networks [Mouly 1992]. This will help us synthesize what we’ve\nlearned so far, allow us to introduce a few more advanced topics as well,\nand provide a lens into what might be in store for 5G mobility management.\nLet’s consider a simple scenario in which a mobile user (e.g., a\npassenger in a car), with a smartphone attaches to a visited 4G/5G network,\nbegins streaming a HD video from a remote server, and then moves from\nthe cell coverage of one 4G/5G base station to another. The four major steps\nin this scenario are shown in Figure 7.28:\n1. Mobile device and base station association. The mobile device\nassociates with a base station in the visited network.\n2. Control-plane configuration of network elements for the mobile device.\nThe visited and home networks establish control-plane state indicating\nthat the mobile device is resident in the visited network.\n3. Data-plane configuration of forwarding tunnels for the mobile device.\nThe visited network and the home network establish tunnels through\nwhich the mobile device and streaming server can send/receive IP\ndatagrams, using indirect routing through the home network’s Packet\nData Network gateway (P-GW).\n4. Mobile device handover from one base station to another. The mobile\ndevice changes its point of attachment to the visited network, via\nhandover from one base station to another.\nLet’s now consider each of these four steps in more detail.\nFigure 7.28 ♦An example 4G/5G mobility scenario\n1. Base station association. Recall that in Section 7.4.2, we studied the\nprocedures by which a mobile device associates with a base station. We\nlearned that the mobile device listens on all frequencies for primary\nsignals being transmitted by base stations in its area. The mobile device\nacquires progressively more information about these base stations,\nultimately selecting the base station with which to associate, and\nbootstrapping a control-­signaling channel with that base station. As part\nof this association, the mobile device provides the base station with its\nInternational Mobile Subscriber Identity (IMSI), which uniquely\nidentifies the mobile device as well as its home network and other\nadditional subscriber information.\n2. Control-plane configuration of LTE network elements for the\nmobile device.Once the mobile-device-to-base-station signaling channel\nhas been established, the base station can contact the MME in the visited\nnetwork. The MME will consult and configure a number of 4G/5G\nelements in both the home and visited networks to establish state on\nbehalf of the mobile node:\nThe MME will use to the IMSI and other information provided by\nthe mobile device to retrieve authentication, encryption, and\navailable network service information for that subscriber. That\ninformation might be in the MME’s local cache, retrieved from\nanother MME that the mobile device had recently contacted, or\nretrieved from the HSS in the mobile device’s home network. The\nmutual authentication process (which we will cover in more detail in\nSection 8.8) ensures that the visited network is sure about the\nidentity of the mobile device and that the device can authenticate the\nnetwork to which it is attaching.\nThe MME informs the HSS in the mobile device’s home network\nthat the mobile device is now resident in the visited network, and the\nHSS updates its database.\nThe base station and the mobile device select parameters for the\ndata-plane channel to be established between the mobile device and\nthe base station (recall that a control plane signaling channel is\nalready in operation).\n3. Data-plane configuration of forwarding tunnels for the mobile\ndevice. The MME next configures the data plane for the mobile device,\nas shown in Figure 7.29. Two tunnels are established. One tunnel is\nbetween the base station and a Serving Gateway in the visited network.\nThe second tunnel is between that Serving Gateway and the PDN\nGateway router in the mobile device’s home network. 4G LTE\nimplements this form of symmetric indirect routing—all traffic to/from\nthe mobile device will be tunneled through the device’s home network.\n4G/5G tunnels use the GPRS Tunneling Protocol (GTP), specified in\n[3GPP GTPv1-U 2019]. The Tunnel Endpoint ID (TEID) in the GTP\nheader indicates which tunnel a datagram belongs, allowing multiple\nflows to be multiplexed and de-multiplexed by GTP between tunnel\nIt is instructive to compare the configuration of tunnels in Figure\n7.29 (the case of mobile roaming in a visited network) with that of\nFigure 7.18 (the case of mobility only within the mobile device’s home\nnetwork). We see that in both cases, the Serving Gateway is co-resident\nin the same network as the mobile device, but PDN Gateway (which is\nalways the PDN Gateway in the mobile device’s home network) may be\nin a different network than the mobile device. This is precisely indirect\nrouting. An alternative to indirect routing, known as local breakout\n[GSMA 2019a] has been specified in which the Serving Gateway\nestablishes a tunnel to the PDN Gateway in the local, visited network. In\npractice, however, local breakout is not widely used [Sauter 2014].\nFigure 7.29 ♦Tunneling in 4G/5G networks between the Serving\nGateway in the visited network and the PDN\ngateway in the home network\nOnce the tunnels have been configured and activated, the mobile\ndevice can now forward packets to/from the Internet via the PDN\ngateway in its home network!\n4. Handover management. A handover occurs when a mobile device\nchanges its association from one base station to another. The handover\nprocess described below is the same, regardless of whether the mobile\ndevice is resident in its home network, or is roaming in a visited\nAs shown in Figure 7.30, datagrams to/from the device are initially\n(before handover) forwarded to the mobile through one base station\n(which we’ll refer to as the source base station), and after handover are\nrouted to the mobile device through another base station (which we’ll\nrefer to as the target base station). As we will see, a handover between\nbase stations results not only in the mobile device transmitting/receiving\nto/from a new base station but also in a change of the base-station side\nof the Serving-Gateway-to-base-station tunnel in Figure 7.29. In the\nsimplest case of handover, when the two base stations are near each\nother and in the same network, all changes occurring as a result of\nhandover are thus relatively local. In particular, the PDN gateway being\nused by the Serving Gateway remains blissfully unaware of device\nmobility. Of course, more complicated handoff scenarios will require the\nuse of more complex mechanisms [Sauter 2014; GSMA 2019a].\nThere may be several reasons for handover to occur. For example,\nthe signal between the current base station and the mobile may have\ndeteriorated to such an extent that communication is severely impaired.\nOr a cell may have become overloaded, handling a large amount of\ntraffic; handing over mobile devices to less congested nearby cells may\nalleviate this congestion. A mobile device periodically measures\ncharacteristics of a beacon signal from its current base station as well as\nsignals from nearby base stations that it can “hear.” These measurements\nare reported once or twice a second to the mobile device’s current\n(source) base station. Based on these measurements, the current loads of\nmobiles in nearby cells, and other factors, the source base station may\nchoose to initiate a handover. The 4G/5G standards do not specify a\nspecific algorithm to be used by a base station to determine whether or\nnot to perform handover, or which target base station to choose; this is\nan active area of research [Zheng 2008; Alexandris 2016].\nFigure 7.30 illustrates the steps involved when a source base station\ndecides to hand over a mobile device to the target base station.\nFigure 7.30 ♦Steps in handing over a mobile device from the\nsource base station to the target base station\n1. The current (source) base station selects the target base station, and\nsends a Handover Request message to the target base station.\n2. The target base station checks whether it has the resources to support the\nmobile device and its quality of service requirements. If so, it pre-\nallocates channel resources (e.g., time slots) on its radio access network\nand other resources for that device. This pre-allocation of resources\nfrees the mobile device from having to go through the time-consuming\nbase-station association protocol discussed earlier, allowing handover to\nbe executed as fast as possible. The target base station replies to the\nsource base station with a Handover Request Acknowledge message,\ncontaining all the information at the target base station that the mobile\ndevice will need to associate with the new base station.\n3. The source base station receives the Handover Request\nAcknowledgement message and informs the mobile device of the target\nbase station’s identity and channel access information. At this point, the\nmobile device can begin sending/receiving datagrams to/from the new\ntarget base station. From the mobile device’s point of view, handover is\nnow complete! However, there is still a bit of work to be done within the\n4. The source base station will also stop forwarding datagrams to the\nmobile device and instead forward any tunneled datagrams it receives to\nthe target base station, which will later forward these datagrams to the\nmobile device.\n5. The target base station informs the MME that it (the target base station)\nwill be the new base station servicing the mobile device. The MME, in\nturn, signals to the Serving Gateway and the target base station to\nreconfigure the Serving-Gateway-to-base-station tunnel to terminate at\nthe target base station, rather than at the source base station.\n6. The target base station confirms back to the source base station that the\ntunnel has been reconfigured, allowing the source base station to release\nresources associated with that mobile device.\n7. At this point, the target base station can also begin delivering datagrams\nto the mobile device, including datagrams forwarded to the target base\nstation by the source base station during handover, as well as datagrams\nnewly arriving on the reconfigured tunnel from the Serving Gateway. It\ncan also forward outgoing datagrams received from the mobile device\ninto the tunnel to the Serving Gateway.\nThe roaming configurations in today’s 4G LTE networks, such as that\ndiscussed above, will also be used in future emerging 5G networks [GSMA\n2019c]. Recall, however, from our discussion in Section 7.4.6 that the 5G\nnetworks will be denser, with significantly smaller cell sizes. This will\nmake handover an even more critically important network function. In\naddition, low handover latency will be critical for many real-time 5G\napplications. The migration of the cellular network control plane to the\nSDN framework that we studied earlier in Chapter 5 [GSMA 2018b;\nCondoluci 2018] promises to enable implementations of a higher-capacity,\nlower-latency 5G cellular network control plane. The application of SDN in\na 5G context is the subject of considerable research [Giust 2015; Ordonez-\nLucena 2017; Nguyen 2016].\n7.6.2 Mobile IP\nToday’s Internet does not have any widely deployed infrastructure that\nprovides the type of services for “on the go” mobile users that we\nencountered for 4G/5G cellular networks. But this is certainly not due to the\nlack of technical solutions for providing such services in an Internet setting!\nIndeed, the Mobile IP architecture and protocols [RFC 5944] that we will\nbriefly discuss below have been standardized by Internet RFCs for more\nthan 20 years, and research has continued on new, more secure and more\ngeneralized mobility solutions [Venkataramani 2014].\nInstead, it has perhaps been the lack of motivating business and use\ncases [Arkko 2012] and the timely development and deployment of\nalternative mobility solutions in cellular networks that has blunted the\ndeployment of Mobile IP. Recall that 20 years ago, 2G cellular networks\nhad already provided a solution for mobile voice services (the “killer app”\nfor mobile users); additionally, next generation 3G networks supporting\nvoice and data were on the horizon. Perhaps the dual technology solution—\nmobile services via cellular networks when we are truly mobile and “on the\ngo” (i.e., the rightmost side of the mobility spectrum in Figure 7.24) and\nInternet services via 802.11 networks or wireline networks when we are\nstationary or moving locally (i.e., the leftmost side of the mobility spectrum\nin Figure 7.24)—that we had 20 years ago and still have today will persist\ninto the future.\nIt will nonetheless be instructive to briefly overview the Mobile IP\nstandard here, as it provides many of the same services as cellular networks\nand implements many of the same basic mobility principles. Earlier editions\nof this textbook have provided a more in-depth study of Mobile IP than we\nwill provide here; the interested reader can find this retired material on this\ntextbook’s website. The Internet architecture and protocols for supporting\nmobility, collectively known as Mobile IP, are defined primarily in RFC\n5944 for IPv4. Mobile IP, like 4G/5G, is a complex standard, and would\nrequire an entire book to describe in detail; indeed one such book is\n[Perkins 1998b]. Our modest goal here is to provide an overview of the\nmost important aspects of Mobile IP.\nThe overall architecture and elements of Mobile IP are strikingly\nsimilar to that of cellular provider networks. There is a strong notion of a\nhome network, in which a mobile device has a permanent IP address, and\nvisited networks (known as “foreign” networks in Mobile IP), where the\nmobile device will be allocated a care-of-address. The home agent in\nMobile IP has a similar function to the LTE HSS: it tracks the location of a\nmobile device by receiving updates from foreign agents in foreign networks\nvisited by that mobile device, just as the HSS receives updates from\nMobility Management Entities (MMEs) in visited networks in which a 4G\nmobile device resides. And both 4G/5G and Mobile IP use indirect routing\nto a mobile node, using tunnels to connect the gateway routers in the home\nand visited/foreign networks. Table 7.3 summarizes the elements of the\nMobile IP architecture, along with a comparison with similar elements in\n4G/5G networks\nTable 7.3 ♦Commonalities between 4G/5G and Mobile IP\narchitectures\nThe mobile IP standard consists of three main pieces:\nAgent discovery. Mobile IP defines the protocols used by a foreign\nagent to advertise its mobility services to a mobile device that wishes to\nattach to its network. Those services will include providing a care-of-\naddress to the mobile device for use in the foreign network, registration\nof the mobile device with the home agent in the mobile device’s home\nnetwork, and forwarding of datagrams to/from the mobile device,\namong other services.\nRegistration with the home agent. Mobile IP defines the protocols used\nby the mobile device and/or foreign agent to register and deregister a\ncare-of-address with a mobile device’s home agent.\nIndirect routing of datagrams. Mobile IP also defines the manner in\nwhich datagrams are forwarded to mobile devices by a home agent,\nincluding rules for forwarding datagrams and handling error conditions,\nand several forms of tunneling [RFC 2003, RFC 2004].\nAgain, our coverage here of Mobile IP has been intentionally brief. The\nChapter 7 Review Questions\nSECTION 7.1\nR1. What does it mean for a wireless network to be operating in\n“infrastructure mode”? If the network is not in infrastructure mode,\nwhat mode of operation is it in, and what is the difference between\nthat mode of operation and infrastructure mode?\nR2. Both MANET and VANET are multi-hop infrastructure-less wireless\nnetworks. What is the difference between them?\nSECTION 7.2\nR3. What are the differences between the following types of wireless\nchannel impairments: path loss, multipath propagation, interference\nfrom other sources?\nR4. As a mobile node gets farther and farther away from a base station,\nwhat are two actions that a base station could take to ensure that the\nloss probability of a transmitted frame does not increase?\nSECTION 7.3\nR5. Describe the role of the beacon frames in 802.11.\nR6. An access point periodically sends beacon frames. What are the\nmanagement applications, a topic we examined in Chapter 5). An intruder\nthat could actively interfere with DNS lookups (as discussed in Section\n2.4), routing computations ­(Sections 5.3 and 5.4), or network management\nfunctions (Sections 5.5 and 5.7) could wreak havoc in the Internet.\nHaving now established the framework, a few of the most important\ndefinitions, and the need for network security, let us next delve into\ncryptography. While the use of cryptography in providing confidentiality is\nself-evident, we’ll see shortly that it is also central to providing end-point\nauthentication and message integrity—making cryptography a cornerstone\nof network security.\n8.2 Principles of Cryptography\nAlthough cryptography has a long history dating back at least as far as Julius ­Caesar,\nmodern cryptographic techniques, including many of those used in the Internet, are based on\nadvances made in the past 30 years. Kahn’s book, The ­Codebreakers [Kahn 1967], and\nSingh’s book, The Code Book: The Science of Secrecy from Ancient Egypt to Quantum\nCryptography [Singh 1999], provide a fascinating look at the long history of cryptography.\nA complete discussion of cryptography itself requires a complete book [Bishop 2003;\nKaufman 2002; Schneier 2015] and so we only touch on the essential aspects of\ncryptography, particularly as they are practiced on the Internet. We also note that while our\nfocus in this section will be on the use of cryptography for confidentiality, we’ll see shortly\nthat cryptographic techniques are inextricably woven into authentication, message integrity,\nnonrepudiation, and more.\nCryptographic techniques allow a sender to disguise data so that an intruder can gain no\ninformation from the intercepted data. The receiver, of course, must be able to recover the\noriginal data from the disguised data. Figure 8.2 illustrates some of the important\nterminology.\nFigure 8.2 ♦Cryptographic components\nSuppose now that Alice wants to send a message to Bob. Alice’s message in its original\nform (e.g., “Bob, I love you. Alice”) is known as ­plaintext, or cleartext. Alice\nencrypts her plaintext message using an encryption algorithm so that the encrypted\nmessage, known as ciphertext, looks unintelligible to any intruder. Interestingly, in many\nmodern cryptographic systems, including those used in the Internet, the encryption\ntechnique itself is known—published, standardized, and available to everyone (e.g., [RFC\n1321; RFC 3447; RFC 2420; NIST 2001]), even a potential intruder! Clearly, if everyone\nknows the method for encoding data, then there must be some secret information that\nprevents an intruder from decrypting the transmitted data. This is where keys come in.\nIn Figure 8.2, Alice provides a key, K , a string of numbers or characters, as input to the\nencryption algorithm. The encryption algorithm takes the key and the plaintext message, m,\nas input and produces ciphertext as output. The notation K (m) refers to the ciphertext form\n(encrypted using the key K ) of the plaintext message, m. The actual encryption algorithm\nthat uses key K  will be evident from the context. Similarly, Bob will provide a key, K , to\nthe decryption algorithm that takes the ciphertext and Bob’s key as input and produces the\noriginal plaintext as output. That is, if Bob receives an encrypted message K (m), he\ndecrypts it by computing K (K (m)) = m. In symmetric key systems, Alice’s and Bob’s keys\nare identical and are secret. In public key systems, a pair of keys is used. One of the keys is\nknown to both Bob and Alice (indeed, it is known to the whole world). The other key is\nknown only by either Bob or Alice (but not both). In the following two subsections, we\nconsider symmetric key and public key systems in more detail.\n8.2.1 Symmetric Key Cryptography\nAll cryptographic algorithms involve substituting one thing for another, for example, taking\na piece of plaintext and then computing and substituting the appropriate ciphertext to create\nthe encrypted message. Before studying a modern key-based cryptographic system, let us\nfirst get our feet wet by studying a very old, very simple symmetric key algorithm attributed\nto Julius Caesar, known as the Caesar cipher (a cipher is a method for encrypting data).\nFor English text, the Caesar cipher would work by taking each letter in the plaintext\nmessage and substituting the letter that is k letters later (allowing wraparound; that is, having\nthe letter z followed by the letter a) in the alphabet. For example, if k = 3, then the letter a in\nplaintext becomes d in ciphertext; b in plaintext becomes e in ciphertext, and so on. Here,\nthe value of k serves as the key. As an example, the plaintext message “bob, i love\nyou. Alice” becomes “ere, l oryh brx. dolfh” in ciphertext. While the\nciphertext does indeed look like gibberish, it wouldn’t take long to break the code if you\nknew that the Caesar cipher was being used, as there are only 25 possible key values.\nAn improvement on the Caesar cipher is the monoalphabetic cipher, which also\nsubstitutes one letter of the alphabet with another letter of the alphabet. ­However, rather than\nsubstituting according to a regular pattern (e.g., substitution with an offset of k for all\nletters), any letter can be substituted for any other letter, as long as each letter has a unique\nsubstitute letter, and vice versa. The substitution rule in Figure 8.3 shows one possible rule\nfor encoding plaintext.\nFigure 8.3 ♦A monoalphabetic cipher\nThe plaintext message “bob, i love you. Alice” becomes “nkn, s gktc\nwky. Mgsbc.” Thus, as in the case of the Caesar cipher, this looks like gibberish. A\nmonoalphabetic cipher would also appear to be better than the Caesar cipher in that there are\n26! (on the order of 10 ) possible pairings of letters rather than 25 possible pairings. A\nbrute-force approach of trying all 10  possible pairings would require far too much work to\nbe a feasible way of breaking the encryption algorithm and decoding the message. However,\nby statistical analysis of the plaintext language, for example, knowing that the letters e and t\nare the most frequently occurring letters in typical English text (accounting for 13 percent\nand 9 percent of letter occurrences), and knowing that particular two-and three-letter\noccurrences of letters appear quite often together (for example, “in,” “it,” “the,” “ion,”\n“ing,” and so forth) make it relatively easy to break this code. If the intruder has some\nof routers in the network (see Chapter 5). In a link-state algorithm, each\nrouter needs to broadcast a link-state message to all other routers in the\nnetwork. A router’s link-state message includes a list of its directly\nconnected neighbors and the direct costs to these neighbors. Once a router\nreceives link-state messages from all of the other routers, it can create a\ncomplete map of the network, run its least-cost routing algorithm, and\nconfigure its forwarding table. One relatively easy attack on the routing\nalgorithm is for Trudy to distribute bogus link-state messages with incorrect\nlink-state information. Thus, the need for message integrity—when router B\nreceives a link-state message from router A, router B should verify that\nrouter A actually created the message and, further, that no one tampered\nwith the message in transit.\nIn this section, we describe a popular message integrity technique that\nis used by many secure networking protocols. But before doing so, we need\nto cover another important topic in cryptography—cryptographic hash\n8.3.1 Cryptographic Hash Functions\nAs shown in Figure 8.7, a hash function takes an input, m, and computes a\nfixed-size string H(m) known as a hash. The Internet checksum (Chapter 3)\nand CRCs (Chapter 6) meet this definition. A cryptographic hash function\nis required to have the following additional property:\nFigure 8.7 ♦Hash functions\nIt is computationally infeasible to find any two different messages x and\ny such that H(x) = H(y).\nInformally, this property means that it is computationally infeasible for\nan intruder to substitute one message for another message that is protected\nby the hash function. That is, if (m, H(m)) are the message and the hash of\nthe message created by the sender, then an intruder cannot forge the\nprotocol in Chapter 3, we will find it instructive here to develop various\nversions of an authentication protocol, which we will call ap (authentication\nprotocol), and poke holes in each version as we proceed. (If you enjoy this\nstepwise evolution of a design, you might also enjoy [Bryant 1988], which\nrecounts a fictitious narrative between designers of an open-\nnetwork authentication system, and their discovery of the many subtle\nissues involved.)\nLet’s assume that Alice needs to authenticate herself to Bob.\nPerhaps the simplest authentication protocol we can imagine is one\nwhere Alice simply sends a message to Bob saying she is Alice. This\nprotocol is shown in Figure 8.15. The flaw here is obvious—there is no way\nfor Bob actually to know that the person sending the message “I am Alice”\nis indeed Alice. For example, Trudy (the intruder) could just as well send\nsuch a message.\nFigure 8.15 ♦Protocol ap1.0 and a failure scenario\nAuthentication Protocol ap2.0\nIf Alice has a well-known network address (e.g., an IP address) from which\nshe always communicates, Bob could attempt to authenticate Alice by\nverifying that the source address on the IP datagram carrying the\nauthentication message matches Alice’s well-known address. In this case,\nAlice would be authenticated. This might stop a very network-naive\nintruder from impersonating Alice, but it wouldn’t stop the determined\nstudent studying this book, or many others!\nFrom our study of the network and data link layers, we know that it is\nnot that hard (for example, if one had access to the operating system code\nand could build one’s own operating system kernel, as is the case with\nLinux and several other freely available operating systems) to create an IP\ndatagram, put whatever IP source address we want (for example, Alice’s\nwell-known IP address) into the IP datagram, and send the datagram over\nthe link-layer protocol to the first-hop router. From then on, the incorrectly\nsource-addressed datagram would be dutifully forwarded to Bob. This\napproach, shown in Figure 8.16, is a form of IP spoofing. IP spoofing can\nbe avoided if Trudy’s first-hop router is configured to forward only\ndatagrams containing Trudy’s IP source address [RFC 2827]. However, this\ncapability is not universally deployed or enforced. Bob would thus be\nfoolish to assume that Trudy’s network manager (who might be Trudy\nherself) had configured Trudy’s first-hop router to forward only\nappropriately addressed datagrams.\nAuthentication Protocol ap3.0\nOne classic approach to authentication is to use a secret password. The\npassword is a shared secret between the authenticator and the person being\nauthenticated. Gmail, Facebook, telnet, FTP, and many other services use\npassword authentication. In protocol ap3.0, Alice thus sends her secret\npassword to Bob, as shown in Figure 8.17.\nSince passwords are so widely used, we might suspect that protocol\nap3.0 is fairly secure. If so, we’d be wrong! The security flaw here is clear.\nIf Trudy eavesdrops on Alice’s communication, then she can learn Alice’s\npassword. Lest you think this is unlikely, consider the fact that when you\nTelnet to another machine and log in, the login password is sent\nunencrypted to the Telnet server. Someone connected to the Telnet client or\nserver’s LAN can possibly sniff (read and store) all packets transmitted on\nthe LAN and thus steal the login password. In fact, this is a well-known\napproach for stealing passwords (see, for example, [Jimenez 1997]). Such a\nthreat is obviously very real, so ap3.0 clearly won’t do.\nFigure 8.16 ♦Protocol ap2.0 and a failure scenario\nAuthentication Protocol ap3.1\nOur next idea for fixing ap3.0 is naturally to encrypt the password. By\nencrypting the password, we can prevent Trudy from learning Alice’s\npassword. If we assume that Alice and Bob share a symmetric secret key,\n, then Alice can encrypt the password and send her identification\nmessage, “I am Alice,” and her encrypted password to Bob. Bob then\ndecrypts the password and, assuming the password is correct, authenticates\nAlice. Bob feels comfortable in authenticating Alice since Alice not only\nknows the password, but also knows the shared secret key value needed to\nencrypt the password. Let’s call this protocol ap3.1.\nWhile it is true that ap3.1 prevents Trudy from learning Alice’s\npassword, the use of cryptography here does not solve the authentication\nproblem. Bob is subject to a playback attack: Trudy need only eavesdrop\non Alice’s communication, record the encrypted version of the password,\nand play back the encrypted version of the password to Bob to pretend that\nshe is Alice. The use of an encrypted password in ap3.1 doesn’t make the\nsituation manifestly different from that of protocol ap3.0 in Figure 8.17.\nFigure 8.17 ♦Protocol ap3.0 and a failure scenario\nAuthentication Protocol ap4.0\nThe failure scenario in Figure 8.17 resulted from the fact that Bob could not\ndistinguish between the original authentication of Alice and the later\nplayback of Alice’s original authentication. That is, Bob could not tell if\nAlice was live (that is, was currently really on the other end of the\nconnection) or whether the messages he was receiving were a recorded\nplayback of a previous authentication of Alice. The very (very) observant\nreader will recall that the three-way TCP handshake protocol needed to\naddress the same problem—the server side of a TCP connection did not\nwant to accept a connection if the received SYN segment was an old copy\n(retransmission) of a SYN segment from an earlier connection. How did the\nTCP server side solve the problem of determining whether the client was\nreally live? It chose an initial sequence number that had not been used in a\nvery long time, sent that number to the client, and then waited for the client\nto respond with an ACK segment containing that number. We can adopt the\nsame idea here for authentication purposes.\nA nonce is a number that a protocol will use only once in a lifetime.\nThat is, once a protocol uses a nonce, it will never use that number again.\nOur ap4.0 protocol uses a nonce as follows:\n1. Alice sends the message “I am Alice” to Bob.\n2. Bob chooses a nonce, R, and sends it to Alice.\n3. Alice encrypts the nonce using Alice and Bob’s symmetric secret key,\n, and sends the encrypted nonce, K\n (R), back to Bob. As in\nprotocol ap3.1, it is the fact that Alice knows K\n and uses it to encrypt\na value that lets Bob know that the message he receives was generated\nby Alice. The nonce is used to ensure that Alice is live.\n4. Bob decrypts the received message. If the decrypted nonce equals the\nnonce he sent Alice, then Alice is authenticated.\nProtocol ap4.0 is illustrated in Figure 8.18. By using the once-in-a-\nlifetime value, R, and then checking the returned value, K\n (R), Bob can\nbe sure that Alice is both who she says she is (since she knows the secret\nkey value needed to encrypt R) and live (since she has encrypted the nonce,\nR, that Bob just created).\nFigure 8.18 ♦Protocol ap4.0 and a failure scenario\nThe use of a nonce and symmetric key cryptography forms the basis of\nap4.0. A natural question is whether we can use a nonce and public key\ncryptography (rather than symmetric key cryptography) to solve the\nauthentication problem. This issue is explored in the problems at the end of\nthe chapter.\n8.5 Securing E-Mail\nIn previous sections, we examined fundamental issues in network security,\nincluding symmetric key and public key cryptography, end-point\nauthentication, key distribution, message integrity, and digital signatures.\nWe are now going to examine how these tools are being used to provide\nsecurity in the Internet.\nInterestingly, it is possible to provide security services in any of the top\nfour layers of the Internet protocol stack. When security is provided for a\nspecific application-layer protocol, the application using the protocol will\nenjoy one or more security services, such as confidentiality, authentication,\nor integrity. When security is provided for a transport-layer protocol, all\napplications that use that protocol enjoy the security services of the\ntransport protocol. When security is provided at the network layer on a\nhost-to-host basis, all transport-layer segments (and hence all application-\nlayer data) enjoy the security services of the network layer. When security\nis provided on a link basis, then the data in all frames traveling over the link\nreceive the security services of the link.\nIn Sections 8.5 through 8.8, we examine how security tools are being\nused in the application, transport, network, and link layers. Being consistent\nwith the general structure of this book, we begin at the top of the protocol\nstack and discuss security at the application layer. Our approach is to use a\nspecific application, e-mail, as a case study for application-layer security.\nWe then move down the protocol stack. We’ll examine the TLS protocol\n(which provides security at the transport layer), IPsec (which provides\nsecurity at the network layer), and the security of the IEEE 802.11 wireless\nLAN protocol.\nYou might be wondering why security functionality is being provided\nat more than one layer in the Internet. Wouldn’t it suffice simply to provide\nthe security functionality at the network layer and be done with it? There\nare two answers to this question. First, although security at the network\nlayer can offer “blanket coverage” by encrypting all the data in the\ndatagrams (that is, all the transport-layer segments) and by authenticating\nall the source IP addresses, it can’t provide user-level security. For example,\na commerce site cannot rely on IP-layer security to authenticate a customer\nwho is purchasing goods at the commerce site. Thus, there is a need for\nsecurity functionality at higher layers as well as blanket coverage at lower\nlayers. Second, it is generally easier to deploy new Internet services,\nincluding security services, at the higher layers of the protocol stack. While\nwaiting for security to be broadly deployed at the network layer, which is\nprobably still many years in the future, many application developers “just\ndo it” and introduce security functionality into their favorite applications. A\nclassic example is Pretty Good Privacy (PGP), which provides secure e-\nmail (discussed later in this section). Requiring only client and server\napplication code, PGP was one of the first security technologies to be\nbroadly used in the Internet.\n8.5.1 Secure E-Mail\nWe now use the cryptographic principles of Sections 8.2 through 8.3 to\ncreate a secure e-mail system. We create this high-level design in an\nincremental manner, at each step introducing new security services. When\ndesigning a secure e-mail system, let us keep in mind the racy example\nintroduced in Section 8.1—the love affair between Alice and Bob. Imagine\nthat Alice wants to send an e-mail message to Bob, and Trudy wants to\nBefore plowing ahead and designing a secure e-mail system for Alice\nand Bob, we should consider which security features would be most\ndesirable for them. First and foremost is confidentiality. As discussed in\nSection 8.1, neither Alice nor Bob wants Trudy to read Alice’s e-mail\nmessage. The second feature that Alice and Bob would most likely want to\nsee in the secure e-mail system is sender authentication. In particular, when\nBob receives the message “I don’t love you anymore. I\nnever want to see you again. Formerly yours,\nAlice,” he would naturally want to be sure that the message came from\nAlice and not from Trudy. Another feature that the two lovers would\nappreciate is message integrity, that is, assurance that the message Alice\nsends is not modified while en route to Bob. Finally, the e-mail system\nshould provide receiver authentication; that is, Alice wants to make sure\nthat she is indeed sending the letter to Bob and not to someone else (for\nexample, Trudy) who is impersonating Bob.\nSo let’s begin by addressing the foremost concern, confidentiality. The\nmost straightforward way to provide confidentiality is for Alice to encrypt\nthe message with symmetric key technology (such as DES or AES) and for\nBob to decrypt the message on receipt. As discussed in Section 8.2, if the\nsymmetric key is long enough, and if only Alice and Bob have the key, then\nit is extremely difficult for anyone else (including Trudy) to read the\nmessage. Although this approach is straightforward, it has the fundamental\ndifficulty that we discussed in Section 8.2—distributing a symmetric key so\nthat only Alice and Bob have copies of it. So we naturally consider an\nalternative approach—public key cryptography (using, for example, RSA).\nIn the public key approach, Bob makes his public key publicly available\n(e.g., in a public key server or on his personal Web page), Alice encrypts\nher message with Bob’s public key, and she sends the encrypted message to\nBob’s e-mail address. When Bob receives the message, he simply decrypts\nit with his private key. Assuming that Alice knows for sure that the public\nkey is Bob’s public key, this approach is an excellent means to provide the\ndesired confidentiality. One problem, however, is that public key encryption\nis relatively inefficient, particularly for long messages.\nTo overcome the efficiency problem, let’s make use of a session key\n(discussed in Section 8.2.2). In particular, Alice (1) selects a random\nsymmetric session key, K , (2) encrypts her message, m, with the symmetric\nkey, (3) encrypts the symmetric key with Bob’s public key, K , (4)\nconcatenates the encrypted message and the encrypted symmetric key to\nform a “package,” and (5) sends the package to Bob’s e-mail address. The\nsteps are illustrated in Figure 8.19. (In this and the subsequent figures, the\ncircled “+” represents concatenation and the circled “−” represents\ndeconcatenation.) When Bob receives the package, he (1) uses his private\nkey, K , to obtain the symmetric key, K , and (2) uses the symmetric key K\nto decrypt the message m.\nHaving designed a secure e-mail system that provides confidentiality,\nlet’s now design another system that provides both sender authentication\nand message integrity. We’ll suppose, for the moment, that Alice and Bob\nare no longer concerned with confidentiality (they want to share their\nfeelings with everyone!), and are concerned only about sender\nauthentication and message integrity. To accomplish this task, we use digital\nsignatures and message digests, as described in Section 8.3. Specifically,\nAlice (1) applies a hash function, H (e.g., MD5), to her message, m, to\nobtain a message digest, (2) signs the result of the hash function with her\nprivate key, K −\nA , to create a digital signature, (3) concatenates the original\n(unencrypted) message with the signature to create a package, and (4) sends\nthe package to Bob’s e-mail address. When Bob receives the package, he\n(1) applies Alice’s public key, K +\nA , to the signed message digest and (2)\ncompares the result of this operation with his own hash, H, of the message.\nThe steps are illustrated in Figure 8.20. As discussed in Section 8.3, if the\ntwo results are the same, Bob can be pretty confident that the message came\nfrom Alice and is unaltered.\nFigure 8.19 ♦Alice used a symmetric session key, K , to send a\nsecret e-mail to Bob\nFigure 8.20 ♦Using hash functions and digital signatures to provide\nsender authentication and message integrity\nNow let’s consider designing an e-mail system that provides\nconfidentiality, sender authentication, and message integrity. This can be\ndone by combining the procedures in Figures 8.19 and 8.20. Alice first\ncreates a preliminary package, exactly as in Figure 8.20, that consists of her\noriginal message along with a digitally signed hash of the message. She\nthen treats this preliminary package as a message in itself and sends this\nnew message through the sender steps in Figure 8.19, creating a new\npackage that is sent to Bob. The steps applied by Alice are shown in Figure\n8.21. When Bob receives the package, he first applies his side of Figure\n8.19 and then his side of Figure 8.20. It should be clear that this design\nachieves the goal of providing confidentiality, sender authentication, and\nmessage integrity. Note that, in this scheme, Alice uses public key\ncryptography twice: once with her own private key and once with Bob’s\npublic key. Similarly, Bob also uses public key cryptography twice—once\nwith his private key and once with Alice’s public key.\nThe secure e-mail design outlined in Figure 8.21 probably provides\nsatisfactory security for most e-mail users for most occasions. However,\nthere is still one important issue that remains to be addressed. The design in\nFigure 8.21 requires Alice to obtain Bob’s public key, and requires Bob to\nobtain Alice’s public key. The distribution of these public keys is a\nnontrivial problem. For example, Trudy might masquerade as Bob and give\nAlice her own public key while saying that it is Bob’s public key, enabling\nher to receive the message meant for Bob. As we learned in Section 8.3, a\npopular approach for securely distributing public keys is to certify the\npublic keys using a CA.\nFigure 8.21 ♦Alice uses symmetric key cyptography, public key\ncryptography, a hash function, and a digital signature\nto provide secrecy, sender authentication, and\nmessage integrity\nWritten by Phil Zimmermann in 1991, Pretty Good Privacy (PGP) is a\nnice example of an e-mail encryption scheme [PGP 2020]. The PGP design\nis, in essence, the same as the design shown in Figure 8.21. Depending on\nthe version, the PGP software uses MD5 or SHA for calculating the\nmessage digest; CAST, triple-DES, or IDEA for symmetric key encryption;\nand RSA for the public key encryption.\nWhen PGP is installed, the software creates a public key pair for the\nuser. The public key can be posted on the user’s Web site or placed in a\npublic key server. The private key is protected by the use of a password.\nThe password has to be entered every time the user accesses the private key.\nPGP gives the user the option of digitally signing the message, encrypting\nthe message, or both digitally signing and encrypting. Figure 8.22 shows a\nPGP signed message. This message appears after the MIME header. The\nencoded data in the message is K −\nA (H(m)), that is, the digitally signed\nmessage digest. As we discussed above, in order for Bob to verify the\nintegrity of the message, he needs to have access to Alice’s public key.\nFigure 8.22 ♦A PGP signed message\nFigure 8.23 shows a secret PGP message. This message also appears\nafter the MIME header. Of course, the plaintext message is not included\nwithin the secret e-mail message. When a sender (such as Alice) wants both\nconfidentiality and integrity, PGP contains a message like that of Figure\n8.23 within the message of Figure 8.22.\nFigure 8.23 ♦A secret PGP message\nPGP also provides a mechanism for public key certification, but the\nmechanism is quite different from the more conventional CA. PGP public\nkeys are certified by a web of trust. Alice herself can certify any\nkey/username pair when she believes the pair really belong together. In\naddition, PGP permits Alice to say that she trusts another user to vouch for\nthe authenticity of more keys. Some PGP users sign each other’s keys by\nholding key-signing parties. Users physically gather, exchange ­public keys,\nand certify each other’s keys by signing them with their private keys.\n8.6 Securing TCP Connections: TLS\nIn the previous section, we saw how cryptographic techniques can provide\nconfidentiality, data integrity, and end-point authentication to a specific\napplication, namely, e-mail. In this section, we’ll drop down a layer in the\nprotocol stack and examine how cryptography can enhance TCP with\nsecurity services, including confidentiality, data integrity, and end-point\nauthentication. This enhanced version of TCP is commonly known as\nTransport Layer Security (TLS), which has been standardized by the\nIETF [RFC 4346]. An earlier and similar version of this protocol is SSL\nThe SSL protocol was originally designed by Netscape, but the basic\nideas behind securing TCP had predated Netscape’s work (for example, see\nWoo [Woo 1994]). Since its inception, SSL and its successor TLS have\nenjoyed broad deployment. TLS is supported by all popular Web browsers\nand Web servers, and it is used by Gmail and essentially all Internet\ncommerce sites (including Amazon, eBay, and TaoBao). Hundreds of\nbillions of dollars are spent over TLS every year. In fact, if you have ever\npurchased anything over the Internet with your credit card, the\ncommunication between your browser and the server for this purchase\nalmost certainly went over TLS. (You can identify that TLS is being used\nby your browser when the URL begins with https: rather than http.)\nTo understand the need for TLS, let’s walk through a typical Internet\ncommerce scenario. Bob is surfing the Web and arrives at the Alice\nIncorporated site, which is selling perfume. The Alice Incorporated site\ndisplays a form in which Bob is supposed to enter the type of perfume and\nquantity desired, his address, and his payment card number. Bob enters this\ninformation, clicks on Submit, and expects to receive (via ordinary postal\nmail) the purchased perfumes; he also expects to receive a charge for his\norder in his next payment card statement. This all sounds good, but if no\nsecurity measures are taken, Bob could be in for a few surprises.\nIf no confidentiality (encryption) is used, an intruder could intercept\nBob’s order and obtain his payment card information. The intruder\ncould then make purchases at Bob’s expense.\nIf no data integrity is used, an intruder could modify Bob’s order,\nhaving him purchase ten times more bottles of perfume than desired.\nFinally, if no server authentication is used, a server could display Alice\nIncorporated’s famous logo when in actuality the site maintained by\nTrudy, who is masquerading as Alice Incorporated. After receiving\nBob’s order, Trudy could take Bob’s money and run. Or Trudy could\ncarry out an identity theft by collecting Bob’s name, address, and credit\ncard number.\nTLS addresses these issues by enhancing TCP with confidentiality, data\nintegrity, server authentication, and client authentication.\nTLS is often used to provide security to transactions that take place\nover HTTP. However, because TLS secures TCP, it can be employed by any\napplication that runs over TCP. TLS provides a simple Application\nProgrammer Interface (API) with sockets, which is similar and analogous to\nTCP’s API. When an application wants to employ TLS, the application\nincludes SSL classes/libraries. As shown in Figure 8.24, although TLS\ntechnically resides in the application layer, from the developer’s ­perspective\nit is a transport protocol that provides TCP’s services enhanced with\nsecurity services.\nFigure 8.24 ♦Although TLS technically resides in the application\nlayer, from the developer’s perspective it is a transport-\nlayer protocol\n8.6.1 The Big Picture\nWe begin by describing a simplified version of TLS, one that will allow us\nto get a big-picture understanding of the why and how of TLS. We will refer\nto this simplified version of TLS as “almost-TLS.” After describing almost-\nTLS, in the next subsection we’ll then describe the real TLS, filling in the\ndetails. Almost-TLS (and TLS) has three phases: handshake, key derivation,\nand data transfer. We now describe these three phases for a communication\nsession between a client (Bob) and a server (Alice), with Alice having a\nprivate/public key pair and a certificate that binds her identity to her public\nDuring the handshake phase, Bob needs to (a) establish a TCP connection\nwith Alice, (b) verify that Alice is really Alice, and (c) send Alice a master\nsecret key, which will be used by both Alice and Bob to generate all the\nsymmetric keys they need for the TLS session. These three steps are shown\nin Figure 8.25. Note that once the TCP connection is established, Bob sends\nAlice a hello message. Alice then responds with her certificate, which\ncontains her public key. As discussed in Section 8.3, because the certificate\nhas been certified by a CA, Bob knows for sure that the public key in the\ncertificate belongs to Alice. Bob then generates a Master Secret (MS)\n(which will only be used for this TLS session), encrypts the MS with\nAlice’s public key to create the Encrypted Master Secret (EMS), and sends\nthe EMS to Alice. Alice decrypts the EMS with her private key to get the\nMS. After this phase, both Bob and Alice (and no one else) know the master\nsecret for this TLS session.\nFigure 8.25 ♦The almost-TLS handshake, beginning with a TCP\nKey Derivation\nIn principle, the MS, now shared by Bob and Alice, could be used as the\nsymmetric session key for all subsequent encryption and data integrity\nchecking. It is, however, generally considered safer for Alice and Bob to\neach use different cryptographic keys, and also to use different keys for\nencryption and integrity checking. Thus, both Alice and Bob use the MS to\ngenerate four keys:\nE  = session encryption key for data sent from Bob to Alice\nM  = session HMAC key for data sent from Bob to Alice, where\nHMAC [RFC 2104] is a standardized hashed message authentication\ncode (MAC) that we encountered in section 8.3.2\nE  = session encryption key for data sent from Alice to Bob\nM  = session HMAC key for data sent from Alice to Bob\nAlice and Bob each generate the four keys from the MS. This could be done\nby simply slicing the MS into four keys. (But in reality TLS it is a little\nmore complicated, as we’ll see.) At the end of the key derivation phase,\nboth Alice and Bob have all four keys. The two encryption keys will be\nused to encrypt data; the two HMAC keys will be used to verify the\nintegrity of the data.\nData Transfer\nNow that Alice and Bob share the same four session keys (E , M , E , and\nM ), they can start to send secured data to each other over the TCP\nconnection. Since TCP is a byte-stream protocol, a natural approach would\nbe for TLS to encrypt application data on the fly and then pass the\nencrypted data on the fly to TCP. But if we were to do this, where would we\nput the HMAC for the integrity check? We certainly do not want to wait\nuntil the end of the TCP session to verify the integrity of all of Bob’s data\nthat was sent over the entire session! To address this issue, TLS breaks the\ndata stream into records, appends an HMAC to each record for integrity\nchecking, and then encrypts the record+HMAC. To create the HMAC, Bob\ninputs the record data along with the key M  into a hash function, as\ndiscussed in Section 8.3. To encrypt the package record+HMAC, Bob uses\nhis session encryption key E . This encrypted package is then passed to\nTCP for transport over the Internet.\nAlthough this approach goes a long way, it still isn’t bullet-proof when\nit comes to providing data integrity for the entire message stream. In\nparticular, suppose Trudy is a woman-in-the-middle and has the ability to\ninsert, delete, and replace segments in the stream of TCP segments sent\nbetween Alice and Bob. Trudy, for example, could capture two segments\nsent by Bob, reverse the order of the segments, adjust the TCP sequence\nnumbers (which are not encrypted), and then send the two reverse-ordered\nsegments to Alice. Assuming that each TCP segment encapsulates exactly\none record, let’s now take a look at how Alice would process these\n1. TCP running in Alice would think everything is fine and pass the two\nrecords to the TLS sublayer.\n2. TLS in Alice would decrypt the two records.\n3. TLS in Alice would use the HMAC in each record to verify the data\nintegrity of the two records.\n4. TLS would then pass the decrypted byte streams of the two records to\nthe application layer; but the complete byte stream received by Alice\nwould not be in the correct order due to reversal of the records!\nYou are encouraged to walk through similar scenarios for when Trudy\nremoves segments or when Trudy replays segments.\nThe solution to this problem, as you probably guessed, is to use\nsequence numbers. TLS does this as follows. Bob maintains a sequence\nnumber counter, which begins at zero and is incremented for each TLS\nrecord he sends. Bob doesn’t actually include a sequence number in the\nrecord itself, but when he calculates the HMAC, he includes the sequence\nnumber in the HMAC calculation. Thus, the HMAC is now a hash of the\ndata plus the HMAC key M  plus the current sequence number. Alice\ntracks Bob’s sequence numbers, allowing her to verify the data integrity of\na record by including the appropriate sequence number in the HMAC\ncalculation. This use of TLS sequence numbers prevents Trudy from\ncarrying out a woman-in-the-middle attack, such as reordering or replaying\nsegments. (Why?)\nThe TLS record (as well as the almost-TLS record) is shown in Figure\n8.26. The record consists of a type field, version field, length field, data\nfield, and HMAC field. Note that the first three fields are not encrypted.\nThe type field indicates whether the record is a handshake message or a\nmessage that contains application data. It is also used to close the TLS\nconnection, as discussed below. TLS at the receiving end uses the length\nfield to extract the TLS records out of the incoming TCP byte stream. The\nversion field is self-explanatory.\nFigure 8.26 ♦Record format for TLS\n8.6.2 A More Complete Picture\nThe previous subsection covered the almost-TLS protocol; it served to give\nus a basic understanding of the why and how of TLS. Now that we have a\nbasic understanding, we can dig a little deeper and examine the essentials of\nthe actual TLS protocol. In parallel to reading this description of the TLS\nprotocol, you are encouraged to complete the Wireshark TLS lab, available\nat the textbook’s Web site.\nTLS Handshake\nSSL does not mandate that Alice and Bob use a specific symmetric key\nalgorithm or a specific public-key algorithm. Instead, TLS allows Alice and\nBob to agree on the cryptographic algorithms at the beginning of the TLS\nsession, during the handshake phase. Additionally, during the handshake\nphase, Alice and Bob send nonces to each other, which are used in the\ncreation of the session keys (E , M , E , and M ). The steps of the real TLS\nhandshake are as follows:\n1. The client sends a list of cryptographic algorithms it supports, along\nwith a ­client nonce.\n2. From the list, the server chooses a symmetric algorithm (for example,\nAES) and a public key algorithm (for example, RSA with a specific key\nlength), and HMAC algorithm (MD5 or SHA-1) along with the HMAC\nkeys. It sends back to the client its choices, as well as a certificate and a\nserver nonce.\n3. The client verifies the certificate, extracts the server’s public key,\ngenerates a Pre-Master Secret (PMS), encrypts the PMS with the\nserver’s public key, and sends the encrypted PMS to the server.\n4. Using the same key derivation function (as specified by the TLS\nstandard), the client and server independently compute the Master\nSecret (MS) from the PMS and nonces. The MS is then sliced up to\ngenerate the two encryption and two HMAC keys. Furthermore, when\nthe chosen symmetric cipher employs CBC (such as 3DES or AES),\nthen two Initialization Vectors (IVs)—one for each side of the\nconnection—are also obtained from the MS. Henceforth, all ­messages\nsent between client and server are encrypted and authenticated (with the\n5. The client sends the HMAC of all the handshake messages.\n6. The server sends the HMAC of all the handshake messages.\nThe last two steps protect the handshake from tampering. To see this,\nobserve that in step 1, the client typically offers a list of algorithms—some\nstrong, some weak. This list of algorithms is sent in cleartext, since the\nencryption algorithms and keys have not yet been agreed upon. Trudy, as a\nwoman-in-the-middle, could delete the stronger algorithms from the list,\nforcing the client to select a weak algorithm. To prevent such a tampering\nattack, in step 5, the client sends the HMAC of the concatenation of all the\nhandshake messages it sent and received. The server can compare this\nHMAC with the HMAC of the handshake messages it received and sent. If\nthere is an inconsistency, the server can terminate the connection. Similarly,\nthe server sends the HMAC of the handshake messages it has seen,\nallowing the client to check for inconsistencies.\nYou may be wondering why there are nonces in steps 1 and 2. Don’t\nsequence numbers suffice for preventing the segment replay attack? The\nanswer is yes, but they don’t alone prevent the “connection replay attack.”\nConsider the following connection replay attack. Suppose Trudy sniffs all\nmessages between Alice and Bob. The next day, Trudy masquerades as Bob\nand sends to Alice exactly the same sequence of messages that Bob sent to\nAlice on the previous day. If Alice doesn’t use nonces, she will respond\nwith exactly the same sequence of messages she sent the previous day.\nAlice will not suspect any funny business, as each message she receives\nwill pass the integrity check. If Alice is an e-commerce server, she will\nthink that Bob is placing a second order (for exactly the same thing). On the\nother hand, by including a nonce in the protocol, Alice will send different\nnonces for each TCP session, causing the encryption keys to be different on\nthe two days. Therefore, when Alice receives played-back TLS records\nfrom Trudy, the records will fail the integrity checks, and the bogus e-\ncommerce transaction will not succeed. In summary, in TLS, nonces are\nused to defend against the “connection replay attack” and sequence\nnumbers are used to defend against replaying individual packets during an\nongoing session.\nConnection Closure\nAt some point, either Bob or Alice will want to end the TLS session. One\napproach would be to let Bob end the TLS session by simply terminating\nthe underlying TCP connection—that is, by having Bob send a TCP FIN\nsegment to Alice. But such a naive design sets the stage for the truncation\nattack whereby Trudy once again gets in the middle of an ongoing TLS\nsession and ends the session early with a TCP FIN. If Trudy were to do this,\nAlice would think she received all of Bob’s data when ­actuality she only\nreceived a portion of it. The solution to this problem is to indicate in the\ntype field whether the record serves to terminate the TLS session.\n(Although the TLS type is sent in the clear, it is authenticated at the receiver\nusing the record’s HMAC.) By including such a field, if Alice were to\nreceive a TCP FIN before ­receiving a closure TLS record, she would know\nthat something funny was going on.\nThis completes our introduction to TLS. We’ve seen that it uses many\nof the cryptography principles discussed in Sections 8.2 and 8.3. Readers\nwho want to explore TLS on yet a deeper level can read Rescorla’s highly\nreadable book on SSL/TLS [Rescorla 2001].\n8.7 Network-Layer Security: IPsec and Virtual\nPrivate Networks\nThe IP security protocol, more commonly known as IPsec, provides\nsecurity at the network layer. IPsec secures IP datagrams between any two\nnetwork-layer entities, including hosts and routers. As we will soon\ndescribe, many institutions (corporations, government branches, non-profit\norganizations, and so on) use IPsec to create virtual private networks\n(VPNs) that run over the public Internet.\nBefore getting into the specifics of IPsec, let’s step back and consider\nwhat it means to provide confidentiality at the network layer. With network-\nlayer confidentiality between a pair of network entities (for example,\nbetween two routers, between two hosts, or between a router and a host),\nthe sending entity encrypts the payloads of all the datagrams it sends to the\nreceiving entity. The encrypted payload could be a TCP segment, a UDP\nsegment, an ICMP message, and so on. If such a network-layer service were\nin place, all data sent from one entity to the other—including e-mail, Web\npages, TCP handshake messages, and management messages (such as\nICMP and SNMP)—would be hidden from any third party that might be\nsniffing the network. For this reason, network-layer security is said to\nprovide “blanket coverage.”\nIn addition to confidentiality, a network-layer security protocol could\npotentially provide other security services. For example, it could provide\nsource authentication, so that the receiving entity can verify the source of\nthe secured datagram. A network-layer security protocol could provide data\nintegrity, so that the receiving entity can check for any tampering of the\ndatagram that may have occurred while the datagram was in transit. A\nnetwork-layer security service could also provide replay-attack prevention,\nmeaning that Bob could detect any duplicate datagrams that an attacker\nmight insert. We will soon see that IPsec indeed provides mechanisms for\nall these security services, that is, for confidentiality, source authentication,\ndata ­integrity, and replay-attack prevention.\n8.7.1 IPsec and Virtual Private Networks (VPNs)\nAn institution that extends over multiple geographical regions often desires\nits own IP network, so that its hosts and servers can send data to each other\nin a secure and confidential manner. To achieve this goal, the institution\ncould actually deploy a stand-alone physical network—including routers,\nlinks, and a DNS ­infrastructure—that is completely separate from the public\nInternet. Such a disjoint network, dedicated to a particular institution, is\ncalled a private network. Not surprisingly, a private network can be very\ncostly, as the institution needs to purchase, install, and maintain its own\nphysical network infrastructure.\nInstead of deploying and maintaining a private network, many\ninstitutions today create VPNs over the existing public Internet. With a\nVPN, the institution’s inter-office traffic is sent over the public Internet\nrather than over a physically independent network. But to provide\nconfidentiality, the inter-office traffic is encrypted before it enters the public\nInternet. A simple example of a VPN is shown in Figure 8.27. Here the\ninstitution consists of a headquarters, a branch office, and traveling\nsalespersons that typically access the Internet from their hotel rooms.\n(There is only one salesperson shown in the figure.) In this VPN, whenever\ntwo hosts within headquarters send IP datagrams to each other or whenever\ntwo hosts within the branch office want to communicate, they use good-old\nvanilla IPv4 (that is, without IPsec services). However, when two of the\ninstitution’s hosts communicate over a path that traverses the public\nInternet, the traffic is encrypted before it enters the Internet.\nFigure 8.27 ♦Virtual private network (VPN)\nTo get a feel for how a VPN works, let’s walk through a simple\nexample in the context of Figure 8.27. When a host in headquarters sends\nan IP datagram to a salesperson in a hotel, the gateway router in\nheadquarters converts the vanilla IPv4 datagram into an IPsec datagram and\nthen forwards this IPsec datagram into the Internet. This IPsec datagram\nactually has a traditional IPv4 header, so that the routers in the public\nInternet process the datagram as if it were an ordinary IPv4 datagram—to\nthem, the datagram is a perfectly ordinary datagram. But, as shown Figure\n8.27, the payload of the IPsec datagram includes an IPsec header, which is\nused for IPsec processing; furthermore, the payload of the IPsec datagram is\nencrypted. When the IPsec datagram arrives at the salesperson’s laptop, the\nOS in the laptop decrypts the payload (and provides other security services,\nsuch as verifying data integrity) and passes the unencrypted payload to the\nupper-layer protocol (for example, to TCP or UDP).\nWe have just given a high-level overview of how an institution can\nemploy IPsec to create a VPN. To see the forest through the trees, we have\nbrushed aside many important details. Let’s now take a closer look.\n8.7.2 The AH and ESP Protocols\nIPsec is a rather complex animal—it is defined in more than a dozen RFCs.\nTwo important RFCs are RFC 4301, which describes the overall IP security\narchitecture, and RFC 6071, which provides an overview of the IPsec\nprotocol suite. Our goal in this textbook, as usual, is not simply to re-hash\nthe dry and arcane RFCs, but instead take a more operational and pedagogic\napproach to describing the protocols.\nIn the IPsec protocol suite, there are two principal protocols: the\nAuthentication Header (AH) protocol and the Encapsulation Security\nPayload (ESP) protocol. When a source IPsec entity (typically a host or a\nrouter) sends secure datagrams to a destination entity (also a host or a\nrouter), it does so with either the AH protocol or the ESP protocol. The AH\nprotocol provides source authentication and data integrity but does not\nprovide confidentiality. The ESP protocol provides source authentication,\ndata integrity, and confidentiality. Because confidentiality is often critical\nfor VPNs and other IPsec applications, the ESP protocol is much more\nwidely used than the AH protocol. In order to de-mystify IPsec and avoid\nmuch of its complication, we will henceforth focus exclusively on the ESP\nprotocol. Readers wanting to learn also about the AH protocol are\nencouraged to explore the RFCs and other online resources.\n8.7.3 Security Associations\nIPsec datagrams are sent between pairs of network entities, such as between\ntwo hosts, between two routers, or between a host and router. Before\nsending IPsec datagrams from source entity to destination entity, the source\nand destination entities create a network-layer logical connection. This\nlogical connection is called a security association (SA). An SA is a simplex\nlogical connection; that is, it is unidirectional from source to destination. If\nboth entities want to send secure datagrams to each other, then two SAs\n(that is, two logical connections) need to be established, one in each\nFor example, consider once again the institutional VPN in Figure 8.27.\nThis institution consists of a headquarters office, a branch office and, say, n\ntraveling salespersons. For the sake of example, let’s suppose that there is\nbi-directional IPsec traffic between headquarters and the branch office and\nbi-directional IPsec traffic between headquarters and the salespersons. In\nthis VPN, how many SAs are there? To answer this question, note that there\nare two SAs between the headquarters gateway router and the branch-office\ngateway router (one in each direction); for each salesperson’s laptop, there\nare two SAs between the headquarters gateway router and the laptop (again,\none in each direction). So, in total, there are (2 + 2n) SAs. Keep in mind,\nhowever, that not all traffic sent into the Internet by the gateway routers or\nby the laptops will be IPsec secured. For example, a host in headquarters\nmay want to access a Web server (such as Amazon or Google) in the public\nInternet. Thus, the gateway router (and the laptops) will emit into the\nInternet both vanilla IPv4 ­datagrams and secured IPsec datagrams.\nFigure 8.28 ♦Security association (SA) from R1 to R2\nLet’s now take a look “inside” an SA. To make the discussion tangible\nand ­concrete, let’s do this in the context of an SA from router R1 to router\nR2 in ­Figure 8.28. (You can think of Router R1 as the headquarters gateway\nrouter and Router R2 as the branch office gateway router from Figure 8.27.)\nRouter R1 will maintain state information about this SA, which will\n4.4.3 of Chapter 4. Indeed, we provided an example there of how\ngeneralized forwarding rules can be used to build a packet-filtering firewall.\nTable 8.6 ♦An access control list for a router interface\nStateful Packet Filters\nIn a traditional packet filter, filtering decisions are made on each packet in\nisolation. Stateful filters actually track TCP connections, and use this\nknowledge to make ­filtering decisions.\nTo understand stateful filters, let’s reexamine the access control list in\nTable 8.6. Although rather restrictive, the access control list in Table 8.6\nnevertheless allows any packet arriving from the outside with ACK = 1 and\nsource port 80 to get through the filter. Such packets could be used by\nattackers in attempts to crash internal systems with malformed packets,\ncarry out denial-of-service attacks, or map the internal network. The naive\nsolution is to block TCP ACK packets as well, but such an approach would\nprevent the organization’s internal users from surfing the Web.\nStateful filters solve this problem by tracking all ongoing TCP\nconnections in a connection table. This is possible because the firewall can\nobserve the beginning of a new connection by observing a three-way\nhandshake (SYN, SYNACK, and ACK); and it can observe the end of a\nconnection when it sees a FIN packet for the connection. The firewall can\nalso (conservatively) assume that the connection is over when it hasn’t seen\nany activity over the connection for, say, 60 seconds. An example\nconnection table for a firewall is shown in Table 8.7. This connection table\nindicates that there are currently three ongoing TCP connections, all of\nwhich have been initiated from within the organization. Additionally, the\nstateful filter includes a new column, “check connection,” in its access\ncontrol list, as shown in Table 8.8. Note that Table 8.8 is identical to the\naccess control list in Table 8.6, except now it indicates that the connection\nshould be checked for two of the rules.\nTable 8.7 ♦Connection table for stateful filter\nLet’s walk through some examples to see how the connection table and\nthe extended access control list work hand-in-hand. Suppose an attacker\nattempts to send a malformed packet into the organization’s network by\nsending a datagram with TCP source port 80 and with the ACK flag set.\nFurther suppose that this packet has source port number 12543 and source\nIP address 150.23.23.155. When this packet reaches the firewall, the\nfirewall checks the access control list in Table 8.7, which indicates that the\nconnection table must also be checked before permitting this packet to enter\nthe organization’s network. The firewall duly checks the connection table,\nsees that this packet is not part of an ongoing TCP connection, and rejects\nthe packet. As a second example, suppose that an internal user wants to surf\nan external Web site. Because this user first sends a TCP SYN segment, the\nuser’s TCP connection gets recorded in the connection table. When the Web\nserver sends back packets (with the ACK bit necessarily set), the firewall\nchecks the table and sees that a corresponding connection is in progress.\nThe firewall will thus let these packets pass, thereby not interfering with the\ninternal user’s Web surfing activity.\nTable 8.8 ♦Access control list for stateful filter\nApplication Gateway\nIn the examples above, we have seen that packet-level filtering allows an\nChapter 8 Review Questions\nSECTION 8.1\nR1. Operational devices such as firewalls and intrusion detection systems\nare used to counter attacks against an organization’s network. What is\nthe basic difference between a firewall and an intrusion detection\nR2. Internet entities (routers, switches, DNS servers, Web servers, user\nend systems, and so on) often need to communicate securely. Give\nthree specific example pairs of Internet entities that may want secure\ncommunication.\nSECTION 8.2\nR3. The encryption technique itself is known—published, standardized,\nand available to everyone, even a potential intruder. Then where does\nthe security of an encryption technique come from?\nR4. What is the difference between known plaintext attack and chosen\nplaintext attack?\nR5. Consider a 16-block cipher. How many possible input blocks does\nthis cipher have? How many possible mappings are there? If we view\neach mapping as a key, then how many possible keys does this cipher\nR6. Suppose N people want to communicate with each of N − 1 other\npeople using symmetric key encryption. All communication between\nany two people, i and j, is visible to all other people in this group of\nN, and no other person in this group should be able to decode their\ncommunication. How many keys are required in the system as a\nwhole? Now suppose that public key encryption is used. How many\nkeys are required in this case?\nR7. Suppose n = 1,000, a = 1,017, and b = 1,006. Use an identity of\nmodular arithmetic to calculate in your head (a · b) mod n.\nR8. Suppose you want to encrypt the message 10010111 by encrypting\nthe decimal number that corresponds to the message. What is the\ndecimal number?\nSECTIONS 8.3-8.4\nR9. In what way does a hash provide a better message integrity check\nthan a checksum (such as the Internet checksum)?\nR10. Can you “decrypt” a hash of a message to get the original message?\nExplain your answer.\nR11. Consider a variation of the MAC algorithm (Figure 8.9) where the\nsender sends (m, H(m) + s), where H(m) + s is the concatenation of\nH(m) and s. Is this variation flawed? Why or why not?\nR12. What does it mean for a signed document to be verifiable and\nnonforgeable?\nR13. In the link-state routing algorithm, we would somehow need to\ndistribute the secret authentication key to each of the routers in the\nautonomous system. How do we distribute the shared authentication\nkey to the communicating entities?\nR14. Name two popular secure networking protocols in which public key\ncertification is used.\nR15. Suppose Alice has a message that she is ready to send to anyone who\nasks. Thousands of people want to obtain Alice’s message, but each\nwants to be sure of the integrity of the message. In this context, do\nyou think a MAC-based or a digital-signature-based integrity scheme\nis more suitable? Why?\nR16. What is the purpose of a nonce in an end-point authentication\nR17. What does it mean to say that a nonce is a once-in-a-lifetime value?\nIn whose lifetime?\nR18. Is the message integrity scheme based on HMAC susceptible to\nplayback attacks? If so, how can a nonce be incorporated into the\nscheme to remove this susceptibility?\nSECTIONS 8.5-8.8\nR19. What is the de facto e-mail encryption scheme? What does it use for\nauthentication and message integrity?\nR20. In the TLS record, there is a field for TLS sequence numbers. True or\nR21. What is the purpose of the random nonces in the TLS handshake?\nR22. Suppose an TLS session employs a block cipher with CBC. True or\nfalse: The server sends to the client the IV in the clear.\nR23. Suppose Bob initiates a TCP connection to Trudy who is pretending\nto be Alice. During the handshake, Trudy sends Bob Alice’s\ncertificate. In what step of the TLS handshake algorithm will Bob\ndiscover that he is not communicating with Alice?\nR24. Consider sending a stream of packets from Host A to Host B using\nIPsec. Typically, a new SA will be established for each packet sent in\nthe stream. True or false?\nR25. Suppose that TCP is being run over IPsec between headquarters and\nthe branch office in Figure 8.28. If TCP retransmits the same packet,\nthen the two corresponding packets sent by R1 packets will have the\nsame sequence number in the ESP header. True or false?\nR26. Is there a fixed encryption algorithm in SSL?\nR27. Consider WEP for 802.11. Suppose that the data is 10001101 and the\nkeystream is 01101010. What is the resulting ciphertext?\nSECTION 8.9\nR28. Stateful packet filters maintain two data structures. Name them and\nbriefly describe what they do.\nR29. Consider a traditional (stateless) packet filter. This packet filter may\nfilter packets based on TCP flag bits as well as other header fields.\nTrue or false?\nR30. In a traditional packet filter, each interface can have its own access\ncontrol list. True or false?\nR31. Why must an application gateway work in conjunction with a router\nfilter to be effective?\nR32. Signature-based IDSs and IPSs inspect into the payloads of TCP and\nUDP segments. True or false?\nP1. Using the monoalphabetic cipher in Figure 8.3, encode the message\n“This is a secret message.”\nP2. Show that Trudy’s known-plaintext attack, in which she knows the\n(ciphertext, plaintext) translation pairs for seven letters, reduces the\nnumber of possible substitutions to be checked in the example in\nSection 8.2.1 by approximately 10 .\nP3. Consider the polyalphabetic system shown in Figure 8.4. Will a\nchosen-plaintext attack that is able to get the plaintext encoding of the\nmessage “The quick brown fox jumps over the lazy dog.” be\nsufficient to decode all messages? Why or why not?\nP4. Consider the block cipher in Figure 8.5. Suppose that each block\ncipher T  simply reverses the order of the eight input bits (so that, for\nexample, 11110000 becomes 00001111). Further suppose that the 64-\nbit scrambler does not modify any bits (so that the output value of the\nmth bit is equal to the input value of the mth bit). (a) With n = 3 and\nthe original 64-bit input equal to 10100000 repeated eight times, what\nis the value of the output? (b) Repeat part (a) but now change the last\nbit of the original 64-bit input from a 0 to a 1. (c) Repeat parts (a) and\n(b) but now suppose that the 64-bit scrambler inverses the order of the\nP5. Encode the plaintext 000001011111 with the 3-bit block cipher in\nTable 8.1 and IV = c(0) = 001. Then show that the receiver can\ndecode the ciphertext, ­knowing IV and K .\nP6. The ciphertext for the 3-bit block cipher in Table 8.1 with plaintext\n010010010 and IV = c(0) = 001 becomes:\nc(1) = K (m(1) ⊕ c(0)) = K (010 ⊕ 001) = K (011) = 100,\nc(2) = K (m(2) ⊕ c(1)) = K (010 ⊕ 100) = K (110) = 000, and\nc(3) = K (m(3) ⊕ c(2)) = K (010 ⊕ 000) = K (010) = 101.\na. Using RSA, choose p = 5 and q = 7, and encode the numbers 12,\n19, and 27 separately. Apply the decryption algorithm to the\nencrypted version to recover the original plaintext message.\nb. Choose p and q of your own and encrypt 1834 as one message\nP8. Consider RSA with p = 7 and q = 13.\na. What are n and z?\nb. Let e be 17. Why is this an acceptable choice for e?\nc. Find d such that de = 1 (mod z).\nd. Encrypt the message m = 9 using the key (n, e). Let c denote the\ncorresponding ciphertext. Show all work.\nP9. In this problem, we explore the Diffie-Hellman (DH) public-key\nencryption algorithm, which allows two entities to agree on a shared\nkey. The DH algorithm makes use of a large prime number p and\nanother large number g less than p. Both p and g are made public (so\nthat an attacker would know them). In DH, Alice and Bob each\nindependently choose secret keys, S  and S , respectively. Alice then\ncomputes her public key, T , by raising g to S  and then taking mod p.\nBob similarly computes his own public key T  by raising g to S  and\nthen taking mod p. Alice and Bob then exchange their public keys\nover the Internet. Alice then calculates the shared secret key S by\nraising T  to S  and then taking mod p. Similarly, Bob calculates the\nshared key S' by raising T  to S  and then taking mod p.\na. Prove that, in general, Alice and Bob obtain the same symmetric\nkey, that is, prove S = S'.\nb. With p = 11 and g = 2, suppose Alice and Bob choose private\nkeys S  = 5 and S  = 12, respectively. Calculate Alice’s and\nBob’s public keys, T  and T . Show all work.\nc. Following up on part (b), now calculate S as the shared\nsymmetric key. Show all work.\nd. Provide a timing diagram that shows how Diffie-Hellman can be\nattacked by a man-in-the-middle. The timing diagram should\nhave three vertical lines, one for Alice, one for Bob, and one for\nthe attacker Trudy.\nP10. Suppose Alice wants to communicate with Bob using symmetric key\ncryptography using a session key K . In Section 8.2, we learned how\npublic-key cryptography can be used to distribute the session key\nfrom Alice to Bob. In this problem, we explore how the session key\ncan be distributed—without public key cryptography—using a key\ndistribution center (KDC). The KDC is a server that shares a unique\nsecret symmetric key with each registered user. For Alice and Bob,\ndenote these keys by K\n. Design a scheme that uses the\nKDC to distribute K  to Alice and Bob. Your scheme should use three\nmessages to distribute the session key: a message from Alice to the\nKDC; a message from the KDC to Alice; and finally a message from\nAlice to Bob. The first message is K\n (A, B). Using the notation,\n, S, A, and B answer the following questions.\na. What is the second message?\nb. What is the third message?\nP11. Compute a third message, different from the two messages in Figure\n8.8, that has the same checksum as the messages in Figure 8.8.\nP12. The sender can mix some randomness into the ciphertext so that\nidentical plaintext blocks produce different ciphertext blocks. But for\neach cipher bit, the sender must now also send a random bit, doubling\nthe required ­bandwidth. Is there any way around this?\nP13. In the BitTorrent P2P file distribution protocol (see Chapter 2), the\nseed breaks the file into blocks, and the peers redistribute the blocks\nto each other. Without any protection, an attacker can easily wreak\nhavoc in a torrent by masquerading as a benevolent peer and sending\nbogus blocks to a small subset of peers in the torrent. These\nunsuspecting peers then redistribute the bogus blocks to other peers,\nwhich in turn redistribute the bogus blocks to even more peers. Thus,\nit is critical for BitTorrent to have a mechanism that allows a peer to\nverify the integrity of a block, so that it doesn’t redistribute bogus\nblocks. Assume that when a peer joins a torrent, it initially gets a\n.torrent file from a fully trusted source. Describe a simple scheme\nthat allows peers to verify the integrity of blocks.\nP14. Solving factorization in polynomial time implies breaking the RSA ­-\ncryptosystem. Is the converse true?\nP15. Consider our authentication protocol in Figure 8.18 in which Alice\nauthenticates herself to Bob, which we saw works well (i.e., we found\nno flaws in it). Now suppose that while Alice is authenticating herself\nto Bob, Bob must authenticate himself to Alice. Give a scenario by\nwhich Trudy, pretending to be Alice, can now authenticate herself to\nBob as Alice. (Hint: Consider that the sequence of operations of the\nprotocol, one with Trudy initiating and one with Bob initiating, can be\narbitrarily interleaved. Pay particular attention to the fact that both\nBob and Alice will use a nonce, and that if care is not taken, the same\nnonce can be used maliciously.)\nP16. A natural question is whether we can use a nonce and public key\ncryptography to solve the end-point authentication problem in Section\n8.4. Consider the following natural protocol: (1) Alice sends the\nmessage “I am Alice” to Bob. (2) Bob chooses a nonce, R, and\nsends it to Alice. (3) Alice uses her private key to encrypt the nonce\nand sends the resulting value to Bob. (4) Bob applies Alice’s public\nkey to the received message. Thus, Bob computes R and authenticates\na. Diagram this protocol, using the notation for public and private\nkeys employed in the textbook.\nb. Suppose that certificates are not used. Describe how Trudy can\nbecome a “woman-in-the-middle” by intercepting Alice’s\nmessages and then ­pretending to be Alice to Bob.\nP17. Figure 8.21 shows the operations that Alice must perform with PGP to\nprovide confidentiality, authentication, and integrity. Diagram the\ncorresponding operations that Bob must perform on the package\nreceived from Alice.\nP18. Suppose Alice wants to send an e-mail to Bob. Bob has a public-\nprivate key pair (K +\nB ), and Alice has Bob’s certificate. But Alice\ndoes not have a public, private key pair. Alice and Bob (and the entire\nworld) share the same hash function H(·).\na. In this situation, is it possible to design a scheme so that Bob can\nverify that Alice created the message? If so, show how with a\nblock diagram for Alice and Bob.\nb. Is it possible to design a scheme that provides confidentiality for\nsending the message from Alice to Bob? If so, show how with a\nblock diagram for Alice and Bob.\nP19. Consider the Wireshark output below for a portion of an SSL session.\na. Is Wireshark packet 112 sent by the client or server?\nb. What is the server’s IP address and port number?\nc. Assuming no loss and no retransmissions, what will be the\nsequence number of the next TCP segment sent by the client?\nd. How many SSL records does Wireshark packet 112 contain?\ne. Does packet 112 contain a Master Secret or an Encrypted Master\nSecret or neither?\nf. Assuming that the handshake type field is 1 byte and each length\nfield is 3 bytes, what are the values of the first and last bytes of\nthe Master Secret (or Encrypted Master Secret)?\ng. The client encrypted handshake message takes into account how\nmany SSL records?\nh. The server encrypted handshake message takes into account how\nmany SSL records?\nP20. In Section 8.6.1, it is shown that without sequence numbers, Trudy (a\nwoman-in-the middle) can wreak havoc in a TLS session by\ninterchanging TCP segments. Can Trudy do something similar by\ndeleting a TCP segment? What does she need to do to succeed at the\ndeletion attack? What effect will it have?\n(Wireshark screenshot reprinted by permission of the\nWireshark Foundation.)\nP21. A router’s link-state message includes a list of its directly connected\nneighbors and the direct costs to these neighbors. Once a router\nreceives link-state messages from all of the other routers, it can create\na complete map of the network, run its least-cost routing algorithm,\nand configure its forwarding table. One relatively easy attack on the\nrouting algorithm is for the attacker to distribute bogus linkstate\nmessages with incorrect link-state information. How can this be\nP22. The following true/false questions pertain to Figure 8.28.\na. When a host in 172.16.1/24 sends a datagram to an Amazon.com\nserver, the router R1 will encrypt the datagram using IPsec.\nb. When a host in 172.16.1/24 sends a datagram to a host in\n172.16.2/24, the router R1 will change the source and destination\naddress of the IP datagram.\nc. Suppose a host in 172.16.1/24 initiates a TCP connection to a\nWeb server in 172.16.2/24. As part of this connection, all\ndatagrams sent by R1 will have protocol number 50 in the left-\nmost IPv4 header field.\nd. Consider sending a TCP segment from a host in 172.16.1/24 to a",
    "unit": "Unit 2",
    "source_type": "textbook",
    "book_priority": 1,
    "source_file": "Computer Networking A Top-Down Approach",
    "chunk_id": "Computer Networking A Top-Down Approach_chunk_0"
  }
]